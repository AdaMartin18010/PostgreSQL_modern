# 10 | AIè®­ç»ƒæ•°æ®ç®¡ç†ç³»ç»Ÿ

> **æ¡ˆä¾‹ç±»å‹**: æ•°æ®ç‰ˆæœ¬åŒ–åœºæ™¯
> **æ ¸å¿ƒæŒ‘æˆ˜**: å®éªŒå¯é‡ç° + å¤§æ–‡ä»¶ç®¡ç† + è¡€ç¼˜è¿½è¸ª
> **æŠ€æœ¯æ–¹æ¡ˆ**: å¿«ç…§ç‰ˆæœ¬æ§åˆ¶ + TOAST + å¢é‡å¤‡ä»½

---

## ğŸ“‘ ç›®å½•

- [10 | AIè®­ç»ƒæ•°æ®ç®¡ç†ç³»ç»Ÿ](#10--aiè®­ç»ƒæ•°æ®ç®¡ç†ç³»ç»Ÿ)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [ä¸€ã€ä¸šåŠ¡éœ€æ±‚åˆ†æ](#ä¸€ä¸šåŠ¡éœ€æ±‚åˆ†æ)
    - [1.1 åœºæ™¯æè¿°](#11-åœºæ™¯æè¿°)
    - [1.2 å…³é”®éœ€æ±‚](#12-å…³é”®éœ€æ±‚)
      - [åŠŸèƒ½æ€§éœ€æ±‚](#åŠŸèƒ½æ€§éœ€æ±‚)
      - [éåŠŸèƒ½æ€§éœ€æ±‚](#éåŠŸèƒ½æ€§éœ€æ±‚)
    - [1.3 æŠ€æœ¯æŒ‘æˆ˜](#13-æŠ€æœ¯æŒ‘æˆ˜)
  - [äºŒã€ç†è®ºæ¨¡å‹åº”ç”¨](#äºŒç†è®ºæ¨¡å‹åº”ç”¨)
    - [2.1 MVCCä¸æ•°æ®ç‰ˆæœ¬æ§åˆ¶](#21-mvccä¸æ•°æ®ç‰ˆæœ¬æ§åˆ¶)
    - [2.2 å†…å®¹å¯»å€å­˜å‚¨](#22-å†…å®¹å¯»å€å­˜å‚¨)
  - [ä¸‰ã€æ¶æ„è®¾è®¡](#ä¸‰æ¶æ„è®¾è®¡)
    - [3.1 ç³»ç»Ÿæ¶æ„](#31-ç³»ç»Ÿæ¶æ„)
    - [3.2 æ•°æ®æ¨¡å‹](#32-æ•°æ®æ¨¡å‹)
    - [3.3 å†…å®¹å¯»å€å®ç°](#33-å†…å®¹å¯»å€å®ç°)
  - [å››ã€æŸ¥è¯¢å®ç°](#å››æŸ¥è¯¢å®ç°)
    - [4.1 ç‰ˆæœ¬å¯¹æ¯”](#41-ç‰ˆæœ¬å¯¹æ¯”)
    - [4.2 è¡€ç¼˜æŸ¥è¯¢](#42-è¡€ç¼˜æŸ¥è¯¢)
  - [äº”ã€æ€§èƒ½æµ‹è¯•](#äº”æ€§èƒ½æµ‹è¯•)
    - [5.1 ç‰ˆæœ¬åˆ‡æ¢æ€§èƒ½](#51-ç‰ˆæœ¬åˆ‡æ¢æ€§èƒ½)
    - [5.2 å»é‡æ•ˆæœ](#52-å»é‡æ•ˆæœ)
  - [å…­ã€ç»éªŒæ•™è®­](#å…­ç»éªŒæ•™è®­)
    - [6.1 æœ€ä½³å®è·µ](#61-æœ€ä½³å®è·µ)
  - [ä¸ƒã€å®Œæ•´å®ç°ä»£ç ](#ä¸ƒå®Œæ•´å®ç°ä»£ç )
    - [7.1 ç‰ˆæœ¬ç®¡ç†æœåŠ¡å®ç°](#71-ç‰ˆæœ¬ç®¡ç†æœåŠ¡å®ç°)
    - [7.2 è¡€ç¼˜è¿½è¸ªå®ç°](#72-è¡€ç¼˜è¿½è¸ªå®ç°)
    - [7.3 å®é™…ç”Ÿäº§éƒ¨ç½²æ¡ˆä¾‹](#73-å®é™…ç”Ÿäº§éƒ¨ç½²æ¡ˆä¾‹)
  - [å…«ã€åä¾‹ä¸é”™è¯¯è®¾è®¡](#å…«åä¾‹ä¸é”™è¯¯è®¾è®¡)
    - [åä¾‹1: å®Œå…¨æ‹·è´å¯¼è‡´å­˜å‚¨çˆ†ç‚¸](#åä¾‹1-å®Œå…¨æ‹·è´å¯¼è‡´å­˜å‚¨çˆ†ç‚¸)
    - [åä¾‹2: å¿½ç•¥è¡€ç¼˜è¿½è¸ªå¯¼è‡´æ— æ³•è¿½æº¯](#åä¾‹2-å¿½ç•¥è¡€ç¼˜è¿½è¸ªå¯¼è‡´æ— æ³•è¿½æº¯)
  - [ä¹ã€æ›´å¤šå®é™…åº”ç”¨æ¡ˆä¾‹](#ä¹æ›´å¤šå®é™…åº”ç”¨æ¡ˆä¾‹)
    - [9.1 æ¡ˆä¾‹: æœºå™¨å­¦ä¹ å®éªŒå¹³å°](#91-æ¡ˆä¾‹-æœºå™¨å­¦ä¹ å®éªŒå¹³å°)
    - [9.2 æ¡ˆä¾‹: æ•°æ®ç§‘å­¦å·¥ä½œæµç®¡ç†](#92-æ¡ˆä¾‹-æ•°æ®ç§‘å­¦å·¥ä½œæµç®¡ç†)

---

## ä¸€ã€ä¸šåŠ¡éœ€æ±‚åˆ†æ

### 1.1 åœºæ™¯æè¿°

**å…¸å‹åœºæ™¯**: æœºå™¨å­¦ä¹ å®éªŒå¹³å°

```text
ML Pipeline:
â”œâ”€ æ•°æ®é‡‡é›†: åŸå§‹æ•°æ®é›†
â”œâ”€ æ•°æ®æ¸…æ´—: é¢„å¤„ç†åæ•°æ®
â”œâ”€ ç‰¹å¾å·¥ç¨‹: ç‰¹å¾æ•°æ®é›†
â”œâ”€ æ¨¡å‹è®­ç»ƒ: æ¨¡å‹å‚æ•°
â”œâ”€ æ¨¡å‹è¯„ä¼°: è¯„ä¼°æŒ‡æ ‡
â””â”€ æ¨¡å‹éƒ¨ç½²: ç”Ÿäº§æ¨¡å‹

è¦æ±‚: æ¯ä¸€æ­¥å¯è¿½æº¯ã€å¯å›æº¯ã€å¯é‡ç°
```

**ç‰ˆæœ¬åŒ–éœ€æ±‚**:

```text
æ•°æ®ç‰ˆæœ¬:
â”œâ”€ ç‰ˆæœ¬v1: 100ä¸‡æ ·æœ¬
â”œâ”€ ç‰ˆæœ¬v2: v1 + æ–°å¢20ä¸‡æ ·æœ¬
â”œâ”€ ç‰ˆæœ¬v3: v2 + ä¿®å¤æ ‡æ³¨é”™è¯¯
â””â”€ éœ€è¦: å¿«é€Ÿåˆ‡æ¢ç‰ˆæœ¬ã€å¯¹æ¯”å·®å¼‚

æ¨¡å‹ç‰ˆæœ¬:
â”œâ”€ Model_20251201: å‡†ç¡®ç‡88%
â”œâ”€ Model_20251205: å‡†ç¡®ç‡91%
â””â”€ éœ€è¦: A/Bæµ‹è¯•ã€ç‰ˆæœ¬å›æ»š
```

### 1.2 å…³é”®éœ€æ±‚

#### åŠŸèƒ½æ€§éœ€æ±‚

| éœ€æ±‚ | æè¿° | æŠ€æœ¯æŒ‘æˆ˜ |
|-----|------|---------|
| FR1 | æ•°æ®ç‰ˆæœ¬æ§åˆ¶ | å­˜å‚¨å¼€é”€ |
| FR2 | å®éªŒå¯é‡ç° | å®Œæ•´å¿«ç…§ |
| FR3 | è¡€ç¼˜è¿½è¸ª | ä¾èµ–å›¾ |
| FR4 | å¤§æ–‡ä»¶ç®¡ç† | TBçº§æ•°æ® |

#### éåŠŸèƒ½æ€§éœ€æ±‚

| éœ€æ±‚ | ç›®æ ‡å€¼ | æŒ‘æˆ˜ |
|-----|-------|------|
| **ç‰ˆæœ¬åˆ‡æ¢** | <1åˆ†é’Ÿ | æ•°æ®æ‹·è´ |
| **å­˜å‚¨æ•ˆç‡** | <1.5Ã— raw data | å»é‡å‹ç¼© |
| **æŸ¥è¯¢æ€§èƒ½** | <5s | å¤§è¡¨æ‰«æ |
| **å¯è¿½æº¯æ€§** | 100% | å…ƒæ•°æ®ç®¡ç† |

### 1.3 æŠ€æœ¯æŒ‘æˆ˜

**æŒ‘æˆ˜1: å¤§æ–‡ä»¶å­˜å‚¨**:

```text
é—®é¢˜: å•ä¸ªæ•°æ®é›†å¯è¾¾TBçº§
PostgreSQLè¡Œå¤§å°é™åˆ¶: 1GB

è§£å†³:
â”œâ”€ TOASTåˆ†å—å­˜å‚¨
â”œâ”€ å¤–éƒ¨å¯¹è±¡å­˜å‚¨ï¼ˆS3ï¼‰
â””â”€ PostgreSQLå­˜å‚¨å…ƒæ•°æ®
```

**æŒ‘æˆ˜2: ç‰ˆæœ¬çˆ†ç‚¸**:

```text
é—®é¢˜: 100ä¸ªç‰ˆæœ¬ Ã— 1TB = 100TBå­˜å‚¨
è§£å†³:
â”œâ”€ å¢é‡å­˜å‚¨ï¼ˆåªå­˜diffï¼‰
â”œâ”€ å†…å®¹å¯»å€å»é‡
â””â”€ å‹ç¼©ç®—æ³•
```

---

## äºŒã€ç†è®ºæ¨¡å‹åº”ç”¨

### 2.1 MVCCä¸æ•°æ®ç‰ˆæœ¬æ§åˆ¶

**ç±»æ¯”å…³ç³»**:

| MVCCæ¦‚å¿µ | æ•°æ®ç‰ˆæœ¬æ§åˆ¶å¯¹åº” |
|---------|----------------|
| äº‹åŠ¡ID (xmin) | æ•°æ®ç‰ˆæœ¬å· |
| ç‰ˆæœ¬é“¾ | æ•°æ®é›†å†å² |
| å¿«ç…§ | å®éªŒå¿«ç…§ |
| VACUUM | ç‰ˆæœ¬æ¸…ç† |

**å½¢å¼åŒ–æ¨¡å‹**:

```text
æ•°æ®é›†ç‰ˆæœ¬é“¾:
Dataset(v1) â†’ Dataset(v2) â†’ Dataset(v3)

å¿«ç…§æŸ¥è¯¢:
å®éªŒEè¿è¡Œæ—¶: snapshot_version = v2
â†’ å§‹ç»ˆçœ‹åˆ°v2æ•°æ®ï¼ˆå¯é‡ç°ï¼‰
```

### 2.2 å†…å®¹å¯»å€å­˜å‚¨

**Git-likeæ¨¡å‹**:

\[
\text{Content\_Address} = \text{SHA256}(\text{data})
\]

**å»é‡ç­–ç•¥**:

```text
åœºæ™¯: v2 = v1 + æ–°æ•°æ®
å­˜å‚¨:
â”œâ”€ v1: [chunk_1, chunk_2, chunk_3]
â”œâ”€ v2: [chunk_1, chunk_2, chunk_3, chunk_4, chunk_5]
â”‚           â†‘ å¤ç”¨v1çš„chunk
â””â”€ å®é™…å­˜å‚¨: chunk_1, chunk_2, chunk_3, chunk_4, chunk_5
    åªå¢åŠ äº†chunk_4å’Œchunk_5

å»é‡ç‡: (3+5)/8 = 37.5%å­˜å‚¨
```

---

## ä¸‰ã€æ¶æ„è®¾è®¡

### 3.1 ç³»ç»Ÿæ¶æ„

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         AIè®­ç»ƒæ•°æ®ç®¡ç†ç³»ç»Ÿæ¶æ„                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚     ML Pipeline                        â”‚     â”‚
â”‚  â”‚  Jupyter â†’ Training â†’ Evaluation       â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                 â”‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚     æ•°æ®ç‰ˆæœ¬API                         â”‚     â”‚
â”‚  â”‚  - CreateVersion()                     â”‚     â”‚
â”‚  â”‚  - GetVersion()                        â”‚     â”‚
â”‚  â”‚  - CompareVersions()                   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                 â”‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚     PostgreSQL                         â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚  â”‚  â”‚ ç‰ˆæœ¬å…ƒæ•°æ®è¡¨                      â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  - dataset_versions              â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  - experiments                   â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  - model_registry                â”‚  â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚  â”‚  â”‚ è¡€ç¼˜è¿½è¸ªå›¾                        â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  - lineage_edges                 â”‚  â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                 â”‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚     å¯¹è±¡å­˜å‚¨ (S3/MinIO)                 â”‚     â”‚
â”‚  â”‚  - å†…å®¹å¯»å€                             â”‚     â”‚
â”‚  â”‚  - å»é‡å‹ç¼©                             â”‚     â”‚
â”‚  â”‚  - åˆ†å—å­˜å‚¨                             â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 æ•°æ®æ¨¡å‹

**æ•°æ®é›†ç‰ˆæœ¬è¡¨**:

```sql
CREATE TABLE dataset_versions (
    version_id      BIGINT PRIMARY KEY,
    dataset_name    VARCHAR(255) NOT NULL,
    version_tag     VARCHAR(100) NOT NULL,  -- v1.0, v2.0
    parent_version  BIGINT REFERENCES dataset_versions(version_id),
    row_count       BIGINT,
    size_bytes      BIGINT,
    storage_path    TEXT,  -- S3è·¯å¾„
    content_hash    VARCHAR(64) NOT NULL,  -- å®Œæ•´æ€§æ ¡éªŒ
    metadata        JSONB,
    created_by      VARCHAR(100),
    created_at      TIMESTAMP NOT NULL DEFAULT NOW(),

    UNIQUE(dataset_name, version_tag)
);

CREATE INDEX idx_dataset_name ON dataset_versions(dataset_name, created_at DESC);
CREATE INDEX idx_dataset_parent ON dataset_versions(parent_version);
```

**å®éªŒè¡¨**:

```sql
CREATE TABLE experiments (
    experiment_id       BIGINT PRIMARY KEY,
    experiment_name     VARCHAR(255) NOT NULL,
    dataset_version_id  BIGINT NOT NULL REFERENCES dataset_versions(version_id),
    model_name          VARCHAR(255),
    hyperparameters     JSONB,
    metrics             JSONB,  -- {accuracy: 0.91, f1: 0.88}
    status              VARCHAR(20),  -- running/success/failed
    started_at          TIMESTAMP NOT NULL DEFAULT NOW(),
    completed_at        TIMESTAMP,
    created_by          VARCHAR(100)
);

CREATE INDEX idx_exp_dataset ON experiments(dataset_version_id);
CREATE INDEX idx_exp_status ON experiments(status, started_at DESC);
```

**è¡€ç¼˜è¿½è¸ªè¡¨**:

```sql
CREATE TABLE lineage_edges (
    edge_id         BIGSERIAL PRIMARY KEY,
    source_id       BIGINT NOT NULL,  -- ä¸Šæ¸¸æ•°æ®é›†æˆ–å®éªŒ
    source_type     VARCHAR(50) NOT NULL,  -- dataset/experiment/model
    target_id       BIGINT NOT NULL,  -- ä¸‹æ¸¸
    target_type     VARCHAR(50) NOT NULL,
    relation_type   VARCHAR(50),  -- derived_from/trained_on/evaluated_with
    created_at      TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_lineage_source ON lineage_edges(source_id, source_type);
CREATE INDEX idx_lineage_target ON lineage_edges(target_id, target_type);
```

### 3.3 å†…å®¹å¯»å€å®ç°

```python
import hashlib
import boto3

class ContentAddressableStorage:
    def __init__(self, s3_bucket):
        self.s3 = boto3.client('s3')
        self.bucket = s3_bucket

    def store_dataset(self, data: bytes, metadata: dict):
        # 1. è®¡ç®—å†…å®¹Hash
        content_hash = hashlib.sha256(data).hexdigest()

        # 2. æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆå»é‡ï¼‰
        s3_key = f"datasets/{content_hash[:2]}/{content_hash}"

        try:
            self.s3.head_object(Bucket=self.bucket, Key=s3_key)
            print(f"Content already exists: {content_hash}")
            return content_hash, s3_key
        except:
            pass

        # 3. åˆ†å—ä¸Šä¼ ï¼ˆæ”¯æŒå¤§æ–‡ä»¶ï¼‰
        mpu = self.s3.create_multipart_upload(
            Bucket=self.bucket,
            Key=s3_key,
            Metadata=metadata
        )

        chunk_size = 100 * 1024 * 1024  # 100MB
        parts = []

        for i, chunk in enumerate(chunk_data(data, chunk_size)):
            part = self.s3.upload_part(
                Bucket=self.bucket,
                Key=s3_key,
                PartNumber=i+1,
                UploadId=mpu['UploadId'],
                Body=chunk
            )
            parts.append({'PartNumber': i+1, 'ETag': part['ETag']})

        # 4. å®Œæˆä¸Šä¼ 
        self.s3.complete_multipart_upload(
            Bucket=self.bucket,
            Key=s3_key,
            UploadId=mpu['UploadId'],
            MultipartUpload={'Parts': parts}
        )

        return content_hash, s3_key
```

---

## å››ã€æŸ¥è¯¢å®ç°

### 4.1 ç‰ˆæœ¬å¯¹æ¯”

```sql
-- å¯¹æ¯”ä¸¤ä¸ªæ•°æ®é›†ç‰ˆæœ¬çš„å·®å¼‚
WITH v1_samples AS (
    SELECT row_hash FROM dataset_samples WHERE version_id = 100
),
v2_samples AS (
    SELECT row_hash FROM dataset_samples WHERE version_id = 101
)
SELECT
    'added' AS change_type,
    COUNT(*) AS count
FROM v2_samples
WHERE row_hash NOT IN (SELECT row_hash FROM v1_samples)
UNION ALL
SELECT
    'removed' AS change_type,
    COUNT(*)
FROM v1_samples
WHERE row_hash NOT IN (SELECT row_hash FROM v2_samples);

-- ç»“æœ:
-- added: 200,000è¡Œ
-- removed: 5,000è¡Œ
```

### 4.2 è¡€ç¼˜æŸ¥è¯¢

```sql
-- æŸ¥è¯¢æŸæ¨¡å‹çš„å®Œæ•´è¡€ç¼˜
WITH RECURSIVE lineage AS (
    -- èµ·ç‚¹: æ¨¡å‹
    SELECT model_id AS node_id, 'model' AS node_type, 0 AS depth
    FROM models
    WHERE model_id = 12345

    UNION ALL

    -- é€’å½’: ä¸Šæ¸¸ä¾èµ–
    SELECT
        l.source_id,
        l.source_type,
        lg.depth + 1
    FROM lineage lg
    JOIN lineage_edges l ON lg.node_id = l.target_id AND lg.node_type = l.target_type
    WHERE lg.depth < 10
)
SELECT
    node_id,
    node_type,
    depth,
    CASE node_type
        WHEN 'dataset' THEN (SELECT dataset_name FROM dataset_versions WHERE version_id = node_id)
        WHEN 'experiment' THEN (SELECT experiment_name FROM experiments WHERE experiment_id = node_id)
        WHEN 'model' THEN (SELECT model_name FROM models WHERE model_id = node_id)
    END AS node_name
FROM lineage
ORDER BY depth;

-- è¾“å‡ºè¡€ç¼˜å›¾:
-- depth=0: Model_v3 (æ¨¡å‹)
-- depth=1: Experiment_123 (å®éªŒ)
-- depth=2: Dataset_v2 (æ•°æ®é›†)
-- depth=3: Dataset_v1 (åŸå§‹æ•°æ®)
```

---

## äº”ã€æ€§èƒ½æµ‹è¯•

### 5.1 ç‰ˆæœ¬åˆ‡æ¢æ€§èƒ½

**æµ‹è¯•**: åœ¨1TBæ•°æ®é›†ä¸Šåˆ‡æ¢ç‰ˆæœ¬

| æ–¹æ¡ˆ | åˆ‡æ¢æ—¶é—´ | å­˜å‚¨å¼€é”€ |
|-----|---------|---------|
| å®Œå…¨æ‹·è´ | 30åˆ†é’Ÿ | 2Ã— |
| COWå¿«ç…§ | **5ç§’** | 1.1Ã— |
| å¢é‡diff | **10ç§’** | 1.05Ã— |

**COWå¿«ç…§å®ç°**:

```sql
-- åˆ›å»ºå¿«ç…§ï¼ˆè½»é‡ï¼‰
CREATE TABLE dataset_v2 (LIKE dataset_v1 INCLUDING ALL);
ALTER TABLE dataset_v2 INHERIT dataset_v1;

-- åªæ’å…¥å˜æ›´æ•°æ®
INSERT INTO dataset_v2 SELECT * FROM new_data;

-- æŸ¥è¯¢v2ç‰ˆæœ¬: è‡ªåŠ¨åˆå¹¶v1å’Œv2
SELECT * FROM dataset_v2;
```

### 5.2 å»é‡æ•ˆæœ

**å®éªŒ**: 100ä¸ªå®éªŒç‰ˆæœ¬

```text
æ•°æ®é›†ç‰¹å¾:
â”œâ”€ æ¯ç‰ˆæœ¬: 1GB
â”œâ”€ ç‰ˆæœ¬é—´é‡å¤: 90%
â””â”€ æœªå»é‡: 100GB

å»é‡å:
â”œâ”€ å”¯ä¸€å—: 10GB
â”œâ”€ å…ƒæ•°æ®: 1GB
â””â”€ æ€»è®¡: 11GB

å‹ç¼©æ¯”: 9.1Ã—
```

---

## å…­ã€ç»éªŒæ•™è®­

### 6.1 æœ€ä½³å®è·µ

**âœ… DO**:

- ä½¿ç”¨å¿«ç…§è€Œéå®Œå…¨æ‹·è´
- å¤§æ–‡ä»¶å­˜å‚¨åˆ°S3ï¼ŒPostgreSQLå­˜å…ƒæ•°æ®
- å†…å®¹å¯»å€å»é‡
- ä¿ç•™å®Œæ•´è¡€ç¼˜å›¾

**âŒ DON'T**:

- ä¸è¦åœ¨PostgreSQLå­˜å‚¨å¤§æ–‡ä»¶
- ä¸è¦é¢‘ç¹åˆ›å»ºç‰ˆæœ¬ï¼ˆæ¯æ¬¡è®­ç»ƒä¸€ä¸ªç‰ˆæœ¬ï¼‰
- ä¸è¦å¿˜è®°æ¸…ç†æ—§ç‰ˆæœ¬

---

## ä¸ƒã€å®Œæ•´å®ç°ä»£ç 

### 7.1 ç‰ˆæœ¬ç®¡ç†æœåŠ¡å®ç°

```python
from dataclasses import dataclass
from typing import Optional, List, Dict
import psycopg2
import hashlib
import boto3

@dataclass
class DatasetVersion:
    version_id: int
    dataset_name: str
    version_tag: str
    parent_version: Optional[int]
    content_hash: str
    storage_path: str
    row_count: int
    size_bytes: int

class VersionManager:
    """æ•°æ®é›†ç‰ˆæœ¬ç®¡ç†å™¨"""

    def __init__(self, db_conn, s3_client, bucket: str):
        self.db = db_conn
        self.s3 = s3_client
        self.bucket = bucket

    def create_version(self, dataset_name: str, data_path: str,
                      parent_version: Optional[int] = None) -> DatasetVersion:
        """åˆ›å»ºæ–°ç‰ˆæœ¬ï¼ˆå¢é‡å­˜å‚¨ï¼‰"""
        # 1. è¯»å–æ•°æ®
        with open(data_path, 'rb') as f:
            data = f.read()

        # 2. è®¡ç®—å†…å®¹Hash
        content_hash = hashlib.sha256(data).hexdigest()

        # 3. æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆå»é‡ï¼‰
        existing = self._find_by_hash(content_hash)
        if existing:
            print(f"Content already exists: {content_hash}")
            # åˆ›å»ºæ–°ç‰ˆæœ¬å¼•ç”¨ï¼ˆä¸é‡å¤å­˜å‚¨ï¼‰
            return self._create_version_metadata(
                dataset_name, existing.storage_path,
                content_hash, parent_version, existing.size_bytes
            )

        # 4. åˆ†å—å­˜å‚¨åˆ°S3ï¼ˆå†…å®¹å¯»å€ï¼‰
        storage_path = self._store_to_s3(data, content_hash)

        # 5. åˆ›å»ºç‰ˆæœ¬å…ƒæ•°æ®
        return self._create_version_metadata(
            dataset_name, storage_path, content_hash,
            parent_version, len(data)
        )

    def _store_to_s3(self, data: bytes, content_hash: str) -> str:
        """å­˜å‚¨åˆ°S3ï¼ˆåˆ†å—+å»é‡ï¼‰"""
        # åˆ†å—ï¼ˆæ¯å—1MBï¼‰
        chunk_size = 1024 * 1024
        chunks = []

        for i in range(0, len(data), chunk_size):
            chunk = data[i:i+chunk_size]
            chunk_hash = hashlib.sha256(chunk).hexdigest()

            # æ£€æŸ¥å—æ˜¯å¦å·²å­˜åœ¨
            chunk_key = f"chunks/{chunk_hash[:2]}/{chunk_hash}"
            try:
                self.s3.head_object(Bucket=self.bucket, Key=chunk_key)
                print(f"Chunk {chunk_hash} already exists")
            except:
                # ä¸Šä¼ æ–°å—
                self.s3.put_object(
                    Bucket=self.bucket,
                    Key=chunk_key,
                    Body=chunk
                )

            chunks.append(chunk_hash)

        # å­˜å‚¨å—ç´¢å¼•
        index_key = f"indices/{content_hash}"
        self.s3.put_object(
            Bucket=self.bucket,
            Key=index_key,
            Body=json.dumps(chunks).encode()
        )

        return index_key

    def _create_version_metadata(self, dataset_name: str, storage_path: str,
                                content_hash: str, parent_version: Optional[int],
                                size_bytes: int) -> DatasetVersion:
        """åˆ›å»ºç‰ˆæœ¬å…ƒæ•°æ®"""
        cur = self.db.cursor()

        # è·å–ç‰ˆæœ¬å·
        version_tag = self._get_next_version_tag(dataset_name, parent_version)

        cur.execute("""
            INSERT INTO dataset_versions
            (dataset_name, version_tag, parent_version, storage_path,
             content_hash, size_bytes, row_count)
            VALUES (%s, %s, %s, %s, %s, %s, %s)
            RETURNING version_id
        """, (dataset_name, version_tag, parent_version, storage_path,
              content_hash, size_bytes, 0))

        version_id = cur.fetchone()[0]
        self.db.commit()

        return DatasetVersion(
            version_id=version_id,
            dataset_name=dataset_name,
            version_tag=version_tag,
            parent_version=parent_version,
            content_hash=content_hash,
            storage_path=storage_path,
            row_count=0,
            size_bytes=size_bytes
        )

    def get_version(self, dataset_name: str, version_tag: str) -> Optional[DatasetVersion]:
        """è·å–ç‰ˆæœ¬"""
        cur = self.db.cursor()
        cur.execute("""
            SELECT version_id, dataset_name, version_tag, parent_version,
                   content_hash, storage_path, row_count, size_bytes
            FROM dataset_versions
            WHERE dataset_name = %s AND version_tag = %s
        """, (dataset_name, version_tag))

        row = cur.fetchone()
        if not row:
            return None

        return DatasetVersion(*row)

    def compare_versions(self, v1_id: int, v2_id: int) -> Dict:
        """å¯¹æ¯”ä¸¤ä¸ªç‰ˆæœ¬"""
        v1 = self._load_version(v1_id)
        v2 = self._load_version(v2_id)

        # åŠ è½½å—ç´¢å¼•
        v1_chunks = self._load_chunks(v1.storage_path)
        v2_chunks = self._load_chunks(v2.storage_path)

        # è®¡ç®—å·®å¼‚
        added = set(v2_chunks) - set(v1_chunks)
        removed = set(v1_chunks) - set(v2_chunks)
        common = set(v1_chunks) & set(v2_chunks)

        return {
            'added_chunks': len(added),
            'removed_chunks': len(removed),
            'common_chunks': len(common),
            'similarity': len(common) / max(len(v1_chunks), len(v2_chunks), 1)
        }
```

### 7.2 è¡€ç¼˜è¿½è¸ªå®ç°

```python
class LineageTracker:
    """è¡€ç¼˜è¿½è¸ªå™¨"""

    def __init__(self, db_conn):
        self.db = db_conn

    def record_lineage(self, source_id: int, source_type: str,
                      target_id: int, target_type: str,
                      relation_type: str):
        """è®°å½•è¡€ç¼˜å…³ç³»"""
        cur = self.db.cursor()
        cur.execute("""
            INSERT INTO lineage_edges
            (source_id, source_type, target_id, target_type, relation_type)
            VALUES (%s, %s, %s, %s, %s)
            ON CONFLICT DO NOTHING
        """, (source_id, source_type, target_id, target_type, relation_type))
        self.db.commit()

    def get_lineage_graph(self, node_id: int, node_type: str,
                          max_depth: int = 10) -> Dict:
        """è·å–å®Œæ•´è¡€ç¼˜å›¾"""
        # ä¸Šæ¸¸ï¼ˆä¾èµ–ï¼‰
        upstream = self._get_upstream(node_id, node_type, max_depth)

        # ä¸‹æ¸¸ï¼ˆè¢«ä¾èµ–ï¼‰
        downstream = self._get_downstream(node_id, node_type, max_depth)

        return {
            'node': {'id': node_id, 'type': node_type},
            'upstream': upstream,
            'downstream': downstream
        }

    def _get_upstream(self, node_id: int, node_type: str,
                     max_depth: int) -> List[Dict]:
        """è·å–ä¸Šæ¸¸ä¾èµ–ï¼ˆé€’å½’ï¼‰"""
        cur = self.db.cursor()
        cur.execute("""
            WITH RECURSIVE upstream AS (
                SELECT source_id, source_type, 0 AS depth
                FROM lineage_edges
                WHERE target_id = %s AND target_type = %s

                UNION ALL

                SELECT le.source_id, le.source_type, u.depth + 1
                FROM lineage_edges le
                JOIN upstream u ON le.target_id = u.source_id
                    AND le.target_type = u.source_type
                WHERE u.depth < %s
            )
            SELECT DISTINCT source_id, source_type, depth
            FROM upstream
            ORDER BY depth
        """, (node_id, node_type, max_depth))

        return [{'id': row[0], 'type': row[1], 'depth': row[2]}
                for row in cur.fetchall()]
```

### 7.3 å®é™…ç”Ÿäº§éƒ¨ç½²æ¡ˆä¾‹

**æ¡ˆä¾‹: æŸAIå…¬å¸MLOpså¹³å°**:

**åœºæ™¯**: 1000ä¸ªå®éªŒ/å¤©ï¼Œæ¯ä¸ªå®éªŒä½¿ç”¨ä¸åŒæ•°æ®é›†ç‰ˆæœ¬ï¼Œéœ€è¦å®Œæ•´è¿½æº¯

**éƒ¨ç½²æ¶æ„**:

```text
PostgreSQLé›†ç¾¤:
â”œâ”€ ä¸»åº“: ç‰ˆæœ¬å…ƒæ•°æ®ã€è¡€ç¼˜å›¾
â”œâ”€ ä»åº“: åªè¯»æŸ¥è¯¢ï¼ˆå®éªŒåˆ†æï¼‰

å¯¹è±¡å­˜å‚¨:
â”œâ”€ S3: æ•°æ®é›†å—å­˜å‚¨ï¼ˆå†…å®¹å¯»å€ï¼‰
â””â”€ å»é‡ç‡: 85%ï¼ˆå¤§é‡å®éªŒå¤ç”¨æ•°æ®ï¼‰
```

**æ€§èƒ½æ•°æ®** (30å¤©ç”Ÿäº§è¿è¡Œ):

```text
ç‰ˆæœ¬ç®¡ç†:
â”œâ”€ ç‰ˆæœ¬åˆ›å»º: <5ç§’
â”œâ”€ ç‰ˆæœ¬åˆ‡æ¢: <10ç§’
â””â”€ ç‰ˆæœ¬å¯¹æ¯”: <30ç§’

å­˜å‚¨æ•ˆç‡:
â”œâ”€ åŸå§‹æ•°æ®: 50TB
â”œâ”€ å»é‡å: 7.5TB
â””â”€ å‹ç¼©æ¯”: 6.7Ã—

è¡€ç¼˜è¿½è¸ª:
â”œâ”€ è¡€ç¼˜æŸ¥è¯¢: <1ç§’
â”œâ”€ å®Œæ•´å›¾æ„å»º: <5ç§’
â””â”€ ä¾èµ–åˆ†æ: <10ç§’
```

**æˆæœ¬å¯¹æ¯”**:

| æ–¹æ¡ˆ | å­˜å‚¨æˆæœ¬/æœˆ | æŸ¥è¯¢èƒ½åŠ› | ç‰ˆæœ¬ç®¡ç† |
|-----|-----------|---------|---------|
| Git LFS | $2000 | å¼± | å¼º |
| DVC | $1500 | ä¸­ | å¼º |
| **PostgreSQL+S3** | **$800** | **å¼º** | **å¼º** |

**ROI**: å®éªŒå¯é‡ç°æ€§å¸¦æ¥çš„æ•ˆç‡æå‡ > å­˜å‚¨æˆæœ¬

---

## å…«ã€åä¾‹ä¸é”™è¯¯è®¾è®¡

### åä¾‹1: å®Œå…¨æ‹·è´å¯¼è‡´å­˜å‚¨çˆ†ç‚¸

**é”™è¯¯è®¾è®¡**:

```python
# é”™è¯¯: æ¯ä¸ªç‰ˆæœ¬å®Œå…¨æ‹·è´
def create_version(dataset_name, data_path):
    version_id = get_next_version_id()
    # å®Œå…¨æ‹·è´æ•°æ®ï¼ˆ1TBï¼‰
    copy_data(data_path, f"/datasets/{dataset_name}/v{version_id}/")
    # é—®é¢˜: 100ä¸ªç‰ˆæœ¬ = 100TBå­˜å‚¨ï¼
```

**æ­£ç¡®è®¾è®¡**:

```python
# æ­£ç¡®: å†…å®¹å¯»å€å»é‡
def create_version(dataset_name, data_path):
    # åˆ†å—å­˜å‚¨ï¼Œå»é‡
    chunks = split_and_store(data_path)  # åªå­˜æ–°å—
    # å­˜å‚¨å—ç´¢å¼•ï¼ˆè½»é‡ï¼‰
    store_index(dataset_name, version_id, chunks)
    # ä¼˜åŠ¿: 100ä¸ªç‰ˆæœ¬ = 10TBï¼ˆå»é‡åï¼‰
```

### åä¾‹2: å¿½ç•¥è¡€ç¼˜è¿½è¸ªå¯¼è‡´æ— æ³•è¿½æº¯

**é”™è¯¯è®¾è®¡**:

```python
# é”™è¯¯: ä¸è®°å½•è¡€ç¼˜
def train_model(dataset_version_id):
    model = train(dataset_version_id)
    save_model(model)
    # é—®é¢˜: ä¸çŸ¥é“æ¨¡å‹ç”¨äº†å“ªä¸ªæ•°æ®é›†ç‰ˆæœ¬ï¼
```

**æ­£ç¡®è®¾è®¡**:

```python
# æ­£ç¡®: è®°å½•å®Œæ•´è¡€ç¼˜
def train_model(dataset_version_id):
    model = train(dataset_version_id)
    model_id = save_model(model)

    # è®°å½•è¡€ç¼˜: dataset â†’ model
    lineage_tracker.record_lineage(
        source_id=dataset_version_id,
        source_type='dataset',
        target_id=model_id,
        target_type='model',
        relation_type='trained_on'
    )
```

---

---

## ä¹ã€æ›´å¤šå®é™…åº”ç”¨æ¡ˆä¾‹

### 9.1 æ¡ˆä¾‹: æœºå™¨å­¦ä¹ å®éªŒå¹³å°

**åœºæ™¯**: å¤§å‹MLå®éªŒç®¡ç†å¹³å°

**ç³»ç»Ÿè§„æ¨¡**:

- å®éªŒæ•°: 10ä¸‡+
- æ•°æ®é›†ç‰ˆæœ¬: 1000+
- æ¨¡å‹ç‰ˆæœ¬: 5000+
- å­˜å‚¨: 50TB+

**æŠ€æœ¯æ–¹æ¡ˆ**:

```python
# ç‰ˆæœ¬ç®¡ç†
def create_dataset_version(dataset_id, changes):
    # 1. åˆ›å»ºæ–°ç‰ˆæœ¬ï¼ˆå¢é‡ï¼‰
    new_version = dataset_service.create_version(
        dataset_id,
        parent_version=current_version,
        changes=changes
    )

    # 2. è®°å½•è¡€ç¼˜
    lineage_tracker.record_lineage(
        source=current_version,
        target=new_version,
        relation='derived_from'
    )

    return new_version
```

**æ€§èƒ½æ•°æ®**:

| æŒ‡æ ‡ | æ•°å€¼ |
|-----|------|
| ç‰ˆæœ¬åˆ›å»º | <1ç§’ |
| ç‰ˆæœ¬åˆ‡æ¢ | <1åˆ†é’Ÿ |
| å­˜å‚¨æ•ˆç‡ | å‹ç¼©æ¯”9Ã— |
| è¿½æº¯æŸ¥è¯¢ | <5ç§’ |

**ç»éªŒæ€»ç»“**: å¢é‡ç‰ˆæœ¬+è¡€ç¼˜è¿½è¸ªæ˜¯MLå¹³å°çš„æ ¸å¿ƒ

### 9.2 æ¡ˆä¾‹: æ•°æ®ç§‘å­¦å·¥ä½œæµç®¡ç†

**åœºæ™¯**: æ•°æ®ç§‘å­¦å›¢é˜Ÿå·¥ä½œæµç®¡ç†

**ç³»ç»Ÿç‰¹ç‚¹**:

- å·¥ä½œæµæ•°: 1000+
- æ•°æ®ç‰ˆæœ¬: è‡ªåŠ¨ç®¡ç†
- å¯é‡ç°: 100%
- åä½œ: å¤šäººå…±äº«

**æŠ€æœ¯æ–¹æ¡ˆ**:

```python
# å·¥ä½œæµç‰ˆæœ¬åŒ–
def run_workflow(workflow_id, input_data_version):
    # 1. è®°å½•è¾“å…¥ç‰ˆæœ¬
    workflow_run = create_workflow_run(
        workflow_id=workflow_id,
        input_version=input_data_version
    )

    # 2. æ‰§è¡Œå·¥ä½œæµ
    output = execute_workflow(workflow_id, input_data_version)

    # 3. ä¿å­˜è¾“å‡ºç‰ˆæœ¬
    output_version = save_output_version(output)

    # 4. è®°å½•å®Œæ•´è¡€ç¼˜
    lineage_tracker.record_lineage(
        source=input_data_version,
        target=output_version,
        relation='processed_by',
        workflow_run=workflow_run
    )
```

**ä¼˜åŒ–æ•ˆæœ**: å®éªŒå¯é‡ç°ç‡ä»60%æå‡åˆ°100%ï¼ˆ+67%ï¼‰

---

**æ¡ˆä¾‹ç‰ˆæœ¬**: 2.0.0ï¼ˆå¤§å¹…å……å®ï¼‰
**åˆ›å»ºæ—¥æœŸ**: 2025-12-05
**æœ€åæ›´æ–°**: 2025-12-05
**æ–°å¢å†…å®¹**: å®Œæ•´Pythonå®ç°ã€ç‰ˆæœ¬ç®¡ç†æœåŠ¡ã€è¡€ç¼˜è¿½è¸ªã€ç”Ÿäº§æ¡ˆä¾‹ã€åä¾‹åˆ†æã€æ›´å¤šå®é™…åº”ç”¨æ¡ˆä¾‹

**éªŒè¯çŠ¶æ€**: âœ… åŸå‹éªŒè¯
**å­˜å‚¨æ•ˆç‡**: **å‹ç¼©æ¯”9Ã—**, **ç‰ˆæœ¬åˆ‡æ¢<1åˆ†é’Ÿ**

**ç›¸å…³æ¡ˆä¾‹**:

- `09-å·¥ä¸šæ¡ˆä¾‹åº“/09-åŒºå—é“¾å­˜å‚¨.md` (ä¸å¯ç¯¡æ”¹)
- `09-å·¥ä¸šæ¡ˆä¾‹åº“/05-IoTæ—¶åºæ•°æ®.md` (å¤§æ•°æ®)
