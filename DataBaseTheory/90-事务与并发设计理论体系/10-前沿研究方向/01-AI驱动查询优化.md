# 01 | AIé©±åŠ¨æŸ¥è¯¢ä¼˜åŒ–

> **ç ”ç©¶ä»·å€¼**: â­â­â­â­ï¼ˆå­¦æœ¯+å·¥ä¸šåŒé«˜ï¼‰
> **æˆç†Ÿåº¦**: ä¸­ç­‰ï¼ˆæœ‰æˆåŠŸæ¡ˆä¾‹ï¼‰
> **æ ¸å¿ƒæŠ€æœ¯**: Transformer + å¼ºåŒ–å­¦ä¹  + æˆæœ¬æ¨¡å‹å­¦ä¹ 

---

## ğŸ“‘ ç›®å½•

- [01 | AIé©±åŠ¨æŸ¥è¯¢ä¼˜åŒ–](#01--aié©±åŠ¨æŸ¥è¯¢ä¼˜åŒ–)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [ä¸€ã€ç ”ç©¶èƒŒæ™¯](#ä¸€ç ”ç©¶èƒŒæ™¯)
    - [1.1 æŸ¥è¯¢ä¼˜åŒ–å™¨ç—›ç‚¹](#11-æŸ¥è¯¢ä¼˜åŒ–å™¨ç—›ç‚¹)
    - [1.2 æœºå™¨å­¦ä¹ ä¼˜åŠ¿](#12-æœºå™¨å­¦ä¹ ä¼˜åŠ¿)
  - [äºŒã€é—®é¢˜å½¢å¼åŒ–](#äºŒé—®é¢˜å½¢å¼åŒ–)
    - [2.1 æŸ¥è¯¢ä¼˜åŒ–ä¸ºåºåˆ—å†³ç­–](#21-æŸ¥è¯¢ä¼˜åŒ–ä¸ºåºåˆ—å†³ç­–)
    - [2.2 ç‰¹å¾æå–](#22-ç‰¹å¾æå–)
  - [ä¸‰ã€æŠ€æœ¯æ–¹æ¡ˆ](#ä¸‰æŠ€æœ¯æ–¹æ¡ˆ)
    - [3.1 æ¶æ„è®¾è®¡](#31-æ¶æ„è®¾è®¡)
    - [3.2 Transformeræ¨¡å‹](#32-transformeræ¨¡å‹)
    - [3.3 å¼ºåŒ–å­¦ä¹ è®­ç»ƒ](#33-å¼ºåŒ–å­¦ä¹ è®­ç»ƒ)
  - [å››ã€å®éªŒè¯„ä¼°](#å››å®éªŒè¯„ä¼°)
    - [4.1 æ•°æ®é›†](#41-æ•°æ®é›†)
    - [4.2 æ€§èƒ½å¯¹æ¯”](#42-æ€§èƒ½å¯¹æ¯”)
    - [4.3 è®­ç»ƒæˆæœ¬](#43-è®­ç»ƒæˆæœ¬)
  - [äº”ã€ç›¸å…³å·¥ä½œ](#äº”ç›¸å…³å·¥ä½œ)
    - [5.1 å­¦æœ¯ç³»ç»Ÿ](#51-å­¦æœ¯ç³»ç»Ÿ)
    - [5.2 å·¥ä¸šç³»ç»Ÿ](#52-å·¥ä¸šç³»ç»Ÿ)
  - [å…­ã€å®Œæ•´å®ç°ä»£ç ](#å…­å®Œæ•´å®ç°ä»£ç )
    - [6.1 SQLç‰¹å¾æå–å™¨](#61-sqlç‰¹å¾æå–å™¨)
    - [6.2 å®Œæ•´Transformeræ¨¡å‹](#62-å®Œæ•´transformeræ¨¡å‹)
    - [6.3 å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¾ªç¯](#63-å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¾ªç¯)
  - [ä¸ƒã€å®é™…éƒ¨ç½²æ¡ˆä¾‹](#ä¸ƒå®é™…éƒ¨ç½²æ¡ˆä¾‹)
    - [æ¡ˆä¾‹1: æŸç”µå•†å¹³å°éƒ¨ç½²Bao](#æ¡ˆä¾‹1-æŸç”µå•†å¹³å°éƒ¨ç½²bao)
    - [æ¡ˆä¾‹2: Google AlloyDB MLä¼˜åŒ–å™¨](#æ¡ˆä¾‹2-google-alloydb-mlä¼˜åŒ–å™¨)
  - [å…«ã€åä¾‹ä¸é”™è¯¯è®¾è®¡](#å…«åä¾‹ä¸é”™è¯¯è®¾è®¡)
    - [åä¾‹1: è¿‡åº¦ä¾èµ–MLæ¨¡å‹](#åä¾‹1-è¿‡åº¦ä¾èµ–mlæ¨¡å‹)
    - [åä¾‹2: å¿½ç•¥æ¨¡å‹æ›´æ–°](#åä¾‹2-å¿½ç•¥æ¨¡å‹æ›´æ–°)

---

## ä¸€ã€ç ”ç©¶èƒŒæ™¯

### 1.1 æŸ¥è¯¢ä¼˜åŒ–å™¨ç—›ç‚¹

**ä¼ ç»Ÿä¼˜åŒ–å™¨ï¼ˆåŸºäºè§„åˆ™+æˆæœ¬ï¼‰**:

```text
é—®é¢˜:
â”œâ”€ æˆæœ¬æ¨¡å‹ä¸å‡†ç¡®ï¼ˆä¼°è®¡è¯¯å·®10-100Ã—ï¼‰
â”œâ”€ æœç´¢ç©ºé—´çˆ†ç‚¸ï¼ˆO(n!) joiné¡ºåºï¼‰
â”œâ”€ æ— æ³•å­¦ä¹ ï¼ˆè§„åˆ™å›ºå®šï¼‰
â””â”€ éš¾ä»¥é€‚åº”æ–°ç¡¬ä»¶ï¼ˆSSD/NVMe/PMEMï¼‰

ç¤ºä¾‹: TPC-H Q9
â”œâ”€ å¯èƒ½æŸ¥è¯¢è®¡åˆ’: >10^12 ç§
â”œâ”€ ä¼ ç»Ÿä¼˜åŒ–å™¨: ç©·ä¸¾+å‰ªæ
â””â”€ æœ€ä¼˜è®¡åˆ’: å¯èƒ½è¢«é”™è¿‡
```

### 1.2 æœºå™¨å­¦ä¹ ä¼˜åŠ¿

**ä¸ºä»€ä¹ˆMLæœ‰æ•ˆ**:

1. **å­¦ä¹ çœŸå®æˆæœ¬** - ä»æ‰§è¡Œå†å²å­¦ä¹ 
2. **ç«¯åˆ°ç«¯ä¼˜åŒ–** - ç›´æ¥ä¼˜åŒ–æ‰§è¡Œæ—¶é—´
3. **é€‚åº”æ€§å¼º** - è´Ÿè½½å˜åŒ–è‡ªåŠ¨è°ƒæ•´
4. **è¶…è¶Šäººå·¥è§„åˆ™** - å‘ç°éšå«æ¨¡å¼

**æˆåŠŸæ¡ˆä¾‹**:

- **Neo** (MIT, 2019): å­¦ä¹ Joiné¡ºåºï¼Œæå‡30%
- **Bao** (Microsoft, 2021): å¼ºåŒ–å­¦ä¹ ï¼Œæå‡50%+

---

## äºŒã€é—®é¢˜å½¢å¼åŒ–

### 2.1 æŸ¥è¯¢ä¼˜åŒ–ä¸ºåºåˆ—å†³ç­–

**è¾“å…¥**: SQLæŸ¥è¯¢ \(Q\)

**è¾“å‡º**: æ‰§è¡Œè®¡åˆ’ \(P\)

**ç›®æ ‡**: æœ€å°åŒ–æ‰§è¡Œæ—¶é—´

\[
P^* = \arg\min_{P \in \mathcal{P}(Q)} \text{ExecutionTime}(P)
\]

**é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹**:

```text
MDP = (S, A, R, T)

çŠ¶æ€ S: éƒ¨åˆ†æ„å»ºçš„æŸ¥è¯¢è®¡åˆ’
â”œâ”€ å·²é€‰æ‹©çš„joiné¡ºåº
â”œâ”€ å·²é€‰æ‹©çš„ç´¢å¼•
â””â”€ å½“å‰å­æŸ¥è¯¢æˆæœ¬ä¼°è®¡

åŠ¨ä½œ A: ä¸‹ä¸€æ­¥ä¼˜åŒ–å†³ç­–
â”œâ”€ é€‰æ‹©joinæ–¹æ³• (Nested Loop/Hash Join/Merge Join)
â”œâ”€ é€‰æ‹©joiné¡ºåº
â””â”€ é€‰æ‹©è®¿é—®è·¯å¾„

å¥–åŠ± R: -æ‰§è¡Œæ—¶é—´ï¼ˆè¶ŠçŸ­è¶Šå¥½ï¼‰

è½¬ç§» T: åº”ç”¨ä¼˜åŒ–å†³ç­–åçš„æ–°çŠ¶æ€
```

### 2.2 ç‰¹å¾æå–

**SQLæŸ¥è¯¢ç‰¹å¾**:

```python
def extract_features(sql_query):
    features = {
        # è¯­æ³•ç‰¹å¾
        'num_tables': count_tables(sql_query),
        'num_joins': count_joins(sql_query),
        'num_predicates': count_where_clauses(sql_query),
        'has_aggregation': has_group_by(sql_query),
        'has_subquery': has_subquery(sql_query),

        # ç»Ÿè®¡ç‰¹å¾
        'table_sizes': [get_table_size(t) for t in tables],
        'selectivity': [estimate_selectivity(p) for p in predicates],
        'join_selectivity': [estimate_join_selectivity(j) for j in joins],

        # ç´¢å¼•ç‰¹å¾
        'available_indexes': [idx for idx in get_indexes(tables)],
        'index_selectivity': [estimate_index_benefit(idx) for idx in indexes],
    }

    return features
```

---

## ä¸‰ã€æŠ€æœ¯æ–¹æ¡ˆ

### 3.1 æ¶æ„è®¾è®¡

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        AIé©±åŠ¨æŸ¥è¯¢ä¼˜åŒ–å™¨æ¶æ„                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚     SQLè§£æ (Parser)                   â”‚     â”‚
â”‚  â”‚  SQL â†’ AST â†’ Logical Plan              â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                     â”‚                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚     ç‰¹å¾æå– (Feature Extractor)        â”‚     â”‚
â”‚  â”‚  - è¯­æ³•ç‰¹å¾                             â”‚     â”‚
â”‚  â”‚  - ç»Ÿè®¡ç‰¹å¾                             â”‚     â”‚
â”‚  â”‚  - ç¡¬ä»¶ç‰¹å¾                             â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                     â”‚                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚     MLæ¨¡å‹ (Transformer)                â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚  â”‚  â”‚ Encoder: SQL â†’ Embedding        â”‚  â”‚     â”‚
â”‚  â”‚  â”‚ Decoder: Embedding â†’ Plan       â”‚  â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚  â”‚  â”‚ å¼ºåŒ–å­¦ä¹ Agent                    â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  State: å½“å‰è®¡åˆ’                 â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  Action: ä¼˜åŒ–å†³ç­–                â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  Reward: -æ‰§è¡Œæ—¶é—´               â”‚  â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                     â”‚                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚     è®¡åˆ’ç”Ÿæˆ (Plan Generator)           â”‚     â”‚
â”‚  â”‚  ML Plan + Traditional Plan             â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                     â”‚                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚     è®¡åˆ’é€‰æ‹© (Plan Selector)            â”‚     â”‚
â”‚  â”‚  - æˆæœ¬å¯¹æ¯”                             â”‚     â”‚
â”‚  â”‚  - ç½®ä¿¡åº¦è¯„ä¼°                           â”‚     â”‚
â”‚  â”‚  - å›é€€æœºåˆ¶                             â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                     â”‚                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚     æ‰§è¡Œ + åé¦ˆ (Execution)             â”‚     â”‚
â”‚  â”‚  - å®é™…æ‰§è¡Œæ—¶é—´                         â”‚     â”‚
â”‚  â”‚  - æ›´æ–°æ¨¡å‹                             â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Transformeræ¨¡å‹

**SQLç¼–ç å™¨**:

```python
import torch.nn as nn

class SQLEncoder(nn.Module):
    def __init__(self, vocab_size=5000, d_model=512, nhead=8, num_layers=6):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)

    def forward(self, sql_tokens):
        # sql_tokens: (batch, seq_len)
        x = self.embedding(sql_tokens)
        x = self.pos_encoding(x)
        # Transformerç¼–ç 
        encoded = self.transformer_encoder(x)
        # æ± åŒ–ä¸ºå›ºå®šç»´åº¦å‘é‡
        sql_embedding = encoded.mean(dim=1)
        return sql_embedding

class PlanDecoder(nn.Module):
    def __init__(self, d_model=512, num_plan_ops=20):
        super().__init__()
        self.fc1 = nn.Linear(d_model, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc_join_order = nn.Linear(128, num_plan_ops)
        self.fc_join_method = nn.Linear(128, 3)  # NL/Hash/Merge
        self.fc_index_choice = nn.Linear(128, num_plan_ops)

    def forward(self, sql_embedding):
        x = F.relu(self.fc1(sql_embedding))
        x = F.relu(self.fc2(x))

        join_order_logits = self.fc_join_order(x)
        join_method_logits = self.fc_join_method(x)
        index_choice_logits = self.fc_index_choice(x)

        return join_order_logits, join_method_logits, index_choice_logits
```

### 3.3 å¼ºåŒ–å­¦ä¹ è®­ç»ƒ

**Baoç®—æ³•ï¼ˆThompson Samplingï¼‰**:

```python
class BaoOptimizer:
    def __init__(self):
        self.models = [Model() for _ in range(10)]  # Ensemble
        self.experience = []

    def optimize(self, query):
        # 1. ä¼ ç»Ÿä¼˜åŒ–å™¨ç”Ÿæˆå€™é€‰è®¡åˆ’
        candidates = traditional_optimizer.generate_plans(query, k=10)

        # 2. MLæ¨¡å‹é¢„æµ‹å„è®¡åˆ’æ‰§è¡Œæ—¶é—´
        predictions = []
        for model in self.models:
            pred = [model.predict(query, plan) for plan in candidates]
            predictions.append(pred)

        # 3. Thompson Samplingé€‰æ‹©
        sampled_model = random.choice(self.models)
        best_idx = np.argmin(predictions[self.models.index(sampled_model)])
        selected_plan = candidates[best_idx]

        # 4. æ‰§è¡Œå¹¶æ”¶é›†åé¦ˆ
        actual_time = execute(selected_plan)

        # 5. æ›´æ–°æ¨¡å‹
        self.experience.append((query, selected_plan, actual_time))

        if len(self.experience) > BATCH_SIZE:
            self.retrain_models()

        return selected_plan

    def retrain_models(self):
        for model in self.models:
            # Bootstrapé‡‡æ ·
            sample = random.sample(self.experience, k=len(self.experience))
            model.fit(sample)
```

---

## å››ã€å®éªŒè¯„ä¼°

### 4.1 æ•°æ®é›†

**TPC-H 100GB**:

- 22ä¸ªæŸ¥è¯¢æ¨¡æ¿
- å¤šè¡¨joinï¼ˆ2-8ä¸ªè¡¨ï¼‰
- å¤æ‚èšåˆ

**JOB (Join Order Benchmark)**:

- 113ä¸ªæŸ¥è¯¢
- åŸºäºIMDBæ•°æ®é›†
- å¤šè¾¾15ä¸ªè¡¨join

### 4.2 æ€§èƒ½å¯¹æ¯”

| æ–¹æ¡ˆ | TPC-Hæ€»æ—¶é—´ | JOBæ€»æ—¶é—´ | å¹³å‡æå‡ |
|-----|------------|-----------|---------|
| PostgreSQLé»˜è®¤ | 3200s | 8500s | Baseline |
| æ‰‹å·¥è°ƒä¼˜ | 2400s | 6200s | +31% |
| Neo (MIT) | 2100s | 5800s | +38% |
| Bao (MSFT) | **1800s** | **4500s** | **+50%** |
| æœ¬æ–¹æ¡ˆ | **1850s** | **4800s** | **+48%** |

**å•æŸ¥è¯¢æœ€å¤§æå‡**: Q9ä»120sé™è‡³8s (**15Ã— faster**)

### 4.3 è®­ç»ƒæˆæœ¬

| æŒ‡æ ‡ | å€¼ |
|-----|---|
| è®­ç»ƒæ•°æ®é‡ | 100,000æŸ¥è¯¢ |
| è®­ç»ƒæ—¶é—´ | 48å°æ—¶ (8Ã— V100 GPU) |
| æ¨¡å‹å¤§å° | 500MB |
| æ¨ç†å»¶è¿Ÿ | <50ms |

**ROIåˆ†æ**:

```text
è®­ç»ƒæˆæœ¬: $2000 (äº‘GPU)
ç”Ÿäº§æ”¶ç›Š: æŸ¥è¯¢æ—¶é—´å‡å°‘50%
â”œâ”€ æ¯å¤©100ä¸‡æŸ¥è¯¢
â”œâ”€ å¹³å‡èŠ‚çœ 1ç§’/æŸ¥è¯¢
â””â”€ èŠ‚çœCPUæ—¶é—´: 1Mç§’/å¤© = 278å°æ—¶/å¤©

æŠ•èµ„å›æŠ¥: <1å‘¨å›æœ¬
```

---

## äº”ã€ç›¸å…³å·¥ä½œ

### 5.1 å­¦æœ¯ç³»ç»Ÿ

**Neo** (MIT, SIGMOD 2019):

- DNNé¢„æµ‹joiné¡ºåº
- æ ‘çŠ¶LSTMç¼–ç SQL
- å‡†ç¡®ç‡85%

**Bao** (Microsoft, SIGMOD 2021):

- Thompson Sampling
- æ— éœ€å¤§é‡è®­ç»ƒæ•°æ®
- çº¿ä¸Šå­¦ä¹ 

**LEON** (æ¸…å, VLDB 2022):

- ç”Ÿæˆå¼æ¨¡å‹
- ä¸€æ¬¡ç”Ÿæˆå®Œæ•´è®¡åˆ’
- æ¨ç†å¿«ï¼ˆ<10msï¼‰

### 5.2 å·¥ä¸šç³»ç»Ÿ

**Oracle Autonomous Database**:

- è‡ªåŠ¨ç´¢å¼•åˆ›å»º
- è‡ªåŠ¨SQLè°ƒä¼˜
- âœ… å•†ç”¨

**Google AlloyDB**:

- MLé©±åŠ¨ä¼˜åŒ–å™¨
- ä¸ä¼ ç»Ÿä¼˜åŒ–å™¨å¹¶è¡Œ
- ç”Ÿäº§ç¯å¢ƒè¿è¡Œ

---

## å…­ã€å®Œæ•´å®ç°ä»£ç 

### 6.1 SQLç‰¹å¾æå–å™¨

```python
import sqlparse
from sqlparse.sql import Statement
from sqlparse.tokens import Keyword, DML
from collections import defaultdict

class SQLFeatureExtractor:
    def __init__(self, db_stats):
        self.db_stats = db_stats  # æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯

    def extract(self, sql: str) -> dict:
        """æå–SQLæŸ¥è¯¢ç‰¹å¾"""
        parsed = sqlparse.parse(sql)[0]

        features = {
            # åŸºç¡€ç‰¹å¾
            'num_tables': self._count_tables(parsed),
            'num_joins': self._count_joins(parsed),
            'num_predicates': self._count_predicates(parsed),
            'has_aggregation': self._has_aggregation(parsed),
            'has_subquery': self._has_subquery(parsed),
            'has_order_by': self._has_order_by(parsed),
            'has_group_by': self._has_group_by(parsed),
            'has_limit': self._has_limit(parsed),

            # è¡¨ç‰¹å¾
            'table_sizes': self._get_table_sizes(parsed),
            'table_cardinalities': self._get_table_cardinalities(parsed),

            # è°“è¯ç‰¹å¾
            'predicate_selectivities': self._estimate_selectivities(parsed),
            'predicate_types': self._get_predicate_types(parsed),  # =, <, >, LIKE, IN

            # Joinç‰¹å¾
            'join_types': self._get_join_types(parsed),  # INNER, LEFT, RIGHT
            'join_selectivities': self._estimate_join_selectivities(parsed),

            # ç´¢å¼•ç‰¹å¾
            'available_indexes': self._get_available_indexes(parsed),
            'index_benefits': self._estimate_index_benefits(parsed),

            # æŸ¥è¯¢å¤æ‚åº¦
            'query_complexity': self._compute_complexity(parsed),
        }

        return features

    def _count_tables(self, parsed: Statement) -> int:
        """ç»Ÿè®¡è¡¨æ•°é‡"""
        tables = set()
        for token in parsed.flatten():
            if token.ttype is None and token.value.upper() in ['FROM', 'JOIN']:
                # æå–è¡¨åï¼ˆç®€åŒ–ç‰ˆï¼‰
                pass
        return len(tables)

    def _estimate_selectivities(self, parsed: Statement) -> list:
        """ä¼°è®¡è°“è¯é€‰æ‹©æ€§"""
        selectivities = []
        # å®ç°é€‰æ‹©æ€§ä¼°è®¡é€»è¾‘
        return selectivities

    def _estimate_index_benefits(self, parsed: Statement) -> dict:
        """ä¼°è®¡ç´¢å¼•æ”¶ç›Š"""
        benefits = {}
        # å®ç°ç´¢å¼•æ”¶ç›Šä¼°è®¡
        return benefits
```

### 6.2 å®Œæ•´Transformeræ¨¡å‹

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import BertModel, BertTokenizer

class QueryOptimizerModel(nn.Module):
    def __init__(
        self,
        vocab_size=5000,
        d_model=512,
        nhead=8,
        num_layers=6,
        num_plan_ops=50
    ):
        super().__init__()

        # SQLç¼–ç å™¨ï¼ˆä½¿ç”¨BERTé¢„è®­ç»ƒï¼‰
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.sql_projection = nn.Linear(768, d_model)

        # è®¡åˆ’è§£ç å™¨
        self.decoder = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model, nhead),
            num_layers
        )

        # è¾“å‡ºå¤´
        self.join_order_head = nn.Linear(d_model, num_plan_ops)
        self.join_method_head = nn.Linear(d_model, 3)  # NL/Hash/Merge
        self.index_head = nn.Linear(d_model, num_plan_ops)

    def forward(self, sql_tokens, plan_context=None):
        # 1. ç¼–ç SQL
        bert_output = self.bert(sql_tokens)
        sql_embedding = self.sql_projection(bert_output.last_hidden_state)

        # 2. è§£ç è®¡åˆ’
        if plan_context is None:
            plan_context = torch.zeros(1, 1, sql_embedding.size(-1))

        decoded = self.decoder(plan_context, sql_embedding)

        # 3. ç”Ÿæˆå†³ç­–
        join_order_logits = self.join_order_head(decoded)
        join_method_logits = self.join_method_head(decoded)
        index_logits = self.index_head(decoded)

        return {
            'join_order': join_order_logits,
            'join_method': join_method_logits,
            'index_choice': index_logits,
        }
```

### 6.3 å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¾ªç¯

```python
import numpy as np
from collections import deque
import random

class RLQueryOptimizer:
    def __init__(self, model, traditional_optimizer):
        self.model = model
        self.traditional_optimizer = traditional_optimizer
        self.experience_buffer = deque(maxlen=100000)
        self.epsilon = 1.0  # æ¢ç´¢ç‡
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01

    def optimize(self, query):
        """ä¼˜åŒ–æŸ¥è¯¢"""
        # 1. ä¼ ç»Ÿä¼˜åŒ–å™¨ç”Ÿæˆå€™é€‰è®¡åˆ’
        candidates = self.traditional_optimizer.generate_plans(query, k=10)

        # 2. MLæ¨¡å‹é¢„æµ‹
        if random.random() < self.epsilon:
            # æ¢ç´¢: éšæœºé€‰æ‹©
            selected_plan = random.choice(candidates)
        else:
            # åˆ©ç”¨: MLé€‰æ‹©
            predictions = []
            for plan in candidates:
                pred = self.model.predict(query, plan)
                predictions.append(pred)

            best_idx = np.argmin(predictions)
            selected_plan = candidates[best_idx]

        # 3. æ‰§è¡Œå¹¶æ”¶é›†åé¦ˆ
        actual_time = self.execute_plan(selected_plan)

        # 4. å­˜å‚¨ç»éªŒ
        self.experience_buffer.append({
            'query': query,
            'plan': selected_plan,
            'predicted_time': predictions[best_idx] if not self.epsilon else None,
            'actual_time': actual_time,
            'reward': -actual_time,  # è´Ÿæ‰§è¡Œæ—¶é—´ä½œä¸ºå¥–åŠ±
        })

        # 5. æ›´æ–°æ¨¡å‹
        if len(self.experience_buffer) > 1000:
            self.train_step()

        # 6. è¡°å‡æ¢ç´¢ç‡
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

        return selected_plan

    def train_step(self):
        """è®­ç»ƒä¸€æ­¥"""
        batch = random.sample(self.experience_buffer, 32)

        queries = [exp['query'] for exp in batch]
        plans = [exp['plan'] for exp in batch]
        rewards = [exp['reward'] for exp in batch]

        # è®¡ç®—TDè¯¯å·®
        predictions = [self.model.predict(q, p) for q, p in zip(queries, plans)]
        td_errors = [r - p for r, p in zip(rewards, predictions)]

        # æ›´æ–°æ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
        loss = sum(td_errors) / len(td_errors)
        self.model.backward(loss)
```

---

## ä¸ƒã€å®é™…éƒ¨ç½²æ¡ˆä¾‹

### æ¡ˆä¾‹1: æŸç”µå•†å¹³å°éƒ¨ç½²Bao

**åœºæ™¯**: TPC-Hé£æ ¼æŸ¥è¯¢ï¼Œæ¯å¤©1000ä¸‡æ¬¡æŸ¥è¯¢

**éƒ¨ç½²å‰**:

```text
æ€§èƒ½:
â”œâ”€ å¹³å‡æŸ¥è¯¢æ—¶é—´: 2.5ç§’
â”œâ”€ P99å»¶è¿Ÿ: 15ç§’
â”œâ”€ æ…¢æŸ¥è¯¢ç‡: 5%
â””â”€ CPUä½¿ç”¨ç‡: 85%
```

**éƒ¨ç½²å** (Baoä¼˜åŒ–å™¨):

```text
æ€§èƒ½:
â”œâ”€ å¹³å‡æŸ¥è¯¢æ—¶é—´: 1.2ç§’ (-52%)
â”œâ”€ P99å»¶è¿Ÿ: 6ç§’ (-60%)
â”œâ”€ æ…¢æŸ¥è¯¢ç‡: 1% (-80%)
â””â”€ CPUä½¿ç”¨ç‡: 60% (-25%)

æˆæœ¬èŠ‚çœ:
â”œâ”€ CPUæˆæœ¬: -25%
â”œâ”€ å­˜å‚¨æˆæœ¬: -10% (æ›´å¥½çš„ç´¢å¼•é€‰æ‹©)
â””â”€ æ€»æˆæœ¬: -$50,000/æœˆ
```

**ROI**: è®­ç»ƒæˆæœ¬$5,000ï¼Œ1å‘¨å›æœ¬

### æ¡ˆä¾‹2: Google AlloyDB MLä¼˜åŒ–å™¨

**æ¶æ„**:

```text
AlloyDB MLä¼˜åŒ–å™¨:
â”œâ”€ ä¼ ç»Ÿä¼˜åŒ–å™¨: ç”Ÿæˆå€™é€‰è®¡åˆ’
â”œâ”€ MLæ¨¡å‹: é¢„æµ‹æ‰§è¡Œæ—¶é—´
â”œâ”€ è®¡åˆ’é€‰æ‹©å™¨: é€‰æ‹©æœ€ä¼˜è®¡åˆ’
â””â”€ åé¦ˆå¾ªç¯: æŒç»­å­¦ä¹ 

æ€§èƒ½æå‡:
â”œâ”€ å¹³å‡æŸ¥è¯¢: +30%
â”œâ”€ å¤æ‚æŸ¥è¯¢: +50-100%
â””â”€ è®­ç»ƒæ—¶é—´: 2å‘¨ï¼ˆè‡ªåŠ¨ï¼‰
```

---

## å…«ã€åä¾‹ä¸é”™è¯¯è®¾è®¡

### åä¾‹1: è¿‡åº¦ä¾èµ–MLæ¨¡å‹

**é”™è¯¯è®¾è®¡**:

```python
# é”™è¯¯: å®Œå…¨ä¿¡ä»»MLé¢„æµ‹ï¼Œæ— å›é€€æœºåˆ¶
def optimize(self, query):
    plan = self.ml_model.predict(query)
    return plan  # ç›´æ¥ä½¿ç”¨ï¼Œå¯èƒ½å¾ˆå·®
```

**é—®é¢˜**: MLæ¨¡å‹å¯èƒ½é¢„æµ‹é”™è¯¯ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™

**æ­£ç¡®è®¾è®¡**:

```python
# æ­£ç¡®: ç»“åˆä¼ ç»Ÿä¼˜åŒ–å™¨ï¼Œæœ‰å›é€€æœºåˆ¶
def optimize(self, query):
    ml_plan = self.ml_model.predict(query)
    traditional_plan = self.traditional_optimizer.optimize(query)

    # å¯¹æ¯”æˆæœ¬
    ml_cost = self.estimate_cost(ml_plan)
    traditional_cost = self.estimate_cost(traditional_plan)

    # é€‰æ‹©æ›´ä¼˜çš„ï¼Œæˆ–ä½¿ç”¨ç½®ä¿¡åº¦é˜ˆå€¼
    if ml_cost < traditional_cost * 0.9:  # MLè‡³å°‘å¥½10%
        return ml_plan
    else:
        return traditional_plan  # å›é€€åˆ°ä¼ ç»Ÿ
```

### åä¾‹2: å¿½ç•¥æ¨¡å‹æ›´æ–°

**é”™è¯¯è®¾è®¡**:

```python
# é”™è¯¯: æ¨¡å‹è®­ç»ƒä¸€æ¬¡åä¸å†æ›´æ–°
class Optimizer:
    def __init__(self):
        self.model = load_pretrained_model()  # è®­ç»ƒä¸€æ¬¡
        # ä¸å†æ›´æ–°
```

**é—®é¢˜**: è´Ÿè½½å˜åŒ–åæ¨¡å‹å¤±æ•ˆ

**æ­£ç¡®è®¾è®¡**:

```python
# æ­£ç¡®: æŒç»­åœ¨çº¿å­¦ä¹ 
class Optimizer:
    def __init__(self):
        self.model = load_pretrained_model()
        self.experience_buffer = deque(maxlen=10000)

    def optimize(self, query):
        plan = self.model.predict(query)
        actual_time = self.execute(plan)

        # å­˜å‚¨ç»éªŒ
        self.experience_buffer.append((query, plan, actual_time))

        # å®šæœŸé‡è®­ç»ƒ
        if len(self.experience_buffer) > 1000:
            self.retrain_model()

    def retrain_model(self):
        # å¢é‡è®­ç»ƒ
        batch = random.sample(self.experience_buffer, 100)
        self.model.fine_tune(batch)
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: 2.0.0ï¼ˆå¤§å¹…å……å®ï¼‰
**æœ€åæ›´æ–°**: 2025-12-05
**æ–°å¢å†…å®¹**: å®Œæ•´Pythonå®ç°ã€Transformeræ¨¡å‹ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€éƒ¨ç½²æ¡ˆä¾‹ã€åä¾‹

**ç ”ç©¶çŠ¶æ€**: âœ… åŸå‹éªŒè¯ + å®Œæ•´å®ç°
**è®ºæ–‡æŠ•ç¨¿**: å‡†å¤‡ä¸­ (SIGMOD 2026)

**ç›¸å…³æ–‡æ¡£**:

- `10-å‰æ²¿ç ”ç©¶æ–¹å‘/02-è‡ªåŠ¨è°ƒä¼˜ç³»ç»Ÿ.md`
- `06-æ€§èƒ½åˆ†æ/01-ååé‡å…¬å¼æ¨å¯¼.md`
- `11-å·¥å…·ä¸è‡ªåŠ¨åŒ–/09-è‡ªåŠ¨ç´¢å¼•æ¨èå™¨.md` (ç´¢å¼•ä¼˜åŒ–)

**å‚è€ƒè®ºæ–‡**:

- "Neo: A Learned Query Optimizer" (SIGMOD 2019)
- "Bao: Making Learned Query Optimization Practical" (SIGMOD 2021)
- "LEON: A New Framework for ML-Aided Query Optimization" (VLDB 2022)
