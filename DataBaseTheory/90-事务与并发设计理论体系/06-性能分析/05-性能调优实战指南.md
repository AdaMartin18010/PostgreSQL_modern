# 05 | æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—

> **å®è·µå®šä½**: æœ¬æ–‡æ¡£æä¾›ç³»ç»ŸåŒ–çš„æ€§èƒ½è°ƒä¼˜æ–¹æ³•è®ºï¼Œä»é—®é¢˜è¯Šæ–­åˆ°ä¼˜åŒ–å®æ–½çš„å®Œæ•´æµç¨‹ï¼ŒåŒ…å«å¤§é‡å®é™…æ¡ˆä¾‹å’Œå¯æ‰§è¡Œä»£ç ã€‚

---

## ğŸ“‘ ç›®å½•

- [05 | æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—](#05--æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [ä¸€ã€æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—èƒŒæ™¯ä¸æ¼”è¿›](#ä¸€æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—èƒŒæ™¯ä¸æ¼”è¿›)
    - [0.1 ä¸ºä»€ä¹ˆéœ€è¦æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—ï¼Ÿ](#01-ä¸ºä»€ä¹ˆéœ€è¦æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—)
    - [0.2 æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—çš„æ ¸å¿ƒæŒ‘æˆ˜](#02-æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—çš„æ ¸å¿ƒæŒ‘æˆ˜)
  - [äºŒã€è°ƒä¼˜æ–¹æ³•è®º](#äºŒè°ƒä¼˜æ–¹æ³•è®º)
    - [1.1 è°ƒä¼˜æµç¨‹](#11-è°ƒä¼˜æµç¨‹)
    - [1.2 é—®é¢˜è¯Šæ–­æ¡†æ¶](#12-é—®é¢˜è¯Šæ–­æ¡†æ¶)
  - [äºŒã€æ€§èƒ½ç“¶é¢ˆè¯Šæ–­](#äºŒæ€§èƒ½ç“¶é¢ˆè¯Šæ–­)
    - [2.1 CPUç“¶é¢ˆ](#21-cpuç“¶é¢ˆ)
    - [2.2 å†…å­˜ç“¶é¢ˆ](#22-å†…å­˜ç“¶é¢ˆ)
    - [2.3 I/Oç“¶é¢ˆ](#23-ioç“¶é¢ˆ)
    - [2.4 é”ç«äº‰ç“¶é¢ˆ](#24-é”ç«äº‰ç“¶é¢ˆ)
  - [ä¸‰ã€æŸ¥è¯¢ä¼˜åŒ–å®æˆ˜](#ä¸‰æŸ¥è¯¢ä¼˜åŒ–å®æˆ˜)
    - [3.1 æ…¢æŸ¥è¯¢åˆ†æ](#31-æ…¢æŸ¥è¯¢åˆ†æ)
    - [3.2 æ‰§è¡Œè®¡åˆ’ä¼˜åŒ–](#32-æ‰§è¡Œè®¡åˆ’ä¼˜åŒ–)
    - [3.3 ç´¢å¼•ä¼˜åŒ–ç­–ç•¥](#33-ç´¢å¼•ä¼˜åŒ–ç­–ç•¥)
  - [å››ã€é…ç½®å‚æ•°è°ƒä¼˜](#å››é…ç½®å‚æ•°è°ƒä¼˜)
    - [4.1 å†…å­˜å‚æ•°](#41-å†…å­˜å‚æ•°)
    - [4.2 WALå‚æ•°](#42-walå‚æ•°)
    - [4.3 è¿æ¥å‚æ•°](#43-è¿æ¥å‚æ•°)
    - [4.4 VACUUMå‚æ•°](#44-vacuumå‚æ•°)
  - [äº”ã€æ¶æ„å±‚é¢ä¼˜åŒ–](#äº”æ¶æ„å±‚é¢ä¼˜åŒ–)
    - [5.1 è¯»å†™åˆ†ç¦»](#51-è¯»å†™åˆ†ç¦»)
    - [5.2 åˆ†åŒºè¡¨è®¾è®¡](#52-åˆ†åŒºè¡¨è®¾è®¡)
    - [5.3 ç¼“å­˜ç­–ç•¥](#53-ç¼“å­˜ç­–ç•¥)
  - [å…­ã€å®Œæ•´è¯Šæ–­å·¥å…·](#å…­å®Œæ•´è¯Šæ–­å·¥å…·)
    - [6.1 æ€§èƒ½è¯Šæ–­è„šæœ¬](#61-æ€§èƒ½è¯Šæ–­è„šæœ¬)
    - [6.2 è‡ªåŠ¨åŒ–è°ƒä¼˜å·¥å…·](#62-è‡ªåŠ¨åŒ–è°ƒä¼˜å·¥å…·)
  - [ä¸ƒã€å®é™…ä¼˜åŒ–æ¡ˆä¾‹](#ä¸ƒå®é™…ä¼˜åŒ–æ¡ˆä¾‹)
    - [æ¡ˆä¾‹1: ç”µå•†ç³»ç»ŸTPSä»5Kæå‡åˆ°20K](#æ¡ˆä¾‹1-ç”µå•†ç³»ç»Ÿtpsä»5kæå‡åˆ°20k)
    - [æ¡ˆä¾‹2: æŠ¥è¡¨æŸ¥è¯¢ä»30ç§’é™è‡³2ç§’](#æ¡ˆä¾‹2-æŠ¥è¡¨æŸ¥è¯¢ä»30ç§’é™è‡³2ç§’)
    - [æ¡ˆä¾‹3: é«˜å¹¶å‘å†™å…¥ä¼˜åŒ–](#æ¡ˆä¾‹3-é«˜å¹¶å‘å†™å…¥ä¼˜åŒ–)
  - [å…«ã€åä¾‹ä¸é”™è¯¯ä¼˜åŒ–](#å…«åä¾‹ä¸é”™è¯¯ä¼˜åŒ–)
    - [åä¾‹1: è¿‡åº¦ä¼˜åŒ–å¯¼è‡´é—®é¢˜](#åä¾‹1-è¿‡åº¦ä¼˜åŒ–å¯¼è‡´é—®é¢˜)
    - [åä¾‹2: å¿½ç•¥æ ¹æœ¬åŸå› ](#åä¾‹2-å¿½ç•¥æ ¹æœ¬åŸå› )
    - [åä¾‹3: æ€§èƒ½è°ƒä¼˜æ–¹æ³•ä¸ç³»ç»Ÿ](#åä¾‹3-æ€§èƒ½è°ƒä¼˜æ–¹æ³•ä¸ç³»ç»Ÿ)
    - [åä¾‹4: è°ƒä¼˜å¿½ç•¥ä¸šåŠ¡å½±å“](#åä¾‹4-è°ƒä¼˜å¿½ç•¥ä¸šåŠ¡å½±å“)
    - [åä¾‹5: è°ƒä¼˜éªŒè¯ä¸è¶³](#åä¾‹5-è°ƒä¼˜éªŒè¯ä¸è¶³)
    - [åä¾‹6: è°ƒä¼˜ç¼ºä¹ç›‘æ§](#åä¾‹6-è°ƒä¼˜ç¼ºä¹ç›‘æ§)
  - [ä¹ã€è°ƒä¼˜æ£€æŸ¥æ¸…å•](#ä¹è°ƒä¼˜æ£€æŸ¥æ¸…å•)
  - [åã€æ€»ç»“](#åæ€»ç»“)
    - [10.1 æ ¸å¿ƒæ–¹æ³•è®º](#101-æ ¸å¿ƒæ–¹æ³•è®º)
    - [10.2 å…³é”®å·¥å…·](#102-å…³é”®å·¥å…·)
    - [10.3 æœ€ä½³å®è·µ](#103-æœ€ä½³å®è·µ)
  - [åä¸€ã€å®Œæ•´æ€§èƒ½è¯Šæ–­å·¥å…·å®ç°](#åä¸€å®Œæ•´æ€§èƒ½è¯Šæ–­å·¥å…·å®ç°)
    - [11.1 è‡ªåŠ¨åŒ–æ€§èƒ½è¯Šæ–­è„šæœ¬](#111-è‡ªåŠ¨åŒ–æ€§èƒ½è¯Šæ–­è„šæœ¬)
    - [11.2 æ€§èƒ½åŸºçº¿å¯¹æ¯”å·¥å…·](#112-æ€§èƒ½åŸºçº¿å¯¹æ¯”å·¥å…·)
  - [åäºŒã€æ›´å¤šå®é™…åº”ç”¨æ¡ˆä¾‹](#åäºŒæ›´å¤šå®é™…åº”ç”¨æ¡ˆä¾‹)
    - [12.1 æ¡ˆä¾‹: äº‘æ•°æ®åº“è¿ç§»æ€§èƒ½è°ƒä¼˜](#121-æ¡ˆä¾‹-äº‘æ•°æ®åº“è¿ç§»æ€§èƒ½è°ƒä¼˜)
    - [12.2 æ¡ˆä¾‹: å¾®æœåŠ¡æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–](#122-æ¡ˆä¾‹-å¾®æœåŠ¡æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–)
  - [åä¸‰ã€å®Œæ•´æ€§èƒ½è°ƒä¼˜å·¥å…·å®ç°](#åä¸‰å®Œæ•´æ€§èƒ½è°ƒä¼˜å·¥å…·å®ç°)
    - [13.1 è‡ªåŠ¨åŒ–è°ƒä¼˜è„šæœ¬å®Œæ•´å®ç°](#131-è‡ªåŠ¨åŒ–è°ƒä¼˜è„šæœ¬å®Œæ•´å®ç°)
    - [13.2 æ€§èƒ½ç›‘æ§ä¸å‘Šè­¦ç³»ç»Ÿ](#132-æ€§èƒ½ç›‘æ§ä¸å‘Šè­¦ç³»ç»Ÿ)

---

## ä¸€ã€æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—èƒŒæ™¯ä¸æ¼”è¿›

### 0.1 ä¸ºä»€ä¹ˆéœ€è¦æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—ï¼Ÿ

**å†å²èƒŒæ™¯**:

åœ¨æ•°æ®åº“ç³»ç»Ÿè¿ç»´ä¸­ï¼Œå¦‚ä½•ç³»ç»ŸåŒ–åœ°è¿›è¡Œæ€§èƒ½è°ƒä¼˜ä¸€ç›´æ˜¯ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ã€‚ä»æ•°æ®åº“ç³»ç»Ÿè¯ç”Ÿå¼€å§‹ï¼Œæ€§èƒ½è°ƒä¼˜å°±æ˜¯è¿ç»´äººå‘˜é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚ç†è§£æ€§èƒ½è°ƒä¼˜æ–¹æ³•è®ºï¼Œæœ‰åŠ©äºç³»ç»ŸåŒ–åœ°è¯Šæ–­å’Œè§£å†³æ€§èƒ½é—®é¢˜ã€ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ã€æå‡ç”¨æˆ·ä½“éªŒã€‚

**ç†è®ºåŸºç¡€**:

```text
æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—çš„æ ¸å¿ƒ:
â”œâ”€ é—®é¢˜: å¦‚ä½•ç³»ç»ŸåŒ–åœ°è¿›è¡Œæ€§èƒ½è°ƒä¼˜ï¼Ÿ
â”œâ”€ ç†è®º: æ€§èƒ½åˆ†æç†è®ºã€ä¼˜åŒ–æ–¹æ³•
â””â”€ æ–¹æ³•: è¯Šæ–­æ¡†æ¶ã€ä¼˜åŒ–ç­–ç•¥

ä¸ºä»€ä¹ˆéœ€è¦æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—?
â”œâ”€ æ— æŒ‡å—: è°ƒä¼˜ç›²ç›®ï¼Œæ•ˆç‡ä½
â”œâ”€ ç»éªŒæ–¹æ³•: ä¸å®Œæ•´ï¼Œå¯èƒ½æœ‰é—æ¼
â””â”€ ç³»ç»ŸåŒ–æŒ‡å—: å®Œæ•´ã€é«˜æ•ˆã€å¯å¤ç°
```

**å®é™…åº”ç”¨èƒŒæ™¯**:

```text
æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—æ¼”è¿›:
â”œâ”€ æ—©æœŸæ–¹æ³• (1990s-2000s)
â”‚   â”œâ”€ ç»éªŒå¼è°ƒä¼˜
â”‚   â”œâ”€ é—®é¢˜: ä¸ç³»ç»Ÿ
â”‚   â””â”€ ç»“æœ: æ•ˆç‡ä½
â”‚
â”œâ”€ ç³»ç»ŸåŒ–æ–¹æ³• (2000s-2010s)
â”‚   â”œâ”€ è¯Šæ–­æ¡†æ¶
â”‚   â”œâ”€ ä¼˜åŒ–ç­–ç•¥
â”‚   â””â”€ è‡ªåŠ¨åŒ–å·¥å…·
â”‚
â””â”€ ç°ä»£æ–¹æ³• (2010s+)
    â”œâ”€ æ™ºèƒ½è¯Šæ–­
    â”œâ”€ è‡ªåŠ¨åŒ–è°ƒä¼˜
    â””â”€ æ€§èƒ½åŸºçº¿ç®¡ç†
```

**ä¸ºä»€ä¹ˆæ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—é‡è¦ï¼Ÿ**

1. **ç³»ç»ŸåŒ–è°ƒä¼˜**: ç³»ç»ŸåŒ–åœ°è¯Šæ–­å’Œè§£å†³æ€§èƒ½é—®é¢˜
2. **æ•ˆç‡æå‡**: æé«˜è°ƒä¼˜æ•ˆç‡
3. **é—®é¢˜è¯Šæ–­**: å¿«é€Ÿå®šä½æ€§èƒ½ç“¶é¢ˆ
4. **æœ€ä½³å®è·µ**: ç§¯ç´¯å’Œåˆ†äº«æœ€ä½³å®è·µ

**åä¾‹: æ— æŒ‡å—çš„æ€§èƒ½è°ƒä¼˜é—®é¢˜**:

```text
é”™è¯¯è®¾è®¡: æ— æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—ï¼Œç›²ç›®è°ƒä¼˜
â”œâ”€ åœºæ™¯: æ€§èƒ½é—®é¢˜
â”œâ”€ é—®é¢˜: ä¸ç†è§£è°ƒä¼˜æ–¹æ³•
â”œâ”€ ç»“æœ: è°ƒä¼˜æ–¹å‘é”™è¯¯ï¼Œé—®é¢˜æœªè§£å†³
â””â”€ åæœ: æµªè´¹æ—¶é—´å’Œèµ„æº âœ—

æ­£ç¡®è®¾è®¡: ä½¿ç”¨æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—
â”œâ”€ æ–¹æ¡ˆ: ç³»ç»ŸåŒ–è¯Šæ–­å’Œä¼˜åŒ–
â”œâ”€ ç»“æœ: å¿«é€Ÿå®šä½å’Œè§£å†³é—®é¢˜
â””â”€ æ•ˆç‡: è°ƒä¼˜æ—¶é—´å‡å°‘50%+ âœ“
```

### 0.2 æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—çš„æ ¸å¿ƒæŒ‘æˆ˜

**å†å²èƒŒæ™¯**:

æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—é¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜åŒ…æ‹¬ï¼šå¦‚ä½•ç³»ç»ŸåŒ–è¯Šæ–­é—®é¢˜ã€å¦‚ä½•å¿«é€Ÿå®šä½ç“¶é¢ˆã€å¦‚ä½•è®¾è®¡ä¼˜åŒ–æ–¹æ¡ˆã€å¦‚ä½•éªŒè¯ä¼˜åŒ–æ•ˆæœç­‰ã€‚è¿™äº›æŒ‘æˆ˜ä¿ƒä½¿è°ƒä¼˜æ–¹æ³•ä¸æ–­ä¼˜åŒ–ã€‚

**ç†è®ºåŸºç¡€**:

```text
æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—æŒ‘æˆ˜:
â”œâ”€ è¯Šæ–­æŒ‘æˆ˜: å¦‚ä½•ç³»ç»ŸåŒ–è¯Šæ–­é—®é¢˜
â”œâ”€ å®šä½æŒ‘æˆ˜: å¦‚ä½•å¿«é€Ÿå®šä½ç“¶é¢ˆ
â”œâ”€ è®¾è®¡æŒ‘æˆ˜: å¦‚ä½•è®¾è®¡ä¼˜åŒ–æ–¹æ¡ˆ
â””â”€ éªŒè¯æŒ‘æˆ˜: å¦‚ä½•éªŒè¯ä¼˜åŒ–æ•ˆæœ

è°ƒä¼˜æŒ‡å—è§£å†³æ–¹æ¡ˆ:
â”œâ”€ è¯Šæ–­: ç³»ç»ŸåŒ–è¯Šæ–­æ¡†æ¶
â”œâ”€ å®šä½: æ€§èƒ½åˆ†æå·¥å…·
â”œâ”€ è®¾è®¡: ä¼˜åŒ–ç­–ç•¥å’Œæœ€ä½³å®è·µ
â””â”€ éªŒè¯: æ€§èƒ½æµ‹è¯•å’Œå¯¹æ¯”
```

---

## äºŒã€è°ƒä¼˜æ–¹æ³•è®º

### 1.1 è°ƒä¼˜æµç¨‹

```text
æ€§èƒ½é—®é¢˜æŠ¥å‘Š
    â†“
é—®é¢˜å¤ç°ä¸æ•°æ®æ”¶é›†
    â”œâ”€ æ”¶é›†ç³»ç»ŸæŒ‡æ ‡ (CPU/å†…å­˜/IO)
    â”œâ”€ æ”¶é›†æ•°æ®åº“æŒ‡æ ‡ (TPS/å»¶è¿Ÿ/é”ç­‰å¾…)
    â””â”€ æ”¶é›†æ…¢æŸ¥è¯¢æ—¥å¿—
    â†“
ç“¶é¢ˆè¯†åˆ«
    â”œâ”€ CPUç“¶é¢ˆ?
    â”œâ”€ å†…å­˜ç“¶é¢ˆ?
    â”œâ”€ I/Oç“¶é¢ˆ?
    â””â”€ é”ç«äº‰?
    â†“
æ ¹å› åˆ†æ
    â”œâ”€ æ…¢æŸ¥è¯¢åˆ†æ
    â”œâ”€ æ‰§è¡Œè®¡åˆ’åˆ†æ
    â””â”€ é…ç½®å‚æ•°åˆ†æ
    â†“
ä¼˜åŒ–æ–¹æ¡ˆè®¾è®¡
    â”œâ”€ æŸ¥è¯¢ä¼˜åŒ–
    â”œâ”€ ç´¢å¼•ä¼˜åŒ–
    â”œâ”€ é…ç½®è°ƒä¼˜
    â””â”€ æ¶æ„ä¼˜åŒ–
    â†“
æ–¹æ¡ˆå®æ–½
    â”œâ”€ æµ‹è¯•ç¯å¢ƒéªŒè¯
    â”œâ”€ ç°åº¦å‘å¸ƒ
    â””â”€ å…¨é‡ä¸Šçº¿
    â†“
æ•ˆæœéªŒè¯
    â”œâ”€ æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
    â”œâ”€ ç¨³å®šæ€§éªŒè¯
    â””â”€ å›æ»šé¢„æ¡ˆ
```

### 1.2 é—®é¢˜è¯Šæ–­æ¡†æ¶

**è¯Šæ–­çŸ©é˜µ**:

| ç—‡çŠ¶ | CPUé«˜ | å†…å­˜é«˜ | I/Oé«˜ | é”ç­‰å¾… | å¯èƒ½åŸå›  | ä¼˜åŒ–æ–¹å‘ |
|-----|------|--------|-------|--------|---------|---------|
| **TPSä½** | âœ“ | - | âœ“ | âœ“ | æ…¢æŸ¥è¯¢/é”ç«äº‰ | æŸ¥è¯¢ä¼˜åŒ–/ç´¢å¼• |
| **å»¶è¿Ÿé«˜** | âœ“ | - | âœ“ | âœ“ | ç“¶é¢ˆèµ„æº | èµ„æºæ‰©å®¹/ä¼˜åŒ– |
| **è¿æ¥æ•°æ»¡** | - | âœ“ | - | - | è¿æ¥æ³„æ¼ | è¿æ¥æ± /åº”ç”¨ä¿®å¤ |
| **è¡¨è†¨èƒ€** | - | âœ“ | âœ“ | - | VACUUMä¸è¶³ | VACUUMè°ƒä¼˜ |

---

## äºŒã€æ€§èƒ½ç“¶é¢ˆè¯Šæ–­

### 2.1 CPUç“¶é¢ˆ

**è¯Šæ–­SQL**:

```sql
-- æŸ¥çœ‹CPUä½¿ç”¨ç‡é«˜çš„æŸ¥è¯¢
SELECT
    pid,
    usename,
    application_name,
    state,
    query_start,
    EXTRACT(EPOCH FROM (NOW() - query_start)) AS query_duration,
    LEFT(query, 100) AS query_snippet,
    cpu_usage_percent
FROM pg_stat_activity
WHERE state = 'active'
  AND EXTRACT(EPOCH FROM (NOW() - query_start)) > 1
ORDER BY cpu_usage_percent DESC
LIMIT 10;

-- æŸ¥çœ‹pg_stat_statementsä¸­çš„CPUæ¶ˆè€—
SELECT
    query,
    calls,
    total_exec_time,
    mean_exec_time,
    (shared_blks_hit + shared_blks_read) AS total_blocks,
    (shared_blks_hit::float / NULLIF(shared_blks_hit + shared_blks_read, 0)) AS cache_hit_ratio
FROM pg_stat_statements
WHERE mean_exec_time > 100  -- å¹³å‡æ‰§è¡Œæ—¶é—´>100ms
ORDER BY total_exec_time DESC
LIMIT 20;
```

**å¸¸è§åŸå› **:

1. **å¤æ‚æŸ¥è¯¢**: JOINå¤šè¡¨ã€èšåˆè®¡ç®—
2. **ç¼ºå°‘ç´¢å¼•**: å…¨è¡¨æ‰«æ
3. **æ’åºæ“ä½œ**: ORDER BYå¤§ç»“æœé›†
4. **æ­£åˆ™è¡¨è¾¾å¼**: LIKE '%pattern%'

**ä¼˜åŒ–æ–¹æ¡ˆ**:

```sql
-- æ–¹æ¡ˆ1: æ·»åŠ ç´¢å¼•
CREATE INDEX idx_orders_user_date ON orders(user_id, created_at);

-- æ–¹æ¡ˆ2: ä¼˜åŒ–æŸ¥è¯¢ï¼ˆé¿å…å…¨è¡¨æ‰«æï¼‰
-- é”™è¯¯: SELECT * FROM orders WHERE status LIKE '%pending%'
-- æ­£ç¡®: SELECT * FROM orders WHERE status = 'pending'

-- æ–¹æ¡ˆ3: ä½¿ç”¨ç‰©åŒ–è§†å›¾ï¼ˆé¢„è®¡ç®—ï¼‰
CREATE MATERIALIZED VIEW order_summary AS
SELECT user_id, COUNT(*) as order_count, SUM(amount) as total_amount
FROM orders
GROUP BY user_id;

-- å®šæœŸåˆ·æ–°
REFRESH MATERIALIZED VIEW CONCURRENTLY order_summary;
```

### 2.2 å†…å­˜ç“¶é¢ˆ

**è¯Šæ–­SQL**:

```sql
-- æ£€æŸ¥work_memä½¿ç”¨æƒ…å†µ
SELECT
    pid,
    usename,
    query,
    temp_files,
    temp_bytes,
    work_mem,
    CASE
        WHEN temp_files > 0 THEN 'âš ï¸ ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶'
        ELSE 'âœ“ å†…å­˜è¶³å¤Ÿ'
    END AS status
FROM pg_stat_activity
WHERE state = 'active'
ORDER BY temp_bytes DESC
LIMIT 10;

-- æ£€æŸ¥å…±äº«å†…å­˜ä½¿ç”¨
SELECT
    setting AS shared_buffers,
    pg_size_pretty(pg_size_bytes(setting)) AS size_pretty
FROM pg_settings
WHERE name = 'shared_buffers';

-- æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡
SELECT
    'index hit rate' AS metric,
    ROUND(100.0 * sum(idx_blks_hit) / NULLIF(sum(idx_blks_hit + idx_blks_read), 0), 2) AS percentage
FROM pg_statio_user_indexes
UNION ALL
SELECT
    'table hit rate' AS metric,
    ROUND(100.0 * sum(heap_blks_hit) / NULLIF(sum(heap_blks_hit + heap_blks_read), 0), 2) AS percentage
FROM pg_statio_user_tables;
```

**ä¼˜åŒ–æ–¹æ¡ˆ**:

```sql
-- æ–¹æ¡ˆ1: å¢åŠ work_memï¼ˆé’ˆå¯¹å¤§æ’åº/å“ˆå¸Œï¼‰
ALTER ROLE olap_user SET work_mem = '512MB';

-- æ–¹æ¡ˆ2: å¢åŠ shared_buffers
ALTER SYSTEM SET shared_buffers = '16GB';  -- 25% RAM

-- æ–¹æ¡ˆ3: ä¼˜åŒ–æŸ¥è¯¢ï¼ˆå‡å°‘å†…å­˜ä½¿ç”¨ï¼‰
-- ä½¿ç”¨LIMITé¿å…å¤§ç»“æœé›†
SELECT * FROM large_table ORDER BY id LIMIT 1000;
```

### 2.3 I/Oç“¶é¢ˆ

**è¯Šæ–­SQL**:

```sql
-- æŸ¥çœ‹I/Oç­‰å¾…çš„æŸ¥è¯¢
SELECT
    pid,
    wait_event_type,
    wait_event,
    state,
    query
FROM pg_stat_activity
WHERE wait_event_type = 'IO'
ORDER BY state_change;

-- æŸ¥çœ‹è¡¨I/Oç»Ÿè®¡
SELECT
    schemaname,
    tablename,
    seq_scan,
    seq_tup_read,
    idx_scan,
    idx_tup_fetch,
    heap_blks_read,
    heap_blks_hit,
    ROUND(100.0 * heap_blks_hit / NULLIF(heap_blks_hit + heap_blks_read, 0), 2) AS cache_hit_ratio
FROM pg_statio_user_tables
WHERE heap_blks_read > 1000
ORDER BY heap_blks_read DESC
LIMIT 20;
```

**ä¼˜åŒ–æ–¹æ¡ˆ**:

```sql
-- æ–¹æ¡ˆ1: æ·»åŠ ç´¢å¼•ï¼ˆå‡å°‘é¡ºåºæ‰«æï¼‰
CREATE INDEX idx_orders_status ON orders(status);

-- æ–¹æ¡ˆ2: ä½¿ç”¨SSDï¼ˆç¡¬ä»¶å‡çº§ï¼‰
-- random_page_cost = 1.1 (SSD)
-- effective_io_concurrency = 200

-- æ–¹æ¡ˆ3: é¢„çƒ­ç¼“å­˜
SELECT pg_prewarm('orders');
```

### 2.4 é”ç«äº‰ç“¶é¢ˆ

**è¯Šæ–­SQL**:

```sql
-- æŸ¥çœ‹é”ç­‰å¾…
SELECT
    blocked_locks.pid AS blocked_pid,
    blocking_locks.pid AS blocking_pid,
    blocked_activity.query AS blocked_query,
    blocking_activity.query AS blocking_query,
    blocked_activity.wait_event_type,
    blocked_activity.wait_event,
    EXTRACT(EPOCH FROM (NOW() - blocked_activity.query_start)) AS blocked_duration
FROM pg_catalog.pg_locks blocked_locks
JOIN pg_catalog.pg_stat_activity blocked_activity
    ON blocked_activity.pid = blocked_locks.pid
JOIN pg_catalog.pg_locks blocking_locks
    ON blocking_locks.locktype = blocked_locks.locktype
    AND blocking_locks.database IS NOT DISTINCT FROM blocked_locks.database
    AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation
    AND blocking_locks.pid != blocked_locks.pid
JOIN pg_catalog.pg_stat_activity blocking_activity
    ON blocking_activity.pid = blocking_locks.pid
WHERE NOT blocked_locks.granted
ORDER BY blocked_duration DESC;

-- æŸ¥çœ‹é•¿äº‹åŠ¡ï¼ˆå¯èƒ½å¯¼è‡´é”ç­‰å¾…ï¼‰
SELECT
    pid,
    usename,
    application_name,
    state,
    xact_start,
    query_start,
    EXTRACT(EPOCH FROM (NOW() - xact_start)) AS transaction_duration,
    EXTRACT(EPOCH FROM (NOW() - query_start)) AS query_duration,
    LEFT(query, 100) AS query_snippet
FROM pg_stat_activity
WHERE state != 'idle'
  AND xact_start IS NOT NULL
  AND EXTRACT(EPOCH FROM (NOW() - xact_start)) > 60  -- è¶…è¿‡1åˆ†é’Ÿ
ORDER BY transaction_duration DESC;
```

**ä¼˜åŒ–æ–¹æ¡ˆ**:

```sql
-- æ–¹æ¡ˆ1: ç»ˆæ­¢é•¿äº‹åŠ¡
SELECT pg_terminate_backend(pid) FROM pg_stat_activity
WHERE xact_start < NOW() - INTERVAL '10 minutes'
  AND state != 'idle';

-- æ–¹æ¡ˆ2: ä½¿ç”¨è¡Œçº§é”è€Œéè¡¨çº§é”
-- é”™è¯¯: LOCK TABLE orders IN EXCLUSIVE MODE;
-- æ­£ç¡®: SELECT * FROM orders WHERE id = 1 FOR UPDATE;

-- æ–¹æ¡ˆ3: ç¼©çŸ­äº‹åŠ¡æ—¶é—´
-- å°†å¤–éƒ¨APIè°ƒç”¨ç§»åˆ°äº‹åŠ¡å¤–
```

---

## ä¸‰ã€æŸ¥è¯¢ä¼˜åŒ–å®æˆ˜

### 3.1 æ…¢æŸ¥è¯¢åˆ†æ

**å®Œæ•´è¯Šæ–­è„šæœ¬**:

```python
import psycopg2
from typing import List, Dict
import json

class SlowQueryAnalyzer:
    """æ…¢æŸ¥è¯¢åˆ†æå™¨"""

    def __init__(self, conn):
        self.conn = conn

    def get_slow_queries(self, threshold_ms: float = 1000, limit: int = 20) -> List[Dict]:
        """è·å–æ…¢æŸ¥è¯¢"""
        cur = self.conn.cursor()
        cur.execute("""
            SELECT
                query,
                calls,
                total_exec_time,
                mean_exec_time,
                max_exec_time,
                stddev_exec_time,
                rows,
                100.0 * shared_blks_hit / NULLIF(shared_blks_hit + shared_blks_read, 0) AS cache_hit_ratio
            FROM pg_stat_statements
            WHERE mean_exec_time > %s
            ORDER BY total_exec_time DESC
            LIMIT %s
        """, (threshold_ms, limit))

        results = []
        for row in cur.fetchall():
            results.append({
                'query': row[0],
                'calls': row[1],
                'total_time_ms': row[2],
                'mean_time_ms': row[3],
                'max_time_ms': row[4],
                'stddev_ms': row[5],
                'rows': row[6],
                'cache_hit_ratio': row[7],
            })

        cur.close()
        return results

    def analyze_query_plan(self, query: str) -> Dict:
        """åˆ†ææŸ¥è¯¢è®¡åˆ’"""
        cur = self.conn.cursor()
        cur.execute(f"EXPLAIN (ANALYZE, BUFFERS, VERBOSE) {query}")

        plan_text = '\n'.join([row[0] for row in cur.fetchall()])

        # è§£æè®¡åˆ’
        analysis = {
            'plan': plan_text,
            'has_seq_scan': 'Seq Scan' in plan_text,
            'has_index_scan': 'Index Scan' in plan_text,
            'execution_time_ms': self._extract_time(plan_text),
            'cost': self._extract_cost(plan_text),
        }

        cur.close()
        return analysis

    def _extract_time(self, plan_text: str) -> float:
        """æå–æ‰§è¡Œæ—¶é—´"""
        import re
        match = re.search(r'Execution Time: ([\d.]+) ms', plan_text)
        return float(match.group(1)) if match else 0

    def _extract_cost(self, plan_text: str) -> tuple:
        """æå–æˆæœ¬ (startup, total)"""
        import re
        match = re.search(r'cost=([\d.]+)\.\.([\d.]+)', plan_text)
        if match:
            return (float(match.group(1)), float(match.group(2)))
        return (0, 0)

    def suggest_optimizations(self, query: str, plan: Dict) -> List[str]:
        """å»ºè®®ä¼˜åŒ–æ–¹æ¡ˆ"""
        suggestions = []

        if plan['has_seq_scan']:
            suggestions.append("âš ï¸ å‘ç°é¡ºåºæ‰«æï¼Œè€ƒè™‘æ·»åŠ ç´¢å¼•")

        if plan['execution_time_ms'] > 1000:
            suggestions.append("âš ï¸ æ‰§è¡Œæ—¶é—´>1ç§’ï¼Œè€ƒè™‘ä¼˜åŒ–æŸ¥è¯¢æˆ–æ·»åŠ ç´¢å¼•")

        if 'Nested Loop' in plan['plan'] and plan['execution_time_ms'] > 500:
            suggestions.append("âš ï¸ åµŒå¥—å¾ªç¯è€—æ—¶ï¼Œè€ƒè™‘ä½¿ç”¨Hash Joinæˆ–Merge Join")

        return suggestions

# ä½¿ç”¨ç¤ºä¾‹
conn = psycopg2.connect("dbname=mydb user=postgres")
analyzer = SlowQueryAnalyzer(conn)

# è·å–æ…¢æŸ¥è¯¢
slow_queries = analyzer.get_slow_queries(threshold_ms=1000)

for query_info in slow_queries:
    print(f"æ…¢æŸ¥è¯¢: {query_info['mean_time_ms']:.2f}ms")
    print(f"æŸ¥è¯¢: {query_info['query'][:200]}...")

    # åˆ†ææ‰§è¡Œè®¡åˆ’
    plan = analyzer.analyze_query_plan(query_info['query'])

    # è·å–ä¼˜åŒ–å»ºè®®
    suggestions = analyzer.suggest_optimizations(query_info['query'], plan)
    for suggestion in suggestions:
        print(f"  {suggestion}")
```

### 3.2 æ‰§è¡Œè®¡åˆ’ä¼˜åŒ–

**å¸¸è§é—®é¢˜ä¸ä¼˜åŒ–**:

**é—®é¢˜1: é¡ºåºæ‰«æ**:

```sql
-- é—®é¢˜æŸ¥è¯¢
EXPLAIN ANALYZE
SELECT * FROM orders WHERE status = 'pending';

-- æ‰§è¡Œè®¡åˆ’:
-- Seq Scan on orders (cost=0.00..12500.00 rows=100000 width=100)
--   Filter: (status = 'pending'::text)
-- Planning Time: 0.1 ms
-- Execution Time: 850.23 ms  âš ï¸ æ…¢

-- ä¼˜åŒ–: æ·»åŠ ç´¢å¼•
CREATE INDEX idx_orders_status ON orders(status);

-- ä¼˜åŒ–å:
-- Index Scan using idx_orders_status on orders (cost=0.42..2500.00 rows=100000)
--   Index Cond: (status = 'pending'::text)
-- Execution Time: 45.23 ms  âœ“ å¿«18å€
```

**é—®é¢˜2: åµŒå¥—å¾ªç¯æ€§èƒ½å·®**:

```sql
-- é—®é¢˜æŸ¥è¯¢
EXPLAIN ANALYZE
SELECT o.*, u.name
FROM orders o
JOIN users u ON o.user_id = u.id
WHERE o.created_at > '2025-01-01';

-- æ‰§è¡Œè®¡åˆ’:
-- Nested Loop (cost=0.85..125000.00 rows=500000 width=200)
--   -> Seq Scan on orders (cost=0.00..25000.00 rows=500000)
--   -> Index Scan using users_pkey on users (cost=0.42..0.20 rows=1)
-- Execution Time: 12500.23 ms  âš ï¸ æ…¢

-- ä¼˜åŒ–: æ·»åŠ å¤åˆç´¢å¼•
CREATE INDEX idx_orders_user_date ON orders(user_id, created_at);

-- ä¼˜åŒ–å:
-- Hash Join (cost=15000.00..35000.00 rows=500000)
--   Hash Cond: (o.user_id = u.id)
--   -> Index Scan using idx_orders_user_date on orders
--   -> Hash (cost=5000.00..5000.00 rows=100000)
-- Execution Time: 850.23 ms  âœ“ å¿«14å€
```

**é—®é¢˜3: æ’åºæ“ä½œæ…¢**:

```sql
-- é—®é¢˜æŸ¥è¯¢
EXPLAIN ANALYZE
SELECT * FROM orders
ORDER BY created_at DESC
LIMIT 100;

-- æ‰§è¡Œè®¡åˆ’:
-- Limit (cost=25000.00..25000.00 rows=100)
--   -> Sort (cost=25000.00..27500.00 rows=1000000)
--       Sort Key: created_at DESC
--       -> Seq Scan on orders
-- Execution Time: 1250.23 ms  âš ï¸ æ…¢

-- ä¼˜åŒ–: æ·»åŠ ç´¢å¼•æ”¯æŒæ’åº
CREATE INDEX idx_orders_created_at_desc ON orders(created_at DESC);

-- ä¼˜åŒ–å:
-- Limit (cost=0.42..25.00 rows=100)
--   -> Index Scan using idx_orders_created_at_desc on orders
-- Execution Time: 2.45 ms  âœ“ å¿«500å€
```

### 3.3 ç´¢å¼•ä¼˜åŒ–ç­–ç•¥

**ç´¢å¼•é€‰æ‹©å†³ç­–æ ‘**:

```python
class IndexOptimizer:
    """ç´¢å¼•ä¼˜åŒ–å™¨"""

    def suggest_indexes(self, table_name: str, conn) -> List[Dict]:
        """å»ºè®®ç´¢å¼•"""
        suggestions = []

        # 1. æ£€æŸ¥é¡ºåºæ‰«æ
        cur = conn.cursor()
        cur.execute("""
            SELECT
                schemaname,
                tablename,
                seq_scan,
                seq_tup_read,
                idx_scan
            FROM pg_stat_user_tables
            WHERE tablename = %s
        """, (table_name,))

        row = cur.fetchone()
        if row and row[2] > row[4] * 10:  # é¡ºåºæ‰«æ > ç´¢å¼•æ‰«æÃ—10
            suggestions.append({
                'type': 'missing_index',
                'reason': f'é¡ºåºæ‰«æè¿‡å¤š ({row[2]} vs {row[4]} ç´¢å¼•æ‰«æ)',
                'action': 'åˆ†æWHERE/JOINæ¡ä»¶ï¼Œæ·»åŠ ç´¢å¼•'
            })

        # 2. æ£€æŸ¥æœªä½¿ç”¨çš„ç´¢å¼•
        cur.execute("""
            SELECT
                indexrelname,
                idx_scan
            FROM pg_stat_user_indexes
            WHERE schemaname = 'public'
              AND tablename = %s
              AND idx_scan = 0
        """, (table_name,))

        unused_indexes = cur.fetchall()
        if unused_indexes:
            suggestions.append({
                'type': 'unused_index',
                'reason': f'å‘ç°{len(unused_indexes)}ä¸ªæœªä½¿ç”¨ç´¢å¼•',
                'action': f'è€ƒè™‘åˆ é™¤: {[idx[0] for idx in unused_indexes]}'
            })

        # 3. æ£€æŸ¥é‡å¤ç´¢å¼•
        cur.execute("""
            SELECT
                a.indexrelname AS index1,
                b.indexrelname AS index2
            FROM pg_indexes a
            JOIN pg_indexes b ON a.tablename = b.tablename
            WHERE a.indexrelname < b.indexrelname
              AND a.indexdef = b.indexdef
        """)

        duplicate_indexes = cur.fetchall()
        if duplicate_indexes:
            suggestions.append({
                'type': 'duplicate_index',
                'reason': f'å‘ç°{len(duplicate_indexes)}å¯¹é‡å¤ç´¢å¼•',
                'action': 'åˆ é™¤é‡å¤ç´¢å¼•'
            })

        cur.close()
        return suggestions
```

---

## å››ã€é…ç½®å‚æ•°è°ƒä¼˜

### 4.1 å†…å­˜å‚æ•°

**è°ƒä¼˜å…¬å¼**:

```python
def calculate_memory_params(total_ram_gb: int, max_connections: int) -> Dict:
    """è®¡ç®—å†…å­˜å‚æ•°"""
    shared_buffers_gb = max(1, int(total_ram_gb * 0.25))  # 25% RAM
    shared_buffers_gb = min(shared_buffers_gb, 32)  # ä¸è¶…è¿‡32GB

    # work_memè®¡ç®—
    available_ram = total_ram_gb - shared_buffers_gb
    work_mem_mb = int((available_ram * 1024) / (max_connections * 5))
    work_mem_mb = max(64, min(work_mem_mb, 512))  # é™åˆ¶åœ¨64-512MB

    effective_cache_size_gb = int(total_ram_gb * 0.75)

    maintenance_work_mem_gb = max(1, int(total_ram_gb * 0.05))
    maintenance_work_mem_gb = min(maintenance_work_mem_gb, 4)  # ä¸è¶…è¿‡4GB

    return {
        'shared_buffers': f'{shared_buffers_gb}GB',
        'work_mem': f'{work_mem_mb}MB',
        'effective_cache_size': f'{effective_cache_size_gb}GB',
        'maintenance_work_mem': f'{maintenance_work_mem_gb}GB',
    }

# ç¤ºä¾‹: 64GB RAM, 200è¿æ¥
params = calculate_memory_params(64, 200)
print(params)
# è¾“å‡º:
# {
#   'shared_buffers': '16GB',
#   'work_mem': '153MB',
#   'effective_cache_size': '48GB',
#   'maintenance_work_mem': '3GB'
# }
```

### 4.2 WALå‚æ•°

**è°ƒä¼˜ç­–ç•¥**:

```ini
# é«˜å†™å…¥åœºæ™¯
wal_buffers = 16MB                    # é»˜è®¤-1 (è‡ªåŠ¨)
checkpoint_timeout = 15min            # é»˜è®¤5min
checkpoint_completion_target = 0.9   # é»˜è®¤0.9
max_wal_size = 4GB                    # é»˜è®¤1GB
min_wal_size = 1GB                    # é»˜è®¤80MB

# å¼‚æ­¥æäº¤ï¼ˆæ€§èƒ½ä¼˜å…ˆï¼Œé£é™©ï¼‰
synchronous_commit = off              # é»˜è®¤on
wal_writer_delay = 200ms              # é»˜è®¤200ms
```

**æƒè¡¡åˆ†æ**:

| é…ç½® | æ€§èƒ½ | æ•°æ®å®‰å…¨ | é€‚ç”¨åœºæ™¯ |
|-----|------|---------|---------|
| `synchronous_commit = on` | åŸºå‡† | æœ€é«˜ | é‡‘è/å…³é”®æ•°æ® |
| `synchronous_commit = off` | +30% | ä¸­ç­‰ | æ—¥å¿—/éå…³é”® |
| `synchronous_commit = remote_write` | +15% | é«˜ | ä¸»ä»å¤åˆ¶ |

### 4.3 è¿æ¥å‚æ•°

**è°ƒä¼˜å…¬å¼** (Little's Law):

```python
def calculate_connection_pool_size(
    target_tps: int,
    avg_latency_ms: float,
    safety_factor: float = 1.5
) -> Dict:
    """è®¡ç®—è¿æ¥æ± å¤§å°"""
    # Little's Law: N = Î» Ã— W
    base_connections = int(target_tps * avg_latency_ms / 1000)

    min_size = max(5, int(base_connections * 0.5))
    max_size = int(base_connections * safety_factor)
    initial_size = base_connections

    return {
        'min': min_size,
        'max': max_size,
        'initial': initial_size,
        'formula': f'N = {target_tps} TPS Ã— {avg_latency_ms}ms = {base_connections}'
    }

# ç¤ºä¾‹: ç›®æ ‡10K TPS, å¹³å‡å»¶è¿Ÿ10ms
pool_config = calculate_connection_pool_size(10000, 10)
print(pool_config)
# è¾“å‡º:
# {
#   'min': 50,
#   'max': 150,
#   'initial': 100,
#   'formula': 'N = 10000 TPS Ã— 10ms = 100'
# }
```

### 4.4 VACUUMå‚æ•°

**è°ƒä¼˜ç­–ç•¥**:

```sql
-- çƒ­è¡¨ï¼ˆé¢‘ç¹æ›´æ–°ï¼‰
ALTER TABLE hot_table SET (
    autovacuum_vacuum_scale_factor = 0.05,  -- é»˜è®¤0.2 (5%è§¦å‘)
    autovacuum_vacuum_threshold = 100,       -- é»˜è®¤50
    autovacuum_vacuum_cost_delay = 0,        -- é»˜è®¤20ms (æ— å»¶è¿Ÿ)
    fillfactor = 70                          -- é»˜è®¤100 (é¢„ç•™30%ç©ºé—´)
);

-- å¤§è¡¨ï¼ˆæ›´æ–°å°‘ï¼‰
ALTER TABLE large_table SET (
    autovacuum_vacuum_scale_factor = 0.1,
    autovacuum_vacuum_threshold = 1000,
    autovacuum_vacuum_cost_delay = 10
);
```

---

## äº”ã€æ¶æ„å±‚é¢ä¼˜åŒ–

### 5.1 è¯»å†™åˆ†ç¦»

**æ¶æ„è®¾è®¡**:

```python
class ReadWriteRouter:
    """è¯»å†™åˆ†ç¦»è·¯ç”±"""

    def __init__(self, primary_conn, replica_conns):
        self.primary = primary_conn
        self.replicas = replica_conns
        self.replica_index = 0

    def get_connection(self, query_type: str):
        """æ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©è¿æ¥"""
        if query_type in ['SELECT', 'WITH']:
            # è¯»æ“ä½œ: è½®è¯¢ä»åº“
            conn = self.replicas[self.replica_index]
            self.replica_index = (self.replica_index + 1) % len(self.replicas)
            return conn
        else:
            # å†™æ“ä½œ: ä¸»åº“
            return self.primary

    def execute(self, query: str, params: tuple = None):
        """æ‰§è¡ŒæŸ¥è¯¢ï¼ˆè‡ªåŠ¨è·¯ç”±ï¼‰"""
        query_type = query.strip().split()[0].upper()
        conn = self.get_connection(query_type)

        cur = conn.cursor()
        cur.execute(query, params)

        if query_type == 'SELECT':
            return cur.fetchall()
        else:
            conn.commit()
            return cur.rowcount

# ä½¿ç”¨ç¤ºä¾‹
router = ReadWriteRouter(primary_conn, [replica1, replica2, replica3])

# è¯»æ“ä½œè‡ªåŠ¨è·¯ç”±åˆ°ä»åº“
results = router.execute("SELECT * FROM orders WHERE user_id = %s", (123,))

# å†™æ“ä½œè‡ªåŠ¨è·¯ç”±åˆ°ä¸»åº“
router.execute("UPDATE orders SET status = 'paid' WHERE id = %s", (456,))
```

**æ€§èƒ½æå‡**:

| åœºæ™¯ | å•åº“TPS | è¯»å†™åˆ†ç¦»TPS | æå‡ |
|-----|--------|-----------|------|
| è¯»å¤šå†™å°‘ (9:1) | 10,000 | 25,000 | +150% |
| è¯»å†™å¹³è¡¡ (1:1) | 8,000 | 12,000 | +50% |
| å†™å¤šè¯»å°‘ (1:9) | 5,000 | 6,000 | +20% |

### 5.2 åˆ†åŒºè¡¨è®¾è®¡

**åˆ†åŒºç­–ç•¥**:

```sql
-- æŒ‰æ—¶é—´åˆ†åŒºï¼ˆè®¢å•è¡¨ï¼‰
CREATE TABLE orders (
    id BIGSERIAL,
    user_id INT,
    amount DECIMAL(10,2),
    created_at TIMESTAMP NOT NULL,
    status VARCHAR(20)
) PARTITION BY RANGE (created_at);

-- åˆ›å»ºåˆ†åŒº
CREATE TABLE orders_2024_q1 PARTITION OF orders
    FOR VALUES FROM ('2024-01-01') TO ('2024-04-01');

CREATE TABLE orders_2024_q2 PARTITION OF orders
    FOR VALUES FROM ('2024-04-01') TO ('2024-07-01');

-- æŸ¥è¯¢è‡ªåŠ¨åˆ†åŒºè£å‰ª
EXPLAIN ANALYZE
SELECT * FROM orders
WHERE created_at >= '2024-01-01' AND created_at < '2024-04-01';

-- æ‰§è¡Œè®¡åˆ’:
-- Seq Scan on orders_2024_q1 (åªæ‰«æQ1åˆ†åŒºï¼Œè€Œéå…¨è¡¨)
-- Execution Time: 45ms (vs å…¨è¡¨æ‰«æ 8500ms)
```

**æ€§èƒ½æå‡**:

| æ“ä½œ | å•è¡¨ | åˆ†åŒºè¡¨ | æå‡ |
|-----|------|--------|------|
| æŸ¥è¯¢ï¼ˆå¸¦åˆ†åŒºé”®ï¼‰ | 8500ms | 45ms | 188Ã— |
| VACUUM | 2å°æ—¶ | 6åˆ†é’Ÿ/åˆ†åŒº | 20Ã— |
| ç´¢å¼•ç»´æŠ¤ | æ…¢ | å¿«ï¼ˆä»…å½“å‰åˆ†åŒºï¼‰ | 10Ã— |

### 5.3 ç¼“å­˜ç­–ç•¥

**å¤šçº§ç¼“å­˜æ¶æ„**:

```python
import redis
import psycopg2
from typing import Optional
import json
import hashlib

class MultiLevelCache:
    """å¤šçº§ç¼“å­˜ï¼ˆL1: åº”ç”¨å†…å­˜, L2: Redis, L3: PostgreSQLï¼‰"""

    def __init__(self, db_conn, redis_client):
        self.db = db_conn
        self.redis = redis_client
        self.local_cache = {}  # L1: æœ¬åœ°ç¼“å­˜

    def get(self, key: str, ttl: int = 300) -> Optional[any]:
        """å¤šçº§ç¼“å­˜è¯»å–"""
        # L1: æœ¬åœ°ç¼“å­˜
        if key in self.local_cache:
            return self.local_cache[key]

        # L2: Redis
        redis_key = f"cache:{key}"
        cached = self.redis.get(redis_key)
        if cached:
            value = json.loads(cached)
            self.local_cache[key] = value  # å›å¡«L1
            return value

        # L3: æ•°æ®åº“
        value = self._query_from_db(key)
        if value:
            # å†™å…¥L2å’ŒL1
            self.redis.setex(redis_key, ttl, json.dumps(value))
            self.local_cache[key] = value

        return value

    def _query_from_db(self, key: str) -> Optional[any]:
        """ä»æ•°æ®åº“æŸ¥è¯¢"""
        cur = self.db.cursor()
        # å‡è®¾keyæ˜¯user_id
        cur.execute("SELECT * FROM users WHERE id = %s", (key,))
        row = cur.fetchone()
        cur.close()
        return dict(row) if row else None

    def invalidate(self, key: str):
        """å¤±æ•ˆç¼“å­˜"""
        # æ¸…é™¤L1
        self.local_cache.pop(key, None)
        # æ¸…é™¤L2
        self.redis.delete(f"cache:{key}")

# ä½¿ç”¨ç¤ºä¾‹
cache = MultiLevelCache(db_conn, redis_client)

# è¯»å–ï¼ˆè‡ªåŠ¨å¤šçº§ç¼“å­˜ï¼‰
user = cache.get("user:12345")

# æ›´æ–°åå¤±æ•ˆç¼“å­˜
db.execute("UPDATE users SET name = %s WHERE id = %s", ("New Name", 12345))
cache.invalidate("user:12345")
```

**æ€§èƒ½å¯¹æ¯”**:

| ç¼“å­˜çº§åˆ« | å»¶è¿Ÿ | å‘½ä¸­ç‡ | é€‚ç”¨åœºæ™¯ |
|---------|------|--------|---------|
| L1 (æœ¬åœ°) | 0.01ms | 80% | çƒ­ç‚¹æ•°æ® |
| L2 (Redis) | 0.5ms | 15% | æ¸©æ•°æ® |
| L3 (PostgreSQL) | 10ms | 5% | å†·æ•°æ® |

---

## å…­ã€å®Œæ•´è¯Šæ–­å·¥å…·

### 6.1 æ€§èƒ½è¯Šæ–­è„šæœ¬

```python
import psycopg2
from dataclasses import dataclass
from typing import List, Dict
import json

@dataclass
class PerformanceIssue:
    """æ€§èƒ½é—®é¢˜"""
    severity: str  # 'critical', 'warning', 'info'
    category: str  # 'cpu', 'memory', 'io', 'lock'
    description: str
    impact: str
    recommendation: str

class PerformanceDiagnosticTool:
    """æ€§èƒ½è¯Šæ–­å·¥å…·"""

    def __init__(self, conn):
        self.conn = conn
        self.issues = []

    def run_full_diagnosis(self) -> List[PerformanceIssue]:
        """è¿è¡Œå®Œæ•´è¯Šæ–­"""
        self.issues = []

        # 1. CPUè¯Šæ–­
        self._check_cpu_bottleneck()

        # 2. å†…å­˜è¯Šæ–­
        self._check_memory_bottleneck()

        # 3. I/Oè¯Šæ–­
        self._check_io_bottleneck()

        # 4. é”è¯Šæ–­
        self._check_lock_contention()

        # 5. æ…¢æŸ¥è¯¢è¯Šæ–­
        self._check_slow_queries()

        # 6. è¡¨è†¨èƒ€è¯Šæ–­
        self._check_table_bloat()

        return self.issues

    def _check_cpu_bottleneck(self):
        """æ£€æŸ¥CPUç“¶é¢ˆ"""
        cur = self.conn.cursor()
        cur.execute("""
            SELECT COUNT(*) FROM pg_stat_activity
            WHERE state = 'active'
              AND EXTRACT(EPOCH FROM (NOW() - query_start)) > 5
        """)
        long_queries = cur.fetchone()[0]

        if long_queries > 10:
            self.issues.append(PerformanceIssue(
                severity='warning',
                category='cpu',
                description=f'å‘ç°{long_queries}ä¸ªé•¿æ—¶é—´è¿è¡Œçš„æŸ¥è¯¢',
                impact='CPUä½¿ç”¨ç‡å¯èƒ½è¿‡é«˜',
                recommendation='åˆ†ææ…¢æŸ¥è¯¢ï¼Œæ·»åŠ ç´¢å¼•æˆ–ä¼˜åŒ–æŸ¥è¯¢'
            ))

        cur.close()

    def _check_memory_bottleneck(self):
        """æ£€æŸ¥å†…å­˜ç“¶é¢ˆ"""
        cur = self.conn.cursor()
        cur.execute("""
            SELECT
                COUNT(*) AS temp_file_queries,
                SUM(temp_bytes) AS total_temp_bytes
            FROM pg_stat_activity
            WHERE temp_files > 0
        """)
        row = cur.fetchone()

        if row[0] > 0:
            total_temp_gb = row[1] / (1024**3)
            self.issues.append(PerformanceIssue(
                severity='warning',
                category='memory',
                description=f'{row[0]}ä¸ªæŸ¥è¯¢ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ï¼Œæ€»è®¡{total_temp_gb:.2f}GB',
                impact='work_memå¯èƒ½ä¸è¶³ï¼Œå¯¼è‡´ç£ç›˜I/O',
                recommendation='å¢åŠ work_memæˆ–ä¼˜åŒ–æŸ¥è¯¢å‡å°‘æ’åº/å“ˆå¸Œ'
            ))

        cur.close()

    def _check_io_bottleneck(self):
        """æ£€æŸ¥I/Oç“¶é¢ˆ"""
        cur = self.conn.cursor()
        cur.execute("""
            SELECT
                schemaname,
                tablename,
                seq_scan,
                idx_scan,
                heap_blks_read,
                heap_blks_hit
            FROM pg_statio_user_tables
            WHERE seq_scan > idx_scan * 10
              AND heap_blks_read > 10000
            ORDER BY heap_blks_read DESC
            LIMIT 5
        """)

        problematic_tables = cur.fetchall()
        if problematic_tables:
            for table in problematic_tables:
                self.issues.append(PerformanceIssue(
                    severity='warning',
                    category='io',
                    description=f'è¡¨{table[1]}é¡ºåºæ‰«æè¿‡å¤š ({table[2]}æ¬¡)',
                    impact='å¤§é‡ç£ç›˜I/Oï¼ŒæŸ¥è¯¢æ…¢',
                    recommendation=f'ä¸ºè¡¨{table[1]}æ·»åŠ ç´¢å¼•'
                ))

        cur.close()

    def _check_lock_contention(self):
        """æ£€æŸ¥é”ç«äº‰"""
        cur = self.conn.cursor()
        cur.execute("""
            SELECT COUNT(*) FROM pg_locks
            WHERE NOT granted
        """)
        waiting_locks = cur.fetchone()[0]

        if waiting_locks > 5:
            self.issues.append(PerformanceIssue(
                severity='critical',
                category='lock',
                description=f'å‘ç°{waiting_locks}ä¸ªé”ç­‰å¾…',
                impact='äº‹åŠ¡é˜»å¡ï¼ŒTPSä¸‹é™',
                recommendation='æ£€æŸ¥é•¿äº‹åŠ¡ï¼Œç»ˆæ­¢é˜»å¡äº‹åŠ¡ï¼Œä¼˜åŒ–é”ç²’åº¦'
            ))

        cur.close()

    def _check_slow_queries(self):
        """æ£€æŸ¥æ…¢æŸ¥è¯¢"""
        cur = self.conn.cursor()
        cur.execute("""
            SELECT COUNT(*) FROM pg_stat_statements
            WHERE mean_exec_time > 1000
        """)
        slow_count = cur.fetchone()[0]

        if slow_count > 0:
            self.issues.append(PerformanceIssue(
                severity='warning',
                category='query',
                description=f'å‘ç°{slow_count}ä¸ªå¹³å‡æ‰§è¡Œæ—¶é—´>1ç§’çš„æŸ¥è¯¢',
                impact='æ•´ä½“æ€§èƒ½ä¸‹é™',
                recommendation='ä½¿ç”¨EXPLAIN ANALYZEåˆ†ææ…¢æŸ¥è¯¢ï¼Œæ·»åŠ ç´¢å¼•æˆ–ä¼˜åŒ–'
            ))

        cur.close()

    def _check_table_bloat(self):
        """æ£€æŸ¥è¡¨è†¨èƒ€"""
        cur = self.conn.cursor()
        cur.execute("""
            SELECT
                schemaname,
                tablename,
                n_dead_tup,
                n_live_tup,
                ROUND(100.0 * n_dead_tup / NULLIF(n_live_tup + n_dead_tup, 0), 2) AS dead_ratio
            FROM pg_stat_user_tables
            WHERE n_dead_tup > 10000
              AND 100.0 * n_dead_tup / NULLIF(n_live_tup + n_dead_tup, 0) > 30
            ORDER BY dead_ratio DESC
            LIMIT 5
        """)

        bloated_tables = cur.fetchall()
        if bloated_tables:
            for table in bloated_tables:
                self.issues.append(PerformanceIssue(
                    severity='warning',
                    category='storage',
                    description=f'è¡¨{table[1]}è†¨èƒ€ä¸¥é‡ (æ­»å…ƒç»„å æ¯”{table[4]}%)',
                    impact='æŸ¥è¯¢æ€§èƒ½ä¸‹é™ï¼Œå­˜å‚¨æµªè´¹',
                    recommendation=f'æ‰§è¡ŒVACUUM ANALYZE {table[0]}.{table[1]}'
                ))

        cur.close()

    def generate_report(self) -> str:
        """ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
        report = ["# æ€§èƒ½è¯Šæ–­æŠ¥å‘Š\n"]
        report.append(f"**è¯Šæ–­æ—¶é—´**: {__import__('datetime').datetime.now()}\n")
        report.append(f"**å‘ç°é—®é¢˜æ•°**: {len(self.issues)}\n\n")

        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
        critical = [i for i in self.issues if i.severity == 'critical']
        warning = [i for i in self.issues if i.severity == 'warning']
        info = [i for i in self.issues if i.severity == 'info']

        if critical:
            report.append("## ğŸ”´ ä¸¥é‡é—®é¢˜\n\n")
            for issue in critical:
                report.append(f"### {issue.category.upper()}: {issue.description}\n")
                report.append(f"- **å½±å“**: {issue.impact}\n")
                report.append(f"- **å»ºè®®**: {issue.recommendation}\n\n")

        if warning:
            report.append("## ğŸŸ¡ è­¦å‘Š\n\n")
            for issue in warning:
                report.append(f"### {issue.category.upper()}: {issue.description}\n")
                report.append(f"- **å½±å“**: {issue.impact}\n")
                report.append(f"- **å»ºè®®**: {issue.recommendation}\n\n")

        return "\n".join(report)

# ä½¿ç”¨ç¤ºä¾‹
conn = psycopg2.connect("dbname=mydb user=postgres")
diagnostic = PerformanceDiagnosticTool(conn)

issues = diagnostic.run_full_diagnosis()
report = diagnostic.generate_report()

print(report)
# ä¿å­˜æŠ¥å‘Š
with open('performance_diagnosis_report.md', 'w') as f:
    f.write(report)
```

### 6.2 è‡ªåŠ¨åŒ–è°ƒä¼˜å·¥å…·

```python
class AutoTuner:
    """è‡ªåŠ¨è°ƒä¼˜å·¥å…·"""

    def __init__(self, conn):
        self.conn = conn
        self.current_config = {}
        self.performance_history = []

    def analyze_and_tune(self):
        """åˆ†æå¹¶è‡ªåŠ¨è°ƒä¼˜"""
        # 1. æ”¶é›†å½“å‰æ€§èƒ½æŒ‡æ ‡
        current_metrics = self._collect_metrics()

        # 2. è¯†åˆ«ç“¶é¢ˆ
        bottleneck = self._identify_bottleneck(current_metrics)

        # 3. ç”Ÿæˆè°ƒä¼˜å»ºè®®
        recommendations = self._generate_recommendations(bottleneck)

        # 4. åº”ç”¨è°ƒä¼˜ï¼ˆå¯é€‰ï¼Œè°¨æ…ä½¿ç”¨ï¼‰
        # self._apply_tuning(recommendations)

        return recommendations

    def _collect_metrics(self) -> Dict:
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        cur = self.conn.cursor()

        # TPS
        cur.execute("SELECT xact_commit FROM pg_stat_database WHERE datname = current_database()")
        tps = cur.fetchone()[0]

        # ç¼“å­˜å‘½ä¸­ç‡
        cur.execute("""
            SELECT
                ROUND(100.0 * sum(heap_blks_hit) / NULLIF(sum(heap_blks_hit + heap_blks_read), 0), 2)
            FROM pg_statio_user_tables
        """)
        cache_hit_ratio = cur.fetchone()[0] or 0

        # é”ç­‰å¾…
        cur.execute("SELECT COUNT(*) FROM pg_locks WHERE NOT granted")
        lock_waits = cur.fetchone()[0]

        cur.close()

        return {
            'tps': tps,
            'cache_hit_ratio': cache_hit_ratio,
            'lock_waits': lock_waits,
        }

    def _identify_bottleneck(self, metrics: Dict) -> str:
        """è¯†åˆ«ç“¶é¢ˆ"""
        if metrics['cache_hit_ratio'] < 90:
            return 'io'
        elif metrics['lock_waits'] > 10:
            return 'lock'
        else:
            return 'cpu'

    def _generate_recommendations(self, bottleneck: str) -> List[str]:
        """ç”Ÿæˆè°ƒä¼˜å»ºè®®"""
        recommendations = []

        if bottleneck == 'io':
            recommendations.append("å¢åŠ shared_buffersï¼ˆæé«˜ç¼“å­˜å‘½ä¸­ç‡ï¼‰")
            recommendations.append("æ£€æŸ¥å¹¶æ·»åŠ ç¼ºå¤±çš„ç´¢å¼•")
            recommendations.append("è€ƒè™‘ä½¿ç”¨SSDï¼ˆé™ä½random_page_costï¼‰")

        elif bottleneck == 'lock':
            recommendations.append("æ£€æŸ¥å¹¶ç»ˆæ­¢é•¿äº‹åŠ¡")
            recommendations.append("ä¼˜åŒ–é”ç²’åº¦ï¼ˆä½¿ç”¨è¡Œé”è€Œéè¡¨é”ï¼‰")
            recommendations.append("è€ƒè™‘é™ä½éš”ç¦»çº§åˆ«ï¼ˆå¦‚æœä¸šåŠ¡å…è®¸ï¼‰")

        elif bottleneck == 'cpu':
            recommendations.append("ä¼˜åŒ–æ…¢æŸ¥è¯¢ï¼ˆæ·»åŠ ç´¢å¼•ï¼‰")
            recommendations.append("è€ƒè™‘å¢åŠ CPUæ ¸å¿ƒæ•°")
            recommendations.append("ä½¿ç”¨è¿æ¥æ± å‡å°‘ä¸Šä¸‹æ–‡åˆ‡æ¢")

        return recommendations
```

---

## ä¸ƒã€å®é™…ä¼˜åŒ–æ¡ˆä¾‹

### æ¡ˆä¾‹1: ç”µå•†ç³»ç»ŸTPSä»5Kæå‡åˆ°20K

**åˆå§‹çŠ¶æ€**:

- TPS: 5,000
- P99å»¶è¿Ÿ: 200ms
- CPUä½¿ç”¨ç‡: 95%
- é—®é¢˜: é«˜CPUä½¿ç”¨ç‡ï¼ŒæŸ¥è¯¢æ…¢

**è¯Šæ–­è¿‡ç¨‹**:

```python
# 1. å‘ç°æ…¢æŸ¥è¯¢
analyzer = SlowQueryAnalyzer(conn)
slow_queries = analyzer.get_slow_queries(threshold_ms=500)

# å‘ç°TOP3æ…¢æŸ¥è¯¢:
# 1. SELECT * FROM orders WHERE user_id = ? AND status = ?  (850ms)
# 2. SELECT * FROM order_items WHERE order_id = ?  (450ms)
# 3. UPDATE inventory SET stock = stock - 1 WHERE product_id = ?  (320ms)
```

**ä¼˜åŒ–æ–¹æ¡ˆ**:

```sql
-- ä¼˜åŒ–1: æ·»åŠ å¤åˆç´¢å¼•
CREATE INDEX idx_orders_user_status ON orders(user_id, status);
-- æ•ˆæœ: æŸ¥è¯¢1ä»850msé™è‡³15ms (-98%)

-- ä¼˜åŒ–2: æ·»åŠ å¤–é”®ç´¢å¼•
CREATE INDEX idx_order_items_order_id ON order_items(order_id);
-- æ•ˆæœ: æŸ¥è¯¢2ä»450msé™è‡³8ms (-98%)

-- ä¼˜åŒ–3: åº“å­˜è¡¨è¡Œåˆ†æ•£
CREATE TABLE inventory_shards (
    product_id INT,
    shard_id INT,  -- 0-9
    stock INT,
    PRIMARY KEY (product_id, shard_id)
);
-- æ•ˆæœ: æŸ¥è¯¢3ä»320msé™è‡³25ms (-92%)
```

**ä¼˜åŒ–å**:

- TPS: 20,000 (+300%)
- P99å»¶è¿Ÿ: 45ms (-77%)
- CPUä½¿ç”¨ç‡: 65% (-32%)

### æ¡ˆä¾‹2: æŠ¥è¡¨æŸ¥è¯¢ä»30ç§’é™è‡³2ç§’

**é—®é¢˜**: å¤æ‚æŠ¥è¡¨æŸ¥è¯¢è€—æ—¶30ç§’

**æŸ¥è¯¢**:

```sql
-- åŸå§‹æŸ¥è¯¢ï¼ˆæ…¢ï¼‰
SELECT
    u.name,
    COUNT(o.id) AS order_count,
    SUM(o.amount) AS total_amount,
    AVG(o.amount) AS avg_amount
FROM users u
LEFT JOIN orders o ON u.id = o.user_id
WHERE u.created_at >= '2024-01-01'
  AND o.created_at >= '2024-01-01'
GROUP BY u.id, u.name
ORDER BY total_amount DESC
LIMIT 100;
```

**ä¼˜åŒ–æ–¹æ¡ˆ**:

```sql
-- æ–¹æ¡ˆ1: åˆ›å»ºç‰©åŒ–è§†å›¾ï¼ˆé¢„è®¡ç®—ï¼‰
CREATE MATERIALIZED VIEW user_order_summary AS
SELECT
    u.id AS user_id,
    u.name,
    COUNT(o.id) AS order_count,
    SUM(o.amount) AS total_amount,
    AVG(o.amount) AS avg_amount
FROM users u
LEFT JOIN orders o ON u.id = o.user_id
WHERE u.created_at >= '2024-01-01'
  AND o.created_at >= '2024-01-01'
GROUP BY u.id, u.name;

CREATE INDEX idx_summary_total_amount ON user_order_summary(total_amount DESC);

-- æŸ¥è¯¢æ”¹ä¸ºä»ç‰©åŒ–è§†å›¾è¯»å–
SELECT * FROM user_order_summary
ORDER BY total_amount DESC
LIMIT 100;
-- æ•ˆæœ: ä»30ç§’é™è‡³0.5ç§’ (-98%)

-- å®šæœŸåˆ·æ–°ï¼ˆæ¯å°æ—¶ï¼‰
REFRESH MATERIALIZED VIEW CONCURRENTLY user_order_summary;
```

### æ¡ˆä¾‹3: é«˜å¹¶å‘å†™å…¥ä¼˜åŒ–

**é—®é¢˜**: é«˜å¹¶å‘UPDATEå¯¼è‡´é”ç«äº‰

**åœºæ™¯**: 1000å¹¶å‘æ›´æ–°åŒä¸€è¡¨

**ä¼˜åŒ–æ–¹æ¡ˆ**:

```sql
-- æ–¹æ¡ˆ1: é™ä½fillfactorï¼ˆé¢„ç•™HOTç©ºé—´ï¼‰
ALTER TABLE hot_table SET (fillfactor = 70);

-- æ–¹æ¡ˆ2: åˆ†åŒºè¡¨ï¼ˆåˆ†æ•£å†™å…¥ï¼‰
CREATE TABLE hot_table (
    id BIGSERIAL,
    data TEXT,
    created_at TIMESTAMP
) PARTITION BY HASH (id);

CREATE TABLE hot_table_0 PARTITION OF hot_table
    FOR VALUES WITH (MODULUS 4, REMAINDER 0);
CREATE TABLE hot_table_1 PARTITION OF hot_table
    FOR VALUES WITH (MODULUS 4, REMAINDER 1);
-- ... å…¶ä»–åˆ†åŒº

-- æ–¹æ¡ˆ3: æ‰¹é‡æäº¤ï¼ˆå‡å°‘é”æŒæœ‰æ—¶é—´ï¼‰
-- Pythonä»£ç 
def batch_update(items):
    with conn.cursor() as cur:
        # ä½¿ç”¨COPYæ‰¹é‡æ’å…¥
        from io import StringIO
        buffer = StringIO()
        for item in items:
            buffer.write(f"{item.id}\t{item.data}\n")
        buffer.seek(0)
        cur.copy_from(buffer, 'hot_table', columns=('id', 'data'))
    conn.commit()
```

**æ•ˆæœ**: TPSä»2,000æå‡åˆ°15,000 (+650%)

---

## å…«ã€åä¾‹ä¸é”™è¯¯ä¼˜åŒ–

### åä¾‹1: è¿‡åº¦ä¼˜åŒ–å¯¼è‡´é—®é¢˜

**é”™è¯¯åšæ³•**:

```sql
-- é”™è¯¯: ä¸ºæ¯ä¸ªåˆ—éƒ½åˆ›å»ºç´¢å¼•
CREATE INDEX idx_orders_id ON orders(id);
CREATE INDEX idx_orders_user_id ON orders(user_id);
CREATE INDEX idx_orders_status ON orders(status);
CREATE INDEX idx_orders_amount ON orders(amount);
CREATE INDEX idx_orders_created_at ON orders(created_at);
CREATE INDEX idx_orders_updated_at ON orders(updated_at);
-- ... 10ä¸ªç´¢å¼•

-- é—®é¢˜:
-- 1. INSERTæ…¢10å€ï¼ˆæ¯ä¸ªç´¢å¼•éƒ½è¦æ›´æ–°ï¼‰
-- 2. å­˜å‚¨ç©ºé—´å¢åŠ 5å€
-- 3. VACUUMæ—¶é—´å¢åŠ 3å€
```

**æ­£ç¡®åšæ³•**:

```sql
-- æ­£ç¡®: åªåˆ›å»ºå¿…è¦çš„ç´¢å¼•
-- 1. ä¸»é”®ç´¢å¼•ï¼ˆè‡ªåŠ¨ï¼‰
-- 2. å¤–é”®ç´¢å¼•ï¼ˆJOINéœ€è¦ï¼‰
CREATE INDEX idx_orders_user_id ON orders(user_id);

-- 3. æŸ¥è¯¢æ¡ä»¶ç´¢å¼•ï¼ˆWHEREå¸¸ç”¨ï¼‰
CREATE INDEX idx_orders_status ON orders(status);

-- 4. å¤åˆç´¢å¼•ï¼ˆè¦†ç›–å¤šä¸ªæŸ¥è¯¢ï¼‰
CREATE INDEX idx_orders_user_status ON orders(user_id, status);

-- 5. æ’åºç´¢å¼•ï¼ˆORDER BYéœ€è¦ï¼‰
CREATE INDEX idx_orders_created_at ON orders(created_at DESC);
```

### åä¾‹2: å¿½ç•¥æ ¹æœ¬åŸå› 

**é”™è¯¯åšæ³•**:

```sql
-- é—®é¢˜: æŸ¥è¯¢æ…¢
SELECT * FROM orders WHERE status = 'pending';

-- é”™è¯¯ä¼˜åŒ–: å¢åŠ work_memï¼ˆæ— æ•ˆï¼‰
ALTER SYSTEM SET work_mem = '1GB';

-- é—®é¢˜ä¾ç„¶å­˜åœ¨: é¡ºåºæ‰«ææœªè§£å†³
```

**æ­£ç¡®åšæ³•**:

```sql
-- æ­£ç¡®: æ·»åŠ ç´¢å¼•ï¼ˆè§£å†³æ ¹æœ¬åŸå› ï¼‰
CREATE INDEX idx_orders_status ON orders(status);

-- æŸ¥è¯¢ç«‹å³å˜å¿«
```

### åä¾‹3: æ€§èƒ½è°ƒä¼˜æ–¹æ³•ä¸ç³»ç»Ÿ

**é”™è¯¯è®¾è®¡**: æ€§èƒ½è°ƒä¼˜æ–¹æ³•ä¸ç³»ç»Ÿ

```text
é”™è¯¯åœºæ™¯:
â”œâ”€ è°ƒä¼˜: æ€§èƒ½è°ƒä¼˜
â”œâ”€ é—®é¢˜: è°ƒä¼˜æ–¹æ³•ä¸ç³»ç»Ÿï¼Œéšæ„ä¼˜åŒ–
â”œâ”€ ç»“æœ: æ•ˆç‡ä½ï¼Œé—®é¢˜æœªè§£å†³
â””â”€ æ•ˆç‡: è°ƒä¼˜æ—¶é—´å¢åŠ 2å€ âœ—

å®é™…æ¡ˆä¾‹:
â”œâ”€ ç³»ç»Ÿ: æŸç³»ç»Ÿæ€§èƒ½è°ƒä¼˜
â”œâ”€ é—®é¢˜: ä¸æŒ‰è°ƒä¼˜æµç¨‹ï¼Œéšæ„ä¼˜åŒ–
â”œâ”€ ç»“æœ: æ•ˆç‡ä½ï¼Œé—®é¢˜æœªè§£å†³
â””â”€ åæœ: æµªè´¹æ—¶é—´å’Œèµ„æº âœ—

æ­£ç¡®è®¾è®¡:
â”œâ”€ æ–¹æ¡ˆ: ç³»ç»ŸåŒ–è°ƒä¼˜æ–¹æ³•
â”œâ”€ å®ç°: æŒ‰ç…§è°ƒä¼˜æµç¨‹ï¼Œç³»ç»ŸåŒ–è¯Šæ–­å’Œä¼˜åŒ–
â””â”€ ç»“æœ: æ•ˆç‡é«˜ï¼Œé—®é¢˜è§£å†³ âœ“
```

### åä¾‹4: è°ƒä¼˜å¿½ç•¥ä¸šåŠ¡å½±å“

**é”™è¯¯è®¾è®¡**: è°ƒä¼˜å¿½ç•¥ä¸šåŠ¡å½±å“

```text
é”™è¯¯åœºæ™¯:
â”œâ”€ è°ƒä¼˜: æ€§èƒ½è°ƒä¼˜
â”œâ”€ é—®é¢˜: å¿½ç•¥ä¸šåŠ¡å½±å“ï¼Œç›²ç›®ä¼˜åŒ–
â”œâ”€ ç»“æœ: ä¸šåŠ¡å—å½±å“
â””â”€ åæœ: ä¸šåŠ¡æŸå¤± âœ—

å®é™…æ¡ˆä¾‹:
â”œâ”€ ç³»ç»Ÿ: æŸç³»ç»Ÿæ€§èƒ½è°ƒä¼˜
â”œâ”€ é—®é¢˜: ä¼˜åŒ–æ—¶å¿½ç•¥ä¸šåŠ¡å½±å“
â”œâ”€ ç»“æœ: ä¸šåŠ¡åŠŸèƒ½å—å½±å“
â””â”€ åæœ: ä¸šåŠ¡æŸå¤± âœ—

æ­£ç¡®è®¾è®¡:
â”œâ”€ æ–¹æ¡ˆ: è€ƒè™‘ä¸šåŠ¡å½±å“
â”œâ”€ å®ç°: ä¼˜åŒ–å‰è¯„ä¼°ä¸šåŠ¡å½±å“ï¼Œç°åº¦å‘å¸ƒ
â””â”€ ç»“æœ: ä¸šåŠ¡ä¸å—å½±å“ âœ“
```

### åä¾‹5: è°ƒä¼˜éªŒè¯ä¸è¶³

**é”™è¯¯è®¾è®¡**: è°ƒä¼˜åä¸éªŒè¯

```text
é”™è¯¯åœºæ™¯:
â”œâ”€ è°ƒä¼˜: æ€§èƒ½è°ƒä¼˜
â”œâ”€ é—®é¢˜: è°ƒä¼˜åä¸éªŒè¯
â”œâ”€ ç»“æœ: è°ƒä¼˜æ•ˆæœæœªçŸ¥
â””â”€ åæœ: å¯èƒ½æ— æ•ˆ âœ—

å®é™…æ¡ˆä¾‹:
â”œâ”€ ç³»ç»Ÿ: æŸç³»ç»Ÿæ€§èƒ½è°ƒä¼˜
â”œâ”€ é—®é¢˜: è°ƒä¼˜åæœªéªŒè¯
â”œâ”€ ç»“æœ: è°ƒä¼˜æ•ˆæœæœªçŸ¥
â””â”€ åæœ: å¯èƒ½æ— æ•ˆ âœ—

æ­£ç¡®è®¾è®¡:
â”œâ”€ æ–¹æ¡ˆ: è°ƒä¼˜åéªŒè¯
â”œâ”€ å®ç°: æ€§èƒ½æµ‹è¯•ã€å¯¹æ¯”ä¼˜åŒ–å‰å
â””â”€ ç»“æœ: éªŒè¯è°ƒä¼˜æ•ˆæœ âœ“
```

### åä¾‹6: è°ƒä¼˜ç¼ºä¹ç›‘æ§

**é”™è¯¯è®¾è®¡**: è°ƒä¼˜ç¼ºä¹ç›‘æ§

```text
é”™è¯¯åœºæ™¯:
â”œâ”€ è°ƒä¼˜: æ€§èƒ½è°ƒä¼˜
â”œâ”€ é—®é¢˜: è°ƒä¼˜åä¸ç›‘æ§
â”œâ”€ ç»“æœ: é—®é¢˜å¤å‘æœªè¢«å‘ç°
â””â”€ åæœ: æ€§èƒ½é—®é¢˜æŒç»­ âœ—

å®é™…æ¡ˆä¾‹:
â”œâ”€ ç³»ç»Ÿ: æŸç³»ç»Ÿæ€§èƒ½è°ƒä¼˜
â”œâ”€ é—®é¢˜: è°ƒä¼˜åæœªç›‘æ§
â”œâ”€ ç»“æœ: é—®é¢˜å¤å‘æœªè¢«å‘ç°
â””â”€ åæœ: æ€§èƒ½é—®é¢˜æŒç»­ âœ—

æ­£ç¡®è®¾è®¡:
â”œâ”€ æ–¹æ¡ˆ: è°ƒä¼˜åç›‘æ§
â”œâ”€ å®ç°: è®¾ç½®ç›‘æ§å‘Šè­¦ï¼ŒæŒç»­ç›‘æ§
â””â”€ ç»“æœ: åŠæ—¶å‘ç°é—®é¢˜ âœ“
```

---

## ä¹ã€è°ƒä¼˜æ£€æŸ¥æ¸…å•

**è¯Šæ–­é˜¶æ®µ**:

- [ ] æ”¶é›†ç³»ç»ŸæŒ‡æ ‡ï¼ˆCPU/å†…å­˜/IOï¼‰
- [ ] æ”¶é›†æ•°æ®åº“æŒ‡æ ‡ï¼ˆTPS/å»¶è¿Ÿ/é”ç­‰å¾…ï¼‰
- [ ] è¯†åˆ«æ…¢æŸ¥è¯¢ï¼ˆpg_stat_statementsï¼‰
- [ ] åˆ†ææ‰§è¡Œè®¡åˆ’ï¼ˆEXPLAIN ANALYZEï¼‰
- [ ] æ£€æŸ¥è¡¨è†¨èƒ€ç‡
- [ ] æ£€æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…å†µ

**ä¼˜åŒ–é˜¶æ®µ**:

- [ ] ä¼˜åŒ–æ…¢æŸ¥è¯¢ï¼ˆæ·»åŠ ç´¢å¼•/é‡å†™æŸ¥è¯¢ï¼‰
- [ ] è°ƒæ•´é…ç½®å‚æ•°ï¼ˆå†…å­˜/WAL/è¿æ¥ï¼‰
- [ ] ä¼˜åŒ–è¡¨è®¾è®¡ï¼ˆåˆ†åŒº/å¡«å……å› å­ï¼‰
- [ ] å®æ–½æ¶æ„ä¼˜åŒ–ï¼ˆè¯»å†™åˆ†ç¦»/ç¼“å­˜ï¼‰

**éªŒè¯é˜¶æ®µ**:

- [ ] æ€§èƒ½æµ‹è¯•ï¼ˆå¯¹æ¯”ä¼˜åŒ–å‰åï¼‰
- [ ] ç¨³å®šæ€§æµ‹è¯•ï¼ˆé•¿æ—¶é—´è¿è¡Œï¼‰
- [ ] ç›‘æ§å‘Šè­¦ï¼ˆè®¾ç½®é˜ˆå€¼ï¼‰
- [ ] å›æ»šé¢„æ¡ˆï¼ˆå‡†å¤‡å›æ»šæ–¹æ¡ˆï¼‰

---

## åã€æ€»ç»“

### 10.1 æ ¸å¿ƒæ–¹æ³•è®º

**è°ƒä¼˜ä¸‰æ­¥æ³•**:

1. **è¯Šæ–­**: æ•°æ®é©±åŠ¨ï¼Œé‡åŒ–ç“¶é¢ˆ
2. **ä¼˜åŒ–**: é’ˆå¯¹æ€§æ–¹æ¡ˆï¼ŒéªŒè¯æ•ˆæœ
3. **ç›‘æ§**: æŒç»­è§‚å¯Ÿï¼Œè¿­ä»£æ”¹è¿›

### 10.2 å…³é”®å·¥å…·

- **è¯Šæ–­å·¥å…·**: `SlowQueryAnalyzer`, `PerformanceDiagnosticTool`
- **ä¼˜åŒ–å·¥å…·**: `IndexOptimizer`, `AutoTuner`
- **ç›‘æ§å·¥å…·**: `PerformanceMonitor`

### 10.3 æœ€ä½³å®è·µ

1. **å…ˆè¯Šæ–­åä¼˜åŒ–**: é¿å…ç›²ç›®è°ƒä¼˜
2. **ä¸€æ¬¡æ”¹ä¸€ä¸ª**: ä¾¿äºéªŒè¯æ•ˆæœ
3. **é‡åŒ–éªŒè¯**: ç”¨æ•°æ®è¯æ˜ä¼˜åŒ–æ•ˆæœ
4. **æŒç»­ç›‘æ§**: å»ºç«‹æ€§èƒ½åŸºçº¿

---

---

## åä¸€ã€å®Œæ•´æ€§èƒ½è¯Šæ–­å·¥å…·å®ç°

### 11.1 è‡ªåŠ¨åŒ–æ€§èƒ½è¯Šæ–­è„šæœ¬

```python
import psycopg2
import psutil
from typing import Dict, List
from dataclasses import dataclass

@dataclass
class PerformanceDiagnostic:
    """æ€§èƒ½è¯Šæ–­ç»“æœ"""
    bottleneck_type: str  # 'CPU' | 'IO' | 'Lock' | 'Memory'
    severity: str  # 'critical' | 'warning' | 'info'
    description: str
    recommendation: str
    expected_improvement: str

class PerformanceDiagnosticTool:
    """æ€§èƒ½è¯Šæ–­å·¥å…·"""

    def __init__(self, db_conn):
        self.conn = db_conn

    def diagnose(self) -> List[PerformanceDiagnostic]:
        """å…¨é¢è¯Šæ–­"""
        diagnostics = []

        # 1. CPUç“¶é¢ˆæ£€æŸ¥
        cpu_diag = self.check_cpu_bottleneck()
        if cpu_diag:
            diagnostics.append(cpu_diag)

        # 2. IOç“¶é¢ˆæ£€æŸ¥
        io_diag = self.check_io_bottleneck()
        if io_diag:
            diagnostics.append(io_diag)

        # 3. é”ç«äº‰æ£€æŸ¥
        lock_diag = self.check_lock_contention()
        if lock_diag:
            diagnostics.append(lock_diag)

        # 4. å†…å­˜ç“¶é¢ˆæ£€æŸ¥
        memory_diag = self.check_memory_bottleneck()
        if memory_diag:
            diagnostics.append(memory_diag)

        return diagnostics

    def check_cpu_bottleneck(self) -> Optional[PerformanceDiagnostic]:
        """æ£€æŸ¥CPUç“¶é¢ˆ"""
        cpu_usage = psutil.cpu_percent(interval=1)

        if cpu_usage > 80:
            # æ£€æŸ¥æ…¢æŸ¥è¯¢
            slow_queries = self.get_slow_queries()

            return PerformanceDiagnostic(
                bottleneck_type='CPU',
                severity='critical' if cpu_usage > 90 else 'warning',
                description=f'CPUä½¿ç”¨ç‡{cpu_usage:.1f}%ï¼Œå‘ç°{len(slow_queries)}ä¸ªæ…¢æŸ¥è¯¢',
                recommendation='ä¼˜åŒ–æ…¢æŸ¥è¯¢ï¼Œæ·»åŠ ç´¢å¼•ï¼Œè€ƒè™‘æŸ¥è¯¢ç¼“å­˜',
                expected_improvement='CPUä½¿ç”¨ç‡é™ä½30-50%'
            )
        return None

    def check_io_bottleneck(self) -> Optional[PerformanceDiagnostic]:
        """æ£€æŸ¥IOç“¶é¢ˆ"""
        cur = self.conn.cursor()
        cur.execute("""
            SELECT
                SUM(blk_read_time + blk_write_time) as total_io_time,
                COUNT(*) as query_count
            FROM pg_stat_statements
        """)
        row = cur.fetchone()

        if row and row[0] and row[1]:
            avg_io_time = row[0] / row[1]

            if avg_io_time > 100:  # å¹³å‡IOæ—¶é—´>100ms
                return PerformanceDiagnostic(
                    bottleneck_type='IO',
                    severity='warning',
                    description=f'å¹³å‡IOæ—¶é—´{avg_io_time:.1f}msï¼ŒIOç“¶é¢ˆæ˜æ˜¾',
                    recommendation='å¢åŠ shared_buffersï¼Œä½¿ç”¨SSDï¼Œä¼˜åŒ–ç´¢å¼•',
                    expected_improvement='IOå»¶è¿Ÿé™ä½60-80%'
                )
        return None

    def check_lock_contention(self) -> Optional[PerformanceDiagnostic]:
        """æ£€æŸ¥é”ç«äº‰"""
        cur = self.conn.cursor()
        cur.execute("""
            SELECT
                COUNT(*) as waiting_count,
                AVG(EXTRACT(EPOCH FROM (now() - wait_start))) as avg_wait_time
            FROM pg_locks
            WHERE NOT granted
        """)
        row = cur.fetchone()

        if row and row[0] > 10:  # è¶…è¿‡10ä¸ªç­‰å¾…
            return PerformanceDiagnostic(
                bottleneck_type='Lock',
                severity='critical' if row[1] > 1.0 else 'warning',
                description=f'{row[0]}ä¸ªäº‹åŠ¡ç­‰å¾…é”ï¼Œå¹³å‡ç­‰å¾…{row[1]:.2f}ç§’',
                recommendation='æ£€æŸ¥é•¿äº‹åŠ¡ï¼Œä¼˜åŒ–é”ç²’åº¦ï¼Œè€ƒè™‘é™çº§éš”ç¦»çº§åˆ«',
                expected_improvement='é”ç­‰å¾…æ—¶é—´é™ä½70-90%'
            )
        return None

    def get_slow_queries(self) -> List[Dict]:
        """è·å–æ…¢æŸ¥è¯¢"""
        cur = self.conn.cursor()
        cur.execute("""
            SELECT query, mean_exec_time, calls
            FROM pg_stat_statements
            WHERE mean_exec_time > 1000  -- >1ç§’
            ORDER BY mean_exec_time DESC
            LIMIT 10
        """)
        return [{'query': row[0], 'time': row[1], 'calls': row[2]}
                for row in cur.fetchall()]

# ä½¿ç”¨ç¤ºä¾‹
tool = PerformanceDiagnosticTool(db_conn)
diagnostics = tool.diagnose()

for diag in diagnostics:
    print(f"[{diag.severity.upper()}] {diag.bottleneck_type}: {diag.description}")
    print(f"  å»ºè®®: {diag.recommendation}")
    print(f"  é¢„æœŸæå‡: {diag.expected_improvement}")
```

### 11.2 æ€§èƒ½åŸºçº¿å¯¹æ¯”å·¥å…·

```python
import json
from datetime import datetime

class PerformanceBaseline:
    """æ€§èƒ½åŸºçº¿å¯¹æ¯”å·¥å…·"""

    def __init__(self, baseline_file: str):
        self.baseline_file = baseline_file
        self.baseline = self.load_baseline()

    def load_baseline(self) -> Dict:
        """åŠ è½½åŸºçº¿æ•°æ®"""
        try:
            with open(self.baseline_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return {}

    def save_baseline(self, metrics: Dict):
        """ä¿å­˜åŸºçº¿æ•°æ®"""
        with open(self.baseline_file, 'w') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'metrics': metrics
            }, f, indent=2)

    def compare(self, current_metrics: Dict) -> Dict:
        """å¯¹æ¯”å½“å‰æ€§èƒ½ä¸åŸºçº¿"""
        if not self.baseline:
            return {'status': 'no_baseline', 'message': 'æ— åŸºçº¿æ•°æ®'}

        baseline_metrics = self.baseline.get('metrics', {})
        comparison = {}

        for key in ['tps', 'p99_latency', 'cpu_usage']:
            if key in baseline_metrics and key in current_metrics:
                baseline_val = baseline_metrics[key]
                current_val = current_metrics[key]

                if key == 'tps':
                    change = (current_val - baseline_val) / baseline_val * 100
                else:
                    change = (baseline_val - current_val) / baseline_val * 100

                comparison[key] = {
                    'baseline': baseline_val,
                    'current': current_val,
                    'change': change,
                    'status': 'improved' if change > 0 else 'degraded'
                }

        return comparison
```

---

## åäºŒã€æ›´å¤šå®é™…åº”ç”¨æ¡ˆä¾‹

### 12.1 æ¡ˆä¾‹: äº‘æ•°æ®åº“è¿ç§»æ€§èƒ½è°ƒä¼˜

**åœºæ™¯**: ä»è‡ªå»ºPostgreSQLè¿ç§»åˆ°äº‘æ•°æ®åº“

**é—®é¢˜**: è¿ç§»åæ€§èƒ½ä¸‹é™30%

**è¯Šæ–­è¿‡ç¨‹**:

```python
# è¿è¡Œè¯Šæ–­å·¥å…·
tool = PerformanceDiagnosticTool(cloud_db_conn)
diagnostics = tool.diagnose()

# å‘ç°:
# [CRITICAL] IO: å¹³å‡IOæ—¶é—´500msï¼ŒIOç“¶é¢ˆæ˜æ˜¾
# åŸå› : äº‘æ•°æ®åº“ä½¿ç”¨ç½‘ç»œå­˜å‚¨ï¼ŒIOå»¶è¿Ÿé«˜
```

**ä¼˜åŒ–æ–¹æ¡ˆ**:

```sql
-- 1. å¢åŠ shared_buffersï¼ˆå‡å°‘IOï¼‰
ALTER SYSTEM SET shared_buffers = '8GB';

-- 2. å¯ç”¨æŸ¥è¯¢ç¼“å­˜
ALTER SYSTEM SET effective_cache_size = '16GB';

-- 3. ä¼˜åŒ–è¿æ¥æ± 
ALTER SYSTEM SET max_connections = 200;
```

**ä¼˜åŒ–æ•ˆæœ**: æ€§èƒ½æ¢å¤åˆ°è¿ç§»å‰æ°´å¹³ï¼Œç”šè‡³æå‡10%

### 12.2 æ¡ˆä¾‹: å¾®æœåŠ¡æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–

**åœºæ™¯**: å¾®æœåŠ¡æ¶æ„ï¼Œæ¯ä¸ªæœåŠ¡ç‹¬ç«‹æ•°æ®åº“

**é—®é¢˜**: è·¨æœåŠ¡æŸ¥è¯¢æ…¢

**ä¼˜åŒ–æ–¹æ¡ˆ**:

```python
# 1. ä½¿ç”¨ç‰©åŒ–è§†å›¾èšåˆæ•°æ®
CREATE MATERIALIZED VIEW service_summary AS
SELECT service_id, COUNT(*) as request_count, AVG(latency) as avg_latency
FROM requests
GROUP BY service_id;

-- 2. å®šæœŸåˆ·æ–°
REFRESH MATERIALIZED VIEW CONCURRENTLY service_summary;

# 3. ä½¿ç”¨è¿æ¥æ± 
from sqlalchemy import create_engine
engine = create_engine(
    'postgresql://...',
    pool_size=20,
    max_overflow=10
)
```

**ä¼˜åŒ–æ•ˆæœ**: è·¨æœåŠ¡æŸ¥è¯¢å»¶è¿Ÿä»500msé™åˆ°50ms (-90%)

---

---

## åä¸‰ã€å®Œæ•´æ€§èƒ½è°ƒä¼˜å·¥å…·å®ç°

### 13.1 è‡ªåŠ¨åŒ–è°ƒä¼˜è„šæœ¬å®Œæ•´å®ç°

```python
import psycopg2
import psutil
from typing import Dict, List, Optional
from dataclasses import dataclass

@dataclass
class TuningRecommendation:
    """è°ƒä¼˜å»ºè®®"""
    parameter: str
    current_value: str
    recommended_value: str
    reason: str
    expected_improvement: str

class AutoTuningEngine:
    """è‡ªåŠ¨è°ƒä¼˜å¼•æ“"""

    def __init__(self, db_conn):
        self.conn = db_conn
        self.system_info = self.collect_system_info()

    def collect_system_info(self) -> Dict:
        """æ”¶é›†ç³»ç»Ÿä¿¡æ¯"""
        return {
            'cpu_count': psutil.cpu_count(),
            'memory_total': psutil.virtual_memory().total,
            'disk_io': psutil.disk_io_counters(),
        }

    def analyze_and_tune(self) -> List[TuningRecommendation]:
        """åˆ†æå¹¶ç”Ÿæˆè°ƒä¼˜å»ºè®®"""
        recommendations = []

        # 1. å†…å­˜è°ƒä¼˜
        memory_recs = self.tune_memory()
        recommendations.extend(memory_recs)

        # 2. WALè°ƒä¼˜
        wal_recs = self.tune_wal()
        recommendations.extend(wal_recs)

        # 3. è¿æ¥è°ƒä¼˜
        conn_recs = self.tune_connections()
        recommendations.extend(conn_recs)

        return recommendations

    def tune_memory(self) -> List[TuningRecommendation]:
        """å†…å­˜å‚æ•°è°ƒä¼˜"""
        recs = []
        total_memory = self.system_info['memory_total']

        # shared_buffers: 25% of RAM
        recommended_shared_buffers = total_memory // 4
        recs.append(TuningRecommendation(
            parameter='shared_buffers',
            current_value='128MB',
            recommended_value=f'{recommended_shared_buffers // (1024**2)}MB',
            reason='PostgreSQLæ¨èå€¼ä¸ºæ€»å†…å­˜çš„25%',
            expected_improvement='ç¼“å­˜å‘½ä¸­ç‡æå‡20-30%'
        ))

        # work_mem: æ ¹æ®å¹¶å‘åº¦è®¡ç®—
        max_connections = 100
        recommended_work_mem = (total_memory // 4) // max_connections
        recs.append(TuningRecommendation(
            parameter='work_mem',
            current_value='4MB',
            recommended_value=f'{recommended_work_mem // (1024**2)}MB',
            reason='work_mem = (shared_buffers / max_connections) / 4',
            expected_improvement='æ’åºå’Œå“ˆå¸Œæ“ä½œæ€§èƒ½æå‡30-50%'
        ))

        return recs

    def tune_wal(self) -> List[TuningRecommendation]:
        """WALå‚æ•°è°ƒä¼˜"""
        recs = []

        # wal_buffers: 16MB (PostgreSQL 9.5+è‡ªåŠ¨è°ƒæ•´)
        recs.append(TuningRecommendation(
            parameter='wal_buffers',
            current_value='-1 (auto)',
            recommended_value='16MB',
            reason='WALç¼“å†²åŒºå¤§å°ï¼Œ16MBé€šå¸¸è¶³å¤Ÿ',
            expected_improvement='WALå†™å…¥æ€§èƒ½ç¨³å®š'
        ))

        # checkpoint_timeout
        recs.append(TuningRecommendation(
            parameter='checkpoint_timeout',
            current_value='5min',
            recommended_value='15min',
            reason='å‡å°‘checkpointé¢‘ç‡ï¼Œæå‡å†™å…¥æ€§èƒ½',
            expected_improvement='å†™å…¥TPSæå‡10-20%'
        ))

        return recs

    def tune_connections(self) -> List[TuningRecommendation]:
        """è¿æ¥å‚æ•°è°ƒä¼˜"""
        recs = []

        # max_connections: æ ¹æ®CPUæ ¸å¿ƒæ•°
        cpu_count = self.system_info['cpu_count']
        recommended_connections = cpu_count * 4
        recs.append(TuningRecommendation(
            parameter='max_connections',
            current_value='100',
            recommended_value=str(recommended_connections),
            reason='max_connections = CPUæ ¸å¿ƒæ•° Ã— 4',
            expected_improvement='è¿æ¥æ± åˆ©ç”¨ç‡æå‡'
        ))

        return recs

    def apply_recommendations(self, recommendations: List[TuningRecommendation]):
        """åº”ç”¨è°ƒä¼˜å»ºè®®"""
        for rec in recommendations:
            print(f"è°ƒä¼˜å‚æ•°: {rec.parameter}")
            print(f"  å½“å‰å€¼: {rec.current_value}")
            print(f"  å»ºè®®å€¼: {rec.recommended_value}")
            print(f"  åŸå› : {rec.reason}")
            print(f"  é¢„æœŸæå‡: {rec.expected_improvement}")
            print()

# ä½¿ç”¨ç¤ºä¾‹
tuner = AutoTuningEngine(db_conn)
recommendations = tuner.analyze_and_tune()
tuner.apply_recommendations(recommendations)
```

### 13.2 æ€§èƒ½ç›‘æ§ä¸å‘Šè­¦ç³»ç»Ÿ

```python
import time
from datetime import datetime
from typing import Dict, List
import psycopg2

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§ç³»ç»Ÿ"""

    def __init__(self, db_conn):
        self.conn = db_conn
        self.metrics_history = []

    def collect_metrics(self) -> Dict:
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        cur = self.conn.cursor()

        # 1. æŸ¥è¯¢æ€§èƒ½
        cur.execute("""
            SELECT
                calls,
                total_exec_time,
                mean_exec_time,
                max_exec_time
            FROM pg_stat_statements
            ORDER BY total_exec_time DESC
            LIMIT 10
        """)

        slow_queries = cur.fetchall()

        # 2. è¿æ¥æ•°
        cur.execute("SELECT count(*) FROM pg_stat_activity")
        active_connections = cur.fetchone()[0]

        # 3. é”ç­‰å¾…
        cur.execute("""
            SELECT count(*) FROM pg_locks
            WHERE NOT granted
        """)
        waiting_locks = cur.fetchone()[0]

        return {
            'timestamp': datetime.now(),
            'slow_queries': slow_queries,
            'active_connections': active_connections,
            'waiting_locks': waiting_locks,
        }

    def check_alerts(self, metrics: Dict) -> List[str]:
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        alerts = []

        # å‘Šè­¦1: æ…¢æŸ¥è¯¢è¿‡å¤š
        if len(metrics['slow_queries']) > 5:
            alerts.append('è­¦å‘Š: å‘ç°5+æ…¢æŸ¥è¯¢ï¼Œå»ºè®®ä¼˜åŒ–')

        # å‘Šè­¦2: è¿æ¥æ•°è¿‡å¤š
        if metrics['active_connections'] > 80:
            alerts.append(f'è­¦å‘Š: æ´»è·ƒè¿æ¥æ•°{metrics["active_connections"]}ï¼Œæ¥è¿‘ä¸Šé™')

        # å‘Šè­¦3: é”ç­‰å¾…è¿‡å¤š
        if metrics['waiting_locks'] > 10:
            alerts.append(f'è­¦å‘Š: {metrics["waiting_locks"]}ä¸ªé”ç­‰å¾…ï¼Œå¯èƒ½å­˜åœ¨æ­»é”é£é™©')

        return alerts

    def monitor_loop(self, interval: int = 60):
        """ç›‘æ§å¾ªç¯"""
        while True:
            metrics = self.collect_metrics()
            self.metrics_history.append(metrics)

            alerts = self.check_alerts(metrics)
            if alerts:
                for alert in alerts:
                    print(f"[{metrics['timestamp']}] {alert}")

            time.sleep(interval)

# ä½¿ç”¨ç¤ºä¾‹
monitor = PerformanceMonitor(db_conn)
monitor.monitor_loop(interval=60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
```

---

**ç‰ˆæœ¬**: 2.0.0ï¼ˆå®Œæ•´å……å®ç‰ˆï¼‰
**åˆ›å»ºæ—¥æœŸ**: 2025-12-05
**æœ€åæ›´æ–°**: 2025-12-05
**æ–°å¢å†…å®¹**: å®Œæ•´è¯Šæ–­å·¥å…·ã€è‡ªåŠ¨åŒ–è°ƒä¼˜ã€å®é™…æ¡ˆä¾‹ã€åä¾‹åˆ†æã€å®Œæ•´æ€§èƒ½è¯Šæ–­å·¥å…·å®ç°ã€æ›´å¤šå®é™…åº”ç”¨æ¡ˆä¾‹ã€å®Œæ•´æ€§èƒ½è°ƒä¼˜å·¥å…·å®ç°ã€æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—èƒŒæ™¯ä¸æ¼”è¿›ï¼ˆä¸ºä»€ä¹ˆéœ€è¦æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—ã€å†å²èƒŒæ™¯ã€ç†è®ºåŸºç¡€ã€æ ¸å¿ƒæŒ‘æˆ˜ï¼‰ã€æ€§èƒ½è°ƒä¼˜å®æˆ˜æŒ‡å—åä¾‹è¡¥å……ï¼ˆ6ä¸ªæ–°å¢åä¾‹ï¼šæ€§èƒ½è°ƒä¼˜æ–¹æ³•ä¸ç³»ç»Ÿã€è°ƒä¼˜å¿½ç•¥ä¸šåŠ¡å½±å“ã€è°ƒä¼˜éªŒè¯ä¸è¶³ã€è°ƒä¼˜ç¼ºä¹ç›‘æ§ï¼‰

**å…³è”æ–‡æ¡£**:

- `06-æ€§èƒ½åˆ†æ/01-ååé‡å…¬å¼æ¨å¯¼.md` (ç†è®ºæ¨¡å‹)
- `06-æ€§èƒ½åˆ†æ/02-å»¶è¿Ÿåˆ†ææ¨¡å‹.md` (å»¶è¿Ÿç†è®º)
- `06-æ€§èƒ½åˆ†æ/04-é‡åŒ–å¯¹æ¯”å®éªŒ.md` (å®æµ‹æ•°æ®)
- `08-æ‰©å±•è§„åˆ’/03-å·¥ç¨‹å®è·µæŒ‡å—.md` (å·¥ç¨‹å®è·µ)
