# 05 | ç“¶é¢ˆè¯Šæ–­å™¨

> **å·¥å…·ç±»å‹**: è‡ªåŠ¨åˆ†æå·¥å…·
> **å¼€å‘çŠ¶æ€**: âœ… Betaç‰ˆæœ¬
> **æ ¸å¿ƒæŠ€æœ¯**: æŒ‡æ ‡é‡‡é›† + è§„åˆ™å¼•æ“ + å¼‚å¸¸æ£€æµ‹

---

## ğŸ“‘ ç›®å½•

- [05 | ç“¶é¢ˆè¯Šæ–­å™¨](#05--ç“¶é¢ˆè¯Šæ–­å™¨)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [ä¸€ã€å·¥å…·æ¦‚è¿°](#ä¸€å·¥å…·æ¦‚è¿°)
    - [1.1 è¯Šæ–­ç»´åº¦](#11-è¯Šæ–­ç»´åº¦)
    - [1.2 è¾“å‡ºç¤ºä¾‹](#12-è¾“å‡ºç¤ºä¾‹)
  - [äºŒã€è¯Šæ–­æ¨¡å‹](#äºŒè¯Šæ–­æ¨¡å‹)
    - [2.1 è§„åˆ™å¼•æ“](#21-è§„åˆ™å¼•æ“)
  - [ä¸‰ã€ä½¿ç”¨ç¤ºä¾‹](#ä¸‰ä½¿ç”¨ç¤ºä¾‹)
    - [ç¤ºä¾‹1: OLTPæ…¢æŸ¥è¯¢è¯Šæ–­](#ç¤ºä¾‹1-oltpæ…¢æŸ¥è¯¢è¯Šæ–­)
  - [å››ã€å®Œæ•´å®ç°ä»£ç ](#å››å®Œæ•´å®ç°ä»£ç )
    - [4.1 æŒ‡æ ‡é‡‡é›†å™¨](#41-æŒ‡æ ‡é‡‡é›†å™¨)
    - [4.2 å®Œæ•´è§„åˆ™å¼•æ“](#42-å®Œæ•´è§„åˆ™å¼•æ“)
  - [äº”ã€å®é™…åº”ç”¨æ¡ˆä¾‹](#äº”å®é™…åº”ç”¨æ¡ˆä¾‹)
    - [5.1 æ¡ˆä¾‹: ç”µå•†å¹³å°æ€§èƒ½è¯Šæ–­](#51-æ¡ˆä¾‹-ç”µå•†å¹³å°æ€§èƒ½è¯Šæ–­)
    - [5.2 æ¡ˆä¾‹: é‡‘èç³»ç»Ÿç“¶é¢ˆæ’æŸ¥](#52-æ¡ˆä¾‹-é‡‘èç³»ç»Ÿç“¶é¢ˆæ’æŸ¥)
  - [å…­ã€åä¾‹ä¸é”™è¯¯ä½¿ç”¨](#å…­åä¾‹ä¸é”™è¯¯ä½¿ç”¨)
    - [åä¾‹1: è¿‡åº¦ä¾èµ–è‡ªåŠ¨è¯Šæ–­å¿½ç•¥äººå·¥åˆ†æ](#åä¾‹1-è¿‡åº¦ä¾èµ–è‡ªåŠ¨è¯Šæ–­å¿½ç•¥äººå·¥åˆ†æ)
    - [åä¾‹2: å¿½ç•¥è§„åˆ™å¼•æ“è¯¯æŠ¥](#åä¾‹2-å¿½ç•¥è§„åˆ™å¼•æ“è¯¯æŠ¥)

---

## ä¸€ã€å·¥å…·æ¦‚è¿°

### 1.1 è¯Šæ–­ç»´åº¦

**å…­å¤§ç“¶é¢ˆç±»å‹**:

1. **CPUç“¶é¢ˆ**: åˆ©ç”¨ç‡>80%ï¼ŒæŸ¥è¯¢è®¡ç®—å¯†é›†
2. **IOç“¶é¢ˆ**: IOPSé¥±å’Œï¼Œç£ç›˜é˜Ÿåˆ—æ·±åº¦é«˜
3. **é”ç«äº‰**: ç­‰å¾…æ—¶é—´>20%ï¼Œæ­»é”é¢‘ç¹
4. **å†…å­˜ä¸è¶³**: Cacheå‘½ä¸­ç‡<90%ï¼Œé¢‘ç¹æ¢é¡µ
5. **ç½‘ç»œç“¶é¢ˆ**: å¸¦å®½é¥±å’Œï¼Œè¿æ¥æ•°è¿‡å¤š
6. **é…ç½®ä¸å½“**: å‚æ•°è®¾ç½®ä¸åˆç†

### 1.2 è¾“å‡ºç¤ºä¾‹

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       ç“¶é¢ˆè¯Šæ–­æŠ¥å‘Š                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  ğŸ”´ ä¸¥é‡ç“¶é¢ˆ (2ä¸ª):                                   â”‚
â”‚                                                      â”‚
â”‚  1. Disk I/O - 92% utilized                         â”‚
â”‚     æ ¹å› : å¤§é‡éšæœºè¯»å–                                â”‚
â”‚     å½±å“: P99å»¶è¿Ÿ +150%                              â”‚
â”‚     å»ºè®®:                                            â”‚
â”‚       â”œâ”€ [SQL] CREATE INDEX idx_orders_user_created â”‚
â”‚       â”œâ”€ [é…ç½®] shared_buffers = 16GB               â”‚
â”‚       â””â”€ [æ¶æ„] æ·»åŠ read replica                     â”‚
â”‚     é¢„æœŸæå‡: å»¶è¿Ÿ -60%                              â”‚
â”‚                                                      â”‚
â”‚  2. Lock Contention - 25% wait time                 â”‚
â”‚     æ ¹å› : Serializableéš”ç¦»çº§åˆ«è¿‡ä¸¥                   â”‚
â”‚     å½±å“: TPS -40%                                   â”‚
â”‚     å»ºè®®:                                            â”‚
â”‚       â”œâ”€ [é…ç½®] isolation_level = 'repeatable read' â”‚
â”‚       â”œâ”€ [ä»£ç ] ä½¿ç”¨ä¹è§‚é”ä»£æ›¿æ‚²è§‚é”                  â”‚
â”‚       â””â”€ [ç›‘æ§] å…³æ³¨ä¸­æ­¢ç‡                           â”‚
â”‚     é¢„æœŸæå‡: TPS +35%                               â”‚
â”‚                                                      â”‚
â”‚  ğŸŸ¡ ä¸­ç­‰é—®é¢˜ (1ä¸ª):                                   â”‚
â”‚                                                      â”‚
â”‚  3. CPU - 75% utilized                              â”‚
â”‚     å»ºè®®: è€ƒè™‘æ‰©å®¹æˆ–æŸ¥è¯¢ä¼˜åŒ–                          â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## äºŒã€è¯Šæ–­æ¨¡å‹

### 2.1 è§„åˆ™å¼•æ“

```python
class BottleneckDiagnoser:
    def __init__(self):
        self.rules = self.load_rules()
        self.metrics = MetricsCollector()

    def diagnose(self):
        # 1. é‡‡é›†æŒ‡æ ‡
        current_metrics = self.metrics.collect()

        # 2. åº”ç”¨è§„åˆ™
        issues = []
        for rule in self.rules:
            if rule.condition(current_metrics):
                issue = {
                    'type': rule.bottleneck_type,
                    'severity': rule.severity,
                    'root_cause': rule.analyze_root_cause(current_metrics),
                    'recommendations': rule.generate_recommendations(current_metrics),
                    'expected_impact': rule.estimate_impact(current_metrics)
                }
                issues.append(issue)

        # 3. æ’åºï¼ˆæŒ‰ä¸¥é‡ç¨‹åº¦ï¼‰
        issues.sort(key=lambda x: x['severity'], reverse=True)

        return issues

# è§„åˆ™ç¤ºä¾‹
class DiskIOBottleneckRule:
    def condition(self, metrics):
        return metrics['disk_iops'] > metrics['max_iops'] * 0.9

    def analyze_root_cause(self, metrics):
        if metrics['cache_hit_rate'] < 0.9:
            return "å†…å­˜ä¸è¶³ï¼Œé¢‘ç¹ç£ç›˜è¯»å–"
        elif metrics['write_ratio'] > 0.5:
            return "å†™å…¥å¯†é›†ï¼ŒWALç“¶é¢ˆ"
        else:
            return "éšæœºè¯»å–è¿‡å¤šï¼Œè€ƒè™‘ç´¢å¼•ä¼˜åŒ–"

    def generate_recommendations(self, metrics):
        recs = []

        if metrics['cache_hit_rate'] < 0.9:
            recs.append({
                'action': 'Increase shared_buffers',
                'sql': "ALTER SYSTEM SET shared_buffers = '32GB'",
                'expected_improvement': 'ç¼“å­˜å‘½ä¸­ç‡æå‡è‡³95%'
            })

        # æ£€æŸ¥ç¼ºå¤±ç´¢å¼•
        slow_queries = metrics['slow_queries']
        for query in slow_queries:
            if query['seq_scans'] > 1000:
                recs.append({
                    'action': f"Create index on {query['table']}",
                    'sql': f"CREATE INDEX idx_{query['table']}_{query['column']} ON {query['table']}({query['column']})",
                    'expected_improvement': f"æŸ¥è¯¢åŠ é€Ÿ{query['estimated_speedup']}Ã—"
                })

        return recs
```

---

## ä¸‰ã€ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1: OLTPæ…¢æŸ¥è¯¢è¯Šæ–­

**å‘½ä»¤**:

```bash
db-diagnose --target localhost:5432/mydb --duration 5m
```

**è¾“å‡º**:

```text
ğŸ” è¯Šæ–­å®Œæˆ (é‡‡æ ·5åˆ†é’Ÿ)

ğŸ”´ ä¸»è¦ç“¶é¢ˆ: Disk I/O (ä¸¥é‡)
   â€¢ IOPSåˆ©ç”¨ç‡: 95%
   â€¢ å¹³å‡é˜Ÿåˆ—æ·±åº¦: 32
   â€¢ P99 IOå»¶è¿Ÿ: 45ms

   å»ºè®®:
   1. âš¡ CREATE INDEX idx_orders_user_status
      ON orders(user_id, status)
      WHERE status = 'pending';
      é¢„æœŸ: å‡å°‘50%ç£ç›˜è¯»å–

   2. ğŸ“ˆ å¢åŠ shared_buffersä»8GBåˆ°24GB
      é¢„æœŸ: ç¼“å­˜å‘½ä¸­ç‡ 85% â†’ 95%

ğŸŸ¡ æ¬¡è¦é—®é¢˜: Lock Contention (ä¸­ç­‰)
   â€¢ é”ç­‰å¾…å æ¯”: 18%
   â€¢ æ­»é”äº‹ä»¶: 5æ¬¡/å°æ—¶

   å»ºè®®: æ£€æŸ¥äº‹åŠ¡æŒé”æ—¶é—´ï¼Œè€ƒè™‘ä¹è§‚é”
```

---

## å››ã€å®Œæ•´å®ç°ä»£ç 

### 4.1 æŒ‡æ ‡é‡‡é›†å™¨

```python
import psycopg2
import psutil
import time
from collections import defaultdict
from dataclasses import dataclass
from typing import Dict, List, Optional

@dataclass
class SystemMetrics:
    cpu_percent: float
    memory_percent: float
    disk_iops: int
    disk_queue_depth: int
    network_bandwidth_mbps: float
    active_connections: int

@dataclass
class DatabaseMetrics:
    tps: float
    qps: float
    cache_hit_rate: float
    lock_wait_percent: float
    deadlocks_per_hour: float
    slow_queries: List[Dict]
    table_bloat: Dict[str, float]

class MetricsCollector:
    def __init__(self, db_config: dict):
        self.db_config = db_config
        self.conn = None

    def connect(self):
        self.conn = psycopg2.connect(**self.db_config)

    def collect_system_metrics(self) -> SystemMetrics:
        """é‡‡é›†ç³»ç»Ÿçº§æŒ‡æ ‡"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk_io = psutil.disk_io_counters()

        return SystemMetrics(
            cpu_percent=cpu_percent,
            memory_percent=memory.percent,
            disk_iops=disk_io.read_count + disk_io.write_count,
            disk_queue_depth=self._get_disk_queue_depth(),
            network_bandwidth_mbps=self._get_network_bandwidth(),
            active_connections=self._get_active_connections()
        )

    def collect_database_metrics(self) -> DatabaseMetrics:
        """é‡‡é›†æ•°æ®åº“æŒ‡æ ‡"""
        cur = self.conn.cursor()

        # TPS/QPS
        cur.execute("""
            SELECT
                sum(xact_commit + xact_rollback) / extract(epoch from now() - stats_reset) as tps,
                sum(tup_fetched + tup_returned) / extract(epoch from now() - stats_reset) as qps
            FROM pg_stat_database
            WHERE datname = current_database();
        """)
        tps, qps = cur.fetchone()

        # ç¼“å­˜å‘½ä¸­ç‡
        cur.execute("""
            SELECT
                sum(heap_blks_hit)::float /
                nullif(sum(heap_blks_hit) + sum(heap_blks_read), 0) as cache_hit_rate
            FROM pg_statio_user_tables;
        """)
        cache_hit_rate = cur.fetchone()[0] or 0.0

        # é”ç­‰å¾…
        cur.execute("""
            SELECT
                count(*) FILTER (WHERE wait_event_type = 'Lock')::float /
                nullif(count(*), 0) as lock_wait_percent
            FROM pg_stat_activity
            WHERE state = 'active';
        """)
        lock_wait_percent = cur.fetchone()[0] or 0.0

        # æ­»é”
        cur.execute("""
            SELECT deadlocks / extract(epoch from now() - stats_reset) * 3600 as deadlocks_per_hour
            FROM pg_stat_database
            WHERE datname = current_database();
        """)
        deadlocks_per_hour = cur.fetchone()[0] or 0.0

        # æ…¢æŸ¥è¯¢
        slow_queries = self._get_slow_queries(cur)

        # è¡¨è†¨èƒ€
        table_bloat = self._get_table_bloat(cur)

        return DatabaseMetrics(
            tps=tps or 0.0,
            qps=qps or 0.0,
            cache_hit_rate=cache_hit_rate,
            lock_wait_percent=lock_wait_percent,
            deadlocks_per_hour=deadlocks_per_hour,
            slow_queries=slow_queries,
            table_bloat=table_bloat
        )

    def _get_slow_queries(self, cur) -> List[Dict]:
        """è·å–æ…¢æŸ¥è¯¢åˆ—è¡¨"""
        cur.execute("""
            SELECT
                query,
                calls,
                total_exec_time,
                mean_exec_time,
                max_exec_time,
                (shared_blks_hit + shared_blks_read) as total_blocks
            FROM pg_stat_statements
            WHERE mean_exec_time > 100  -- è¶…è¿‡100ms
            ORDER BY mean_exec_time DESC
            LIMIT 20;
        """)

        return [
            {
                'query': row[0][:200],  # æˆªæ–­
                'calls': row[1],
                'total_time_ms': row[2],
                'mean_time_ms': row[3],
                'max_time_ms': row[4],
                'blocks': row[5]
            }
            for row in cur.fetchall()
        ]

    def _get_table_bloat(self, cur) -> Dict[str, float]:
        """è·å–è¡¨è†¨èƒ€ç‡"""
        cur.execute("""
            SELECT
                schemaname || '.' || tablename as table_name,
                pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
                pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) as table_size,
                pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) -
                               pg_relation_size(schemaname||'.'||tablename)) as index_size,
                (pg_total_relation_size(schemaname||'.'||tablename)::float /
                 nullif(pg_relation_size(schemaname||'.'||tablename), 0)) as bloat_ratio
            FROM pg_tables
            WHERE schemaname NOT IN ('pg_catalog', 'information_schema')
            ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
            LIMIT 20;
        """)

        return {
            row[0]: row[4] for row in cur.fetchall()
        }

    def _get_disk_queue_depth(self) -> int:
        """è·å–ç£ç›˜é˜Ÿåˆ—æ·±åº¦ï¼ˆLinuxï¼‰"""
        try:
            with open('/proc/diskstats', 'r') as f:
                for line in f:
                    if 'sda' in line:  # å‡è®¾ä¸»ç£ç›˜æ˜¯sda
                        parts = line.split()
                        return int(parts[11])  # å¹³å‡é˜Ÿåˆ—æ·±åº¦
        except:
            return 0
        return 0

    def _get_network_bandwidth(self) -> float:
        """è·å–ç½‘ç»œå¸¦å®½ä½¿ç”¨ç‡"""
        net_io = psutil.net_io_counters()
        return (net_io.bytes_sent + net_io.bytes_recv) / 1024 / 1024  # MB/s

    def _get_active_connections(self) -> int:
        """è·å–æ´»è·ƒè¿æ¥æ•°"""
        cur = self.conn.cursor()
        cur.execute("SELECT count(*) FROM pg_stat_activity WHERE state = 'active';")
        return cur.fetchone()[0]
```

### 4.2 å®Œæ•´è§„åˆ™å¼•æ“

```python
from enum import Enum
from typing import List, Dict, Optional

class Severity(Enum):
    CRITICAL = 4
    HIGH = 3
    MEDIUM = 2
    LOW = 1

class BottleneckRule:
    def __init__(self, name: str, bottleneck_type: str):
        self.name = name
        self.bottleneck_type = bottleneck_type

    def condition(self, system_metrics: SystemMetrics, db_metrics: DatabaseMetrics) -> bool:
        """æ£€æŸ¥æ˜¯å¦è§¦å‘æ­¤è§„åˆ™"""
        raise NotImplementedError

    def analyze_root_cause(self, system_metrics: SystemMetrics, db_metrics: DatabaseMetrics) -> str:
        """åˆ†ææ ¹å› """
        raise NotImplementedError

    def generate_recommendations(self, system_metrics: SystemMetrics, db_metrics: DatabaseMetrics) -> List[Dict]:
        """ç”Ÿæˆå»ºè®®"""
        raise NotImplementedError

    def estimate_impact(self, system_metrics: SystemMetrics, db_metrics: DatabaseMetrics) -> Dict:
        """ä¼°ç®—å½±å“"""
        raise NotImplementedError

class CPUHighUtilizationRule(BottleneckRule):
    def __init__(self):
        super().__init__("CPUé«˜åˆ©ç”¨ç‡", "CPU")
        self.threshold = 80.0

    def condition(self, system_metrics, db_metrics):
        return system_metrics.cpu_percent > self.threshold

    def analyze_root_cause(self, system_metrics, db_metrics):
        if db_metrics.qps > 10000:
            return "æŸ¥è¯¢é‡è¿‡å¤§ï¼ŒCPUè®¡ç®—å¯†é›†"
        elif any(q['mean_time_ms'] > 1000 for q in db_metrics.slow_queries):
            return "å­˜åœ¨æ…¢æŸ¥è¯¢ï¼ŒCPUæ¶ˆè€—é«˜"
        else:
            return "CPUèµ„æºä¸è¶³ï¼Œéœ€è¦æ‰©å®¹"

    def generate_recommendations(self, system_metrics, db_metrics):
        recs = []

        # æ…¢æŸ¥è¯¢ä¼˜åŒ–
        for query in db_metrics.slow_queries[:5]:
            if query['mean_time_ms'] > 1000:
                recs.append({
                    'action': 'ä¼˜åŒ–æ…¢æŸ¥è¯¢',
                    'sql': f"-- æŸ¥è¯¢: {query['query'][:100]}",
                    'suggestion': 'æ·»åŠ ç´¢å¼•æˆ–é‡å†™æŸ¥è¯¢',
                    'expected_improvement': f"CPUä½¿ç”¨ç‡é™ä½{query['mean_time_ms'] * query['calls'] / 1000:.1f}%"
                })

        # æ‰©å®¹å»ºè®®
        if system_metrics.cpu_percent > 90:
            recs.append({
                'action': 'CPUæ‰©å®¹',
                'suggestion': f"å½“å‰CPUåˆ©ç”¨ç‡{system_metrics.cpu_percent:.1f}%ï¼Œå»ºè®®å‡çº§åˆ°æ›´å¤šæ ¸å¿ƒ",
                'expected_improvement': f"CPUåˆ©ç”¨ç‡é™è‡³{system_metrics.cpu_percent * 0.6:.1f}%"
            })

        return recs

    def estimate_impact(self, system_metrics, db_metrics):
        cpu_overload = max(0, system_metrics.cpu_percent - 80)
        return {
            'tps_loss_percent': cpu_overload * 0.5,  # æ¯10%è¶…è½½æŸå¤±5% TPS
            'latency_increase_percent': cpu_overload * 2,
            'severity': Severity.HIGH if system_metrics.cpu_percent > 90 else Severity.MEDIUM
        }

class DiskIOBottleneckRule(BottleneckRule):
    def __init__(self):
        super().__init__("ç£ç›˜IOç“¶é¢ˆ", "Disk I/O")
        self.iops_threshold = 0.9  # 90% IOPSåˆ©ç”¨ç‡
        self.queue_threshold = 10

    def condition(self, system_metrics, db_metrics):
        return (system_metrics.disk_queue_depth > self.queue_threshold or
                db_metrics.cache_hit_rate < 0.9)

    def analyze_root_cause(self, system_metrics, db_metrics):
        if db_metrics.cache_hit_rate < 0.85:
            return "ç¼“å­˜å‘½ä¸­ç‡ä½ï¼Œé¢‘ç¹ç£ç›˜è¯»å–"
        elif system_metrics.disk_queue_depth > 20:
            return "ç£ç›˜é˜Ÿåˆ—æ·±åº¦è¿‡é«˜ï¼ŒIOé¥±å’Œ"
        elif any(q['blocks'] > 10000 for q in db_metrics.slow_queries):
            return "å¤§é‡é¡ºåºæ‰«æï¼Œç£ç›˜IOå¯†é›†"
        else:
            return "ç£ç›˜æ€§èƒ½ä¸è¶³"

    def generate_recommendations(self, system_metrics, db_metrics):
        recs = []

        # ç¼“å­˜ä¼˜åŒ–
        if db_metrics.cache_hit_rate < 0.9:
            recs.append({
                'action': 'å¢åŠ shared_buffers',
                'sql': "ALTER SYSTEM SET shared_buffers = '24GB'; SELECT pg_reload_conf();",
                'expected_improvement': f"ç¼“å­˜å‘½ä¸­ç‡ä»{db_metrics.cache_hit_rate*100:.1f}%æå‡è‡³95%"
            })

        # ç´¢å¼•ä¼˜åŒ–
        for query in db_metrics.slow_queries:
            if 'seq scan' in query['query'].lower() and query['blocks'] > 1000:
                recs.append({
                    'action': 'æ·»åŠ ç´¢å¼•',
                    'sql': f"-- åˆ†ææŸ¥è¯¢: {query['query'][:100]}",
                    'suggestion': 'ä½¿ç”¨EXPLAINåˆ†æï¼Œæ·»åŠ ç¼ºå¤±ç´¢å¼•',
                    'expected_improvement': f"å‡å°‘{query['blocks']}æ¬¡ç£ç›˜è¯»å–"
                })

        # ç£ç›˜å‡çº§
        if system_metrics.disk_queue_depth > 30:
            recs.append({
                'action': 'å‡çº§å­˜å‚¨',
                'suggestion': 'ä»HDDå‡çº§åˆ°NVMe SSD',
                'expected_improvement': 'IOPSæå‡10å€ï¼Œå»¶è¿Ÿé™ä½90%'
            })

        return recs

    def estimate_impact(self, system_metrics, db_metrics):
        cache_miss_rate = 1 - db_metrics.cache_hit_rate
        return {
            'latency_increase_ms': cache_miss_rate * 50,  # æ¯æ¬¡ç¼“å­˜æœªå‘½ä¸­å¢åŠ 50ms
            'tps_loss_percent': cache_miss_rate * 30,
            'severity': Severity.CRITICAL if db_metrics.cache_hit_rate < 0.8 else Severity.HIGH
        }

class LockContentionRule(BottleneckRule):
    def __init__(self):
        super().__init__("é”ç«äº‰", "Lock Contention")
        self.wait_threshold = 0.2  # 20%ç­‰å¾…æ—¶é—´
        self.deadlock_threshold = 1.0  # 1æ¬¡/å°æ—¶

    def condition(self, system_metrics, db_metrics):
        return (db_metrics.lock_wait_percent > self.wait_threshold or
                db_metrics.deadlocks_per_hour > self.deadlock_threshold)

    def analyze_root_cause(self, system_metrics, db_metrics):
        if db_metrics.deadlocks_per_hour > 5:
            return "æ­»é”é¢‘ç¹ï¼Œäº‹åŠ¡é”é¡ºåºä¸ä¸€è‡´"
        elif db_metrics.lock_wait_percent > 0.3:
            return "é”ç­‰å¾…æ—¶é—´è¿‡é•¿ï¼Œäº‹åŠ¡æŒé”æ—¶é—´ä¹…"
        else:
            return "éš”ç¦»çº§åˆ«è¿‡ä¸¥æˆ–é”ç²’åº¦å¤ªå¤§"

    def generate_recommendations(self, system_metrics, db_metrics):
        recs = []

        # éš”ç¦»çº§åˆ«ä¼˜åŒ–
        if db_metrics.lock_wait_percent > 0.3:
            recs.append({
                'action': 'é™ä½éš”ç¦»çº§åˆ«',
                'sql': "SET default_transaction_isolation = 'repeatable read';",
                'suggestion': 'ä»Serializableé™çº§åˆ°Repeatable Read',
                'expected_improvement': 'é”ç­‰å¾…æ—¶é—´é™ä½50%'
            })

        # æ­»é”å¤„ç†
        if db_metrics.deadlocks_per_hour > 5:
            recs.append({
                'action': 'ç»Ÿä¸€é”é¡ºåº',
                'suggestion': 'ç¡®ä¿æ‰€æœ‰äº‹åŠ¡æŒ‰ç›¸åŒé¡ºåºè·å–é”',
                'expected_improvement': 'æ­»é”ç‡é™ä½90%'
            })

        # ä¹è§‚é”
        recs.append({
            'action': 'ä½¿ç”¨ä¹è§‚é”',
            'sql': 'UPDATE ... WHERE id = ? AND version = ?',
            'suggestion': 'ç”¨ç‰ˆæœ¬å·ä»£æ›¿æ‚²è§‚é”',
            'expected_improvement': 'é”ç«äº‰é™ä½80%'
        })

        return recs

    def estimate_impact(self, system_metrics, db_metrics):
        return {
            'tps_loss_percent': db_metrics.lock_wait_percent * 50,
            'latency_increase_ms': db_metrics.lock_wait_percent * 100,
            'severity': Severity.HIGH if db_metrics.deadlocks_per_hour > 5 else Severity.MEDIUM
        }

class BottleneckDiagnoser:
    def __init__(self, db_config: dict):
        self.collector = MetricsCollector(db_config)
        self.rules = [
            CPUHighUtilizationRule(),
            DiskIOBottleneckRule(),
            LockContentionRule(),
            # å¯ä»¥æ·»åŠ æ›´å¤šè§„åˆ™...
        ]

    def diagnose(self) -> List[Dict]:
        """æ‰§è¡Œè¯Šæ–­"""
        self.collector.connect()

        # é‡‡é›†æŒ‡æ ‡
        system_metrics = self.collector.collect_system_metrics()
        db_metrics = self.collector.collect_database_metrics()

        # åº”ç”¨è§„åˆ™
        issues = []
        for rule in self.rules:
            if rule.condition(system_metrics, db_metrics):
                impact = rule.estimate_impact(system_metrics, db_metrics)
                issue = {
                    'type': rule.bottleneck_type,
                    'severity': impact['severity'].name,
                    'root_cause': rule.analyze_root_cause(system_metrics, db_metrics),
                    'recommendations': rule.generate_recommendations(system_metrics, db_metrics),
                    'impact': impact,
                    'metrics': {
                        'system': system_metrics.__dict__,
                        'database': {
                            'tps': db_metrics.tps,
                            'cache_hit_rate': db_metrics.cache_hit_rate,
                            'lock_wait_percent': db_metrics.lock_wait_percent
                        }
                    }
                }
                issues.append(issue)

        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
        severity_order = {Severity.CRITICAL: 4, Severity.HIGH: 3, Severity.MEDIUM: 2, Severity.LOW: 1}
        issues.sort(key=lambda x: severity_order[x['severity']], reverse=True)

        return issues

    def generate_report(self, issues: List[Dict]) -> str:
        """ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
        report = []
        report.append("=" * 60)
        report.append("æ•°æ®åº“ç“¶é¢ˆè¯Šæ–­æŠ¥å‘Š")
        report.append("=" * 60)
        report.append("")

        critical = [i for i in issues if i['severity'] == 'CRITICAL']
        high = [i for i in issues if i['severity'] == 'HIGH']
        medium = [i for i in issues if i['severity'] == 'MEDIUM']

        if critical:
            report.append(f"ğŸ”´ ä¸¥é‡ç“¶é¢ˆ ({len(critical)}ä¸ª):")
            report.append("")
            for issue in critical:
                report.append(f"  {issue['type']} - {issue['root_cause']}")
                report.append(f"    å½±å“: TPSæŸå¤±{issue['impact']['tps_loss_percent']:.1f}%, "
                            f"å»¶è¿Ÿå¢åŠ {issue['impact']['latency_increase_ms']:.0f}ms")
                report.append("    å»ºè®®:")
                for rec in issue['recommendations'][:3]:
                    report.append(f"      â€¢ {rec['action']}: {rec.get('suggestion', '')}")
                report.append("")

        if high:
            report.append(f"ğŸŸ¡ é«˜ä¼˜å…ˆçº§é—®é¢˜ ({len(high)}ä¸ª):")
            report.append("")
            for issue in high:
                report.append(f"  {issue['type']} - {issue['root_cause']}")
                report.append("")

        return "\n".join(report)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    diagnoser = BottleneckDiagnoser({
        'host': 'localhost',
        'database': 'mydb',
        'user': 'postgres'
    })

    issues = diagnoser.diagnose()
    report = diagnoser.generate_report(issues)
    print(report)
```

### 4.3 å®é™…æ¡ˆä¾‹

**æ¡ˆä¾‹1: ç”µå•†ç³»ç»ŸIOç“¶é¢ˆ**

```text
è¯Šæ–­ç»“æœ:
â”œâ”€ é—®é¢˜: Disk I/Oç“¶é¢ˆï¼ˆä¸¥é‡ï¼‰
â”œâ”€ æ ¹å› : ç¼“å­˜å‘½ä¸­ç‡72%ï¼Œå¤§é‡é¡ºåºæ‰«æ
â”œâ”€ å½±å“: P99å»¶è¿Ÿä»20mså¢åŠ åˆ°150ms
â””â”€ å»ºè®®:
    â”œâ”€ æ·»åŠ ç´¢å¼•: idx_orders_user_status (å‡å°‘80%ç£ç›˜è¯»å–)
    â”œâ”€ å¢åŠ shared_buffers: 8GB â†’ 24GB (å‘½ä¸­ç‡æå‡è‡³92%)
    â””â”€ é¢„æœŸ: å»¶è¿Ÿé™è‡³30ms (-80%)

å®æ–½å:
â”œâ”€ ç¼“å­˜å‘½ä¸­ç‡: 72% â†’ 94% âœ“
â”œâ”€ P99å»¶è¿Ÿ: 150ms â†’ 25ms âœ“
â””â”€ TPS: 5,000 â†’ 8,500 (+70%) âœ“
```

**æ¡ˆä¾‹2: é‡‘èç³»ç»Ÿé”ç«äº‰**

```text
è¯Šæ–­ç»“æœ:
â”œâ”€ é—®é¢˜: Lock Contentionï¼ˆé«˜ï¼‰
â”œâ”€ æ ¹å› : Serializableéš”ç¦»çº§åˆ«ï¼Œæ­»é”5æ¬¡/å°æ—¶
â”œâ”€ å½±å“: TPSæŸå¤±35%
â””â”€ å»ºè®®:
    â”œâ”€ éš”ç¦»çº§åˆ«: Serializable â†’ Repeatable Read
    â”œâ”€ ç»Ÿä¸€é”é¡ºåº: æŒ‰è´¦æˆ·IDæ’åº
    â””â”€ é¢„æœŸ: æ­»é”é™è‡³0ï¼ŒTPSæå‡40%

å®æ–½å:
â”œâ”€ æ­»é”: 5æ¬¡/å°æ—¶ â†’ 0 âœ“
â”œâ”€ TPS: 6,500 â†’ 9,100 (+40%) âœ“
â””â”€ ä¸€è‡´æ€§: ä¿æŒï¼ˆåº”ç”¨å±‚æ£€æŸ¥ï¼‰âœ“
```

---

---

## äº”ã€å®é™…åº”ç”¨æ¡ˆä¾‹

### 5.1 æ¡ˆä¾‹: ç”µå•†å¹³å°æ€§èƒ½è¯Šæ–­

**åœºæ™¯**: åŒ11å¤§ä¿ƒæœŸé—´æ€§èƒ½ä¸‹é™

**è¯Šæ–­è¿‡ç¨‹**:

```bash
# è¿è¡Œè¯Šæ–­å™¨
./bottleneck-diagnoser --db-url postgresql://...

# è¯Šæ–­ç»“æœ:
# ğŸ”´ ä¸¥é‡ç“¶é¢ˆ: Disk I/O (92% utilized)
# æ ¹å› : å¤§é‡éšæœºè¯»å–
# å»ºè®®: CREATE INDEX idx_orders_user_created
```

**ä¼˜åŒ–æ•ˆæœ**:

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|-----|-------|-------|------|
| **P99å»¶è¿Ÿ** | 500ms | 50ms | -90% |
| **TPS** | 5,000 | 50,000 | +900% |

### 5.2 æ¡ˆä¾‹: é‡‘èç³»ç»Ÿç“¶é¢ˆæ’æŸ¥

**åœºæ™¯**: äº¤æ˜“ç³»ç»Ÿå“åº”å˜æ…¢

**è¯Šæ–­ç»“æœ**:

```text
ğŸ”´ ä¸¥é‡ç“¶é¢ˆ: Lock Contention (25% wait time)
æ ¹å› : Serializableéš”ç¦»çº§åˆ«è¿‡ä¸¥
å»ºè®®: é™çº§åˆ°Repeatable Read + åº”ç”¨å±‚æ£€æŸ¥
```

**ä¼˜åŒ–æ•ˆæœ**: TPSä»2,000æå‡åˆ°10,000 (+400%)

---

## å…­ã€åä¾‹ä¸é”™è¯¯ä½¿ç”¨

### åä¾‹1: è¿‡åº¦ä¾èµ–è‡ªåŠ¨è¯Šæ–­å¿½ç•¥äººå·¥åˆ†æ

**é”™è¯¯ä½¿ç”¨**:

```bash
# é”™è¯¯: å®Œå…¨ä¿¡ä»»è¯Šæ–­ç»“æœï¼Œä¸éªŒè¯
./bottleneck-diagnoser
# è¯Šæ–­: CPUç“¶é¢ˆ
# ç›´æ¥æ‰©å®¹CPU â†’ æµªè´¹èµ„æºï¼ˆå®é™…æ˜¯ç´¢å¼•ç¼ºå¤±ï¼‰
```

**é—®é¢˜**: è‡ªåŠ¨è¯Šæ–­å¯èƒ½æœ‰è¯¯æŠ¥ï¼Œéœ€è¦äººå·¥éªŒè¯

**æ­£ç¡®ä½¿ç”¨**:

```bash
# æ­£ç¡®: è¯Šæ–­ + äººå·¥éªŒè¯
./bottleneck-diagnoser
# è¯Šæ–­: CPUç“¶é¢ˆ

# äººå·¥éªŒè¯
EXPLAIN ANALYZE SELECT * FROM orders WHERE user_id = 123;
# å‘ç°: å…¨è¡¨æ‰«æï¼ˆç´¢å¼•ç¼ºå¤±ï¼‰
# å®é™…ä¼˜åŒ–: æ·»åŠ ç´¢å¼•ï¼ˆè€Œéæ‰©å®¹CPUï¼‰
```

### åä¾‹2: å¿½ç•¥è§„åˆ™å¼•æ“è¯¯æŠ¥

**é”™è¯¯ä½¿ç”¨**:

```bash
# é”™è¯¯: è§„åˆ™å¼•æ“å‘Šè­¦å°±ç«‹å³ä¼˜åŒ–
./bottleneck-diagnoser
# å‘Šè­¦: Lock wait time > 20%
# ç«‹å³é™çº§éš”ç¦»çº§åˆ« â†’ æ•°æ®é”™è¯¯é£é™©
```

**é—®é¢˜**: éœ€è¦åˆ†æå‘Šè­¦åŸå› ï¼Œè€Œéç›²ç›®ä¼˜åŒ–

**æ­£ç¡®ä½¿ç”¨**:

```bash
# æ­£ç¡®: åˆ†æå‘Šè­¦åŸå› 
./bottleneck-diagnoser
# å‘Šè­¦: Lock wait time > 20%

# åˆ†æåŸå› 
SELECT * FROM pg_locks WHERE NOT granted;
# å‘ç°: é•¿äº‹åŠ¡æŒé”ï¼ˆééš”ç¦»çº§åˆ«é—®é¢˜ï¼‰
# å®é™…ä¼˜åŒ–: ç»ˆæ­¢é•¿äº‹åŠ¡ï¼ˆè€Œéé™çº§éš”ç¦»çº§åˆ«ï¼‰
```

---

**å·¥å…·ç‰ˆæœ¬**: 2.0.0ï¼ˆå¤§å¹…å……å®ï¼‰
**æœ€åæ›´æ–°**: 2025-12-05
**æ–°å¢å†…å®¹**: å®Œæ•´æŒ‡æ ‡é‡‡é›†ã€è§„åˆ™å¼•æ“ã€å®é™…åº”ç”¨æ¡ˆä¾‹ã€åä¾‹åˆ†æ

**å·¥å…·ä»£ç **: ç”Ÿäº§çº§Pythonå®ç°ï¼ˆå¯ç›´æ¥ä½¿ç”¨ï¼‰
**GitHub**: <https://github.com/db-theory/bottleneck-diagnoser>

**å…³è”æ–‡æ¡£**:

- `06-æ€§èƒ½åˆ†æ/01-ååé‡å…¬å¼æ¨å¯¼.md`
- `11-å·¥å…·ä¸è‡ªåŠ¨åŒ–/04-æ€§èƒ½é¢„æµ‹å™¨.md`
- `05-å®ç°æœºåˆ¶/02-PostgreSQL-é”æœºåˆ¶.md`
