# 09 | è‡ªåŠ¨ç´¢å¼•æ¨èå™¨

> **å·¥å…·ç±»å‹**: åˆ†ææ¨èå·¥å…·
> **å¼€å‘çŠ¶æ€**: âœ… Betaç‰ˆæœ¬
> **æ ¸å¿ƒæŠ€æœ¯**: æ…¢æŸ¥è¯¢åˆ†æ + æ”¶ç›Šè¯„ä¼° + MLæ’åº

---

## ğŸ“‘ ç›®å½•

- [09 | è‡ªåŠ¨ç´¢å¼•æ¨èå™¨](#09--è‡ªåŠ¨ç´¢å¼•æ¨èå™¨)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [ä¸€ã€å·¥å…·æ¦‚è¿°](#ä¸€å·¥å…·æ¦‚è¿°)
    - [1.1 åŠŸèƒ½å®šä½](#11-åŠŸèƒ½å®šä½)
  - [äºŒã€æ¨èç®—æ³•](#äºŒæ¨èç®—æ³•)
    - [2.1 ç´¢å¼•å€™é€‰ç”Ÿæˆ](#21-ç´¢å¼•å€™é€‰ç”Ÿæˆ)
    - [2.2 å†—ä½™ç´¢å¼•æ£€æµ‹](#22-å†—ä½™ç´¢å¼•æ£€æµ‹)
  - [ä¸‰ã€æ”¶ç›Šè¯„ä¼°](#ä¸‰æ”¶ç›Šè¯„ä¼°)
    - [3.1 ROIè®¡ç®—æ¨¡å‹](#31-roiè®¡ç®—æ¨¡å‹)
  - [å››ã€ä½¿ç”¨æŒ‡å—](#å››ä½¿ç”¨æŒ‡å—)
    - [4.1 CLIä½¿ç”¨](#41-cliä½¿ç”¨)
    - [4.2 ç”ŸæˆSQLè„šæœ¬](#42-ç”Ÿæˆsqlè„šæœ¬)
  - [äº”ã€å®Œæ•´å®ç°ä»£ç ](#äº”å®Œæ•´å®ç°ä»£ç )
    - [5.1 SQLè§£æä¸ç´¢å¼•å€™é€‰ç”Ÿæˆ](#51-sqlè§£æä¸ç´¢å¼•å€™é€‰ç”Ÿæˆ)
    - [5.2 ç´¢å¼•æ”¶ç›Šè¯„ä¼°](#52-ç´¢å¼•æ”¶ç›Šè¯„ä¼°)
    - [5.3 å®Œæ•´æ¨èå™¨å®ç°](#53-å®Œæ•´æ¨èå™¨å®ç°)
    - [5.4 å®é™…æ¡ˆä¾‹](#54-å®é™…æ¡ˆä¾‹)

---

## ä¸€ã€å·¥å…·æ¦‚è¿°

### 1.1 åŠŸèƒ½å®šä½

**æ ¸å¿ƒä»·å€¼**: è‡ªåŠ¨è¯†åˆ«ç¼ºå¤±ç´¢å¼•ï¼Œé‡åŒ–æ”¶ç›Š

**è¾“å…¥æ•°æ®æº**:

- pg_stat_statements (æ…¢æŸ¥è¯¢)
- pg_stat_user_tables (è¡¨ç»Ÿè®¡)
- pg_indexes (ç°æœ‰ç´¢å¼•)
- EXPLAINåˆ†æç»“æœ

**è¾“å‡º**:

- ç¼ºå¤±ç´¢å¼•åˆ—è¡¨ï¼ˆæŒ‰ROIæ’åºï¼‰
- å†—ä½™ç´¢å¼•è¯†åˆ«ï¼ˆå¯å®‰å…¨åˆ é™¤ï¼‰
- æ”¶ç›Š/æˆæœ¬åˆ†æ

---

## äºŒã€æ¨èç®—æ³•

### 2.1 ç´¢å¼•å€™é€‰ç”Ÿæˆ

```python
class IndexRecommender:
    def analyze_query(self, query_text, execution_stats):
        # 1. è§£æSQL
        ast = parse_sql(query_text)

        # 2. æå–ç´¢å¼•å€™é€‰
        candidates = []

        # WHEREæ¡ä»¶åˆ—
        for predicate in ast.where_clauses:
            if predicate.operator in ['=', 'IN', '>', '<']:
                candidates.append({
                    'table': predicate.table,
                    'columns': [predicate.column],
                    'type': 'B-Tree',
                    'reason': f"WHERE {predicate.column} filter"
                })

        # JOINæ¡ä»¶åˆ—
        for join in ast.joins:
            candidates.append({
                'table': join.left_table,
                'columns': [join.left_column, join.right_column],
                'type': 'B-Tree',
                'reason': f"JOIN condition"
            })

        # ORDER BYåˆ—
        if ast.order_by:
            candidates.append({
                'table': ast.from_table,
                'columns': [col for col in ast.order_by],
                'type': 'B-Tree',
                'reason': "ORDER BY optimization"
            })

        # 3. å»é‡å’Œåˆå¹¶
        merged = self.merge_candidates(candidates)

        # 4. è¯„ä¼°æ”¶ç›Š
        for candidate in merged:
            candidate['benefit'] = self.estimate_benefit(query_text, candidate, execution_stats)
            candidate['cost'] = self.estimate_cost(candidate)
            candidate['roi'] = candidate['benefit'] / candidate['cost']

        return merged

    def estimate_benefit(self, query, index_spec, stats):
        # æ‰§è¡ŒEXPLAIN with hypothetical index
        explain_result = self.explain_with_hypothetical_index(query, index_spec)

        current_cost = stats['total_time']
        predicted_cost = explain_result['total_cost']

        # æ”¶ç›Š = èŠ‚çœçš„æ—¶é—´ Ã— æŸ¥è¯¢é¢‘ç‡
        time_saved = max(0, current_cost - predicted_cost)
        query_freq = stats['calls_per_day']

        daily_benefit = time_saved * query_freq  # ç§’/å¤©

        return daily_benefit

    def estimate_cost(self, index_spec):
        # ä¼°ç®—ç´¢å¼•å¤§å°
        table_size = get_table_size(index_spec['table'])
        column_sizes = sum([get_column_size(col) for col in index_spec['columns']])

        index_size_mb = (table_size / avg_row_size) * column_sizes / 1024 / 1024

        # ç»´æŠ¤æˆæœ¬
        write_overhead = index_size_mb * 0.1  # æ¯MBç´¢å¼•å¢åŠ 0.1mså†™å…¥å»¶è¿Ÿ

        return {
            'storage_mb': index_size_mb,
            'write_overhead_ms': write_overhead
        }
```

### 2.2 å†—ä½™ç´¢å¼•æ£€æµ‹

```sql
-- æ£€æµ‹å†—ä½™ç´¢å¼•
WITH index_columns AS (
    SELECT
        indexrelid::regclass AS index_name,
        indrelid::regclass AS table_name,
        array_agg(attname ORDER BY attnum) AS columns
    FROM pg_index
    JOIN pg_attribute ON attrelid = indrelid AND attnum = ANY(indkey)
    WHERE indisvalid
    GROUP BY indexrelid, indrelid
)
SELECT
    i1.index_name AS redundant_index,
    i2.index_name AS covering_index,
    i1.columns AS redundant_columns,
    i2.columns AS covering_columns
FROM index_columns i1
JOIN index_columns i2 ON i1.table_name = i2.table_name
WHERE i1.indexrelid < i2.indexrelid  -- é¿å…é‡å¤
  AND i1.columns <@ i2.columns  -- i1æ˜¯i2çš„å‰ç¼€
  AND pg_relation_size(i1.indexrelid) > 100 * 1024 * 1024;  -- å¤§äº100MB

-- è¾“å‡º: å¯ä»¥å®‰å…¨åˆ é™¤redundant_index
```

---

## ä¸‰ã€æ”¶ç›Šè¯„ä¼°

### 3.1 ROIè®¡ç®—æ¨¡å‹

\[
\text{ROI} = \frac{\text{Daily\_Time\_Saved} \times \text{Cost\_Per\_Second}}{\text{Storage\_Cost} + \text{Maintenance\_Cost}}
\]

**ç¤ºä¾‹**:

```text
ç´¢å¼•: idx_orders_user_created
â”œâ”€ å­˜å‚¨æˆæœ¬: 200MB Ã— $0.1/GB/æœˆ = $0.02/æœˆ
â”œâ”€ ç»´æŠ¤æˆæœ¬: å†™å…¥å»¶è¿Ÿ +2ms Ã— 1000 writes/s = $5/æœˆ
â”œâ”€ æ”¶ç›Š: 1000 queries/å¤© Ã— 50msèŠ‚çœ Ã— $0.001/ç§’ = $50/å¤© = $1500/æœˆ
â””â”€ ROI: $1500 / ($0.02 + $5) â‰ˆ 300Ã— ğŸš€

ç»“è®º: å¼ºçƒˆæ¨èåˆ›å»º
```

---

## å››ã€ä½¿ç”¨æŒ‡å—

### 4.1 CLIä½¿ç”¨

```bash
# åˆ†ææ…¢æŸ¥è¯¢å¹¶æ¨èç´¢å¼•
db-index-advisor \
  --host localhost \
  --database mydb \
  --analyze-duration 1h \
  --min-roi 10

# è¾“å‡º Top 10 æ¨è:
# 1. idx_orders_user_created (ROI: 300Ã—) â­â­â­â­â­
# 2. idx_products_category (ROI: 150Ã—) â­â­â­â­
# ...
```

### 4.2 ç”ŸæˆSQLè„šæœ¬

```bash
# ç”Ÿæˆå¯æ‰§è¡Œçš„DDLè„šæœ¬
db-index-advisor --output create_indexes.sql

# create_indexes.sqlå†…å®¹:
-- Index Recommendation Report
-- Generated: 2025-12-05
-- Estimated Total Benefit: 15,000 queries/day faster

-- Recommendation #1 (ROI: 300Ã—)
CREATE INDEX CONCURRENTLY idx_orders_user_created
  ON orders(user_id, created_at)
  WHERE status = 'pending';
-- Expected: 1000 queries/day, 50ms faster each

-- Recommendation #2 (ROI: 150Ã—)
CREATE INDEX CONCURRENTLY idx_products_category
  ON products(category)
  INCLUDE (price, stock);
-- Expected: 500 queries/day, 80ms faster each
```

---

## äº”ã€å®Œæ•´å®ç°ä»£ç 

### 5.1 SQLè§£æä¸ç´¢å¼•å€™é€‰ç”Ÿæˆ

```python
import psycopg2
import re
from collections import defaultdict
from dataclasses import dataclass
from typing import List, Dict, Optional
import sqlparse
from sqlparse.sql import Statement, Token
from sqlparse.tokens import Keyword, Name

@dataclass
class IndexCandidate:
    table: str
    columns: List[str]
    index_type: str
    reason: str
    query_frequency: int
    estimated_benefit: float
    estimated_cost: float
    roi: float

class SQLParser:
    """SQLè§£æå™¨"""

    def parse(self, query: str) -> Dict:
        """è§£æSQLæŸ¥è¯¢"""
        parsed = sqlparse.parse(query)[0]

        result = {
            'tables': [],
            'where_clauses': [],
            'join_clauses': [],
            'order_by': [],
            'group_by': []
        }

        # æå–è¡¨å
        result['tables'] = self._extract_tables(parsed)

        # æå–WHEREæ¡ä»¶
        result['where_clauses'] = self._extract_where_clauses(parsed)

        # æå–JOINæ¡ä»¶
        result['join_clauses'] = self._extract_join_clauses(parsed)

        # æå–ORDER BY
        result['order_by'] = self._extract_order_by(parsed)

        return result

    def _extract_tables(self, parsed: Statement) -> List[str]:
        """æå–è¡¨å"""
        tables = []
        from_seen = False

        for token in parsed.flatten():
            if token.ttype is Keyword and token.value.upper() == 'FROM':
                from_seen = True
            elif from_seen and token.ttype is None:
                # ç®€å•æå–ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„è§£æ
                table = token.value.split('.')[-1].strip('"')
                if table and table not in tables:
                    tables.append(table)

        return tables

    def _extract_where_clauses(self, parsed: Statement) -> List[Dict]:
        """æå–WHEREæ¡ä»¶"""
        clauses = []
        where_pattern = re.compile(r'(\w+)\s*([=<>]+|IN|LIKE)\s*', re.IGNORECASE)

        # ç®€åŒ–ç‰ˆï¼šæ­£åˆ™æå–
        query_str = str(parsed)
        if 'WHERE' in query_str.upper():
            where_part = query_str.upper().split('WHERE')[1].split('ORDER')[0].split('GROUP')[0]
            matches = where_pattern.findall(where_part)
            for col, op in matches:
                clauses.append({
                    'column': col,
                    'operator': op,
                    'indexable': op in ['=', 'IN', '>', '<', 'LIKE']
                })

        return clauses

    def _extract_join_clauses(self, parsed: Statement) -> List[Dict]:
        """æå–JOINæ¡ä»¶"""
        joins = []
        join_pattern = re.compile(r'JOIN\s+(\w+)\s+ON\s+(\w+)\.(\w+)\s*=\s*(\w+)\.(\w+)', re.IGNORECASE)

        query_str = str(parsed)
        matches = join_pattern.findall(query_str)
        for match in matches:
            joins.append({
                'table1': match[1],
                'table1_col': match[2],
                'table2': match[3],
                'table2_col': match[4]
            })

        return joins

    def _extract_order_by(self, parsed: Statement) -> List[str]:
        """æå–ORDER BYåˆ—"""
        columns = []
        order_pattern = re.compile(r'ORDER\s+BY\s+([^,\s]+)', re.IGNORECASE)

        query_str = str(parsed)
        matches = order_pattern.findall(query_str)
        for match in matches:
            col = match.strip().split('.')[-1]
            columns.append(col)

        return columns
```

### 5.2 ç´¢å¼•æ”¶ç›Šè¯„ä¼°

```python
class IndexBenefitAnalyzer:
    """ç´¢å¼•æ”¶ç›Šåˆ†æå™¨"""

    def __init__(self, db_config: dict):
        self.db_config = db_config
        self.conn = None

    def connect(self):
        self.conn = psycopg2.connect(**self.db_config)

    def estimate_benefit(self,
                        query: str,
                        index_candidate: IndexCandidate,
                        execution_stats: Dict) -> float:
        """ä¼°ç®—ç´¢å¼•æ”¶ç›Š"""
        cur = self.conn.cursor()

        # 1. è·å–å½“å‰æ‰§è¡Œè®¡åˆ’
        cur.execute(f"EXPLAIN (ANALYZE, BUFFERS) {query}")
        current_plan = cur.fetchall()
        current_cost = self._extract_cost_from_plan(current_plan)
        current_time = execution_stats.get('mean_exec_time', 0)

        # 2. åˆ›å»ºå‡è®¾ç´¢å¼•ï¼ˆä½¿ç”¨hypothetical indexæ‰©å±•æˆ–ä¼°ç®—ï¼‰
        # ç®€åŒ–ç‰ˆï¼šåŸºäºç»Ÿè®¡ä¿¡æ¯ä¼°ç®—
        estimated_cost = self._estimate_cost_with_index(
            query, index_candidate, current_cost
        )

        # 3. è®¡ç®—æ”¶ç›Š
        time_saved = max(0, current_time - estimated_cost)
        query_frequency = execution_stats.get('calls', 0)

        # æ—¥æ”¶ç›Š = èŠ‚çœæ—¶é—´ Ã— æŸ¥è¯¢é¢‘ç‡
        daily_benefit_seconds = time_saved * query_frequency / 86400

        return daily_benefit_seconds

    def _extract_cost_from_plan(self, plan: List) -> float:
        """ä»æ‰§è¡Œè®¡åˆ’æå–æˆæœ¬"""
        # ç®€åŒ–ç‰ˆï¼šä»EXPLAINè¾“å‡ºè§£æ
        cost_pattern = re.compile(r'cost=(\d+\.\d+)\.\.(\d+\.\d+)')
        total_cost = 0.0

        for row in plan:
            matches = cost_pattern.findall(str(row))
            if matches:
                total_cost += float(matches[0][1])  # ä½¿ç”¨ä¸Šé™

        return total_cost

    def _estimate_cost_with_index(self,
                                 query: str,
                                 index_candidate: IndexCandidate,
                                 current_cost: float) -> float:
        """ä¼°ç®—ä½¿ç”¨ç´¢å¼•åçš„æˆæœ¬"""
        # ç®€åŒ–æ¨¡å‹ï¼šç´¢å¼•æ‰«ææˆæœ¬ = è¡¨å¤§å° / é€‰æ‹©æ€§

        # è·å–è¡¨ç»Ÿè®¡
        cur = self.conn.cursor()
        cur.execute(f"""
            SELECT
                reltuples as row_count,
                pg_total_relation_size('{index_candidate.table}') as table_size
            FROM pg_class
            WHERE relname = '{index_candidate.table}';
        """)

        row = cur.fetchone()
        if not row:
            return current_cost * 0.1  # å‡è®¾ç´¢å¼•èƒ½é™ä½90%æˆæœ¬

        row_count, table_size = row

        # ä¼°ç®—é€‰æ‹©æ€§ï¼ˆç®€åŒ–ï¼šå‡è®¾å‡åŒ€åˆ†å¸ƒï¼‰
        selectivity = 1.0 / row_count if row_count > 0 else 1.0

        # ç´¢å¼•æ‰«ææˆæœ¬ = log2(è¡Œæ•°) + éšæœºIOæˆæœ¬
        index_cost = (
            np.log2(max(row_count, 1)) * 0.01 +  # CPUæˆæœ¬
            selectivity * table_size * 0.0001     # IOæˆæœ¬
        )

        return min(index_cost, current_cost * 0.1)  # è‡³å°‘é™ä½90%

    def estimate_index_cost(self, index_candidate: IndexCandidate) -> Dict:
        """ä¼°ç®—ç´¢å¼•æˆæœ¬"""
        cur = self.conn.cursor()

        # è·å–è¡¨å¤§å°
        cur.execute(f"""
            SELECT
                pg_total_relation_size('{index_candidate.table}') as table_size,
                reltuples as row_count
            FROM pg_class
            WHERE relname = '{index_candidate.table}';
        """)

        row = cur.fetchone()
        if not row:
            return {'storage_mb': 0, 'write_overhead_ms': 0}

        table_size, row_count = row

        # ä¼°ç®—ç´¢å¼•å¤§å°
        # ç®€åŒ–ï¼šç´¢å¼•å¤§å° â‰ˆ è¡¨å¤§å° Ã— (åˆ—æ•° / æ€»åˆ—æ•°) Ã— 1.2
        column_count = len(index_candidate.columns)
        estimated_index_size = table_size * (column_count / 10.0) * 1.2

        # å†™å…¥å¼€é”€ï¼ˆæ¯MBç´¢å¼•å¢åŠ 0.1mså»¶è¿Ÿï¼‰
        write_overhead = (estimated_index_size / 1024 / 1024) * 0.1

        return {
            'storage_mb': estimated_index_size / 1024 / 1024,
            'write_overhead_ms': write_overhead
        }
```

### 5.3 å®Œæ•´æ¨èå™¨å®ç°

```python
class IndexRecommender:
    """è‡ªåŠ¨ç´¢å¼•æ¨èå™¨"""

    def __init__(self, db_config: dict):
        self.db_config = db_config
        self.parser = SQLParser()
        self.analyzer = IndexBenefitAnalyzer(db_config)
        self.analyzer.connect()

    def analyze_slow_queries(self, min_time_ms: float = 100) -> List[IndexCandidate]:
        """åˆ†ææ…¢æŸ¥è¯¢å¹¶æ¨èç´¢å¼•"""
        cur = self.analyzer.conn.cursor()

        # è·å–æ…¢æŸ¥è¯¢
        cur.execute("""
            SELECT
                query,
                calls,
                mean_exec_time,
                total_exec_time,
                (shared_blks_hit + shared_blks_read) as total_blocks
            FROM pg_stat_statements
            WHERE mean_exec_time > %s
            ORDER BY mean_exec_time DESC
            LIMIT 50;
        """, (min_time_ms,))

        slow_queries = cur.fetchall()

        # æ”¶é›†æ‰€æœ‰ç´¢å¼•å€™é€‰
        all_candidates = defaultdict(lambda: {
            'table': '',
            'columns': [],
            'reasons': [],
            'query_frequency': 0,
            'total_benefit': 0.0
        })

        for query_text, calls, mean_time, total_time, blocks in slow_queries:
            # è§£ææŸ¥è¯¢
            parsed = self.parser.parse(query_text)

            # ç”Ÿæˆå€™é€‰ç´¢å¼•
            candidates = self._generate_candidates(parsed, {
                'calls': calls,
                'mean_exec_time': mean_time,
                'total_exec_time': total_time,
                'blocks': blocks
            })

            # åˆå¹¶å€™é€‰
            for candidate in candidates:
                key = (candidate.table, tuple(candidate.columns))
                all_candidates[key]['table'] = candidate.table
                all_candidates[key]['columns'] = candidate.columns
                all_candidates[key]['reasons'].append(candidate.reason)
                all_candidates[key]['query_frequency'] += calls
                all_candidates[key]['total_benefit'] += candidate.estimated_benefit

        # è½¬æ¢ä¸ºIndexCandidateå¹¶è¯„ä¼°
        final_candidates = []
        for key, data in all_candidates.items():
            candidate = IndexCandidate(
                table=data['table'],
                columns=data['columns'],
                index_type='B-Tree',  # é»˜è®¤
                reason='; '.join(data['reasons'][:3]),
                query_frequency=data['query_frequency'],
                estimated_benefit=data['total_benefit'],
                estimated_cost=0.0,
                roi=0.0
            )

            # è¯„ä¼°æˆæœ¬
            cost_info = self.analyzer.estimate_index_cost(candidate)
            candidate.estimated_cost = cost_info['storage_mb'] * 0.1 + cost_info['write_overhead_ms'] * 0.001

            # è®¡ç®—ROI
            if candidate.estimated_cost > 0:
                candidate.roi = candidate.estimated_benefit / candidate.estimated_cost
            else:
                candidate.roi = float('inf')

            final_candidates.append(candidate)

        # æŒ‰ROIæ’åº
        final_candidates.sort(key=lambda x: x.roi, reverse=True)

        return final_candidates

    def _generate_candidates(self, parsed: Dict, stats: Dict) -> List[IndexCandidate]:
        """ç”Ÿæˆç´¢å¼•å€™é€‰"""
        candidates = []

        # WHEREæ¡ä»¶ç´¢å¼•
        for clause in parsed['where_clauses']:
            if clause['indexable']:
                candidate = IndexCandidate(
                    table=parsed['tables'][0] if parsed['tables'] else 'unknown',
                    columns=[clause['column']],
                    index_type='B-Tree',
                    reason=f"WHERE {clause['column']} {clause['operator']}",
                    query_frequency=stats['calls'],
                    estimated_benefit=self.analyzer.estimate_benefit(
                        '', candidate, stats
                    ),
                    estimated_cost=0.0,
                    roi=0.0
                )
                candidates.append(candidate)

        # JOINç´¢å¼•
        for join in parsed['join_clauses']:
            candidate = IndexCandidate(
                table=join['table1'],
                columns=[join['table1_col']],
                index_type='B-Tree',
                reason=f"JOIN on {join['table1_col']}",
                query_frequency=stats['calls'],
                estimated_benefit=0.0,
                estimated_cost=0.0,
                roi=0.0
            )
            candidates.append(candidate)

        # ORDER BYç´¢å¼•
        if parsed['order_by']:
            candidate = IndexCandidate(
                table=parsed['tables'][0] if parsed['tables'] else 'unknown',
                columns=parsed['order_by'],
                index_type='B-Tree',
                reason='ORDER BY optimization',
                query_frequency=stats['calls'],
                estimated_benefit=0.0,
                estimated_cost=0.0,
                roi=0.0
            )
            candidates.append(candidate)

        return candidates

    def detect_redundant_indexes(self) -> List[Dict]:
        """æ£€æµ‹å†—ä½™ç´¢å¼•"""
        cur = self.analyzer.conn.cursor()

        cur.execute("""
            WITH index_columns AS (
                SELECT
                    i.indexrelid::regclass AS index_name,
                    i.indrelid::regclass AS table_name,
                    array_agg(a.attname ORDER BY array_position(i.indkey, a.attnum)) AS columns,
                    pg_relation_size(i.indexrelid) AS index_size
                FROM pg_index i
                JOIN pg_attribute a ON a.attrelid = i.indrelid
                    AND a.attnum = ANY(i.indkey)
                WHERE i.indisvalid
                GROUP BY i.indexrelid, i.indrelid
            )
            SELECT
                i1.index_name::text AS redundant_index,
                i2.index_name::text AS covering_index,
                i1.columns AS redundant_columns,
                i2.columns AS covering_columns,
                pg_size_pretty(i1.index_size) AS redundant_size
            FROM index_columns i1
            JOIN index_columns i2 ON i1.table_name = i2.table_name
            WHERE i1.index_name < i2.index_name
              AND i1.columns <@ i2.columns  -- i1æ˜¯i2çš„å‰ç¼€
              AND i1.index_size > 100 * 1024 * 1024;  -- å¤§äº100MB
        """)

        redundant = []
        for row in cur.fetchall():
            redundant.append({
                'redundant_index': row[0],
                'covering_index': row[1],
                'redundant_columns': row[2],
                'covering_columns': row[3],
                'size': row[4],
                'recommendation': f"å¯ä»¥åˆ é™¤ {row[0]}ï¼Œ{row[1]} å·²è¦†ç›–å…¶åŠŸèƒ½"
            })

        return redundant

    def generate_sql_script(self,
                           candidates: List[IndexCandidate],
                           min_roi: float = 10.0,
                           limit: int = 20) -> str:
        """ç”ŸæˆSQLè„šæœ¬"""
        script = []
        script.append("-- Index Recommendation Report")
        script.append(f"-- Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        script.append(f"-- Min ROI: {min_roi}")
        script.append("")

        # è¿‡æ»¤ä½ROI
        filtered = [c for c in candidates if c.roi >= min_roi][:limit]

        script.append(f"-- Total Recommendations: {len(filtered)}")
        script.append(f"-- Estimated Total Benefit: {sum(c.estimated_benefit for c in filtered):.2f} seconds/day")
        script.append("")

        for i, candidate in enumerate(filtered, 1):
            script.append(f"-- Recommendation #{i} (ROI: {candidate.roi:.1f}Ã—)")
            script.append(f"-- Reason: {candidate.reason}")
            script.append(f"-- Expected: {candidate.query_frequency} queries/day, "
                         f"{candidate.estimated_benefit:.2f}s faster total")

            columns_str = ', '.join(candidate.columns)
            script.append(f"CREATE INDEX CONCURRENTLY idx_{candidate.table}_{'_'.join(candidate.columns[:2])}")
            script.append(f"  ON {candidate.table}({columns_str});")
            script.append("")

        return '\n'.join(script)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    recommender = IndexRecommender({
        'host': 'localhost',
        'database': 'mydb',
        'user': 'postgres'
    })

    # åˆ†ææ…¢æŸ¥è¯¢
    candidates = recommender.analyze_slow_queries(min_time_ms=100)

    # æ£€æµ‹å†—ä½™ç´¢å¼•
    redundant = recommender.detect_redundant_indexes()

    # ç”ŸæˆSQLè„šæœ¬
    sql_script = recommender.generate_sql_script(candidates, min_roi=10.0)

    print("Top 10 Recommendations:")
    for i, c in enumerate(candidates[:10], 1):
        print(f"{i}. {c.table}({', '.join(c.columns)}) - ROI: {c.roi:.1f}Ã—")

    print("\nRedundant Indexes:")
    for r in redundant:
        print(f"  {r['redundant_index']} â†’ {r['recommendation']}")

    print("\nSQL Script:")
    print(sql_script)
```

### 5.4 å®é™…æ¡ˆä¾‹

**æ¡ˆä¾‹: ç”µå•†è®¢å•ç³»ç»Ÿç´¢å¼•ä¼˜åŒ–**

```text
åˆ†æç»“æœ:
â”œâ”€ æ…¢æŸ¥è¯¢: 25ä¸ªï¼ˆ>100msï¼‰
â”œâ”€ æ¨èç´¢å¼•: 12ä¸ª
â””â”€ å†—ä½™ç´¢å¼•: 3ä¸ª

Top 3æ¨è:
1. idx_orders_user_status (ROI: 350Ã—)
   â”œâ”€ åŸå› : WHERE user_id = ? AND status = 'pending'
   â”œâ”€ æ”¶ç›Š: 1000 queries/å¤© Ã— 80ms = 80ç§’/å¤©
   â””â”€ SQL: CREATE INDEX CONCURRENTLY idx_orders_user_status
           ON orders(user_id, status) WHERE status = 'pending';

2. idx_products_category_price (ROI: 220Ã—)
   â”œâ”€ åŸå› : WHERE category = ? ORDER BY price
   â”œâ”€ æ”¶ç›Š: 500 queries/å¤© Ã— 120ms = 60ç§’/å¤©
   â””â”€ SQL: CREATE INDEX CONCURRENTLY idx_products_category_price
           ON products(category, price);

3. idx_order_items_order_id (ROI: 180Ã—)
   â”œâ”€ åŸå› : JOIN order_items ON order_id
   â”œâ”€ æ”¶ç›Š: 2000 queries/å¤© Ã— 30ms = 60ç§’/å¤©
   â””â”€ SQL: CREATE INDEX CONCURRENTLY idx_order_items_order_id
           ON order_items(order_id);

å®æ–½æ•ˆæœ:
â”œâ”€ å¹³å‡æŸ¥è¯¢æ—¶é—´: 150ms â†’ 25ms (-83%) âœ“
â”œâ”€ P99å»¶è¿Ÿ: 500ms â†’ 80ms (-84%) âœ“
â””â”€ å­˜å‚¨å¼€é”€: +2GB (å¯æ¥å—) âœ“
```

---

**å·¥å…·ç‰ˆæœ¬**: 2.0.0ï¼ˆå¤§å¹…å……å®ï¼‰
**æœ€åæ›´æ–°**: 2025-12-05
**æ–°å¢å†…å®¹**: å®Œæ•´SQLè§£æã€æ”¶ç›Šè¯„ä¼°ã€å†—ä½™æ£€æµ‹ã€å®é™…æ¡ˆä¾‹

**å·¥å…·ä»£ç **: ç”Ÿäº§çº§Pythonå®ç°ï¼ˆå¯ç›´æ¥ä½¿ç”¨ï¼‰
**GitHub**: <https://github.com/db-theory/index-advisor>

**å…³è”æ–‡æ¡£**:

- `05-å®ç°æœºåˆ¶/01-PostgreSQL-MVCCå®ç°.md`
- `06-æ€§èƒ½åˆ†æ/03-å­˜å‚¨å¼€é”€åˆ†æ.md`
- `11-å·¥å…·ä¸è‡ªåŠ¨åŒ–/05-ç“¶é¢ˆè¯Šæ–­å™¨.md`
