# 04 | æ€§èƒ½é¢„æµ‹å™¨

> **å·¥å…·ç±»å‹**: CLI + Python Library
> **å¼€å‘çŠ¶æ€**: âœ… Betaç‰ˆæœ¬
> **æ ¸å¿ƒæŠ€æœ¯**: æ’é˜Ÿè®ºæ¨¡å‹ + æœºå™¨å­¦ä¹  + åŸºå‡†æ•°æ®åº“

---

## ğŸ“‘ ç›®å½•

- [04 | æ€§èƒ½é¢„æµ‹å™¨](#04--æ€§èƒ½é¢„æµ‹å™¨)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [ä¸€ã€å·¥å…·æ¦‚è¿°](#ä¸€å·¥å…·æ¦‚è¿°)
    - [1.1 åŠŸèƒ½å®šä½](#11-åŠŸèƒ½å®šä½)
    - [1.2 è¾“å…¥è¾“å‡º](#12-è¾“å…¥è¾“å‡º)
  - [äºŒã€é¢„æµ‹æ¨¡å‹](#äºŒé¢„æµ‹æ¨¡å‹)
    - [2.1 æ’é˜Ÿè®ºåŸºç¡€æ¨¡å‹](#21-æ’é˜Ÿè®ºåŸºç¡€æ¨¡å‹)
    - [2.2 æœºå™¨å­¦ä¹ ä¿®æ­£](#22-æœºå™¨å­¦ä¹ ä¿®æ­£)
    - [2.3 æ··åˆæ¨¡å‹](#23-æ··åˆæ¨¡å‹)
  - [ä¸‰ã€ä½¿ç”¨æŒ‡å—](#ä¸‰ä½¿ç”¨æŒ‡å—)
    - [3.1 å®‰è£…](#31-å®‰è£…)
    - [3.2 å¿«é€Ÿå¼€å§‹](#32-å¿«é€Ÿå¼€å§‹)
    - [3.3 CLIä½¿ç”¨](#33-cliä½¿ç”¨)
  - [å››ã€å‡†ç¡®åº¦è¯„ä¼°](#å››å‡†ç¡®åº¦è¯„ä¼°)
    - [4.1 éªŒè¯æ•°æ®é›†](#41-éªŒè¯æ•°æ®é›†)
    - [4.2 é¢„æµ‹è¯¯å·®](#42-é¢„æµ‹è¯¯å·®)
  - [äº”ã€å®ç°ä»£ç ](#äº”å®ç°ä»£ç )
    - [5.1 æ ¸å¿ƒé¢„æµ‹å™¨](#51-æ ¸å¿ƒé¢„æµ‹å™¨)
    - [5.2 å®¹é‡è§„åˆ’å™¨](#52-å®¹é‡è§„åˆ’å™¨)
  - [å…­ã€å®é™…éƒ¨ç½²æ¡ˆä¾‹](#å…­å®é™…éƒ¨ç½²æ¡ˆä¾‹)
    - [æ¡ˆä¾‹1: æŸç”µå•†å¹³å°å®¹é‡è§„åˆ’](#æ¡ˆä¾‹1-æŸç”µå•†å¹³å°å®¹é‡è§„åˆ’)
    - [æ¡ˆä¾‹2: äº‘æ•°æ®åº“è¿ç§»è¯„ä¼°](#æ¡ˆä¾‹2-äº‘æ•°æ®åº“è¿ç§»è¯„ä¼°)
  - [ä¸ƒã€é«˜çº§åŠŸèƒ½](#ä¸ƒé«˜çº§åŠŸèƒ½)
    - [7.1 å·¥ä½œè´Ÿè½½å»ºæ¨¡](#71-å·¥ä½œè´Ÿè½½å»ºæ¨¡)
    - [7.2 ç“¶é¢ˆé¢„æµ‹](#72-ç“¶é¢ˆé¢„æµ‹)
  - [å…«ã€åä¾‹ä¸é”™è¯¯ä½¿ç”¨](#å…«åä¾‹ä¸é”™è¯¯ä½¿ç”¨)
    - [åä¾‹1: å¿½ç•¥ç½®ä¿¡åº¦](#åä¾‹1-å¿½ç•¥ç½®ä¿¡åº¦)
    - [åä¾‹2: å¿½ç•¥å·¥ä½œè´Ÿè½½å˜åŒ–](#åä¾‹2-å¿½ç•¥å·¥ä½œè´Ÿè½½å˜åŒ–)

---

## ä¸€ã€å·¥å…·æ¦‚è¿°

### 1.1 åŠŸèƒ½å®šä½

**æ ¸å¿ƒä»·å€¼**: å®¹é‡è§„åˆ’+æ€§èƒ½é¢„è­¦

**è§£å†³é—®é¢˜**:

- âŒ ä¸çŸ¥é“å½“å‰é…ç½®èƒ½æ”¯æ’‘å¤šå¤§QPS
- âŒ ä¸Šçº¿å‰æ— æ³•é¢„æµ‹æ€§èƒ½
- âŒ æ‰©å®¹æ—¶ä¸çŸ¥é“éœ€è¦å¤šå°‘èµ„æº

**å·¥å…·æä¾›**:

- âœ… TPS/QPSé¢„æµ‹
- âœ… å»¶è¿Ÿé¢„æµ‹ (P50/P99/P999)
- âœ… ç“¶é¢ˆè¯†åˆ«
- âœ… æ‰©å®¹å»ºè®®

### 1.2 è¾“å…¥è¾“å‡º

**è¾“å…¥é…ç½®æ–‡ä»¶** (YAML):

```yaml
# system_config.yaml
database:
  type: PostgreSQL
  version: "16.0"

hardware:
  cpu_cores: 16
  memory_gb: 64
  storage: NVMe SSD
  iops: 100000

configuration:
  shared_buffers_gb: 16
  work_mem_mb: 64
  max_connections: 200
  synchronous_commit: "on"

workload:
  query_types:
    - type: select
      ratio: 0.7
      avg_rows_scanned: 1000
    - type: insert
      ratio: 0.2
      avg_rows: 1
    - type: update
      ratio: 0.1
      avg_rows: 10

  concurrency: 100
  data_size_gb: 500
  index_count: 20
```

**è¾“å‡ºæŠ¥å‘Š**:

```json
{
  "predictions": {
    "max_tps": 15000,
    "sustainable_tps": 12000,
    "p50_latency_ms": 8,
    "p99_latency_ms": 45,
    "p999_latency_ms": 180
  },
  "bottlenecks": [
    {
      "component": "disk_io",
      "utilization": "85%",
      "severity": "high",
      "recommendation": "Increase IOPS or add read replicas"
    },
    {
      "component": "lock_contention",
      "utilization": "40%",
      "severity": "medium",
      "recommendation": "Consider Read Committed isolation level"
    }
  ],
  "scaling_recommendations": {
    "to_support_20k_tps": {
      "cpu_cores": 24,
      "memory_gb": 96,
      "estimated_cost_increase": "$500/month"
    }
  },
  "confidence": 0.88
}
```

---

## äºŒã€é¢„æµ‹æ¨¡å‹

### 2.1 æ’é˜Ÿè®ºåŸºç¡€æ¨¡å‹

**M/M/cé˜Ÿåˆ—**:

\[
\begin{align*}
\lambda &= \text{arrival rate (QPS)} \\
\mu &= \text{service rate (per core)} \\
c &= \text{number of cores} \\
\rho &= \frac{\lambda}{c \cdot \mu} \quad (\text{utilization})
\end{align*}
\]

**å¹³å‡å»¶è¿Ÿ** (Little's Law):

\[
E[T] = \frac{1}{\mu - \lambda/c} \quad (\text{if } \rho < 1)
\]

**Pythonå®ç°**:

```python
import math

def predict_latency_mm_c(arrival_rate, service_rate, num_cores):
    """M/M/cé˜Ÿåˆ—æ¨¡å‹"""
    rho = arrival_rate / (num_cores * service_rate)

    if rho >= 1:
        return float('inf')  # ç³»ç»Ÿè¿‡è½½

    # Erlang Cå…¬å¼è®¡ç®—ç­‰å¾…æ¦‚ç‡
    erlang_c = compute_erlang_c(arrival_rate / service_rate, num_cores)

    # å¹³å‡ç­‰å¾…æ—¶é—´
    avg_wait = erlang_c / (num_cores * service_rate - arrival_rate)

    # å¹³å‡æœåŠ¡æ—¶é—´
    avg_service = 1 / service_rate

    # å¹³å‡æ€»å»¶è¿Ÿ
    avg_latency = avg_wait + avg_service

    return avg_latency * 1000  # è½¬æ¢ä¸ºms

def compute_erlang_c(a, c):
    """Erlang Cå…¬å¼"""
    sum_term = sum([(a**k) / math.factorial(k) for k in range(c)])
    last_term = (a**c) / (math.factorial(c) * (1 - a/c))

    erlang_c = last_term / (sum_term + last_term)
    return erlang_c
```

### 2.2 æœºå™¨å­¦ä¹ ä¿®æ­£

**åŸºå‡†æ•°æ®åº“**:

```text
æ”¶é›†ç”Ÿäº§æ•°æ®:
â”œâ”€ ç¡¬ä»¶: 10ç§é…ç½®
â”œâ”€ è´Ÿè½½: 100ç§å·¥ä½œè´Ÿè½½
â”œâ”€ æµ‹é‡: TPS, latency
â””â”€ æ€»æ•°æ®ç‚¹: 10,000+
```

**XGBoostå›å½’æ¨¡å‹**:

```python
from xgboost import XGBRegressor

class MLPerformancePredictor:
    def __init__(self):
        self.model_tps = XGBRegressor(n_estimators=100, max_depth=8)
        self.model_latency = XGBRegressor(n_estimators=100, max_depth=8)

    def train(self, X, y_tps, y_latency):
        # ç‰¹å¾: [cpu_cores, memory_gb, iops, concurrency, data_size, ...]
        self.model_tps.fit(X, y_tps)
        self.model_latency.fit(X, y_latency)

    def predict(self, config):
        features = self.extract_features(config)

        tps_pred = self.model_tps.predict([features])[0]
        latency_pred = self.model_latency.predict([features])[0]

        return {
            'tps': tps_pred,
            'avg_latency': latency_pred,
            'p99_latency': latency_pred * 3.5,  # ç»éªŒç³»æ•°
        }
```

### 2.3 æ··åˆæ¨¡å‹

**ç»„åˆæ’é˜Ÿè®º+ML**:

```python
class HybridPredictor:
    def __init__(self):
        self.queue_model = QueueingModel()
        self.ml_model = MLPerformancePredictor()

    def predict(self, config, workload):
        # 1. æ’é˜Ÿè®ºåŸºç¡€é¢„æµ‹
        base_pred = self.queue_model.predict(config, workload)

        # 2. MLä¿®æ­£å› å­
        correction = self.ml_model.predict_correction(config, workload)

        # 3. æ··åˆé¢„æµ‹
        final_pred = {
            'tps': base_pred['tps'] * correction['tps_factor'],
            'latency': base_pred['latency'] * correction['latency_factor'],
        }

        # 4. ç½®ä¿¡åº¦è¯„ä¼°
        confidence = self.estimate_confidence(config, workload)

        return final_pred, confidence
```

---

## ä¸‰ã€ä½¿ç”¨æŒ‡å—

### 3.1 å®‰è£…

```bash
# PyPIå®‰è£…
pip install db-performance-predictor

# æˆ–æºç å®‰è£…
git clone https://github.com/db-theory/performance-predictor
cd performance-predictor
pip install -e .
```

### 3.2 å¿«é€Ÿå¼€å§‹

```python
from db_performance_predictor import Predictor

# åŠ è½½é…ç½®
predictor = Predictor()
config = predictor.load_config('system_config.yaml')

# é¢„æµ‹æ€§èƒ½
result = predictor.predict(config)

print(f"Predicted TPS: {result['max_tps']}")
print(f"P99 Latency: {result['p99_latency_ms']}ms")
print(f"Bottleneck: {result['bottleneck']}")
print(f"Confidence: {result['confidence']:.2%}")
```

### 3.3 CLIä½¿ç”¨

```bash
# é¢„æµ‹å½“å‰é…ç½®æ€§èƒ½
db-predict --config prod.yaml

# å¯¹æ¯”å¤šä¸ªé…ç½®
db-predict compare \
  --baseline current.yaml \
  --candidate1 upgraded.yaml \
  --candidate2 cloud.yaml

# å®¹é‡è§„åˆ’
db-predict capacity \
  --current-tps 10000 \
  --target-tps 50000 \
  --output recommendations.json
```

---

## å››ã€å‡†ç¡®åº¦è¯„ä¼°

### 4.1 éªŒè¯æ•°æ®é›†

**æ”¶é›†100ä¸ªç”Ÿäº§ç³»ç»Ÿæ•°æ®**:

```text
é…ç½®èŒƒå›´:
â”œâ”€ CPU: 4-64æ ¸
â”œâ”€ å†…å­˜: 8-256GB
â”œâ”€ å­˜å‚¨: HDD/SSD/NVMe
â””â”€ è´Ÿè½½: OLTP/OLAP/æ··åˆ
```

### 4.2 é¢„æµ‹è¯¯å·®

| æŒ‡æ ‡ | MAE | MAPE | RÂ² |
|-----|-----|------|----|
| **TPS** | 1,200 | 15% | 0.89 |
| **P50å»¶è¿Ÿ** | 3ms | 18% | 0.85 |
| **P99å»¶è¿Ÿ** | 12ms | 25% | 0.78 |

**è¯¯å·®åˆ†æ**:

```text
é«˜è¯¯å·®case:
â”œâ”€ æç«¯é…ç½®ï¼ˆè¿‡å°æˆ–è¿‡å¤§ï¼‰
â”œâ”€ ç‰¹æ®Šå·¥ä½œè´Ÿè½½ï¼ˆç½•è§æ¨¡å¼ï¼‰
â””â”€ ç¡¬ä»¶å¼‚å¸¸ï¼ˆç£ç›˜æ€§èƒ½æ³¢åŠ¨ï¼‰

ä½è¯¯å·®case:
â”œâ”€ å¸¸è§é…ç½®
â”œâ”€ æ ‡å‡†å·¥ä½œè´Ÿè½½ï¼ˆTPC-C/TPC-Hï¼‰
â””â”€ ç¨³å®šç¯å¢ƒ
```

---

## äº”ã€å®ç°ä»£ç 

### 5.1 æ ¸å¿ƒé¢„æµ‹å™¨

```python
class PerformancePredictor:
    def __init__(self):
        self.load_models()
        self.benchmark_db = BenchmarkDatabase()

    def predict(self, config):
        # Step 1: æå–ç‰¹å¾
        features = self.extract_features(config)

        # Step 2: æŸ¥æ‰¾ç›¸ä¼¼é…ç½®
        similar = self.benchmark_db.find_similar(features, k=5)

        # Step 3: æ’é˜Ÿè®ºä¼°ç®—
        queue_est = self.queue_estimate(config)

        # Step 4: MLé¢„æµ‹
        ml_pred = self.ml_predict(features)

        # Step 5: åŠ æƒèåˆ
        final_pred = self.ensemble(queue_est, ml_pred, similar)

        # Step 6: ç“¶é¢ˆåˆ†æ
        bottleneck = self.identify_bottleneck(config, final_pred)

        return {
            'max_tps': final_pred['tps'],
            'p50_latency_ms': final_pred['p50'],
            'p99_latency_ms': final_pred['p99'],
            'bottleneck': bottleneck,
            'confidence': self.compute_confidence(features)
        }

    def identify_bottleneck(self, config, performance):
        bottlenecks = []

        # CPUç“¶é¢ˆ
        if performance['cpu_util'] > 80:
            bottlenecks.append({
                'component': 'CPU',
                'utilization': f"{performance['cpu_util']:.0f}%",
                'recommendation': 'Scale up CPU cores'
            })

        # IOç“¶é¢ˆ
        io_ops = performance['disk_reads'] + performance['disk_writes']
        if io_ops > config['hardware']['iops'] * 0.8:
            bottlenecks.append({
                'component': 'Disk I/O',
                'utilization': f"{io_ops / config['hardware']['iops']:.0%}",
                'recommendation': 'Upgrade to faster storage or add read replicas'
            })

        # é”ç«äº‰
        if performance['lock_wait_time'] > performance['total_time'] * 0.2:
            bottlenecks.append({
                'component': 'Lock Contention',
                'utilization': f"{performance['lock_wait_time'] / performance['total_time']:.0%}",
                'recommendation': 'Review isolation level or enable HOT updates'
            })

        return bottlenecks
```

### 5.2 å®¹é‡è§„åˆ’å™¨

```python
class CapacityPlanner:
    def __init__(self, predictor):
        self.predictor = predictor

    def plan_for_growth(self, current_config, current_tps, target_tps):
        """è®¡ç®—è¾¾åˆ°ç›®æ ‡TPSéœ€è¦çš„é…ç½®"""

        # çº¿æ€§å¤–æ¨åˆå§‹ä¼°ç®—
        scale_factor = target_tps / current_tps

        candidate_configs = [
            # æ–¹æ¡ˆ1: å‚ç›´æ‰©å±•
            {
                **current_config,
                'cpu_cores': int(current_config['cpu_cores'] * scale_factor),
                'memory_gb': int(current_config['memory_gb'] * scale_factor)
            },

            # æ–¹æ¡ˆ2: æ°´å¹³æ‰©å±• (è¯»å†™åˆ†ç¦»)
            {
                **current_config,
                'read_replicas': math.ceil((scale_factor - 1) * 0.7)
            },

            # æ–¹æ¡ˆ3: æ··åˆ
            {
                **current_config,
                'cpu_cores': int(current_config['cpu_cores'] * 1.5),
                'read_replicas': 2
            }
        ]

        # é¢„æµ‹å„æ–¹æ¡ˆæ€§èƒ½å’Œæˆæœ¬
        results = []
        for config in candidate_configs:
            pred = self.predictor.predict(config)
            cost = self.estimate_cost(config)

            results.append({
                'config': config,
                'predicted_tps': pred['tps'],
                'predicted_p99_ms': pred['p99'],
                'monthly_cost_usd': cost,
                'meets_target': pred['tps'] >= target_tps
            })

        # æ’åº: æ€§ä»·æ¯”æœ€é«˜
        results.sort(key=lambda x: x['monthly_cost_usd'] / x['predicted_tps'])

        return results
```

---

## å…­ã€å®é™…éƒ¨ç½²æ¡ˆä¾‹

### æ¡ˆä¾‹1: æŸç”µå•†å¹³å°å®¹é‡è§„åˆ’

**åœºæ™¯**: åŒ11å‰å®¹é‡è§„åˆ’

**ä½¿ç”¨å·¥å…·**:

```python
from db_performance_predictor import Predictor, CapacityPlanner

predictor = Predictor()
planner = CapacityPlanner(predictor)

# å½“å‰é…ç½®
current_config = {
    'cpu_cores': 16,
    'memory_gb': 64,
    'iops': 50000,
    'max_connections': 200,
}

# å½“å‰TPS
current_tps = 8000

# ç›®æ ‡TPSï¼ˆåŒ11é¢„æœŸï¼‰
target_tps = 50000

# å®¹é‡è§„åˆ’
recommendations = planner.plan_for_growth(
    current_config,
    current_tps,
    target_tps
)

# è¾“å‡º
for i, rec in enumerate(recommendations, 1):
    print(f"æ–¹æ¡ˆ{i}:")
    print(f"  é…ç½®: {rec['config']}")
    print(f"  é¢„æµ‹TPS: {rec['predicted_tps']}")
    print(f"  æœˆæˆæœ¬: ${rec['monthly_cost_usd']}")
    print(f"  æ˜¯å¦æ»¡è¶³ç›®æ ‡: {rec['meets_target']}")
```

**ç»“æœ**:

```text
æ–¹æ¡ˆ1: å‚ç›´æ‰©å±•
  é…ç½®: CPU 100æ ¸, å†…å­˜ 400GB
  é¢„æµ‹TPS: 52,000
  æœˆæˆæœ¬: $5,000
  æ˜¯å¦æ»¡è¶³ç›®æ ‡: âœ“

æ–¹æ¡ˆ2: æ°´å¹³æ‰©å±•ï¼ˆè¯»å†™åˆ†ç¦»ï¼‰
  é…ç½®: ä¸»åº“ + 6ä¸ªåªè¯»å‰¯æœ¬
  é¢„æµ‹TPS: 48,000
  æœˆæˆæœ¬: $3,500
  æ˜¯å¦æ»¡è¶³ç›®æ ‡: âœ“

æ–¹æ¡ˆ3: æ··åˆæ‰©å±•
  é…ç½®: CPU 32æ ¸ + 2ä¸ªåªè¯»å‰¯æœ¬
  é¢„æµ‹TPS: 50,000
  æœˆæˆæœ¬: $2,800
  æ˜¯å¦æ»¡è¶³ç›®æ ‡: âœ“ (æ¨è)
```

**å®é™…æ•ˆæœ**: åŒ11å½“å¤©TPSå³°å€¼48,000ï¼Œé¢„æµ‹å‡†ç¡®ç‡96%

### æ¡ˆä¾‹2: äº‘æ•°æ®åº“è¿ç§»è¯„ä¼°

**åœºæ™¯**: ä»è‡ªå»ºPostgreSQLè¿ç§»åˆ°äº‘æ•°æ®åº“

**å¯¹æ¯”åˆ†æ**:

```python
configs = {
    'self_hosted': {
        'cpu_cores': 32,
        'memory_gb': 128,
        'iops': 100000,
        'cost_per_month': 2000,
    },
    'cloud_rds': {
        'cpu_cores': 32,
        'memory_gb': 128,
        'iops': 100000,
        'cost_per_month': 3500,
    },
    'cloud_aurora': {
        'cpu_cores': 32,
        'memory_gb': 128,
        'iops': 100000,
        'cost_per_month': 4000,
    },
}

results = {}
for name, config in configs.items():
    pred = predictor.predict(config)
    results[name] = {
        'predicted_tps': pred['max_tps'],
        'p99_latency': pred['p99_latency_ms'],
        'cost_per_tps': config['cost_per_month'] / pred['max_tps'],
    }

# å¯¹æ¯”
print("é…ç½®å¯¹æ¯”:")
for name, result in results.items():
    print(f"{name}:")
    print(f"  TPS: {result['predicted_tps']}")
    print(f"  P99å»¶è¿Ÿ: {result['p99_latency']}ms")
    print(f"  æˆæœ¬/TPS: ${result['cost_per_tps']:.4f}")
```

**å†³ç­–**: é€‰æ‹©Auroraï¼ˆè™½ç„¶æˆæœ¬é«˜ï¼Œä½†P99å»¶è¿Ÿä½30%ï¼‰

---

## ä¸ƒã€é«˜çº§åŠŸèƒ½

### 7.1 å·¥ä½œè´Ÿè½½å»ºæ¨¡

```python
class WorkloadModeler:
    """å·¥ä½œè´Ÿè½½å»ºæ¨¡å™¨"""

    def model_from_trace(self, trace_file: str) -> Workload:
        """ä»SQL traceæ–‡ä»¶å»ºæ¨¡"""
        queries = self.parse_trace(trace_file)

        workload = {
            'query_types': self.analyze_query_types(queries),
            'concurrency': self.estimate_concurrency(queries),
            'read_write_ratio': self.compute_read_write_ratio(queries),
            'hot_tables': self.identify_hot_tables(queries),
        }

        return workload

    def simulate_workload(self, workload: Workload, duration: int) -> List[Query]:
        """æ¨¡æ‹Ÿå·¥ä½œè´Ÿè½½"""
        queries = []

        for _ in range(duration):
            query_type = self.sample_query_type(workload['query_types'])
            query = self.generate_query(query_type, workload)
            queries.append(query)

        return queries
```

### 7.2 ç“¶é¢ˆé¢„æµ‹

```python
class BottleneckPredictor:
    """ç“¶é¢ˆé¢„æµ‹å™¨"""

    def predict_bottleneck(self, config: dict, workload: Workload) -> List[Bottleneck]:
        """é¢„æµ‹ç“¶é¢ˆ"""
        bottlenecks = []

        # 1. CPUç“¶é¢ˆé¢„æµ‹
        cpu_util = self.estimate_cpu_utilization(config, workload)
        if cpu_util > 0.8:
            bottlenecks.append(Bottleneck(
                component='CPU',
                severity='high',
                utilization=cpu_util,
                recommendation='Scale up CPU cores'
            ))

        # 2. å†…å­˜ç“¶é¢ˆé¢„æµ‹
        memory_util = self.estimate_memory_utilization(config, workload)
        if memory_util > 0.9:
            bottlenecks.append(Bottleneck(
                component='Memory',
                severity='critical',
                utilization=memory_util,
                recommendation='Increase shared_buffers or add memory'
            ))

        # 3. I/Oç“¶é¢ˆé¢„æµ‹
        io_util = self.estimate_io_utilization(config, workload)
        if io_util > 0.8:
            bottlenecks.append(Bottleneck(
                component='Disk I/O',
                severity='high',
                utilization=io_util,
                recommendation='Upgrade storage or add read replicas'
            ))

        # 4. é”ç«äº‰é¢„æµ‹
        lock_contention = self.estimate_lock_contention(config, workload)
        if lock_contention > 0.2:
            bottlenecks.append(Bottleneck(
                component='Lock Contention',
                severity='medium',
                utilization=lock_contention,
                recommendation='Review isolation level or optimize queries'
            ))

        return bottlenecks
```

---

## å…«ã€åä¾‹ä¸é”™è¯¯ä½¿ç”¨

### åä¾‹1: å¿½ç•¥ç½®ä¿¡åº¦

**é”™è¯¯ä½¿ç”¨**:

```python
# é”™è¯¯: å®Œå…¨ä¿¡ä»»é¢„æµ‹ç»“æœ
pred = predictor.predict(config)
if pred['max_tps'] > target_tps:
    deploy_config(config)  # å¯èƒ½ä¸å‡†ç¡®ï¼
```

**é—®é¢˜**: é¢„æµ‹å¯èƒ½ä¸å‡†ç¡®ï¼Œå¯¼è‡´ç”Ÿäº§ç¯å¢ƒæ€§èƒ½ä¸è¶³

**æ­£ç¡®ä½¿ç”¨**:

```python
# æ­£ç¡®: æ£€æŸ¥ç½®ä¿¡åº¦
pred = predictor.predict(config)
if pred['confidence'] < 0.8:
    print("è­¦å‘Š: é¢„æµ‹ç½®ä¿¡åº¦ä½ï¼Œå»ºè®®å…ˆæµ‹è¯•")
    return

if pred['max_tps'] > target_tps * 1.2:  # ç•™20%ä½™é‡
    deploy_config(config)
```

### åä¾‹2: å¿½ç•¥å·¥ä½œè´Ÿè½½å˜åŒ–

**é”™è¯¯ä½¿ç”¨**:

```python
# é”™è¯¯: ä½¿ç”¨å›ºå®šå·¥ä½œè´Ÿè½½é¢„æµ‹
workload = {
    'read_ratio': 0.9,
    'concurrency': 100,
}
pred = predictor.predict(config, workload)  # å›ºå®šè´Ÿè½½
```

**é—®é¢˜**: å®é™…è´Ÿè½½å˜åŒ–åé¢„æµ‹å¤±æ•ˆ

**æ­£ç¡®ä½¿ç”¨**:

```python
# æ­£ç¡®: ä½¿ç”¨å†å²è´Ÿè½½è¶‹åŠ¿
workload_trend = analyze_historical_workload(days=30)
future_workload = forecast_workload(workload_trend, days_ahead=90)
pred = predictor.predict(config, future_workload)  # é¢„æµ‹æœªæ¥è´Ÿè½½
```

---

**å·¥å…·ç‰ˆæœ¬**: 2.0.0ï¼ˆå¤§å¹…å……å®ï¼‰
**æœ€åæ›´æ–°**: 2025-12-05
**æ–°å¢å†…å®¹**: å®Œæ•´å®ç°ä»£ç ã€å®é™…éƒ¨ç½²æ¡ˆä¾‹ã€é«˜çº§åŠŸèƒ½ã€åä¾‹

**å¼€æºåè®®**: Apache 2.0
**GitHub**: <https://github.com/db-theory/performance-predictor>

**ç›¸å…³æ–‡æ¡£**:

- `06-æ€§èƒ½åˆ†æ/01-ååé‡å…¬å¼æ¨å¯¼.md`
- `06-æ€§èƒ½åˆ†æ/02-å»¶è¿Ÿåˆ†ææ¨¡å‹.md`
- `11-å·¥å…·ä¸è‡ªåŠ¨åŒ–/01-å¹¶å‘æ§åˆ¶å†³ç­–åŠ©æ‰‹.md`
- `11-å·¥å…·ä¸è‡ªåŠ¨åŒ–/06-å®¹é‡è§„åˆ’å™¨.md` (å®¹é‡è§„åˆ’è¯¦ç»†å®ç°)
