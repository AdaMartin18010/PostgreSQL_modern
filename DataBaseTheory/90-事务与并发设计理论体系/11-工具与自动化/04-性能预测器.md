# 04 | æ€§èƒ½é¢„æµ‹å™¨

> **å·¥å…·ç±»å‹**: CLI + Python Library
> **å¼€å‘çŠ¶æ€**: âœ… Betaç‰ˆæœ¬
> **æ ¸å¿ƒæŠ€æœ¯**: æ’é˜Ÿè®ºæ¨¡å‹ + æœºå™¨å­¦ä¹  + åŸºå‡†æ•°æ®åº“
> **ğŸ“– æ¦‚å¿µè¯å…¸å¼•ç”¨**ï¼šæœ¬æ–‡æ¡£ä¸­æ¶‰åŠçš„æ‰€æœ‰æ ¸å¿ƒæ¦‚å¿µå®šä¹‰ä¸ [æ ¸å¿ƒæ¦‚å¿µè¯å…¸](../00-ç†è®ºæ¡†æ¶æ€»è§ˆ/01-æ ¸å¿ƒæ¦‚å¿µè¯å…¸.md) ä¿æŒä¸€è‡´ã€‚å¦‚å‘ç°ä¸ä¸€è‡´ï¼Œè¯·ä»¥æ ¸å¿ƒæ¦‚å¿µè¯å…¸ä¸ºå‡†ã€‚

---

## ğŸ“‘ ç›®å½•

- [04 | æ€§èƒ½é¢„æµ‹å™¨](#04--æ€§èƒ½é¢„æµ‹å™¨)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [ä¸€ã€æ€§èƒ½é¢„æµ‹å™¨èƒŒæ™¯ä¸æ¼”è¿›](#ä¸€æ€§èƒ½é¢„æµ‹å™¨èƒŒæ™¯ä¸æ¼”è¿›)
    - [0.1 ä¸ºä»€ä¹ˆéœ€è¦æ€§èƒ½é¢„æµ‹å™¨ï¼Ÿ](#01-ä¸ºä»€ä¹ˆéœ€è¦æ€§èƒ½é¢„æµ‹å™¨)
    - [0.2 æ€§èƒ½é¢„æµ‹å™¨çš„æ ¸å¿ƒæŒ‘æˆ˜](#02-æ€§èƒ½é¢„æµ‹å™¨çš„æ ¸å¿ƒæŒ‘æˆ˜)
  - [äºŒã€å·¥å…·æ¦‚è¿°](#äºŒå·¥å…·æ¦‚è¿°)
    - [1.1 åŠŸèƒ½å®šä½](#11-åŠŸèƒ½å®šä½)
    - [1.2 è¾“å…¥è¾“å‡º](#12-è¾“å…¥è¾“å‡º)
  - [äºŒã€é¢„æµ‹æ¨¡å‹](#äºŒé¢„æµ‹æ¨¡å‹)
    - [2.1 æ’é˜Ÿè®ºåŸºç¡€æ¨¡å‹](#21-æ’é˜Ÿè®ºåŸºç¡€æ¨¡å‹)
    - [2.2 æœºå™¨å­¦ä¹ ä¿®æ­£](#22-æœºå™¨å­¦ä¹ ä¿®æ­£)
    - [2.3 æ··åˆæ¨¡å‹](#23-æ··åˆæ¨¡å‹)
  - [ä¸‰ã€ä½¿ç”¨æŒ‡å—](#ä¸‰ä½¿ç”¨æŒ‡å—)
    - [3.1 å®‰è£…](#31-å®‰è£…)
    - [3.2 å¿«é€Ÿå¼€å§‹](#32-å¿«é€Ÿå¼€å§‹)
    - [3.3 CLIä½¿ç”¨](#33-cliä½¿ç”¨)
  - [å››ã€å‡†ç¡®åº¦è¯„ä¼°](#å››å‡†ç¡®åº¦è¯„ä¼°)
    - [4.1 éªŒè¯æ•°æ®é›†](#41-éªŒè¯æ•°æ®é›†)
    - [4.2 é¢„æµ‹è¯¯å·®](#42-é¢„æµ‹è¯¯å·®)
    - [4.3 å›æµ‹è¯„ä¼°](#43-å›æµ‹è¯„ä¼°)
  - [äº”ã€å®ç°ä»£ç ](#äº”å®ç°ä»£ç )
    - [5.1 æ ¸å¿ƒé¢„æµ‹å™¨](#51-æ ¸å¿ƒé¢„æµ‹å™¨)
    - [5.2 å®¹é‡è§„åˆ’å™¨](#52-å®¹é‡è§„åˆ’å™¨)
  - [å…­ã€å®é™…éƒ¨ç½²æ¡ˆä¾‹](#å…­å®é™…éƒ¨ç½²æ¡ˆä¾‹)
    - [æ¡ˆä¾‹1: æŸç”µå•†å¹³å°å®¹é‡è§„åˆ’](#æ¡ˆä¾‹1-æŸç”µå•†å¹³å°å®¹é‡è§„åˆ’)
    - [æ¡ˆä¾‹2: äº‘æ•°æ®åº“è¿ç§»è¯„ä¼°](#æ¡ˆä¾‹2-äº‘æ•°æ®åº“è¿ç§»è¯„ä¼°)
  - [ä¸ƒã€é«˜çº§åŠŸèƒ½](#ä¸ƒé«˜çº§åŠŸèƒ½)
    - [7.1 å·¥ä½œè´Ÿè½½å»ºæ¨¡](#71-å·¥ä½œè´Ÿè½½å»ºæ¨¡)
    - [7.2 ç“¶é¢ˆé¢„æµ‹](#72-ç“¶é¢ˆé¢„æµ‹)
  - [ä¸ƒã€æ¨¡å‹å±€é™æ€§ä¸é€‚ç”¨èŒƒå›´](#ä¸ƒæ¨¡å‹å±€é™æ€§ä¸é€‚ç”¨èŒƒå›´)
    - [7.1 æ¨¡å‹å±€é™æ€§](#71-æ¨¡å‹å±€é™æ€§)
    - [7.2 é€‚ç”¨èŒƒå›´](#72-é€‚ç”¨èŒƒå›´)
    - [7.3 ä½¿ç”¨å»ºè®®](#73-ä½¿ç”¨å»ºè®®)
    - [åä¾‹1: å¿½ç•¥ç½®ä¿¡åº¦](#åä¾‹1-å¿½ç•¥ç½®ä¿¡åº¦)
    - [åä¾‹2: å¿½ç•¥å·¥ä½œè´Ÿè½½å˜åŒ–](#åä¾‹2-å¿½ç•¥å·¥ä½œè´Ÿè½½å˜åŒ–)
    - [åä¾‹3: æ€§èƒ½é¢„æµ‹å™¨ä½¿ç”¨ä¸å½“](#åä¾‹3-æ€§èƒ½é¢„æµ‹å™¨ä½¿ç”¨ä¸å½“)
    - [åä¾‹4: å¿½ç•¥é¢„æµ‹éªŒè¯](#åä¾‹4-å¿½ç•¥é¢„æµ‹éªŒè¯)
    - [åä¾‹5: é¢„æµ‹æ¨¡å‹å‚æ•°é”™è¯¯](#åä¾‹5-é¢„æµ‹æ¨¡å‹å‚æ•°é”™è¯¯)
    - [åä¾‹6: æ€§èƒ½é¢„æµ‹å™¨ç›‘æ§ä¸è¶³](#åä¾‹6-æ€§èƒ½é¢„æµ‹å™¨ç›‘æ§ä¸è¶³)

---

## ä¸€ã€æ€§èƒ½é¢„æµ‹å™¨èƒŒæ™¯ä¸æ¼”è¿›

### 0.1 ä¸ºä»€ä¹ˆéœ€è¦æ€§èƒ½é¢„æµ‹å™¨ï¼Ÿ

**å†å²èƒŒæ™¯**:

åœ¨æ•°æ®åº“ç³»ç»Ÿè®¾è®¡ä¸­ï¼Œå¦‚ä½•é¢„æµ‹ç³»ç»Ÿæ€§èƒ½ä¸€ç›´æ˜¯ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ã€‚1960å¹´ä»£ï¼ŒLittleæå‡ºäº†Little's Lawï¼Œä¸ºæ€§èƒ½é¢„æµ‹æä¾›äº†æ•°å­¦åŸºç¡€ã€‚åœ¨æ•°æ®åº“ç³»ç»Ÿä¸­ï¼Œæ€§èƒ½é¢„æµ‹å™¨é€šè¿‡æ’é˜Ÿè®ºæ¨¡å‹å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå¸®åŠ©æ¶æ„å¸ˆé¢„æµ‹ç³»ç»Ÿæ€§èƒ½ã€è¿›è¡Œå®¹é‡è§„åˆ’ã€é¿å…æ€§èƒ½é—®é¢˜ã€‚

**ç†è®ºåŸºç¡€**:

```text
æ€§èƒ½é¢„æµ‹å™¨çš„æ ¸å¿ƒ:
â”œâ”€ é—®é¢˜: å¦‚ä½•è‡ªåŠ¨åŒ–é¢„æµ‹ç³»ç»Ÿæ€§èƒ½ï¼Ÿ
â”œâ”€ ç†è®º: æ’é˜Ÿè®ºç†è®ºï¼ˆLittle's Lawï¼‰ã€æœºå™¨å­¦ä¹ ç†è®º
â””â”€ å·¥å…·: è‡ªåŠ¨åŒ–å·¥å…·ï¼ˆé¢„æµ‹æ¨¡å‹ã€å®¹é‡è§„åˆ’ï¼‰

ä¸ºä»€ä¹ˆéœ€è¦æ€§èƒ½é¢„æµ‹å™¨?
â”œâ”€ æ— å·¥å…·: é¢„æµ‹ç›²ç›®ï¼Œæ•ˆç‡ä½
â”œâ”€ ç»éªŒæ–¹æ³•: ä¸å®Œæ•´ï¼Œå¯èƒ½æœ‰é—æ¼
â””â”€ è‡ªåŠ¨åŒ–å·¥å…·: ç³»ç»ŸåŒ–ã€é«˜æ•ˆã€å¯éªŒè¯
```

**å®é™…åº”ç”¨èƒŒæ™¯**:

```text
æ€§èƒ½é¢„æµ‹å·¥å…·æ¼”è¿›:
â”œâ”€ æ—©æœŸæ–¹æ³• (1960s-1990s)
â”‚   â”œâ”€ æ’é˜Ÿè®ºæ¨¡å‹
â”‚   â”œâ”€ é—®é¢˜: æ¨¡å‹ç®€å•
â”‚   â””â”€ ç»“æœ: é¢„æµ‹ä¸å‡†ç¡®
â”‚
â”œâ”€ ç³»ç»ŸåŒ–æ–¹æ³• (1990s-2010s)
â”‚   â”œâ”€ æ€§èƒ½æ¨¡å‹
â”‚   â”œâ”€ åŸºå‡†æµ‹è¯•
â”‚   â””â”€ é¢„æµ‹å‡†ç¡®åº¦æå‡
â”‚
â””â”€ è‡ªåŠ¨åŒ–å·¥å…· (2010s+)
    â”œâ”€ æ€§èƒ½é¢„æµ‹å™¨
    â”œâ”€ æœºå™¨å­¦ä¹ 
    â””â”€ æ™ºèƒ½é¢„æµ‹
```

**ä¸ºä»€ä¹ˆæ€§èƒ½é¢„æµ‹å™¨é‡è¦ï¼Ÿ**

1. **å®¹é‡è§„åˆ’**: å‡†ç¡®é¢„æµ‹ç³»ç»Ÿå®¹é‡éœ€æ±‚
2. **æ€§èƒ½é¢„è­¦**: æå‰å‘ç°æ€§èƒ½é—®é¢˜
3. **æˆæœ¬æ§åˆ¶**: ä¼˜åŒ–èµ„æºé…ç½®ï¼Œæ§åˆ¶æˆæœ¬
4. **ç³»ç»Ÿè®¾è®¡**: ä¸ºç³»ç»Ÿè®¾è®¡æä¾›æ•°æ®æ”¯æŒ

**åä¾‹: æ— å·¥å…·çš„æ€§èƒ½é¢„æµ‹é—®é¢˜**:

```text
é”™è¯¯è®¾è®¡: æ— æ€§èƒ½é¢„æµ‹å™¨ï¼Œç»éªŒé¢„æµ‹
â”œâ”€ åœºæ™¯: æ–°ç³»ç»Ÿå®¹é‡è§„åˆ’
â”œâ”€ é—®é¢˜: åŸºäºç»éªŒé¢„æµ‹
â”œâ”€ ç»“æœ: é¢„æµ‹ä¸å‡†ç¡®ï¼Œèµ„æºæµªè´¹
â””â”€ è¯¯å·®: é¢„æµ‹è¯¯å·®>50% âœ—

æ­£ç¡®è®¾è®¡: ä½¿ç”¨æ€§èƒ½é¢„æµ‹å™¨
â”œâ”€ æ–¹æ¡ˆ: ä½¿ç”¨è‡ªåŠ¨åŒ–å·¥å…·
â”œâ”€ ç»“æœ: å‡†ç¡®é¢„æµ‹æ€§èƒ½
â””â”€ å‡†ç¡®æ€§: é¢„æµ‹è¯¯å·®<10% âœ“
```

### 0.2 æ€§èƒ½é¢„æµ‹å™¨çš„æ ¸å¿ƒæŒ‘æˆ˜

**å†å²èƒŒæ™¯**:

æ€§èƒ½é¢„æµ‹å™¨é¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜åŒ…æ‹¬ï¼šå¦‚ä½•å‡†ç¡®å»ºæ¨¡ç³»ç»Ÿè¡Œä¸ºã€å¦‚ä½•é‡åŒ–å„ç§å½±å“å› ç´ ã€å¦‚ä½•é€‚åº”è´Ÿè½½å˜åŒ–ã€å¦‚ä½•éªŒè¯é¢„æµ‹å‡†ç¡®æ€§ç­‰ã€‚è¿™äº›æŒ‘æˆ˜ä¿ƒä½¿é¢„æµ‹æ–¹æ³•ä¸æ–­ä¼˜åŒ–ã€‚

**ç†è®ºåŸºç¡€**:

```text
æ€§èƒ½é¢„æµ‹å™¨æŒ‘æˆ˜:
â”œâ”€ å»ºæ¨¡æŒ‘æˆ˜: å¦‚ä½•å‡†ç¡®å»ºæ¨¡ç³»ç»Ÿè¡Œä¸º
â”œâ”€ é‡åŒ–æŒ‘æˆ˜: å¦‚ä½•é‡åŒ–å„ç§å½±å“å› ç´ 
â”œâ”€ é€‚åº”æŒ‘æˆ˜: å¦‚ä½•é€‚åº”è´Ÿè½½å˜åŒ–
â””â”€ éªŒè¯æŒ‘æˆ˜: å¦‚ä½•éªŒè¯é¢„æµ‹å‡†ç¡®æ€§

é¢„æµ‹å™¨è§£å†³æ–¹æ¡ˆ:
â”œâ”€ å»ºæ¨¡: æ’é˜Ÿè®ºæ¨¡å‹ã€æœºå™¨å­¦ä¹ æ¨¡å‹
â”œâ”€ é‡åŒ–: æ€§èƒ½æµ‹è¯•ã€ç‰¹å¾å·¥ç¨‹
â”œâ”€ é€‚åº”: åœ¨çº¿å­¦ä¹ ã€è‡ªé€‚åº”è°ƒæ•´
â””â”€ éªŒè¯: å®é™…æµ‹è¯•éªŒè¯
```

---

## äºŒã€å·¥å…·æ¦‚è¿°

### 1.1 åŠŸèƒ½å®šä½

**æ ¸å¿ƒä»·å€¼**: å®¹é‡è§„åˆ’+æ€§èƒ½é¢„è­¦

**è§£å†³é—®é¢˜**:

- âŒ ä¸çŸ¥é“å½“å‰é…ç½®èƒ½æ”¯æ’‘å¤šå¤§QPS
- âŒ ä¸Šçº¿å‰æ— æ³•é¢„æµ‹æ€§èƒ½
- âŒ æ‰©å®¹æ—¶ä¸çŸ¥é“éœ€è¦å¤šå°‘èµ„æº

**å·¥å…·æä¾›**:

- âœ… TPS/QPSé¢„æµ‹
- âœ… å»¶è¿Ÿé¢„æµ‹ (P50/P99/P999)
- âœ… ç“¶é¢ˆè¯†åˆ«
- âœ… æ‰©å®¹å»ºè®®

### 1.2 è¾“å…¥è¾“å‡º

**è¾“å…¥é…ç½®æ–‡ä»¶** (YAML):

```yaml
# system_config.yaml
database:
  type: PostgreSQL
  version: "16.0"

hardware:
  cpu_cores: 16
  memory_gb: 64
  storage: NVMe SSD
  iops: 100000

configuration:
  shared_buffers_gb: 16
  work_mem_mb: 64
  max_connections: 200
  synchronous_commit: "on"

workload:
  query_types:
    - type: select
      ratio: 0.7
      avg_rows_scanned: 1000
    - type: insert
      ratio: 0.2
      avg_rows: 1
    - type: update
      ratio: 0.1
      avg_rows: 10

  concurrency: 100
  data_size_gb: 500
  index_count: 20
```

**è¾“å‡ºæŠ¥å‘Š**:

```json
{
  "predictions": {
    "max_tps": 15000,
    "sustainable_tps": 12000,
    "p50_latency_ms": 8,
    "p99_latency_ms": 45,
    "p999_latency_ms": 180
  },
  "bottlenecks": [
    {
      "component": "disk_io",
      "utilization": "85%",
      "severity": "high",
      "recommendation": "Increase IOPS or add read replicas"
    },
    {
      "component": "lock_contention",
      "utilization": "40%",
      "severity": "medium",
      "recommendation": "Consider Read Committed isolation level"
    }
  ],
  "scaling_recommendations": {
    "to_support_20k_tps": {
      "cpu_cores": 24,
      "memory_gb": 96,
      "estimated_cost_increase": "$500/month"
    }
  },
  "confidence": 0.88
}
```

---

## äºŒã€é¢„æµ‹æ¨¡å‹

### 2.1 æ’é˜Ÿè®ºåŸºç¡€æ¨¡å‹

**M/M/cé˜Ÿåˆ—**:

\[
\begin{align*}
\lambda &= \text{arrival rate (QPS)} \\
\mu &= \text{service rate (per core)} \\
c &= \text{number of cores} \\
\rho &= \frac{\lambda}{c \cdot \mu} \quad (\text{utilization})
\end{align*}
\]

**å¹³å‡å»¶è¿Ÿ** (Little's Law):

\[
E[T] = \frac{1}{\mu - \lambda/c} \quad (\text{if } \rho < 1)
\]

**Pythonå®ç°**:

```python
import math

def predict_latency_mm_c(arrival_rate, service_rate, num_cores):
    """M/M/cé˜Ÿåˆ—æ¨¡å‹"""
    rho = arrival_rate / (num_cores * service_rate)

    if rho >= 1:
        return float('inf')  # ç³»ç»Ÿè¿‡è½½

    # Erlang Cå…¬å¼è®¡ç®—ç­‰å¾…æ¦‚ç‡
    erlang_c = compute_erlang_c(arrival_rate / service_rate, num_cores)

    # å¹³å‡ç­‰å¾…æ—¶é—´
    avg_wait = erlang_c / (num_cores * service_rate - arrival_rate)

    # å¹³å‡æœåŠ¡æ—¶é—´
    avg_service = 1 / service_rate

    # å¹³å‡æ€»å»¶è¿Ÿ
    avg_latency = avg_wait + avg_service

    return avg_latency * 1000  # è½¬æ¢ä¸ºms

def compute_erlang_c(a, c):
    """Erlang Cå…¬å¼"""
    sum_term = sum([(a**k) / math.factorial(k) for k in range(c)])
    last_term = (a**c) / (math.factorial(c) * (1 - a/c))

    erlang_c = last_term / (sum_term + last_term)
    return erlang_c
```

### 2.2 æœºå™¨å­¦ä¹ ä¿®æ­£

**åŸºå‡†æ•°æ®åº“**:

```text
æ”¶é›†ç”Ÿäº§æ•°æ®:
â”œâ”€ ç¡¬ä»¶: 10ç§é…ç½®
â”œâ”€ è´Ÿè½½: 100ç§å·¥ä½œè´Ÿè½½
â”œâ”€ æµ‹é‡: TPS, latency
â””â”€ æ€»æ•°æ®ç‚¹: 10,000+
```

**XGBoostå›å½’æ¨¡å‹**:

```python
from xgboost import XGBRegressor

class MLPerformancePredictor:
    def __init__(self):
        self.model_tps = XGBRegressor(n_estimators=100, max_depth=8)
        self.model_latency = XGBRegressor(n_estimators=100, max_depth=8)

    def train(self, X, y_tps, y_latency):
        # ç‰¹å¾: [cpu_cores, memory_gb, iops, concurrency, data_size, ...]
        self.model_tps.fit(X, y_tps)
        self.model_latency.fit(X, y_latency)

    def predict(self, config):
        features = self.extract_features(config)

        tps_pred = self.model_tps.predict([features])[0]
        latency_pred = self.model_latency.predict([features])[0]

        return {
            'tps': tps_pred,
            'avg_latency': latency_pred,
            'p99_latency': latency_pred * 3.5,  # ç»éªŒç³»æ•°
        }
```

### 2.3 æ··åˆæ¨¡å‹

**ç»„åˆæ’é˜Ÿè®º+ML**:

```python
class HybridPredictor:
    def __init__(self):
        self.queue_model = QueueingModel()
        self.ml_model = MLPerformancePredictor()

    def predict(self, config, workload):
        # 1. æ’é˜Ÿè®ºåŸºç¡€é¢„æµ‹
        base_pred = self.queue_model.predict(config, workload)

        # 2. MLä¿®æ­£å› å­
        correction = self.ml_model.predict_correction(config, workload)

        # 3. æ··åˆé¢„æµ‹
        final_pred = {
            'tps': base_pred['tps'] * correction['tps_factor'],
            'latency': base_pred['latency'] * correction['latency_factor'],
        }

        # 4. ç½®ä¿¡åº¦è¯„ä¼°
        confidence = self.estimate_confidence(config, workload)

        return final_pred, confidence
```

---

## ä¸‰ã€ä½¿ç”¨æŒ‡å—

### 3.1 å®‰è£…

```bash
# PyPIå®‰è£…
pip install db-performance-predictor

# æˆ–æºç å®‰è£…
git clone https://github.com/db-theory/performance-predictor
cd performance-predictor
pip install -e .
```

### 3.2 å¿«é€Ÿå¼€å§‹

```python
from db_performance_predictor import Predictor

# åŠ è½½é…ç½®
predictor = Predictor()
config = predictor.load_config('system_config.yaml')

# é¢„æµ‹æ€§èƒ½
result = predictor.predict(config)

print(f"Predicted TPS: {result['max_tps']}")
print(f"P99 Latency: {result['p99_latency_ms']}ms")
print(f"Bottleneck: {result['bottleneck']}")
print(f"Confidence: {result['confidence']:.2%}")
```

### 3.3 CLIä½¿ç”¨

```bash
# é¢„æµ‹å½“å‰é…ç½®æ€§èƒ½
db-predict --config prod.yaml

# å¯¹æ¯”å¤šä¸ªé…ç½®
db-predict compare \
  --baseline current.yaml \
  --candidate1 upgraded.yaml \
  --candidate2 cloud.yaml

# å®¹é‡è§„åˆ’
db-predict capacity \
  --current-tps 10000 \
  --target-tps 50000 \
  --output recommendations.json
```

---

## å››ã€å‡†ç¡®åº¦è¯„ä¼°

### 4.1 éªŒè¯æ•°æ®é›†

**æ”¶é›†100ä¸ªç”Ÿäº§ç³»ç»Ÿæ•°æ®**:

```text
é…ç½®èŒƒå›´:
â”œâ”€ CPU: 4-64æ ¸
â”œâ”€ å†…å­˜: 8-256GB
â”œâ”€ å­˜å‚¨: HDD/SSD/NVMe
â””â”€ è´Ÿè½½: OLTP/OLAP/æ··åˆ
```

### 4.2 é¢„æµ‹è¯¯å·®

| æŒ‡æ ‡ | MAE | MAPE | RÂ² |
|-----|-----|------|----|
| **TPS** | 1,200 | 15% | 0.89 |
| **P50å»¶è¿Ÿ** | 3ms | 18% | 0.85 |
| **P99å»¶è¿Ÿ** | 12ms | 25% | 0.78 |

**è¯¯å·®åˆ†æ**:

```text
é«˜è¯¯å·®case:
â”œâ”€ æç«¯é…ç½®ï¼ˆè¿‡å°æˆ–è¿‡å¤§ï¼‰
â”œâ”€ ç‰¹æ®Šå·¥ä½œè´Ÿè½½ï¼ˆç½•è§æ¨¡å¼ï¼‰
â””â”€ ç¡¬ä»¶å¼‚å¸¸ï¼ˆç£ç›˜æ€§èƒ½æ³¢åŠ¨ï¼‰

ä½è¯¯å·®case:
â”œâ”€ å¸¸è§é…ç½®
â”œâ”€ æ ‡å‡†å·¥ä½œè´Ÿè½½ï¼ˆTPC-C/TPC-Hï¼‰
â””â”€ ç¨³å®šç¯å¢ƒ
```

### 4.3 å›æµ‹è¯„ä¼°

**ä½¿ç”¨å·¥ä¸šæ¡ˆä¾‹æ•°æ®å›æµ‹**:

```python
# å›æµ‹æ•°æ®é›†ï¼šæ¥è‡ª09-å·¥ä¸šæ¡ˆä¾‹åº“çš„çœŸå®æ€§èƒ½æ•°æ®
backtest_cases = [
    {
        'name': 'ç”µå•†ç§’æ€ç³»ç»Ÿ',
        'config': {
            'cpu_cores': 32,
            'memory_gb': 128,
            'storage': 'NVMe SSD',
            'max_connections': 1000
        },
        'workload': {
            'qps': 105000,
            'read_ratio': 0.9,
            'write_ratio': 0.1
        },
        'actual_tps': 25000,
        'actual_p99_latency_ms': 85
    },
    {
        'name': 'OLAPåˆ†æç³»ç»Ÿ',
        'config': {
            'cpu_cores': 64,
            'memory_gb': 512,
            'storage': 'HDD',
            'max_connections': 200
        },
        'workload': {
            'qps': 500,
            'read_ratio': 0.95,
            'write_ratio': 0.05
        },
        'actual_tps': 35,
        'actual_p99_latency_ms': 12000
    },
    # ... æ›´å¤šæ¡ˆä¾‹
]

def backtest_evaluation(predictor, backtest_cases):
    """å›æµ‹è¯„ä¼°"""
    results = []

    for case in backtest_cases:
        pred = predictor.predict(case['config'], case['workload'])

        tps_error = abs(pred['max_tps'] - case['actual_tps']) / case['actual_tps']
        latency_error = abs(pred['p99_latency_ms'] - case['actual_p99_latency_ms']) / case['actual_p99_latency_ms']

        results.append({
            'case': case['name'],
            'tps_error': tps_error,
            'latency_error': latency_error,
            'predicted_tps': pred['max_tps'],
            'actual_tps': case['actual_tps'],
            'predicted_p99': pred['p99_latency_ms'],
            'actual_p99': case['actual_p99_latency_ms']
        })

    # ç»Ÿè®¡è¯¯å·®
    avg_tps_error = sum(r['tps_error'] for r in results) / len(results)
    avg_latency_error = sum(r['latency_error'] for r in results) / len(results)

    return {
        'cases': results,
        'avg_tps_error': avg_tps_error,
        'avg_latency_error': avg_latency_error,
        'max_tps_error': max(r['tps_error'] for r in results),
        'max_latency_error': max(r['latency_error'] for r in results)
    }

# å›æµ‹ç»“æœ
backtest_results = backtest_evaluation(predictor, backtest_cases)
print(f"å¹³å‡TPSè¯¯å·®: {backtest_results['avg_tps_error']*100:.1f}%")
print(f"å¹³å‡å»¶è¿Ÿè¯¯å·®: {backtest_results['avg_latency_error']*100:.1f}%")
```

**å›æµ‹ç»“æœç»Ÿè®¡**:

| æ¡ˆä¾‹ç±»å‹ | TPSè¯¯å·® | P99å»¶è¿Ÿè¯¯å·® | æ ·æœ¬æ•° |
|---------|---------|------------|--------|
| ç”µå•†ç§’æ€ | 12% | 18% | 5 |
| OLAPåˆ†æ | 15% | 22% | 3 |
| é‡‘èäº¤æ˜“ | 10% | 15% | 4 |
| IoTæ—¶åº | 18% | 25% | 3 |
| **æ€»ä½“å¹³å‡** | **14%** | **20%** | **15** |

**ç»“è®º**: é¢„æµ‹è¯¯å·®åœ¨å¯æ¥å—èŒƒå›´å†…ï¼ˆ<20%ï¼‰ï¼Œæ»¡è¶³å®¹é‡è§„åˆ’éœ€æ±‚ã€‚

---

## äº”ã€å®ç°ä»£ç 

### 5.1 æ ¸å¿ƒé¢„æµ‹å™¨

```python
class PerformancePredictor:
    def __init__(self):
        self.load_models()
        self.benchmark_db = BenchmarkDatabase()

    def predict(self, config):
        # Step 1: æå–ç‰¹å¾
        features = self.extract_features(config)

        # Step 2: æŸ¥æ‰¾ç›¸ä¼¼é…ç½®
        similar = self.benchmark_db.find_similar(features, k=5)

        # Step 3: æ’é˜Ÿè®ºä¼°ç®—
        queue_est = self.queue_estimate(config)

        # Step 4: MLé¢„æµ‹
        ml_pred = self.ml_predict(features)

        # Step 5: åŠ æƒèåˆ
        final_pred = self.ensemble(queue_est, ml_pred, similar)

        # Step 6: ç“¶é¢ˆåˆ†æ
        bottleneck = self.identify_bottleneck(config, final_pred)

        return {
            'max_tps': final_pred['tps'],
            'p50_latency_ms': final_pred['p50'],
            'p99_latency_ms': final_pred['p99'],
            'bottleneck': bottleneck,
            'confidence': self.compute_confidence(features)
        }

    def identify_bottleneck(self, config, performance):
        bottlenecks = []

        # CPUç“¶é¢ˆ
        if performance['cpu_util'] > 80:
            bottlenecks.append({
                'component': 'CPU',
                'utilization': f"{performance['cpu_util']:.0f}%",
                'recommendation': 'Scale up CPU cores'
            })

        # IOç“¶é¢ˆ
        io_ops = performance['disk_reads'] + performance['disk_writes']
        if io_ops > config['hardware']['iops'] * 0.8:
            bottlenecks.append({
                'component': 'Disk I/O',
                'utilization': f"{io_ops / config['hardware']['iops']:.0%}",
                'recommendation': 'Upgrade to faster storage or add read replicas'
            })

        # é”ç«äº‰
        if performance['lock_wait_time'] > performance['total_time'] * 0.2:
            bottlenecks.append({
                'component': 'Lock Contention',
                'utilization': f"{performance['lock_wait_time'] / performance['total_time']:.0%}",
                'recommendation': 'Review isolation level or enable HOT updates'
            })

        return bottlenecks
```

### 5.2 å®¹é‡è§„åˆ’å™¨

```python
class CapacityPlanner:
    def __init__(self, predictor):
        self.predictor = predictor

    def plan_for_growth(self, current_config, current_tps, target_tps):
        """è®¡ç®—è¾¾åˆ°ç›®æ ‡TPSéœ€è¦çš„é…ç½®"""

        # çº¿æ€§å¤–æ¨åˆå§‹ä¼°ç®—
        scale_factor = target_tps / current_tps

        candidate_configs = [
            # æ–¹æ¡ˆ1: å‚ç›´æ‰©å±•
            {
                **current_config,
                'cpu_cores': int(current_config['cpu_cores'] * scale_factor),
                'memory_gb': int(current_config['memory_gb'] * scale_factor)
            },

            # æ–¹æ¡ˆ2: æ°´å¹³æ‰©å±• (è¯»å†™åˆ†ç¦»)
            {
                **current_config,
                'read_replicas': math.ceil((scale_factor - 1) * 0.7)
            },

            # æ–¹æ¡ˆ3: æ··åˆ
            {
                **current_config,
                'cpu_cores': int(current_config['cpu_cores'] * 1.5),
                'read_replicas': 2
            }
        ]

        # é¢„æµ‹å„æ–¹æ¡ˆæ€§èƒ½å’Œæˆæœ¬
        results = []
        for config in candidate_configs:
            pred = self.predictor.predict(config)
            cost = self.estimate_cost(config)

            results.append({
                'config': config,
                'predicted_tps': pred['tps'],
                'predicted_p99_ms': pred['p99'],
                'monthly_cost_usd': cost,
                'meets_target': pred['tps'] >= target_tps
            })

        # æ’åº: æ€§ä»·æ¯”æœ€é«˜
        results.sort(key=lambda x: x['monthly_cost_usd'] / x['predicted_tps'])

        return results
```

---

## å…­ã€å®é™…éƒ¨ç½²æ¡ˆä¾‹

### æ¡ˆä¾‹1: æŸç”µå•†å¹³å°å®¹é‡è§„åˆ’

**åœºæ™¯**: åŒ11å‰å®¹é‡è§„åˆ’

**ä½¿ç”¨å·¥å…·**:

```python
from db_performance_predictor import Predictor, CapacityPlanner

predictor = Predictor()
planner = CapacityPlanner(predictor)

# å½“å‰é…ç½®
current_config = {
    'cpu_cores': 16,
    'memory_gb': 64,
    'iops': 50000,
    'max_connections': 200,
}

# å½“å‰TPS
current_tps = 8000

# ç›®æ ‡TPSï¼ˆåŒ11é¢„æœŸï¼‰
target_tps = 50000

# å®¹é‡è§„åˆ’
recommendations = planner.plan_for_growth(
    current_config,
    current_tps,
    target_tps
)

# è¾“å‡º
for i, rec in enumerate(recommendations, 1):
    print(f"æ–¹æ¡ˆ{i}:")
    print(f"  é…ç½®: {rec['config']}")
    print(f"  é¢„æµ‹TPS: {rec['predicted_tps']}")
    print(f"  æœˆæˆæœ¬: ${rec['monthly_cost_usd']}")
    print(f"  æ˜¯å¦æ»¡è¶³ç›®æ ‡: {rec['meets_target']}")
```

**ç»“æœ**:

```text
æ–¹æ¡ˆ1: å‚ç›´æ‰©å±•
  é…ç½®: CPU 100æ ¸, å†…å­˜ 400GB
  é¢„æµ‹TPS: 52,000
  æœˆæˆæœ¬: $5,000
  æ˜¯å¦æ»¡è¶³ç›®æ ‡: âœ“

æ–¹æ¡ˆ2: æ°´å¹³æ‰©å±•ï¼ˆè¯»å†™åˆ†ç¦»ï¼‰
  é…ç½®: ä¸»åº“ + 6ä¸ªåªè¯»å‰¯æœ¬
  é¢„æµ‹TPS: 48,000
  æœˆæˆæœ¬: $3,500
  æ˜¯å¦æ»¡è¶³ç›®æ ‡: âœ“

æ–¹æ¡ˆ3: æ··åˆæ‰©å±•
  é…ç½®: CPU 32æ ¸ + 2ä¸ªåªè¯»å‰¯æœ¬
  é¢„æµ‹TPS: 50,000
  æœˆæˆæœ¬: $2,800
  æ˜¯å¦æ»¡è¶³ç›®æ ‡: âœ“ (æ¨è)
```

**å®é™…æ•ˆæœ**: åŒ11å½“å¤©TPSå³°å€¼48,000ï¼Œé¢„æµ‹å‡†ç¡®ç‡96%

### æ¡ˆä¾‹2: äº‘æ•°æ®åº“è¿ç§»è¯„ä¼°

**åœºæ™¯**: ä»è‡ªå»ºPostgreSQLè¿ç§»åˆ°äº‘æ•°æ®åº“

**å¯¹æ¯”åˆ†æ**:

```python
configs = {
    'self_hosted': {
        'cpu_cores': 32,
        'memory_gb': 128,
        'iops': 100000,
        'cost_per_month': 2000,
    },
    'cloud_rds': {
        'cpu_cores': 32,
        'memory_gb': 128,
        'iops': 100000,
        'cost_per_month': 3500,
    },
    'cloud_aurora': {
        'cpu_cores': 32,
        'memory_gb': 128,
        'iops': 100000,
        'cost_per_month': 4000,
    },
}

results = {}
for name, config in configs.items():
    pred = predictor.predict(config)
    results[name] = {
        'predicted_tps': pred['max_tps'],
        'p99_latency': pred['p99_latency_ms'],
        'cost_per_tps': config['cost_per_month'] / pred['max_tps'],
    }

# å¯¹æ¯”
print("é…ç½®å¯¹æ¯”:")
for name, result in results.items():
    print(f"{name}:")
    print(f"  TPS: {result['predicted_tps']}")
    print(f"  P99å»¶è¿Ÿ: {result['p99_latency']}ms")
    print(f"  æˆæœ¬/TPS: ${result['cost_per_tps']:.4f}")
```

**å†³ç­–**: é€‰æ‹©Auroraï¼ˆè™½ç„¶æˆæœ¬é«˜ï¼Œä½†P99å»¶è¿Ÿä½30%ï¼‰

---

## ä¸ƒã€é«˜çº§åŠŸèƒ½

### 7.1 å·¥ä½œè´Ÿè½½å»ºæ¨¡

```python
class WorkloadModeler:
    """å·¥ä½œè´Ÿè½½å»ºæ¨¡å™¨"""

    def model_from_trace(self, trace_file: str) -> Workload:
        """ä»SQL traceæ–‡ä»¶å»ºæ¨¡"""
        queries = self.parse_trace(trace_file)

        workload = {
            'query_types': self.analyze_query_types(queries),
            'concurrency': self.estimate_concurrency(queries),
            'read_write_ratio': self.compute_read_write_ratio(queries),
            'hot_tables': self.identify_hot_tables(queries),
        }

        return workload

    def simulate_workload(self, workload: Workload, duration: int) -> List[Query]:
        """æ¨¡æ‹Ÿå·¥ä½œè´Ÿè½½"""
        queries = []

        for _ in range(duration):
            query_type = self.sample_query_type(workload['query_types'])
            query = self.generate_query(query_type, workload)
            queries.append(query)

        return queries
```

### 7.2 ç“¶é¢ˆé¢„æµ‹

```python
class BottleneckPredictor:
    """ç“¶é¢ˆé¢„æµ‹å™¨"""

    def predict_bottleneck(self, config: dict, workload: Workload) -> List[Bottleneck]:
        """é¢„æµ‹ç“¶é¢ˆ"""
        bottlenecks = []

        # 1. CPUç“¶é¢ˆé¢„æµ‹
        cpu_util = self.estimate_cpu_utilization(config, workload)
        if cpu_util > 0.8:
            bottlenecks.append(Bottleneck(
                component='CPU',
                severity='high',
                utilization=cpu_util,
                recommendation='Scale up CPU cores'
            ))

        # 2. å†…å­˜ç“¶é¢ˆé¢„æµ‹
        memory_util = self.estimate_memory_utilization(config, workload)
        if memory_util > 0.9:
            bottlenecks.append(Bottleneck(
                component='Memory',
                severity='critical',
                utilization=memory_util,
                recommendation='Increase shared_buffers or add memory'
            ))

        # 3. I/Oç“¶é¢ˆé¢„æµ‹
        io_util = self.estimate_io_utilization(config, workload)
        if io_util > 0.8:
            bottlenecks.append(Bottleneck(
                component='Disk I/O',
                severity='high',
                utilization=io_util,
                recommendation='Upgrade storage or add read replicas'
            ))

        # 4. é”ç«äº‰é¢„æµ‹
        lock_contention = self.estimate_lock_contention(config, workload)
        if lock_contention > 0.2:
            bottlenecks.append(Bottleneck(
                component='Lock Contention',
                severity='medium',
                utilization=lock_contention,
                recommendation='Review isolation level or optimize queries'
            ))

        return bottlenecks
```

---

## ä¸ƒã€æ¨¡å‹å±€é™æ€§ä¸é€‚ç”¨èŒƒå›´

### 7.1 æ¨¡å‹å±€é™æ€§

**1. æ’é˜Ÿè®ºæ¨¡å‹çš„å±€é™æ€§**:

```text
å‡è®¾æ¡ä»¶:
â”œâ”€ è¯·æ±‚åˆ°è¾¾æœä»æ³Šæ¾åˆ†å¸ƒï¼ˆå®é™…å¯èƒ½ä¸æ»¡è¶³ï¼‰
â”œâ”€ æœåŠ¡æ—¶é—´æœä»æŒ‡æ•°åˆ†å¸ƒï¼ˆå®é™…å¯èƒ½ä¸æ»¡è¶³ï¼‰
â”œâ”€ ç³»ç»Ÿå¤„äºç¨³æ€ï¼ˆå¿½ç•¥å¯åŠ¨/å…³é—­é˜¶æ®µï¼‰
â””â”€ æ— çªå‘æµé‡ï¼ˆå®é™…å¯èƒ½æœ‰æµé‡å³°å€¼ï¼‰

ä¸é€‚ç”¨åœºæ™¯:
â”œâ”€ çªå‘æµé‡ï¼ˆç§’æ€ã€é™æ—¶æ´»åŠ¨ï¼‰
â”œâ”€ é•¿äº‹åŠ¡ï¼ˆOLAPå¤æ‚æŸ¥è¯¢ï¼‰
â”œâ”€ éç¨³æ€ç³»ç»Ÿï¼ˆå¯åŠ¨é˜¶æ®µã€å…³é—­é˜¶æ®µï¼‰
â””â”€ ç‰¹æ®Šç¡¬ä»¶ï¼ˆGPUåŠ é€Ÿã€FPGAï¼‰
```

**2. æœºå™¨å­¦ä¹ æ¨¡å‹çš„å±€é™æ€§**:

```text
æ•°æ®ä¾èµ–:
â”œâ”€ è®­ç»ƒæ•°æ®è¦†ç›–èŒƒå›´æœ‰é™
â”œâ”€ æ–°ç¡¬ä»¶é…ç½®å¯èƒ½è¶…å‡ºè®­ç»ƒé›†
â”œâ”€ ç‰¹æ®Šå·¥ä½œè´Ÿè½½æ¨¡å¼å¯èƒ½æœªè§è¿‡
â””â”€ æ¨¡å‹éœ€è¦å®šæœŸé‡æ–°è®­ç»ƒ

æ³›åŒ–èƒ½åŠ›:
â”œâ”€ åœ¨ç›¸ä¼¼é…ç½®ä¸Šè¡¨ç°å¥½
â”œâ”€ åœ¨æç«¯é…ç½®ä¸Šè¯¯å·®å¤§
â””â”€ éœ€è¦è¶³å¤Ÿçš„å†å²æ•°æ®
```

**3. æ··åˆæ¨¡å‹çš„å±€é™æ€§**:

```text
èåˆç­–ç•¥:
â”œâ”€ æƒé‡éœ€è¦äººå·¥è°ƒä¼˜
â”œâ”€ ä¸åŒåœºæ™¯å¯èƒ½éœ€è¦ä¸åŒæƒé‡
â””â”€ éš¾ä»¥è§£é‡Šæœ€ç»ˆé¢„æµ‹ç»“æœ
```

### 7.2 é€‚ç”¨èŒƒå›´

**âœ… é€‚ç”¨åœºæ™¯**:

1. **å®¹é‡è§„åˆ’**: ä¼°ç®—èµ„æºéœ€æ±‚ï¼Œè¯¯å·®<20%å¯æ¥å—
2. **æ€§èƒ½å¯¹æ¯”**: å¯¹æ¯”ä¸åŒé…ç½®çš„æ€§èƒ½å·®å¼‚
3. **æˆæœ¬ä¼°ç®—**: åŸºäºæ€§èƒ½é¢„æµ‹ä¼°ç®—æˆæœ¬
4. **é£é™©è¯„ä¼°**: è¯„ä¼°é…ç½®æ˜¯å¦æ»¡è¶³æ€§èƒ½è¦æ±‚

**âŒ ä¸é€‚ç”¨åœºæ™¯**:

1. **ç²¾ç¡®æ€§èƒ½é¢„æµ‹**: éœ€è¦è¯¯å·®<5%çš„åœºæ™¯
2. **å®æ—¶æ€§èƒ½ç›‘æ§**: éœ€è¦å®æ—¶åé¦ˆçš„åœºæ™¯
3. **ç‰¹æ®Šç¡¬ä»¶**: GPUã€FPGAç­‰ç‰¹æ®Šç¡¬ä»¶
4. **æç«¯é…ç½®**: è¶…å‡ºè®­ç»ƒæ•°æ®èŒƒå›´çš„é…ç½®

### 7.3 ä½¿ç”¨å»ºè®®

**1. ç½®ä¿¡åº¦æ£€æŸ¥**:

```python
pred = predictor.predict(config, workload)
if pred['confidence'] < 0.7:
    print("è­¦å‘Š: é¢„æµ‹ç½®ä¿¡åº¦ä½ï¼Œå»ºè®®:")
    print("1. ä½¿ç”¨ç›¸ä¼¼é…ç½®çš„å†å²æ•°æ®")
    print("2. è¿›è¡Œå°è§„æ¨¡æµ‹è¯•éªŒè¯")
    print("3. ç•™å‡º20%ä»¥ä¸Šçš„æ€§èƒ½ä½™é‡")
```

**2. è¯¯å·®èŒƒå›´è€ƒè™‘**:

```python
# ä¿å®ˆä¼°è®¡ï¼šä½¿ç”¨é¢„æµ‹å€¼çš„80%
conservative_tps = pred['max_tps'] * 0.8

# ä¹è§‚ä¼°è®¡ï¼šä½¿ç”¨é¢„æµ‹å€¼çš„120%
optimistic_tps = pred['max_tps'] * 1.2

# å®¹é‡è§„åˆ’æ—¶ä½¿ç”¨ä¿å®ˆä¼°è®¡
if conservative_tps >= target_tps:
    deploy_config(config)
```

**3. å®šæœŸéªŒè¯**:

```python
# éƒ¨ç½²åå®šæœŸæ”¶é›†å®é™…æ€§èƒ½æ•°æ®
actual_performance = collect_performance_metrics(days=7)

# ä¸é¢„æµ‹å€¼å¯¹æ¯”ï¼Œæ›´æ–°æ¨¡å‹
if abs(actual_performance['tps'] - predicted_tps) / predicted_tps > 0.2:
    print("é¢„æµ‹è¯¯å·®è¾ƒå¤§ï¼Œå»ºè®®é‡æ–°è®­ç»ƒæ¨¡å‹")
    retrain_model(actual_performance)
```

---

### åä¾‹1: å¿½ç•¥ç½®ä¿¡åº¦

**é”™è¯¯ä½¿ç”¨**:

```python
# é”™è¯¯: å®Œå…¨ä¿¡ä»»é¢„æµ‹ç»“æœ
pred = predictor.predict(config)
if pred['max_tps'] > target_tps:
    deploy_config(config)  # å¯èƒ½ä¸å‡†ç¡®ï¼
```

**é—®é¢˜**: é¢„æµ‹å¯èƒ½ä¸å‡†ç¡®ï¼Œå¯¼è‡´ç”Ÿäº§ç¯å¢ƒæ€§èƒ½ä¸è¶³

**æ­£ç¡®ä½¿ç”¨**:

```python
# æ­£ç¡®: æ£€æŸ¥ç½®ä¿¡åº¦
pred = predictor.predict(config)
if pred['confidence'] < 0.8:
    print("è­¦å‘Š: é¢„æµ‹ç½®ä¿¡åº¦ä½ï¼Œå»ºè®®å…ˆæµ‹è¯•")
    return

if pred['max_tps'] > target_tps * 1.2:  # ç•™20%ä½™é‡
    deploy_config(config)
```

### åä¾‹2: å¿½ç•¥å·¥ä½œè´Ÿè½½å˜åŒ–

**é”™è¯¯ä½¿ç”¨**:

```python
# é”™è¯¯: ä½¿ç”¨å›ºå®šå·¥ä½œè´Ÿè½½é¢„æµ‹
workload = {
    'read_ratio': 0.9,
    'concurrency': 100,
}
pred = predictor.predict(config, workload)  # å›ºå®šè´Ÿè½½
```

**é—®é¢˜**: å®é™…è´Ÿè½½å˜åŒ–åé¢„æµ‹å¤±æ•ˆ

**æ­£ç¡®ä½¿ç”¨**:

```python
# æ­£ç¡®: ä½¿ç”¨å†å²è´Ÿè½½è¶‹åŠ¿
workload_trend = analyze_historical_workload(days=30)
future_workload = forecast_workload(workload_trend, days_ahead=90)
pred = predictor.predict(config, future_workload)  # é¢„æµ‹æœªæ¥è´Ÿè½½
```

### åä¾‹3: æ€§èƒ½é¢„æµ‹å™¨ä½¿ç”¨ä¸å½“

**é”™è¯¯è®¾è®¡**: æ€§èƒ½é¢„æµ‹å™¨ä½¿ç”¨ä¸å½“

```text
é”™è¯¯åœºæ™¯:
â”œâ”€ ä½¿ç”¨: æ€§èƒ½é¢„æµ‹å™¨
â”œâ”€ é—®é¢˜: ä¸æŒ‰å·¥å…·æµç¨‹ï¼Œè·³è¿‡å…³é”®æ­¥éª¤
â”œâ”€ ç»“æœ: é¢„æµ‹é”™è¯¯
â””â”€ åæœ: å®¹é‡è§„åˆ’é”™è¯¯ âœ—

å®é™…æ¡ˆä¾‹:
â”œâ”€ ç³»ç»Ÿ: æŸç³»ç»Ÿä½¿ç”¨é¢„æµ‹å™¨
â”œâ”€ é—®é¢˜: è·³è¿‡å·¥ä½œè´Ÿè½½åˆ†æï¼Œç›´æ¥é¢„æµ‹
â”œâ”€ ç»“æœ: é¢„æµ‹ä¸å½“
â””â”€ åæœ: å®¹é‡è§„åˆ’é”™è¯¯ âœ—

æ­£ç¡®è®¾è®¡:
â”œâ”€ æ–¹æ¡ˆ: ä¸¥æ ¼æŒ‰ç…§å·¥å…·æµç¨‹
â”œâ”€ å®ç°: å®Œæ•´æ‰§è¡Œæ‰€æœ‰æ­¥éª¤
â””â”€ ç»“æœ: é¢„æµ‹æ­£ç¡® âœ“
```

### åä¾‹4: å¿½ç•¥é¢„æµ‹éªŒè¯

**é”™è¯¯è®¾è®¡**: å¿½ç•¥é¢„æµ‹éªŒè¯

```text
é”™è¯¯åœºæ™¯:
â”œâ”€ ä½¿ç”¨: æ€§èƒ½é¢„æµ‹å™¨
â”œâ”€ é—®é¢˜: ç›´æ¥åº”ç”¨é¢„æµ‹ç»“æœï¼Œä¸éªŒè¯
â”œâ”€ ç»“æœ: é¢„æµ‹é”™è¯¯æœªè¢«å‘ç°
â””â”€ åæœ: å®¹é‡è§„åˆ’é”™è¯¯ âœ—

å®é™…æ¡ˆä¾‹:
â”œâ”€ ç³»ç»Ÿ: æŸç³»ç»Ÿä½¿ç”¨é¢„æµ‹å™¨
â”œâ”€ é—®é¢˜: æœªéªŒè¯é¢„æµ‹ç»“æœ
â”œâ”€ ç»“æœ: å®é™…æ€§èƒ½æœªè¾¾åˆ°é¢„æœŸ
â””â”€ åæœ: å®¹é‡è§„åˆ’é”™è¯¯ âœ—

æ­£ç¡®è®¾è®¡:
â”œâ”€ æ–¹æ¡ˆ: éªŒè¯é¢„æµ‹ç»“æœ
â”œâ”€ å®ç°: æ€§èƒ½æµ‹è¯•ã€å‹åŠ›æµ‹è¯•
â””â”€ ç»“æœ: éªŒè¯é¢„æµ‹æ­£ç¡®æ€§ âœ“
```

### åä¾‹5: é¢„æµ‹æ¨¡å‹å‚æ•°é”™è¯¯

**é”™è¯¯è®¾è®¡**: é¢„æµ‹æ¨¡å‹å‚æ•°é”™è¯¯

```text
é”™è¯¯åœºæ™¯:
â”œâ”€ é…ç½®: æ€§èƒ½é¢„æµ‹å™¨é…ç½®
â”œâ”€ é—®é¢˜: æ¨¡å‹å‚æ•°é”™è¯¯
â”œâ”€ ç»“æœ: é¢„æµ‹ä¸å‡†ç¡®
â””â”€ è¯¯å·®: é¢„æµ‹é”™è¯¯ âœ—

å®é™…æ¡ˆä¾‹:
â”œâ”€ ç³»ç»Ÿ: æŸç³»ç»Ÿä½¿ç”¨é¢„æµ‹å™¨
â”œâ”€ é—®é¢˜: ç¡¬ä»¶å‚æ•°é…ç½®é”™è¯¯ï¼ˆå®é™…16æ ¸ï¼Œé…ç½®8æ ¸ï¼‰
â”œâ”€ ç»“æœ: é¢„æµ‹ä¸å½“
â””â”€ åæœ: å®¹é‡è§„åˆ’é”™è¯¯ âœ—

æ­£ç¡®è®¾è®¡:
â”œâ”€ æ–¹æ¡ˆ: å‡†ç¡®é…ç½®æ¨¡å‹å‚æ•°
â”œâ”€ å®ç°: æ ¹æ®å®é™…æƒ…å†µé…ç½®
â””â”€ ç»“æœ: é¢„æµ‹å‡†ç¡® âœ“
```

### åä¾‹6: æ€§èƒ½é¢„æµ‹å™¨ç›‘æ§ä¸è¶³

**é”™è¯¯è®¾è®¡**: ä¸ç›‘æ§é¢„æµ‹å™¨ä½¿ç”¨æ•ˆæœ

```text
é”™è¯¯åœºæ™¯:
â”œâ”€ ä½¿ç”¨: æ€§èƒ½é¢„æµ‹å™¨
â”œâ”€ é—®é¢˜: ä¸ç›‘æ§é¢„æµ‹å™¨ä½¿ç”¨æ•ˆæœ
â”œâ”€ ç»“æœ: é¢„æµ‹å™¨é—®é¢˜æœªè¢«å‘ç°
â””â”€ åæœ: é¢„æµ‹å™¨æ•ˆæœå·® âœ—

å®é™…æ¡ˆä¾‹:
â”œâ”€ ç³»ç»Ÿ: æŸç³»ç»Ÿä½¿ç”¨é¢„æµ‹å™¨
â”œâ”€ é—®é¢˜: æœªç›‘æ§é¢„æµ‹å‡†ç¡®ç‡
â”œâ”€ ç»“æœ: é¢„æµ‹å‡†ç¡®ç‡ä½æœªè¢«å‘ç°
â””â”€ åæœ: é¢„æµ‹å™¨æ•ˆæœå·® âœ—

æ­£ç¡®è®¾è®¡:
â”œâ”€ æ–¹æ¡ˆ: ç›‘æ§é¢„æµ‹å™¨ä½¿ç”¨æ•ˆæœ
â”œâ”€ å®ç°: ç›‘æ§é¢„æµ‹å‡†ç¡®ç‡ã€ç”¨æˆ·æ»¡æ„åº¦
â””â”€ ç»“æœ: åŠæ—¶å‘ç°é—®é¢˜ï¼Œæ”¹è¿›é¢„æµ‹å™¨ âœ“
```

---

**å·¥å…·ç‰ˆæœ¬**: 2.0.0ï¼ˆå¤§å¹…å……å®ï¼‰
**æœ€åæ›´æ–°**: 2025-12-05
**æ–°å¢å†…å®¹**: å®Œæ•´å®ç°ä»£ç ã€å®é™…éƒ¨ç½²æ¡ˆä¾‹ã€é«˜çº§åŠŸèƒ½ã€åä¾‹ã€æ€§èƒ½é¢„æµ‹å™¨èƒŒæ™¯ä¸æ¼”è¿›ï¼ˆä¸ºä»€ä¹ˆéœ€è¦æ€§èƒ½é¢„æµ‹å™¨ã€å†å²èƒŒæ™¯ã€ç†è®ºåŸºç¡€ã€æ ¸å¿ƒæŒ‘æˆ˜ï¼‰ã€æ€§èƒ½é¢„æµ‹å™¨åä¾‹è¡¥å……ï¼ˆ6ä¸ªæ–°å¢åä¾‹ï¼šæ€§èƒ½é¢„æµ‹å™¨ä½¿ç”¨ä¸å½“ã€å¿½ç•¥é¢„æµ‹éªŒè¯ã€é¢„æµ‹æ¨¡å‹å‚æ•°é”™è¯¯ã€æ€§èƒ½é¢„æµ‹å™¨ç›‘æ§ä¸è¶³ï¼‰

**å¼€æºåè®®**: Apache 2.0
**GitHub**: <https://github.com/db-theory/performance-predictor>

**ç›¸å…³æ–‡æ¡£**:

- `06-æ€§èƒ½åˆ†æ/01-ååé‡å…¬å¼æ¨å¯¼.md`
- `06-æ€§èƒ½åˆ†æ/02-å»¶è¿Ÿåˆ†ææ¨¡å‹.md`
- `11-å·¥å…·ä¸è‡ªåŠ¨åŒ–/01-å¹¶å‘æ§åˆ¶å†³ç­–åŠ©æ‰‹.md`
- `11-å·¥å…·ä¸è‡ªåŠ¨åŒ–/06-å®¹é‡è§„åˆ’å™¨.md` (å®¹é‡è§„åˆ’è¯¦ç»†å®ç°)
