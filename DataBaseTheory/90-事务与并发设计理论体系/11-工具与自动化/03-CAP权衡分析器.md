# 03 | CAPæƒè¡¡åˆ†æå™¨

> **å·¥å…·ç±»å‹**: Webè¯„ä¼°å·¥å…·
> **å¼€å‘çŠ¶æ€**: âœ… Betaç‰ˆæœ¬
> **æ ¸å¿ƒæŠ€æœ¯**: CAPè®¡ç®—å™¨ + åœºæ™¯åº“ + ç³»ç»Ÿå¯¹æ¯”

---

## ğŸ“‘ ç›®å½•

- [03 | CAPæƒè¡¡åˆ†æå™¨](#03--capæƒè¡¡åˆ†æå™¨)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [ä¸€ã€å·¥å…·æ¦‚è¿°](#ä¸€å·¥å…·æ¦‚è¿°)
    - [1.1 åŠŸèƒ½å®šä½](#11-åŠŸèƒ½å®šä½)
    - [1.2 è¾“å‡ºæŠ¥å‘Š](#12-è¾“å‡ºæŠ¥å‘Š)
  - [äºŒã€CAPè¯„åˆ†æ¨¡å‹](#äºŒcapè¯„åˆ†æ¨¡å‹)
    - [2.1 è¯„åˆ†ç®—æ³•](#21-è¯„åˆ†ç®—æ³•)
    - [2.2 ç³»ç»ŸåŒ¹é…ç®—æ³•](#22-ç³»ç»ŸåŒ¹é…ç®—æ³•)
  - [ä¸‰ã€å®Œæ•´å®ç°ä»£ç ](#ä¸‰å®Œæ•´å®ç°ä»£ç )
    - [3.1 CAPè¯„åˆ†ç®—æ³•å®Œæ•´å®ç°](#31-capè¯„åˆ†ç®—æ³•å®Œæ•´å®ç°)
    - [3.2 Web APIå®ç°](#32-web-apiå®ç°)
    - [3.3 å®é™…æ¡ˆä¾‹](#33-å®é™…æ¡ˆä¾‹)
  - [å››ã€ä½¿ç”¨æŒ‡å—](#å››ä½¿ç”¨æŒ‡å—)
    - [4.1 Webç•Œé¢ä½¿ç”¨](#41-webç•Œé¢ä½¿ç”¨)
    - [4.2 APIè°ƒç”¨](#42-apiè°ƒç”¨)
  - [äº”ã€åä¾‹ä¸é”™è¯¯ä½¿ç”¨](#äº”åä¾‹ä¸é”™è¯¯ä½¿ç”¨)
    - [åä¾‹1: å¿½ç•¥ä¸šåŠ¡éœ€æ±‚ç›²ç›®ä½¿ç”¨å·¥å…·](#åä¾‹1-å¿½ç•¥ä¸šåŠ¡éœ€æ±‚ç›²ç›®ä½¿ç”¨å·¥å…·)
    - [åä¾‹2: è¯„åˆ†æƒé‡è®¾ç½®ä¸åˆç†](#åä¾‹2-è¯„åˆ†æƒé‡è®¾ç½®ä¸åˆç†)
  - [å…­ã€å®é™…åº”ç”¨æ¡ˆä¾‹](#å…­å®é™…åº”ç”¨æ¡ˆä¾‹)
    - [6.1 æ¡ˆä¾‹: æŸå…¬å¸åˆ†å¸ƒå¼ç³»ç»Ÿé€‰å‹](#61-æ¡ˆä¾‹-æŸå…¬å¸åˆ†å¸ƒå¼ç³»ç»Ÿé€‰å‹)
    - [6.2 æ¡ˆä¾‹: äº‘æ•°æ®åº“CAPé€‰æ‹©](#62-æ¡ˆä¾‹-äº‘æ•°æ®åº“capé€‰æ‹©)

---

## ä¸€ã€å·¥å…·æ¦‚è¿°

### 1.1 åŠŸèƒ½å®šä½

**æ ¸å¿ƒä»·å€¼**: 5åˆ†é’Ÿç¡®å®šåˆ†å¸ƒå¼ç³»ç»ŸCAPå®šä½

**è¾“å…¥å‚æ•°**:

```yaml
requirements:
  consistency:
    level: strong  # weak/eventual/strong/strict
    latency_tolerance_ms: 100

  availability:
    target: 99.99  # å¹´å®•æœºæ—¶é—´
    acceptable_downtime_sec: 60

  partition_tolerance:
    network_type: datacenter  # datacenter/wan/global
    expected_partition_duration_sec: 10

  workload:
    read_ratio: 0.8
    write_ratio: 0.2
    qps: 50000
```

### 1.2 è¾“å‡ºæŠ¥å‘Š

```json
{
  "cap_analysis": {
    "classification": "CP System",
    "consistency_score": 95,
    "availability_score": 75,
    "partition_tolerance_score": 85,
    "tradeoff_summary": "Prioritizes consistency over availability during network partitions"
  },
  "recommendations": [
    {
      "system": "PostgreSQL + Streaming Replication",
      "match_score": 92,
      "pros": ["Strong consistency", "Rich SQL", "ACID transactions"],
      "cons": ["Writes unavailable during partition", "Single-master bottleneck"],
      "suitable_scenarios": ["Financial systems", "Order processing"]
    },
    {
      "system": "etcd (Raft)",
      "match_score": 88,
      "pros": ["Linearizable", "Automatic failover", "Watch API"],
      "cons": ["Limited to key-value", "Write latency"],
      "suitable_scenarios": ["Configuration storage", "Service discovery"]
    }
  ]
}
```

---

## äºŒã€CAPè¯„åˆ†æ¨¡å‹

### 2.1 è¯„åˆ†ç®—æ³•

```python
class CAPAnalyzer:
    def analyze(self, requirements):
        # 1. è®¡ç®—Cå¾—åˆ†
        c_score = self.compute_consistency_score(requirements['consistency'])

        # 2. è®¡ç®—Aå¾—åˆ†
        a_score = self.compute_availability_score(requirements['availability'])

        # 3. è®¡ç®—På¾—åˆ†
        p_score = self.compute_partition_tolerance_score(requirements['partition_tolerance'])

        # 4. å½’ä¸€åŒ–åˆ°100åˆ†
        total = c_score + a_score + p_score
        normalized = {
            'C': (c_score / total) * 100,
            'A': (a_score / total) * 100,
            'P': (p_score / total) * 100
        }

        # 5. CAPåˆ†ç±»
        classification = self.classify_cap(normalized)

        return {
            'scores': normalized,
            'classification': classification
        }

    def classify_cap(self, scores):
        if scores['C'] > 60 and scores['P'] > 60:
            return 'CP System'
        elif scores['A'] > 60 and scores['P'] > 60:
            return 'AP System'
        elif scores['C'] > 60 and scores['A'] > 60:
            return 'CA System (Not partition-tolerant)'
        else:
            return 'Balanced System'
```

### 2.2 ç³»ç»ŸåŒ¹é…ç®—æ³•

```python
def match_systems(cap_scores, requirements):
    systems_database = [
        {
            'name': 'PostgreSQL',
            'cap_profile': {'C': 95, 'A': 70, 'P': 60},
            'best_for': ['CP', 'CA'],
            'features': ['ACID', 'SQL', 'MVCC']
        },
        {
            'name': 'Cassandra',
            'cap_profile': {'C': 40, 'A': 95, 'P': 95},
            'best_for': ['AP'],
            'features': ['Tunable consistency', 'Multi-master', 'Horizontal scaling']
        },
        {
            'name': 'etcd',
            'cap_profile': {'C': 100, 'A': 75, 'P': 90},
            'best_for': ['CP'],
            'features': ['Linearizable', 'Raft', 'Watch']
        },
        # ... æ›´å¤šç³»ç»Ÿ
    ]

    # è®¡ç®—åŒ¹é…åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
    matches = []
    for system in systems_database:
        similarity = cosine_similarity(cap_scores, system['cap_profile'])
        matches.append({
            'system': system['name'],
            'match_score': similarity * 100,
            'system_info': system
        })

    return sorted(matches, key=lambda x: x['match_score'], reverse=True)
```

---

## ä¸‰ã€å®Œæ•´å®ç°ä»£ç 

### 3.1 CAPè¯„åˆ†ç®—æ³•å®Œæ•´å®ç°

```python
from dataclasses import dataclass
from typing import Dict, List, Optional
from enum import Enum
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class ConsistencyLevel(Enum):
    WEAK = 1
    EVENTUAL = 2
    STRONG = 3
    STRICT = 4

class CAPAnalyzer:
    """CAPæƒè¡¡åˆ†æå™¨"""

    def __init__(self):
        self.systems_database = self._load_systems_database()

    def analyze(self, requirements: Dict) -> Dict:
        """åˆ†æCAPéœ€æ±‚"""
        # 1. è®¡ç®—C/A/På¾—åˆ†
        c_score = self._compute_consistency_score(requirements.get('consistency', {}))
        a_score = self._compute_availability_score(requirements.get('availability', {}))
        p_score = self._compute_partition_tolerance_score(requirements.get('partition_tolerance', {}))

        # 2. å½’ä¸€åŒ–
        total = c_score + a_score + p_score
        if total == 0:
            total = 1

        normalized = {
            'C': (c_score / total) * 100,
            'A': (a_score / total) * 100,
            'P': (p_score / total) * 100
        }

        # 3. CAPåˆ†ç±»
        classification = self._classify_cap(normalized)

        # 4. ç³»ç»ŸåŒ¹é…
        recommendations = self._match_systems(normalized, requirements)

        return {
            'scores': normalized,
            'classification': classification,
            'recommendations': recommendations,
            'tradeoff_summary': self._generate_tradeoff_summary(classification, normalized)
        }

    def _compute_consistency_score(self, consistency_req: Dict) -> float:
        """è®¡ç®—ä¸€è‡´æ€§å¾—åˆ†"""
        level = consistency_req.get('level', 'eventual')
        latency_tolerance = consistency_req.get('latency_tolerance_ms', 1000)

        # ä¸€è‡´æ€§çº§åˆ«æƒé‡
        level_weights = {
            'weak': 20,
            'eventual': 40,
            'strong': 80,
            'strict': 100
        }

        base_score = level_weights.get(level, 40)

        # å»¶è¿Ÿå®¹å¿åº¦å½±å“ï¼ˆå®¹å¿åº¦è¶Šä½ï¼Œä¸€è‡´æ€§è¦æ±‚è¶Šé«˜ï¼‰
        latency_factor = max(0, 1 - (latency_tolerance / 1000))

        return base_score * (1 + latency_factor * 0.5)

    def _compute_availability_score(self, availability_req: Dict) -> float:
        """è®¡ç®—å¯ç”¨æ€§å¾—åˆ†"""
        target = availability_req.get('target', 99.9)  # 99.9%
        acceptable_downtime = availability_req.get('acceptable_downtime_sec', 3600)

        # å¯ç”¨æ€§ç›®æ ‡æƒé‡ï¼ˆ99.9% = 60åˆ†, 99.99% = 80åˆ†, 99.999% = 100åˆ†ï¼‰
        if target >= 99.999:
            base_score = 100
        elif target >= 99.99:
            base_score = 80
        elif target >= 99.9:
            base_score = 60
        else:
            base_score = 40

        # å¯æ¥å—å®•æœºæ—¶é—´å½±å“
        downtime_factor = max(0, 1 - (acceptable_downtime / 86400))  # ç›¸å¯¹äº1å¤©

        return base_score * (1 + downtime_factor * 0.3)

    def _compute_partition_tolerance_score(self, partition_req: Dict) -> float:
        """è®¡ç®—åˆ†åŒºå®¹é”™å¾—åˆ†"""
        network_type = partition_req.get('network_type', 'datacenter')
        expected_duration = partition_req.get('expected_partition_duration_sec', 60)

        # ç½‘ç»œç±»å‹æƒé‡
        network_weights = {
            'datacenter': 60,   # åŒæ•°æ®ä¸­å¿ƒï¼Œåˆ†åŒºæ¦‚ç‡ä½
            'wan': 80,          # è·¨WANï¼Œåˆ†åŒºæ¦‚ç‡ä¸­
            'global': 100       # å…¨çƒåˆ†å¸ƒï¼Œåˆ†åŒºæ¦‚ç‡é«˜
        }

        base_score = network_weights.get(network_type, 60)

        # é¢„æœŸåˆ†åŒºæ—¶é•¿å½±å“ï¼ˆæ—¶é•¿è¶Šé•¿ï¼Œå®¹é”™è¦æ±‚è¶Šé«˜ï¼‰
        duration_factor = min(1.0, expected_duration / 3600)  # ç›¸å¯¹äº1å°æ—¶

        return base_score * (1 + duration_factor * 0.5)

    def _classify_cap(self, scores: Dict) -> str:
        """CAPåˆ†ç±»"""
        c, a, p = scores['C'], scores['A'], scores['P']

        if c > 60 and p > 60:
            return 'CP System'
        elif a > 60 and p > 60:
            return 'AP System'
        elif c > 60 and a > 60:
            return 'CA System (Not partition-tolerant)'
        else:
            return 'Balanced System'

    def _generate_tradeoff_summary(self, classification: str, scores: Dict) -> str:
        """ç”Ÿæˆæƒè¡¡æ‘˜è¦"""
        summaries = {
            'CP System': 'åœ¨ç½‘ç»œåˆ†åŒºæ—¶ä¼˜å…ˆä¿è¯ä¸€è‡´æ€§ï¼Œå¯èƒ½ç‰ºç‰²å¯ç”¨æ€§',
            'AP System': 'åœ¨ç½‘ç»œåˆ†åŒºæ—¶ä¼˜å…ˆä¿è¯å¯ç”¨æ€§ï¼Œæ¥å—æœ€ç»ˆä¸€è‡´æ€§',
            'CA System (Not partition-tolerant)': 'å•æ•°æ®ä¸­å¿ƒéƒ¨ç½²ï¼Œä¸å¤„ç†ç½‘ç»œåˆ†åŒº',
            'Balanced System': 'å¹³è¡¡ä¸€è‡´æ€§å’Œå¯ç”¨æ€§ï¼Œæ ¹æ®åœºæ™¯åŠ¨æ€è°ƒæ•´'
        }
        return summaries.get(classification, 'æœªçŸ¥ç±»å‹')

    def _load_systems_database(self) -> List[Dict]:
        """åŠ è½½ç³»ç»Ÿæ•°æ®åº“"""
        return [
            {
                'name': 'PostgreSQL + Streaming Replication',
                'cap_profile': {'C': 95, 'A': 70, 'P': 60},
                'best_for': ['CP', 'CA'],
                'features': ['ACID', 'SQL', 'MVCC', 'åŒæ­¥å¤åˆ¶'],
                'pros': ['å¼ºä¸€è‡´æ€§', 'ä¸°å¯ŒSQL', 'ACIDäº‹åŠ¡', 'æˆç†Ÿç¨³å®š'],
                'cons': ['åˆ†åŒºæ—¶å†™å…¥ä¸å¯ç”¨', 'å•ä¸»ç“¶é¢ˆ', 'æ‰©å±•æ€§æœ‰é™'],
                'suitable_scenarios': ['é‡‘èç³»ç»Ÿ', 'è®¢å•å¤„ç†', 'è´¦æˆ·ç®¡ç†'],
                'latency': '5-50ms',
                'throughput': '10K-50K TPS'
            },
            {
                'name': 'Cassandra',
                'cap_profile': {'C': 40, 'A': 95, 'P': 95},
                'best_for': ['AP'],
                'features': ['å¯è°ƒä¸€è‡´æ€§', 'å¤šä¸»å¤åˆ¶', 'æ°´å¹³æ‰©å±•', 'æœ€ç»ˆä¸€è‡´'],
                'pros': ['é«˜å¯ç”¨', 'å…¨çƒåˆ†å¸ƒ', 'æ°´å¹³æ‰©å±•', 'å†™å…¥æ€§èƒ½é«˜'],
                'cons': ['æœ€ç»ˆä¸€è‡´æ€§', 'æ— ACID', 'æŸ¥è¯¢èƒ½åŠ›æœ‰é™'],
                'suitable_scenarios': ['ç¤¾äº¤ç½‘ç»œ', 'IoTæ•°æ®', 'æ—¥å¿—å­˜å‚¨'],
                'latency': '2-10ms',
                'throughput': '100K+ TPS'
            },
            {
                'name': 'etcd (Raft)',
                'cap_profile': {'C': 100, 'A': 75, 'P': 90},
                'best_for': ['CP'],
                'features': ['çº¿æ€§ä¸€è‡´æ€§', 'Raftå…±è¯†', 'Watch API', 'è‡ªåŠ¨æ•…éšœè½¬ç§»'],
                'pros': ['å¼ºä¸€è‡´æ€§', 'è‡ªåŠ¨æ•…éšœè½¬ç§»', 'é…ç½®ç®¡ç†'],
                'cons': ['ä»…é”®å€¼å­˜å‚¨', 'å†™å…¥å»¶è¿Ÿ', 'å°‘æ•°æ´¾ä¸å¯ç”¨'],
                'suitable_scenarios': ['é…ç½®å­˜å‚¨', 'æœåŠ¡å‘ç°', 'åˆ†å¸ƒå¼é”'],
                'latency': '5-50ms',
                'throughput': '10K TPS'
            },
            {
                'name': 'MongoDB Replica Set',
                'cap_profile': {'C': 85, 'A': 80, 'P': 70},
                'best_for': ['CP', 'CA'],
                'features': ['æ–‡æ¡£å­˜å‚¨', 'å‰¯æœ¬é›†', 'å¯è°ƒä¸€è‡´æ€§', 'è‡ªåŠ¨æ•…éšœè½¬ç§»'],
                'pros': ['çµæ´»æ•°æ®æ¨¡å‹', 'æ°´å¹³æ‰©å±•', 'å¯è°ƒä¸€è‡´æ€§'],
                'cons': ['æœ€ç»ˆä¸€è‡´è¯»', 'åˆ†ç‰‡å¤æ‚', 'äº‹åŠ¡é™åˆ¶'],
                'suitable_scenarios': ['å†…å®¹ç®¡ç†', 'ç”¨æˆ·ç”»åƒ', 'æ—¥å¿—åˆ†æ'],
                'latency': '5-30ms',
                'throughput': '20K-100K TPS'
            },
            {
                'name': 'CockroachDB',
                'cap_profile': {'C': 90, 'A': 85, 'P': 90},
                'best_for': ['CP'],
                'features': ['åˆ†å¸ƒå¼SQL', 'Raftå¤åˆ¶', 'ä¸²è¡ŒåŒ–éš”ç¦»', 'å…¨å±€äº‹åŠ¡'],
                'pros': ['å¼ºä¸€è‡´æ€§', 'åˆ†å¸ƒå¼SQL', 'è‡ªåŠ¨åˆ†ç‰‡', 'è·¨åŒºåŸŸ'],
                'cons': ['å»¶è¿Ÿè¾ƒé«˜', 'æˆæœ¬é«˜', 'å¤æ‚åº¦é«˜'],
                'suitable_scenarios': ['å…¨çƒåˆ†å¸ƒå¼åº”ç”¨', 'å¤šç§Ÿæˆ·SaaS'],
                'latency': '10-100ms',
                'throughput': '5K-20K TPS'
            },
            {
                'name': 'DynamoDB',
                'cap_profile': {'C': 50, 'A': 95, 'P': 95},
                'best_for': ['AP'],
                'features': ['æ‰˜ç®¡æœåŠ¡', 'è‡ªåŠ¨æ‰©å±•', 'æœ€ç»ˆä¸€è‡´', 'å¼ºä¸€è‡´å¯é€‰'],
                'pros': ['å®Œå…¨æ‰˜ç®¡', 'è‡ªåŠ¨æ‰©å±•', 'å…¨çƒåˆ†å¸ƒ', 'æŒ‰éœ€ä»˜è´¹'],
                'cons': ['æˆæœ¬é«˜', 'æŸ¥è¯¢èƒ½åŠ›æœ‰é™', 'ä¾›åº”å•†é”å®š'],
                'suitable_scenarios': ['Serverlessåº”ç”¨', 'ç§»åŠ¨åç«¯'],
                'latency': '1-10ms',
                'throughput': 'æ— é™ï¼ˆæ‰˜ç®¡ï¼‰'
            }
        ]

    def _match_systems(self, cap_scores: Dict, requirements: Dict) -> List[Dict]:
        """åŒ¹é…ç³»ç»Ÿ"""
        target_vector = np.array([[cap_scores['C'], cap_scores['A'], cap_scores['P']]])

        matches = []
        for system in self.systems_database:
            system_vector = np.array([[system['cap_profile']['C'],
                                      system['cap_profile']['A'],
                                      system['cap_profile']['P']]])

            # ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = cosine_similarity(target_vector, system_vector)[0][0]

            # é¢å¤–åŠ åˆ†ï¼šåœºæ™¯åŒ¹é…
            scenario_bonus = 0.0
            workload = requirements.get('workload', {})
            if 'financial' in str(requirements.get('scenario', '')).lower():
                if 'financial' in ' '.join(system['suitable_scenarios']).lower():
                    scenario_bonus = 0.1

            match_score = (similarity + scenario_bonus) * 100

            matches.append({
                'system': system['name'],
                'match_score': match_score,
                'cap_profile': system['cap_profile'],
                'pros': system['pros'],
                'cons': system['cons'],
                'suitable_scenarios': system['suitable_scenarios'],
                'features': system['features'],
                'latency': system['latency'],
                'throughput': system['throughput']
            })

        return sorted(matches, key=lambda x: x['match_score'], reverse=True)
```

### 3.2 Web APIå®ç°

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional

app = FastAPI()
analyzer = CAPAnalyzer()

class ConsistencyRequirement(BaseModel):
    level: str  # weak/eventual/strong/strict
    latency_tolerance_ms: Optional[int] = 1000

class AvailabilityRequirement(BaseModel):
    target: float  # 99.9, 99.99, 99.999
    acceptable_downtime_sec: Optional[int] = 3600

class PartitionToleranceRequirement(BaseModel):
    network_type: str  # datacenter/wan/global
    expected_partition_duration_sec: Optional[int] = 60

class WorkloadRequirement(BaseModel):
    read_ratio: Optional[float] = 0.8
    write_ratio: Optional[float] = 0.2
    qps: Optional[int] = 10000

class CAPAnalysisRequest(BaseModel):
    consistency: ConsistencyRequirement
    availability: AvailabilityRequirement
    partition_tolerance: PartitionToleranceRequirement
    workload: Optional[WorkloadRequirement] = None
    scenario: Optional[str] = None

@app.post("/api/cap/analyze")
async def analyze_cap(request: CAPAnalysisRequest):
    """CAPåˆ†æAPI"""
    try:
        requirements = {
            'consistency': request.consistency.dict(),
            'availability': request.availability.dict(),
            'partition_tolerance': request.partition_tolerance.dict(),
            'workload': request.workload.dict() if request.workload else {},
            'scenario': request.scenario
        }

        result = analyzer.analyze(requirements)

        return {
            'success': True,
            'cap_analysis': {
                'classification': result['classification'],
                'scores': result['scores'],
                'tradeoff_summary': result['tradeoff_summary']
            },
            'recommendations': result['recommendations'][:5]  # Top 5
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/cap/systems")
async def list_systems():
    """åˆ—å‡ºæ‰€æœ‰ç³»ç»Ÿ"""
    return {
        'systems': [
            {
                'name': s['name'],
                'cap_profile': s['cap_profile'],
                'best_for': s['best_for']
            }
            for s in analyzer.systems_database
        ]
    }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    import uvicorn
    uvicorn.run(app, host='0.0.0.0', port=8080)
```

### 3.3 å®é™…æ¡ˆä¾‹

**æ¡ˆä¾‹1: é‡‘èäº¤æ˜“ç³»ç»Ÿ**

```python
request = CAPAnalysisRequest(
    consistency=ConsistencyRequirement(level='strict', latency_tolerance_ms=50),
    availability=AvailabilityRequirement(target=99.99, acceptable_downtime_sec=300),
    partition_tolerance=PartitionToleranceRequirement(network_type='datacenter'),
    scenario='financial'
)

result = analyzer.analyze(request.dict())

# è¾“å‡º:
# classification: 'CP System'
# scores: {'C': 95, 'A': 75, 'P': 65}
# Top recommendation: PostgreSQL + Streaming Replication (match_score: 92)
```

**æ¡ˆä¾‹2: ç¤¾äº¤ç½‘ç»œç³»ç»Ÿ**

```python
request = CAPAnalysisRequest(
    consistency=ConsistencyRequirement(level='eventual', latency_tolerance_ms=500),
    availability=AvailabilityRequirement(target=99.99, acceptable_downtime_sec=60),
    partition_tolerance=PartitionToleranceRequirement(network_type='global'),
    scenario='social'
)

result = analyzer.analyze(request.dict())

# è¾“å‡º:
# classification: 'AP System'
# scores: {'C': 45, 'A': 90, 'P': 95}
# Top recommendation: Cassandra (match_score: 95)
```

---

## å››ã€ä½¿ç”¨æŒ‡å—

### 4.1 Webç•Œé¢ä½¿ç”¨

```bash
# è®¿é—®åœ¨çº¿å·¥å…·
https://tools.db-theory.org/cap-analyzer

# æˆ–æœ¬åœ°è¿è¡Œ
docker run -p 8080:8080 db-tools/cap-analyzer:latest
```

### 4.2 APIè°ƒç”¨

```python
import requests

response = requests.post('https://api.db-theory.org/cap/analyze', json={
    'requirements': {
        'consistency': {'level': 'strong'},
        'availability': {'target': 99.99},
        'partition_tolerance': {'network_type': 'wan'}
    }
})

result = response.json()
print(f"Classification: {result['cap_analysis']['classification']}")
print(f"Top recommendation: {result['recommendations'][0]['system']}")
```

---

---

## äº”ã€åä¾‹ä¸é”™è¯¯ä½¿ç”¨

### åä¾‹1: å¿½ç•¥ä¸šåŠ¡éœ€æ±‚ç›²ç›®ä½¿ç”¨å·¥å…·

**é”™è¯¯ä½¿ç”¨**:

```python
# é”™è¯¯: å®Œå…¨ä¾èµ–å·¥å…·æ¨è
result = analyzer.analyze(requirements)
system = result['recommendations'][0]['system']
# ç›´æ¥ä½¿ç”¨ï¼Œä¸éªŒè¯æ˜¯å¦é€‚åˆä¸šåŠ¡
```

**é—®é¢˜**: å·¥å…·æ˜¯è¾…åŠ©ï¼Œæœ€ç»ˆå†³ç­–éœ€ç»“åˆä¸šåŠ¡

**æ­£ç¡®ä½¿ç”¨**:

```python
# æ­£ç¡®: å·¥å…·æ¨è + ä¸šåŠ¡éªŒè¯
result = analyzer.analyze(requirements)
recommendations = result['recommendations']

# ç»“åˆä¸šåŠ¡éœ€æ±‚é€‰æ‹©
for rec in recommendations:
    if validate_business_requirements(rec):
        return rec
```

### åä¾‹2: è¯„åˆ†æƒé‡è®¾ç½®ä¸åˆç†

**é”™è¯¯ä½¿ç”¨**:

```python
# é”™è¯¯: æ‰€æœ‰ç»´åº¦æƒé‡ç›¸åŒ
weights = {
    'consistency': 1.0,
    'availability': 1.0,
    'performance': 1.0
}
# å¿½ç•¥ä¸šåŠ¡ä¼˜å…ˆçº§
```

**é—®é¢˜**: ä¸åŒä¸šåŠ¡åœºæ™¯ä¼˜å…ˆçº§ä¸åŒ

**æ­£ç¡®ä½¿ç”¨**:

```python
# æ­£ç¡®: æ ¹æ®ä¸šåŠ¡è®¾ç½®æƒé‡
if business_type == 'financial':
    weights = {'consistency': 0.5, 'availability': 0.3, 'performance': 0.2}
elif business_type == 'social':
    weights = {'consistency': 0.2, 'availability': 0.5, 'performance': 0.3}
```

---

---

## å…­ã€å®é™…åº”ç”¨æ¡ˆä¾‹

### 6.1 æ¡ˆä¾‹: æŸå…¬å¸åˆ†å¸ƒå¼ç³»ç»Ÿé€‰å‹

**åœºæ™¯**: å¤§å‹äº’è”ç½‘å…¬å¸æ–°ç³»ç»Ÿé€‰å‹

**ä½¿ç”¨å·¥å…·**: CAPæƒè¡¡åˆ†æå™¨

**è¾“å…¥å‚æ•°**:

```yaml
requirements:
  consistency: strong
  availability: 99.99
  partition_tolerance: required
  workload:
    read_ratio: 0.8
    qps: 100000
```

**åˆ†æç»“æœ**:

```json
{
  "recommended_systems": [
    {
      "name": "PostgreSQL (åŒæ­¥å¤åˆ¶)",
      "cap": "CP",
      "score": 85,
      "reason": "å¼ºä¸€è‡´æ€§è¦æ±‚ï¼Œå¯æ¥å—åˆ†åŒºæ—¶ä¸å¯ç”¨"
    },
    {
      "name": "Cassandra",
      "cap": "AP",
      "score": 60,
      "reason": "é«˜å¯ç”¨ï¼Œä½†æœ€ç»ˆä¸€è‡´æ€§ä¸ç¬¦åˆè¦æ±‚"
    }
  ],
  "final_decision": "PostgreSQL (åŒæ­¥å¤åˆ¶)"
}
```

**å†³ç­–æ•ˆæœ**: ç³»ç»Ÿé€‰å‹æ—¶é—´ä»1ä¸ªæœˆé™åˆ°3å¤©ï¼ˆ-90%ï¼‰

### 6.2 æ¡ˆä¾‹: äº‘æ•°æ®åº“CAPé€‰æ‹©

**åœºæ™¯**: äº‘æ•°æ®åº“æœåŠ¡CAPé…ç½®

**ä½¿ç”¨å·¥å…·**: CAPæƒè¡¡åˆ†æå™¨

**åˆ†æè¿‡ç¨‹**:

- ä¸ºä¸åŒç§Ÿæˆ·æ¨èä¸åŒCAPé…ç½®
- é‡‘èç§Ÿæˆ·: CPï¼ˆå¼ºä¸€è‡´æ€§ï¼‰
- ç¤¾äº¤ç§Ÿæˆ·: APï¼ˆé«˜å¯ç”¨ï¼‰

**æŠ€æœ¯æ–¹æ¡ˆ**:

```python
# å¤šç§Ÿæˆ·CAPé…ç½®
def configure_cap_for_tenant(tenant_id, requirements):
    analyzer = CAPAnalyzer()
    result = analyzer.analyze(requirements)

    if result['cap'] == 'CP':
        # é…ç½®åŒæ­¥å¤åˆ¶
        configure_sync_replication(tenant_id)
    elif result['cap'] == 'AP':
        # é…ç½®å¼‚æ­¥å¤åˆ¶
        configure_async_replication(tenant_id)
```

**ä¼˜åŒ–æ•ˆæœ**: ç§Ÿæˆ·æ»¡æ„åº¦æå‡30%

---

**å·¥å…·ç‰ˆæœ¬**: 2.0.0ï¼ˆå¤§å¹…å……å®ï¼‰
**æœ€åæ›´æ–°**: 2025-12-05
**æ–°å¢å†…å®¹**: å®Œæ•´è¯„åˆ†ç®—æ³•ã€ç³»ç»Ÿæ•°æ®åº“ã€Web APIã€å®é™…æ¡ˆä¾‹ã€åä¾‹åˆ†æã€å®é™…åº”ç”¨æ¡ˆä¾‹

**å·¥å…·ä»£ç **: ç”Ÿäº§çº§Pythonå®ç°ï¼ˆFastAPIï¼‰
**GitHub**: <https://github.com/db-theory/cap-analyzer>

**å…³è”æ–‡æ¡£**:

- `01-æ ¸å¿ƒç†è®ºæ¨¡å‹/04-CAPç†è®ºä¸æƒè¡¡.md`
- `04-åˆ†å¸ƒå¼æ‰©å±•/05-CAPå®è·µæ¡ˆä¾‹.md`
- `09-å·¥ä¸šæ¡ˆä¾‹åº“/03-ç¤¾äº¤ç½‘ç»œç³»ç»Ÿ.md` (APæ¡ˆä¾‹)
- `09-å·¥ä¸šæ¡ˆä¾‹åº“/02-é‡‘èäº¤æ˜“ç³»ç»Ÿ.md` (CPæ¡ˆä¾‹)
