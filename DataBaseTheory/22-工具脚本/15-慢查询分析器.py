#!/usr/bin/env python3
"""
PostgreSQLæ…¢æŸ¥è¯¢åˆ†æå™¨
åŠŸèƒ½: åˆ†æpg_stat_statementsï¼Œæä¾›ä¼˜åŒ–å»ºè®®
"""

import psycopg2
from psycopg2.extras import RealDictCursor
import argparse
import re
from collections import defaultdict

class SlowQueryAnalyzer:
    """æ…¢æŸ¥è¯¢åˆ†æå™¨"""
    
    def __init__(self, conn_str: str):
        self.conn = psycopg2.connect(conn_str, cursor_factory=RealDictCursor)
        self.cursor = self.conn.cursor()
        self.slow_queries = []
    
    def analyze_slow_queries(self, threshold_ms: float = 100):
        """åˆ†ææ…¢æŸ¥è¯¢"""
        
        # æ£€æŸ¥æ‰©å±•
        self.cursor.execute("SELECT COUNT(*) FROM pg_extension WHERE extname = 'pg_stat_statements';")
        if self.cursor.fetchone()['count'] == 0:
            print("é”™è¯¯: pg_stat_statementsæœªå®‰è£…")
            return []
        
        # è·å–æ…¢æŸ¥è¯¢
        self.cursor.execute("""
            SELECT 
                queryid,
                query,
                calls,
                total_exec_time / 1000 AS total_sec,
                mean_exec_time AS avg_ms,
                min_exec_time AS min_ms,
                max_exec_time AS max_ms,
                stddev_exec_time AS stddev_ms,
                rows,
                100.0 * shared_blks_hit / NULLIF(shared_blks_hit + shared_blks_read, 0) AS cache_hit_ratio
            FROM pg_stat_statements
            WHERE mean_exec_time > %s
              AND calls > 10
            ORDER BY total_exec_time DESC
            LIMIT 50;
        """, (threshold_ms,))
        
        self.slow_queries = [dict(row) for row in self.cursor.fetchall()]
        return self.slow_queries
    
    def classify_query_type(self, query: str) -> str:
        """åˆ†ç±»æŸ¥è¯¢ç±»å‹"""
        query_upper = query.upper()
        
        if query_upper.startswith('SELECT'):
            if 'JOIN' in query_upper:
                return 'SELECT_JOIN'
            elif 'GROUP BY' in query_upper:
                return 'SELECT_AGGREGATE'
            else:
                return 'SELECT_SIMPLE'
        elif query_upper.startswith('INSERT'):
            return 'INSERT'
        elif query_upper.startswith('UPDATE'):
            return 'UPDATE'
        elif query_upper.startswith('DELETE'):
            return 'DELETE'
        else:
            return 'OTHER'
    
    def suggest_optimizations(self, query_info: dict) -> list:
        """æä¾›ä¼˜åŒ–å»ºè®®"""
        
        suggestions = []
        query = query_info['query']
        query_upper = query.upper()
        
        # å»ºè®®1: SELECT *
        if 'SELECT *' in query_upper:
            suggestions.append({
                'type': 'SELECT',
                'issue': 'ä½¿ç”¨SELECT *',
                'suggestion': 'åªé€‰æ‹©éœ€è¦çš„åˆ—',
                'impact': 'HIGH'
            })
        
        # å»ºè®®2: ç¼ºå°‘LIMIT
        if 'SELECT' in query_upper and 'LIMIT' not in query_upper and query_info['rows'] > 1000:
            suggestions.append({
                'type': 'SELECT',
                'issue': f"è¿”å›{query_info['rows']}è¡Œä½†æ— LIMIT",
                'suggestion': 'æ·»åŠ LIMITé™åˆ¶è¿”å›è¡Œæ•°',
                'impact': 'MEDIUM'
            })
        
        # å»ºè®®3: ç¼“å­˜å‘½ä¸­ç‡ä½
        if query_info['cache_hit_ratio'] and query_info['cache_hit_ratio'] < 90:
            suggestions.append({
                'type': 'CACHE',
                'issue': f"ç¼“å­˜å‘½ä¸­ç‡{query_info['cache_hit_ratio']:.1f}%",
                'suggestion': 'å¯èƒ½ç¼ºå°‘ç´¢å¼•æˆ–shared_buffersä¸è¶³',
                'impact': 'HIGH'
            })
        
        # å»ºè®®4: é«˜æ ‡å‡†å·®ï¼ˆæ€§èƒ½ä¸ç¨³å®šï¼‰
        if query_info['stddev_ms'] > query_info['avg_ms'] * 0.5:
            suggestions.append({
                'type': 'STABILITY',
                'issue': f"æ‰§è¡Œæ—¶é—´æ³¢åŠ¨å¤§(stddev={query_info['stddev_ms']:.1f}ms)",
                'suggestion': 'æ£€æŸ¥æ˜¯å¦æœ‰é—´æ­‡æ€§é”ç­‰å¾…æˆ–èµ„æºç«äº‰',
                'impact': 'MEDIUM'
            })
        
        # å»ºè®®5: ORæ¡ä»¶
        if ' OR ' in query_upper and 'WHERE' in query_upper:
            suggestions.append({
                'type': 'LOGIC',
                'issue': 'åŒ…å«ORæ¡ä»¶',
                'suggestion': 'è€ƒè™‘ä½¿ç”¨UNIONæˆ–INå­å¥',
                'impact': 'MEDIUM'
            })
        
        # å»ºè®®6: å‡½æ•°åŒ…è£¹ç´¢å¼•åˆ—
        patterns = [
            (r'WHERE\s+\w+\([^)]*\w+\s*\)', 'WHEREå­å¥ä¸­ä½¿ç”¨å‡½æ•°åŒ…è£¹åˆ—'),
            (r'LOWER\s*\(', 'ä½¿ç”¨LOWER()å‡½æ•°ï¼Œè€ƒè™‘åˆ›å»ºè¡¨è¾¾å¼ç´¢å¼•'),
            (r'UPPER\s*\(', 'ä½¿ç”¨UPPER()å‡½æ•°ï¼Œè€ƒè™‘åˆ›å»ºè¡¨è¾¾å¼ç´¢å¼•')
        ]
        
        for pattern, issue in patterns:
            if re.search(pattern, query_upper):
                suggestions.append({
                    'type': 'INDEX',
                    'issue': issue,
                    'suggestion': 'åˆ›å»ºè¡¨è¾¾å¼ç´¢å¼•æˆ–æ”¹å†™æŸ¥è¯¢',
                    'impact': 'HIGH'
                })
                break
        
        # å»ºè®®7: å­æŸ¥è¯¢
        if ' IN (' in query_upper and 'SELECT' in query_upper.split(' IN (')[1]:
            suggestions.append({
                'type': 'SUBQUERY',
                'issue': 'INå­æŸ¥è¯¢',
                'suggestion': 'è€ƒè™‘ä½¿ç”¨JOINæˆ–EXISTS',
                'impact': 'MEDIUM'
            })
        
        return suggestions
    
    def generate_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        
        print("\n" + "="*80)
        print("PostgreSQLæ…¢æŸ¥è¯¢åˆ†ææŠ¥å‘Š")
        print("="*80 + "\n")
        
        if not self.slow_queries:
            print("âœ… æ— æ…¢æŸ¥è¯¢ï¼ˆæ‰€æœ‰æŸ¥è¯¢<100msï¼‰")
            return
        
        print(f"å‘ç° {len(self.slow_queries)} ä¸ªæ…¢æŸ¥è¯¢\n")
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        by_type = defaultdict(list)
        for q in self.slow_queries:
            qtype = self.classify_query_type(q['query'])
            by_type[qtype].append(q)
        
        print("æŒ‰ç±»å‹åˆ†å¸ƒ:")
        for qtype, queries in sorted(by_type.items(), key=lambda x: -len(x[1])):
            total_time = sum(q['total_sec'] for q in queries)
            print(f"  {qtype}: {len(queries)}ä¸ªæŸ¥è¯¢, æ€»è€—æ—¶{total_time:.2f}ç§’")
        print()
        
        # è¯¦ç»†åˆ†æTop 10
        print("="*80)
        print("Top 10æ…¢æŸ¥è¯¢è¯¦ç»†åˆ†æ")
        print("="*80 + "\n")
        
        for i, query_info in enumerate(self.slow_queries[:10], 1):
            print(f"\nã€{i}ã€‘QueryID: {query_info['queryid']}")
            print(f"æŸ¥è¯¢: {query_info['query'][:200]}...")
            print(f"è°ƒç”¨æ¬¡æ•°: {query_info['calls']}")
            print(f"å¹³å‡è€—æ—¶: {query_info['avg_ms']:.2f}ms")
            print(f"æœ€å°/æœ€å¤§: {query_info['min_ms']:.2f}ms / {query_info['max_ms']:.2f}ms")
            print(f"æ ‡å‡†å·®: {query_info['stddev_ms']:.2f}ms")
            print(f"æ€»è€—æ—¶: {query_info['total_sec']:.2f}ç§’")
            if query_info['cache_hit_ratio']:
                print(f"ç¼“å­˜å‘½ä¸­ç‡: {query_info['cache_hit_ratio']:.1f}%")
            
            # ä¼˜åŒ–å»ºè®®
            suggestions = self.suggest_optimizations(query_info)
            if suggestions:
                print(f"\nä¼˜åŒ–å»ºè®®:")
                for sug in suggestions:
                    impact_color = "ğŸ”´" if sug['impact'] == 'HIGH' else "ğŸŸ¡"
                    print(f"  {impact_color} [{sug['type']}] {sug['issue']}")
                    print(f"     å»ºè®®: {sug['suggestion']}")
            
            print("-"*80)
    
    def export_explain_plans(self, output_file: str):
        """å¯¼å‡ºæŸ¥è¯¢è®¡åˆ’"""
        
        with open(output_file, 'w') as f:
            for query_info in self.slow_queries[:20]:
                f.write(f"\n{'='*80}\n")
                f.write(f"QueryID: {query_info['queryid']}\n")
                f.write(f"Query: {query_info['query']}\n")
                f.write(f"{'='*80}\n\n")
                
                try:
                    self.cursor.execute(f"EXPLAIN (ANALYZE, BUFFERS) {query_info['query']}")
                    plan = self.cursor.fetchall()
                    for row in plan:
                        f.write(f"{row[0]}\n")
                except Exception as e:
                    f.write(f"EXPLAINå¤±è´¥: {e}\n")
                
                f.write("\n")
        
        print(f"\nâœ… æŸ¥è¯¢è®¡åˆ’å·²å¯¼å‡ºåˆ°: {output_file}")
    
    def close(self):
        self.cursor.close()
        self.conn.close()

def main():
    parser = argparse.ArgumentParser(description='PostgreSQLæ…¢æŸ¥è¯¢åˆ†æå™¨')
    parser.add_argument('--host', default='localhost')
    parser.add_argument('--port', type=int, default=5432)
    parser.add_argument('--dbname', required=True)
    parser.add_argument('--user', default='postgres')
    parser.add_argument('--password')
    parser.add_argument('--threshold', type=float, default=100, 
                       help='æ…¢æŸ¥è¯¢é˜ˆå€¼(ms)')
    parser.add_argument('--export-plans', help='å¯¼å‡ºEXPLAINè®¡åˆ’åˆ°æ–‡ä»¶')
    
    args = parser.parse_args()
    
    conn_str = f"host={args.host} port={args.port} dbname={args.dbname} user={args.user}"
    if args.password:
        conn_str += f" password={args.password}"
    
    try:
        analyzer = SlowQueryAnalyzer(conn_str)
        
        # åˆ†æ
        analyzer.analyze_slow_queries(args.threshold)
        
        # ç”ŸæˆæŠ¥å‘Š
        analyzer.generate_report()
        
        # å¯¼å‡ºè®¡åˆ’
        if args.export_plans:
            analyzer.export_explain_plans(args.export_plans)
        
        analyzer.close()
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == '__main__':
    main()
```

**ä½¿ç”¨**:
```bash
# åˆ†ææ…¢æŸ¥è¯¢ï¼ˆ>100msï¼‰
python3 15-æ…¢æŸ¥è¯¢åˆ†æå™¨.py --dbname mydb --user postgres

# è‡ªå®šä¹‰é˜ˆå€¼ï¼ˆ>50msï¼‰
python3 15-æ…¢æŸ¥è¯¢åˆ†æå™¨.py --dbname mydb --threshold 50

# å¯¼å‡ºEXPLAINè®¡åˆ’
python3 15-æ…¢æŸ¥è¯¢åˆ†æå™¨.py --dbname mydb --export-plans slow_queries_plans.txt

# å®šæ—¶åˆ†æ
0 */4 * * * python3 /path/to/15-æ…¢æŸ¥è¯¢åˆ†æå™¨.py --dbname mydb --export-plans /var/log/slow_queries_$(date +\%Y\%m\%d_\%H).txt
```
