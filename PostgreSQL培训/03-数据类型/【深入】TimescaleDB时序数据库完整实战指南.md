# ã€æ·±å…¥ã€‘TimescaleDBæ—¶åºæ•°æ®åº“å®Œæ•´å®æˆ˜æŒ‡å—

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0 | **åˆ›å»ºæ—¥æœŸ**: 2025-01 | **é€‚ç”¨ç‰ˆæœ¬**: PostgreSQL 13+, TimescaleDB 2.13+
> **éš¾åº¦ç­‰çº§**: â­â­â­â­ é«˜çº§ | **é¢„è®¡å­¦ä¹ æ—¶é—´**: 8-10å°æ—¶

---

## ğŸ“‹ ç›®å½•

- [ã€æ·±å…¥ã€‘TimescaleDBæ—¶åºæ•°æ®åº“å®Œæ•´å®æˆ˜æŒ‡å—](#æ·±å…¥timescaledbæ—¶åºæ•°æ®åº“å®Œæ•´å®æˆ˜æŒ‡å—)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1. è¯¾ç¨‹æ¦‚è¿°](#1-è¯¾ç¨‹æ¦‚è¿°)
    - [1.1 ä»€ä¹ˆæ˜¯TimescaleDBï¼Ÿ](#11-ä»€ä¹ˆæ˜¯timescaledb)
      - [æ ¸å¿ƒç‰¹æ€§](#æ ¸å¿ƒç‰¹æ€§)
      - [é€‚ç”¨åœºæ™¯](#é€‚ç”¨åœºæ™¯)
    - [1.2 TimescaleDB vs å…¶ä»–æ–¹æ¡ˆ](#12-timescaledb-vs-å…¶ä»–æ–¹æ¡ˆ)
  - [2. æ—¶åºæ•°æ®åº“åŸºç¡€](#2-æ—¶åºæ•°æ®åº“åŸºç¡€)
    - [2.1 æ—¶åºæ•°æ®ç‰¹å¾](#21-æ—¶åºæ•°æ®ç‰¹å¾)
      - [ç¤ºä¾‹ï¼šIoTä¼ æ„Ÿå™¨æ•°æ®](#ç¤ºä¾‹iotä¼ æ„Ÿå™¨æ•°æ®)
    - [2.2 æ—¶åºæ•°æ®å­˜å‚¨æŒ‘æˆ˜](#22-æ—¶åºæ•°æ®å­˜å‚¨æŒ‘æˆ˜)
  - [3. TimescaleDBæ¶æ„](#3-timescaledbæ¶æ„)
    - [3.1 æ ¸å¿ƒæ¦‚å¿µ](#31-æ ¸å¿ƒæ¦‚å¿µ)
    - [3.2 Chunkç®¡ç†](#32-chunkç®¡ç†)
    - [3.3 æ¶æ„å›¾](#33-æ¶æ„å›¾)
  - [4. å®‰è£…ä¸é…ç½®](#4-å®‰è£…ä¸é…ç½®)
    - [4.1 å®‰è£…TimescaleDB](#41-å®‰è£…timescaledb)
      - [Ubuntu/Debian](#ubuntudebian)
      - [Docker](#docker)
      - [Docker Compose](#docker-compose)
    - [4.2 åˆå§‹åŒ–](#42-åˆå§‹åŒ–)
    - [4.3 æ€§èƒ½è°ƒä¼˜é…ç½®](#43-æ€§èƒ½è°ƒä¼˜é…ç½®)
  - [5. Hypertableè¶…è¡¨](#5-hypertableè¶…è¡¨)
    - [5.1 åˆ›å»ºHypertable](#51-åˆ›å»ºhypertable)
    - [5.2 å¤šç»´åˆ†åŒº](#52-å¤šç»´åˆ†åŒº)
    - [5.3 Hypertableç®¡ç†](#53-hypertableç®¡ç†)
  - [6. æ•°æ®å†™å…¥ä¼˜åŒ–](#6-æ•°æ®å†™å…¥ä¼˜åŒ–)
    - [6.1 æ‰¹é‡å†™å…¥](#61-æ‰¹é‡å†™å…¥)
    - [6.2 å¹¶è¡Œå†™å…¥](#62-å¹¶è¡Œå†™å…¥)
    - [6.3 æ— åºå†™å…¥ä¼˜åŒ–](#63-æ— åºå†™å…¥ä¼˜åŒ–)
  - [7. æ—¶åºæŸ¥è¯¢](#7-æ—¶åºæŸ¥è¯¢)
    - [7.1 æ—¶é—´æ¡¶èšåˆï¼ˆtime\_bucketï¼‰](#71-æ—¶é—´æ¡¶èšåˆtime_bucket)
    - [7.2 Gap Fillingï¼ˆå¡«è¡¥ç¼ºå¤±ï¼‰](#72-gap-fillingå¡«è¡¥ç¼ºå¤±)
    - [7.3 çª—å£å‡½æ•°](#73-çª—å£å‡½æ•°)
    - [7.4 Downsamplingï¼ˆé™é‡‡æ ·ï¼‰](#74-downsamplingé™é‡‡æ ·)
  - [8. è¿ç»­èšåˆ](#8-è¿ç»­èšåˆ)
    - [8.1 Continuous AggregateåŸºç¡€](#81-continuous-aggregateåŸºç¡€)
    - [8.2 å®æ—¶èšåˆ](#82-å®æ—¶èšåˆ)
    - [8.3 å¤šçº§èšåˆ](#83-å¤šçº§èšåˆ)
    - [8.4 è¿ç»­èšåˆç®¡ç†](#84-è¿ç»­èšåˆç®¡ç†)
  - [9. æ•°æ®å‹ç¼©ä¸ä¿ç•™](#9-æ•°æ®å‹ç¼©ä¸ä¿ç•™)
    - [9.1 æ•°æ®å‹ç¼©](#91-æ•°æ®å‹ç¼©)
      - [å¯ç”¨å‹ç¼©](#å¯ç”¨å‹ç¼©)
      - [å‹ç¼©åŸç†](#å‹ç¼©åŸç†)
    - [9.2 æ•°æ®ä¿ç•™ç­–ç•¥](#92-æ•°æ®ä¿ç•™ç­–ç•¥)
    - [9.3 åˆ†å±‚å­˜å‚¨](#93-åˆ†å±‚å­˜å‚¨)
  - [10. é«˜çº§ç‰¹æ€§](#10-é«˜çº§ç‰¹æ€§)
    - [10.1 Hyperfunctionsï¼ˆé«˜çº§æ—¶åºå‡½æ•°ï¼‰](#101-hyperfunctionsé«˜çº§æ—¶åºå‡½æ•°)
    - [10.2 æ•°æ®åˆ†å±‚æŸ¥è¯¢](#102-æ•°æ®åˆ†å±‚æŸ¥è¯¢)
    - [10.3 åˆ†å¸ƒå¼Hypertableï¼ˆå¤šèŠ‚ç‚¹ï¼‰](#103-åˆ†å¸ƒå¼hypertableå¤šèŠ‚ç‚¹)
  - [11. æ€§èƒ½ä¼˜åŒ–](#11-æ€§èƒ½ä¼˜åŒ–)
    - [11.1 ç´¢å¼•ç­–ç•¥](#111-ç´¢å¼•ç­–ç•¥)
    - [11.2 æŸ¥è¯¢ä¼˜åŒ–](#112-æŸ¥è¯¢ä¼˜åŒ–)
    - [11.3 æ‰¹é‡æ“ä½œä¼˜åŒ–](#113-æ‰¹é‡æ“ä½œä¼˜åŒ–)
    - [11.4 ç›‘æ§æŸ¥è¯¢](#114-ç›‘æ§æŸ¥è¯¢)
  - [12. ç”Ÿäº§å®æˆ˜æ¡ˆä¾‹](#12-ç”Ÿäº§å®æˆ˜æ¡ˆä¾‹)
    - [12.1 æ¡ˆä¾‹1ï¼šIoTè®¾å¤‡ç›‘æ§å¹³å°](#121-æ¡ˆä¾‹1iotè®¾å¤‡ç›‘æ§å¹³å°)
      - [éœ€æ±‚](#éœ€æ±‚)
      - [å®ç°](#å®ç°)
    - [12.2 æ¡ˆä¾‹2ï¼šé‡‘èå¸‚åœºæ•°æ®](#122-æ¡ˆä¾‹2é‡‘èå¸‚åœºæ•°æ®)
    - [12.3 æ¡ˆä¾‹3ï¼šAPMï¼ˆåº”ç”¨æ€§èƒ½ç›‘æ§ï¼‰](#123-æ¡ˆä¾‹3apmåº”ç”¨æ€§èƒ½ç›‘æ§)
  - [13. æœ€ä½³å®è·µ](#13-æœ€ä½³å®è·µ)
    - [13.1 è®¾è®¡åŸåˆ™](#131-è®¾è®¡åŸåˆ™)
      - [âœ… æ¨èåšæ³•](#-æ¨èåšæ³•)
    - [13.2 è¿ç»´Checklist](#132-è¿ç»´checklist)
    - [13.3 æ€§èƒ½è°ƒä¼˜Checklist](#133-æ€§èƒ½è°ƒä¼˜checklist)
  - [14. FAQä¸ç–‘éš¾è§£ç­”](#14-faqä¸ç–‘éš¾è§£ç­”)
    - [Q1: Hypertable vs PostgreSQLåˆ†åŒºè¡¨ï¼Ÿ](#q1-hypertable-vs-postgresqlåˆ†åŒºè¡¨)
    - [Q2: chunkæ•°é‡è¿‡å¤šæ€ä¹ˆåŠï¼Ÿ](#q2-chunkæ•°é‡è¿‡å¤šæ€ä¹ˆåŠ)
    - [Q3: å‹ç¼©åèƒ½æ›´æ–°æ•°æ®å—ï¼Ÿ](#q3-å‹ç¼©åèƒ½æ›´æ–°æ•°æ®å—)
    - [Q4: å¦‚ä½•è¿ç§»ç°æœ‰PostgreSQLæ—¶åºæ•°æ®åˆ°TimescaleDBï¼Ÿ](#q4-å¦‚ä½•è¿ç§»ç°æœ‰postgresqlæ—¶åºæ•°æ®åˆ°timescaledb)
    - [Q5: TimescaleDBå¯ä»¥ç”¨äºéæ—¶åºæ•°æ®å—ï¼Ÿ](#q5-timescaledbå¯ä»¥ç”¨äºéæ—¶åºæ•°æ®å—)
  - [ğŸ“š å»¶ä¼¸é˜…è¯»](#-å»¶ä¼¸é˜…è¯»)
    - [å®˜æ–¹èµ„æº](#å®˜æ–¹èµ„æº)
    - [ç›¸å…³æŠ€æœ¯](#ç›¸å…³æŠ€æœ¯)
    - [æ¨èé˜…è¯»](#æ¨èé˜…è¯»)
  - [âœ… å­¦ä¹ æ£€æŸ¥æ¸…å•](#-å­¦ä¹ æ£€æŸ¥æ¸…å•)
  - [ğŸ’¡ ä¸‹ä¸€æ­¥å­¦ä¹ ](#-ä¸‹ä¸€æ­¥å­¦ä¹ )

---

## 1. è¯¾ç¨‹æ¦‚è¿°

### 1.1 ä»€ä¹ˆæ˜¯TimescaleDBï¼Ÿ

**TimescaleDB** æ˜¯PostgreSQLçš„æ—¶åºæ•°æ®åº“æ‰©å±•ï¼Œä¸“ä¸ºæ—¶é—´åºåˆ—æ•°æ®ä¼˜åŒ–ï¼Œæä¾›10-100å€çš„æ’å…¥æ€§èƒ½å’Œè‡ªåŠ¨æ•°æ®ç®¡ç†ã€‚

#### æ ¸å¿ƒç‰¹æ€§

| ç‰¹æ€§ | è¯´æ˜ | ä¼˜åŠ¿ |
|------|------|------|
| **Hypertable** | è‡ªåŠ¨åˆ†åŒºç®¡ç† | æ— éœ€æ‰‹åŠ¨åˆ›å»ºåˆ†åŒº |
| **é«˜é€Ÿå†™å…¥** | æ‰¹é‡æ’å…¥ä¼˜åŒ– | 100ä¸‡+ rows/ç§’ |
| **è¿ç»­èšåˆ** | å®æ—¶ç‰©åŒ–è§†å›¾ | ç§’çº§å»¶è¿Ÿ |
| **æ•°æ®å‹ç¼©** | åˆ—å¼å‹ç¼© | èŠ‚çœ90%+å­˜å‚¨ |
| **æ•°æ®ä¿ç•™** | è‡ªåŠ¨è¿‡æœŸåˆ é™¤ | æ— éœ€ç»´æŠ¤è„šæœ¬ |
| **SQLå…¼å®¹** | 100% PostgreSQL | é›¶å­¦ä¹ æˆæœ¬ |
| **æ—¶åºå‡½æ•°** | ä¸“ç”¨åˆ†æå‡½æ•° | gap filling, LOCFç­‰ |

#### é€‚ç”¨åœºæ™¯

**âœ… ç†æƒ³åœºæ™¯**:

- IoTè®¾å¤‡ç›‘æ§ï¼ˆä¼ æ„Ÿå™¨æ•°æ®ï¼‰
- åº”ç”¨æ€§èƒ½ç›‘æ§ï¼ˆAPMï¼‰
- é‡‘èå¸‚åœºæ•°æ®ï¼ˆè‚¡ç¥¨ã€åŠ å¯†è´§å¸ï¼‰
- æ—¥å¿—ä¸äº‹ä»¶æ•°æ®
- æ°”è±¡ä¸ç¯å¢ƒç›‘æµ‹
- å·¥ä¸šç”Ÿäº§ç›‘æ§

**âŒ ä¸é€‚åˆ**:

- éæ—¶åºæ•°æ®ä¸ºä¸»
- å¤§é‡æ›´æ–°/åˆ é™¤æ“ä½œ
- å¤æ‚äº‹åŠ¡å¤„ç†

### 1.2 TimescaleDB vs å…¶ä»–æ–¹æ¡ˆ

```text
TimescaleDB vs InfluxDB:
âœ… SQLæ ‡å‡†ï¼ˆå­¦ä¹ æˆæœ¬ä½ï¼‰
âœ… å…³ç³»æ•°æ®+æ—¶åºæ•°æ®æ··åˆ
âœ… å¤æ‚æŸ¥è¯¢æ”¯æŒæ›´å¥½
âœ… ACIDäº‹åŠ¡ä¿è¯
âŒ çº¯æ—¶åºå†™å…¥æ€§èƒ½ç•¥é€Š

TimescaleDB vs ClickHouse:
âœ… å®æ—¶å†™å…¥ï¼ˆéæ‰¹é‡ï¼‰
âœ… æ›´æ–°/åˆ é™¤æ”¯æŒ
âœ… OLTP+OLAPæ··åˆ
âŒ çº¯OLAPåˆ†ææ€§èƒ½ç•¥é€Š
âŒ æ•°æ®å‹ç¼©æ¯”ä¸å¦‚ClickHouse

TimescaleDB vs Prometheus:
âœ… æ›´çµæ´»çš„æ•°æ®æ¨¡å‹
âœ… æ›´é•¿çš„æ•°æ®ä¿ç•™
âœ… æ›´å¤æ‚çš„æŸ¥è¯¢
âœ… å…³ç³»æ•°æ®JOIN
âŒ MetricsæŠ“å–ç”Ÿæ€ä¸å¦‚Prometheus

TimescaleDB vs åŸç”ŸPostgreSQLåˆ†åŒº:
âœ… è‡ªåŠ¨åˆ†åŒºç®¡ç†ï¼ˆæ— éœ€æ‰‹åŠ¨åˆ›å»ºï¼‰
âœ… è‡ªåŠ¨å‹ç¼©ã€ä¿ç•™ç­–ç•¥
âœ… è¿ç»­èšåˆï¼ˆå¢é‡æ›´æ–°ï¼‰
âœ… æ—¶åºä¸“ç”¨å‡½æ•°
âœ… æ€§èƒ½ä¼˜åŒ–ï¼ˆæ‰¹é‡æ’å…¥ã€æŸ¥è¯¢ï¼‰
```

---

## 2. æ—¶åºæ•°æ®åº“åŸºç¡€

### 2.1 æ—¶åºæ•°æ®ç‰¹å¾

```text
æ—¶åºæ•°æ®çš„å…¸å‹ç‰¹å¾ï¼š

1. æ—¶é—´æˆ³ï¼šæ¯æ¡è®°å½•å¿…æœ‰æ—¶é—´æ ‡è¯†
2. åªè¿½åŠ ï¼šåŸºæœ¬åªå†™å…¥ï¼Œæå°‘æ›´æ–°
3. æ—¶é—´é¡ºåºï¼šæŒ‰æ—¶é—´æ’åºæŸ¥è¯¢
4. èšåˆåˆ†æï¼šæ—¶é—´çª—å£èšåˆï¼ˆavg, max, minï¼‰
5. æ•°æ®é‡å¤§ï¼šæŒç»­ä¸æ–­äº§ç”Ÿ
6. å†·çƒ­åˆ†å±‚ï¼šè¿‘æœŸæ•°æ®çƒ­ï¼Œå†å²æ•°æ®å†·
```

#### ç¤ºä¾‹ï¼šIoTä¼ æ„Ÿå™¨æ•°æ®

```sql
-- å…¸å‹æ—¶åºæ•°æ®è¡¨
CREATE TABLE sensor_data (
    time TIMESTAMPTZ NOT NULL,      -- æ—¶é—´æˆ³
    device_id INT NOT NULL,          -- è®¾å¤‡IDï¼ˆç»´åº¦ï¼‰
    location TEXT,                   -- ä½ç½®ï¼ˆç»´åº¦ï¼‰
    temperature DOUBLE PRECISION,    -- æ¸©åº¦ï¼ˆæŒ‡æ ‡ï¼‰
    humidity DOUBLE PRECISION,       -- æ¹¿åº¦ï¼ˆæŒ‡æ ‡ï¼‰
    pressure DOUBLE PRECISION        -- æ°”å‹ï¼ˆæŒ‡æ ‡ï¼‰
);

-- æŸ¥è¯¢æ¨¡å¼ï¼šæ—¶é—´èŒƒå›´ + èšåˆ
SELECT
    time_bucket('1 hour', time) AS hour,
    device_id,
    AVG(temperature) AS avg_temp,
    MAX(humidity) AS max_humidity
FROM sensor_data
WHERE time >= NOW() - INTERVAL '24 hours'
GROUP BY hour, device_id
ORDER BY hour DESC;
```

### 2.2 æ—¶åºæ•°æ®å­˜å‚¨æŒ‘æˆ˜

| æŒ‘æˆ˜ | ä¼ ç»ŸPostgreSQL | TimescaleDBè§£å†³æ–¹æ¡ˆ |
|------|----------------|---------------------|
| **é«˜å¹¶å‘å†™å…¥** | å•è¡¨é”ç«äº‰ | è‡ªåŠ¨åˆ†ç‰‡ï¼Œå¹¶è¡Œå†™å…¥ |
| **æµ·é‡æ•°æ®** | ç´¢å¼•è†¨èƒ€ | è‡ªåŠ¨åˆ†åŒº+å‹ç¼© |
| **æŸ¥è¯¢æ€§èƒ½** | å…¨è¡¨æ‰«æ | æ—¶é—´ç´¢å¼•+åˆ†åŒºè£å‰ª |
| **æ•°æ®è€åŒ–** | æ‰‹åŠ¨åˆ é™¤ | è‡ªåŠ¨ä¿ç•™ç­–ç•¥ |
| **èšåˆåˆ†æ** | æ¯æ¬¡é‡ç®— | è¿ç»­èšåˆï¼ˆå¢é‡ï¼‰ |

---

## 3. TimescaleDBæ¶æ„

### 3.1 æ ¸å¿ƒæ¦‚å¿µ

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Hypertableï¼ˆé€»è¾‘è¡¨ï¼‰            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  SELECT * FROM sensor_data;     â”‚ â”‚
â”‚  â”‚  â†’ è‡ªåŠ¨è·¯ç”±åˆ°ç›¸å…³Chunk           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    è‡ªåŠ¨åˆ†åŒºï¼ˆæŒ‰æ—¶é—´ï¼‰
           â”‚
     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚           â”‚         â”‚          â”‚
  â”Œâ”€â”€â–¼â”€â”€â”    â”Œâ”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
  â”‚Chunkâ”‚    â”‚Chunkâ”‚  â”‚Chunkâ”‚   â”‚ Chunk â”‚
  â”‚ 1   â”‚    â”‚ 2   â”‚  â”‚ 3   â”‚   â”‚  ...  â”‚
  â”‚(1å¤©)â”‚    â”‚(1å¤©)â”‚  â”‚(1å¤©)â”‚   â”‚       â”‚
  â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜
  æœªå‹ç¼©     å‹ç¼©     å‹ç¼©       å·²å½’æ¡£

ç‰¹ç‚¹ï¼š
- Hypertableï¼šç”¨æˆ·è§†è§’çš„å•è¡¨
- Chunkï¼šå†…éƒ¨å®é™…åˆ†åŒºï¼ˆæŒ‰æ—¶é—´è‡ªåŠ¨åˆ›å»ºï¼‰
- é€æ˜ï¼šåº”ç”¨æ— éœ€å…³å¿ƒåˆ†åŒºç»†èŠ‚
- è‡ªåŠ¨ï¼šåˆ›å»ºã€å‹ç¼©ã€åˆ é™¤å…¨è‡ªåŠ¨
```

### 3.2 Chunkç®¡ç†

```sql
-- Chunkæ˜¯å®é™…å­˜å‚¨æ•°æ®çš„åˆ†åŒº
-- ç¤ºä¾‹ï¼šsensor_dataçš„chunkç»“æ„

_timescaledb_internal._hyper_1_1_chunk  -- 2025-01-01
_timescaledb_internal._hyper_1_2_chunk  -- 2025-01-02
_timescaledb_internal._hyper_1_3_chunk  -- 2025-01-03
...

-- ç”¨æˆ·æŸ¥è¯¢Hypertable
SELECT * FROM sensor_data WHERE time >= '2025-01-02';

-- å†…éƒ¨å®é™…æŸ¥è¯¢ï¼ˆåˆ†åŒºè£å‰ªï¼‰
SELECT * FROM _timescaledb_internal._hyper_1_2_chunk
UNION ALL
SELECT * FROM _timescaledb_internal._hyper_1_3_chunk;
-- è‡ªåŠ¨è·³è¿‡ä¸ç›¸å…³çš„chunk
```

### 3.3 æ¶æ„å›¾

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Application Layer                   â”‚
â”‚  JDBC / psycopg2 / Go pgx / ...            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         TimescaleDB Extension               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Hypertable Manager                  â”‚  â”‚
â”‚  â”‚  - è‡ªåŠ¨åˆ›å»º/åˆ é™¤chunk                 â”‚  â”‚
â”‚  â”‚  - æŸ¥è¯¢è·¯ç”±                          â”‚  â”‚
â”‚  â”‚  - åˆ†åŒºè£å‰ª                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Continuous Aggregate Engine         â”‚  â”‚
â”‚  â”‚  - å¢é‡æ›´æ–°ç‰©åŒ–è§†å›¾                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Compression Engine                  â”‚  â”‚
â”‚  â”‚  - åˆ—å¼å‹ç¼©                          â”‚  â”‚
â”‚  â”‚  - è‡ªåŠ¨å‹ç¼©ç­–ç•¥                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Retention Policy                    â”‚  â”‚
â”‚  â”‚  - è‡ªåŠ¨åˆ é™¤è¿‡æœŸæ•°æ®                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PostgreSQL Core                     â”‚
â”‚  Storage / Index / Transaction / WAL        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. å®‰è£…ä¸é…ç½®

### 4.1 å®‰è£…TimescaleDB

#### Ubuntu/Debian

```bash
# æ·»åŠ TimescaleDBä»“åº“
sudo apt-get install -y gnupg postgresql-common apt-transport-https lsb-release wget
sudo sh -c "echo 'deb https://packagecloud.io/timescale/timescaledb/ubuntu/ $(lsb_release -c -s) main' > /etc/apt/sources.list.d/timescaledb.list"
wget --quiet -O - https://packagecloud.io/timescale/timescaledb/gpgkey | sudo apt-key add -

# æ›´æ–°å¹¶å®‰è£…
sudo apt-get update
sudo apt-get install -y timescaledb-2-postgresql-15

# è¿è¡Œé…ç½®è„šæœ¬
sudo timescaledb-tune --quiet --yes

# é‡å¯PostgreSQL
sudo systemctl restart postgresql
```

#### Docker

```bash
# å¿«é€Ÿå¯åŠ¨
docker run -d --name timescaledb \
  -p 5432:5432 \
  -e POSTGRES_PASSWORD=password \
  timescale/timescaledb:latest-pg15

# è¿æ¥
docker exec -it timescaledb psql -U postgres
```

#### Docker Compose

```yaml
version: '3.8'
services:
  timescaledb:
    image: timescale/timescaledb:latest-pg15
    container_name: timescaledb
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: tsdb
    volumes:
      - timescaledb-data:/var/lib/postgresql/data
    command:
      - "postgres"
      - "-c"
      - "shared_preload_libraries=timescaledb"
      - "-c"
      - "max_connections=200"
      - "-c"
      - "shared_buffers=512MB"

volumes:
  timescaledb-data:
```

### 4.2 åˆå§‹åŒ–

```sql
-- åˆ›å»ºæ‰©å±•
CREATE EXTENSION IF NOT EXISTS timescaledb;

-- éªŒè¯å®‰è£…
SELECT default_version, installed_version
FROM pg_available_extensions
WHERE name = 'timescaledb';

-- æŸ¥çœ‹ç‰ˆæœ¬ä¿¡æ¯
SELECT extversion FROM pg_extension WHERE extname = 'timescaledb';

-- æŸ¥çœ‹é…ç½®
SHOW timescaledb.telemetry_level;
SHOW timescaledb.max_background_workers;
```

### 4.3 æ€§èƒ½è°ƒä¼˜é…ç½®

```sql
-- postgresql.confæ¨èé…ç½®ï¼ˆ16GB RAMæœåŠ¡å™¨ï¼‰

-- åŸºç¡€é…ç½®
shared_preload_libraries = 'timescaledb'
max_connections = 200

-- å†…å­˜é…ç½®
shared_buffers = 4GB
effective_cache_size = 12GB
maintenance_work_mem = 1GB
work_mem = 64MB

-- WALé…ç½®
wal_buffers = 16MB
min_wal_size = 1GB
max_wal_size = 4GB
checkpoint_completion_target = 0.9

-- TimescaleDBç‰¹å®šé…ç½®
timescaledb.max_background_workers = 8
timescaledb.last_tuned = '2025-01-01 00:00:00'
timescaledb.last_tuned_version = '0.15.0'

-- å¹¶è¡ŒæŸ¥è¯¢
max_parallel_workers_per_gather = 4
max_parallel_workers = 8

-- æ‰§è¡Œé…ç½®å
SELECT pg_reload_conf();
```

---

## 5. Hypertableè¶…è¡¨

### 5.1 åˆ›å»ºHypertable

```sql
-- 1. åˆ›å»ºæ™®é€šè¡¨
CREATE TABLE sensor_data (
    time TIMESTAMPTZ NOT NULL,
    device_id INT NOT NULL,
    temperature DOUBLE PRECISION,
    humidity DOUBLE PRECISION,
    cpu_usage DOUBLE PRECISION
);

-- 2. è½¬æ¢ä¸ºHypertable
SELECT create_hypertable(
    'sensor_data',           -- è¡¨å
    'time',                  -- æ—¶é—´åˆ—
    chunk_time_interval => INTERVAL '1 day',  -- æ¯ä¸ªchunkçš„æ—¶é—´è·¨åº¦
    if_not_exists => TRUE
);

-- 3. åˆ›å»ºç´¢å¼•
CREATE INDEX sensor_data_device_time_idx
ON sensor_data (device_id, time DESC);

CREATE INDEX sensor_data_time_idx
ON sensor_data (time DESC);
```

### 5.2 å¤šç»´åˆ†åŒº

```sql
-- æŒ‰æ—¶é—´+ç©ºé—´ç»´åº¦åˆ†åŒº
CREATE TABLE metrics (
    time TIMESTAMPTZ NOT NULL,
    device_id INT NOT NULL,
    location TEXT NOT NULL,
    value DOUBLE PRECISION
);

SELECT create_hypertable(
    'metrics',
    'time',
    partitioning_column => 'device_id',  -- ç©ºé—´åˆ†åŒºåˆ—
    number_partitions => 4,              -- ç©ºé—´åˆ†åŒºæ•°
    chunk_time_interval => INTERVAL '1 day'
);

-- ç»“æœï¼š4ä¸ªç©ºé—´åˆ†åŒº Ã— Nä¸ªæ—¶é—´åˆ†åŒº = 4Nä¸ªchunk
-- ä¼˜åŠ¿ï¼šå¹¶è¡Œå†™å…¥ã€æŸ¥è¯¢æ—¶å¯ä»¥åŒæ—¶è£å‰ªæ—¶é—´å’Œç©ºé—´
```

### 5.3 Hypertableç®¡ç†

```sql
-- æŸ¥çœ‹æ‰€æœ‰Hypertable
SELECT * FROM timescaledb_information.hypertables;

-- æŸ¥çœ‹ç‰¹å®šHypertableçš„chunk
SELECT * FROM timescaledb_information.chunks
WHERE hypertable_name = 'sensor_data'
ORDER BY range_start DESC;

-- æŸ¥çœ‹Hypertableè¯¦ç»†ä¿¡æ¯
SELECT
    hypertable_name,
    num_chunks,
    table_size,
    index_size,
    total_size
FROM timescaledb_information.hypertable
WHERE hypertable_name = 'sensor_data';

-- åˆ é™¤Hypertable
DROP TABLE sensor_data;  -- è‡ªåŠ¨æ¸…ç†æ‰€æœ‰chunk
```

---

## 6. æ•°æ®å†™å…¥ä¼˜åŒ–

### 6.1 æ‰¹é‡å†™å…¥

```sql
-- âŒ æ…¢ï¼šé€è¡Œæ’å…¥
DO $$
BEGIN
    FOR i IN 1..10000 LOOP
        INSERT INTO sensor_data (time, device_id, temperature)
        VALUES (NOW(), 1, 25.0 + random());
    END LOOP;
END $$;

-- âœ… å¿«ï¼šæ‰¹é‡æ’å…¥
INSERT INTO sensor_data (time, device_id, temperature, humidity)
SELECT
    NOW() - (random() * INTERVAL '1 day'),
    (random() * 100)::INT,
    20 + random() * 15,
    40 + random() * 40
FROM generate_series(1, 1000000);

-- âœ… æœ€å¿«ï¼šCOPY
COPY sensor_data (time, device_id, temperature, humidity)
FROM STDIN CSV;
-- ... å¤§é‡æ•°æ® ...
\.
```

### 6.2 å¹¶è¡Œå†™å…¥

```python
# Pythonå¹¶è¡Œå†™å…¥ç¤ºä¾‹
import psycopg2
from multiprocessing import Pool
import random
from datetime import datetime, timedelta

def insert_batch(worker_id):
    conn = psycopg2.connect("dbname=tsdb user=postgres")
    cur = conn.cursor()

    # æ¯ä¸ªworkeræ’å…¥100ä¸‡æ¡
    batch_size = 10000
    for batch in range(100):
        data = [
            (
                datetime.now() - timedelta(seconds=random.randint(0, 86400)),
                random.randint(1, 1000),
                20 + random.random() * 15,
                40 + random.random() * 40
            )
            for _ in range(batch_size)
        ]

        cur.executemany(
            "INSERT INTO sensor_data (time, device_id, temperature, humidity) VALUES (%s, %s, %s, %s)",
            data
        )
        conn.commit()

        if batch % 10 == 0:
            print(f"Worker {worker_id}: {batch * batch_size} rows inserted")

    cur.close()
    conn.close()

# å¯åŠ¨8ä¸ªå¹¶è¡Œworker
if __name__ == '__main__':
    with Pool(8) as pool:
        pool.map(insert_batch, range(8))
```

### 6.3 æ— åºå†™å…¥ä¼˜åŒ–

```sql
-- é—®é¢˜ï¼šæ—¶é—´æˆ³ä¹±åºæ’å…¥ï¼ˆIoTæ•°æ®å»¶è¿Ÿåˆ°è¾¾ï¼‰
-- TimescaleDBä¼˜åŒ–ï¼šè‡ªåŠ¨è·¯ç”±åˆ°æ­£ç¡®çš„chunk

-- é…ç½®ï¼šå…è®¸è¾ƒå¤§çš„æ—¶é—´çª—å£ä¹±åº
SELECT set_chunk_time_interval('sensor_data', INTERVAL '1 day');

-- å¦‚æœæ•°æ®å¯èƒ½å»¶è¿Ÿæ•°å¤©ï¼Œå¢å¤§chunké—´éš”
SELECT set_chunk_time_interval('sensor_data', INTERVAL '7 days');

-- æˆ–ä½¿ç”¨æ›´ç»†ç²’åº¦çš„chunk + åˆå¹¶
-- ï¼ˆé€‚ç”¨äºå¤§éƒ¨åˆ†æ•°æ®æŒ‰æ—¶é—´æœ‰åºï¼Œå°‘é‡ä¹±åºï¼‰
```

---

## 7. æ—¶åºæŸ¥è¯¢

### 7.1 æ—¶é—´æ¡¶èšåˆï¼ˆtime_bucketï¼‰

```sql
-- æŒ‰å°æ—¶èšåˆ
SELECT
    time_bucket('1 hour', time) AS hour,
    device_id,
    AVG(temperature) AS avg_temp,
    MAX(temperature) AS max_temp,
    MIN(temperature) AS min_temp,
    COUNT(*) AS sample_count
FROM sensor_data
WHERE time >= NOW() - INTERVAL '24 hours'
GROUP BY hour, device_id
ORDER BY hour DESC, device_id;

-- æŒ‰5åˆ†é’Ÿèšåˆ
SELECT
    time_bucket('5 minutes', time) AS bucket,
    AVG(cpu_usage) AS avg_cpu,
    MAX(cpu_usage) AS max_cpu
FROM sensor_data
WHERE time >= NOW() - INTERVAL '1 hour'
  AND device_id = 123
GROUP BY bucket
ORDER BY bucket DESC;

-- æŒ‰å¤©èšåˆï¼ˆä½¿ç”¨originå¯¹é½åˆ°å‡Œæ™¨ï¼‰
SELECT
    time_bucket('1 day', time, TIMESTAMPTZ '2025-01-01 00:00:00') AS day,
    COUNT(*) AS daily_count
FROM sensor_data
GROUP BY day
ORDER BY day DESC;
```

### 7.2 Gap Fillingï¼ˆå¡«è¡¥ç¼ºå¤±ï¼‰

```sql
-- é—®é¢˜ï¼šè®¾å¤‡ç¦»çº¿å¯¼è‡´æ•°æ®ç¼ºå¤±
SELECT
    time_bucket('1 hour', time) AS hour,
    AVG(temperature) AS avg_temp
FROM sensor_data
WHERE device_id = 123
  AND time >= '2025-01-01' AND time < '2025-01-02'
GROUP BY hour
ORDER BY hour;

-- ç»“æœå¯èƒ½ç¼ºå°‘æŸäº›å°æ—¶

-- è§£å†³ï¼štime_bucket_gapfill
SELECT
    time_bucket_gapfill('1 hour', time) AS hour,
    AVG(temperature) AS avg_temp,
    locf(AVG(temperature)) AS avg_temp_filled  -- Last Observation Carried Forward
FROM sensor_data
WHERE device_id = 123
  AND time >= '2025-01-01' AND time < '2025-01-02'
GROUP BY hour
ORDER BY hour;

-- æˆ–ä½¿ç”¨interpolateï¼ˆçº¿æ€§æ’å€¼ï¼‰
SELECT
    time_bucket_gapfill('1 hour', time) AS hour,
    interpolate(AVG(temperature)) AS avg_temp_interpolated
FROM sensor_data
WHERE device_id = 123
  AND time >= '2025-01-01' AND time < '2025-01-02'
GROUP BY hour
ORDER BY hour;
```

### 7.3 çª—å£å‡½æ•°

```sql
-- è®¡ç®—ç§»åŠ¨å¹³å‡ï¼ˆæ»‘åŠ¨çª—å£ï¼‰
SELECT
    time,
    device_id,
    temperature,
    AVG(temperature) OVER (
        PARTITION BY device_id
        ORDER BY time
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    ) AS moving_avg_10
FROM sensor_data
WHERE device_id = 123
  AND time >= NOW() - INTERVAL '1 hour'
ORDER BY time DESC;

-- è®¡ç®—å˜åŒ–ç‡
SELECT
    time,
    device_id,
    temperature,
    temperature - LAG(temperature) OVER (
        PARTITION BY device_id ORDER BY time
    ) AS temp_change
FROM sensor_data
WHERE device_id = 123
  AND time >= NOW() - INTERVAL '1 hour'
ORDER BY time DESC;

-- æ£€æµ‹å¼‚å¸¸ï¼ˆè¶…è¿‡3å€æ ‡å‡†å·®ï¼‰
WITH stats AS (
    SELECT
        device_id,
        AVG(temperature) AS mean,
        STDDEV(temperature) AS stddev
    FROM sensor_data
    WHERE time >= NOW() - INTERVAL '7 days'
    GROUP BY device_id
)
SELECT
    sd.time,
    sd.device_id,
    sd.temperature,
    s.mean,
    s.stddev,
    (sd.temperature - s.mean) / s.stddev AS z_score
FROM sensor_data sd
JOIN stats s ON sd.device_id = s.device_id
WHERE sd.time >= NOW() - INTERVAL '1 hour'
  AND ABS((sd.temperature - s.mean) / s.stddev) > 3
ORDER BY sd.time DESC;
```

### 7.4 Downsamplingï¼ˆé™é‡‡æ ·ï¼‰

```sql
-- å°†é«˜é¢‘æ•°æ®é™é‡‡æ ·åˆ°ä½é¢‘
CREATE TABLE sensor_data_hourly AS
SELECT
    time_bucket('1 hour', time) AS hour,
    device_id,
    AVG(temperature) AS avg_temperature,
    MAX(temperature) AS max_temperature,
    MIN(temperature) AS min_temperature,
    AVG(humidity) AS avg_humidity,
    COUNT(*) AS sample_count
FROM sensor_data
WHERE time >= '2025-01-01'
GROUP BY hour, device_id;

-- åˆ›å»ºä¸ºHypertable
SELECT create_hypertable(
    'sensor_data_hourly',
    'hour',
    chunk_time_interval => INTERVAL '7 days'
);

-- æŸ¥è¯¢æ—¶ä¼˜å…ˆä½¿ç”¨é™é‡‡æ ·è¡¨
SELECT * FROM sensor_data_hourly
WHERE hour >= NOW() - INTERVAL '30 days';
```

---

## 8. è¿ç»­èšåˆ

### 8.1 Continuous AggregateåŸºç¡€

```sql
-- åˆ›å»ºè¿ç»­èšåˆï¼ˆç±»ä¼¼ç‰©åŒ–è§†å›¾ï¼Œä½†å¢é‡æ›´æ–°ï¼‰
CREATE MATERIALIZED VIEW sensor_hourly
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 hour', time) AS hour,
    device_id,
    AVG(temperature) AS avg_temp,
    MAX(temperature) AS max_temp,
    MIN(temperature) AS min_temp,
    COUNT(*) AS sample_count
FROM sensor_data
GROUP BY hour, device_id;

-- æ·»åŠ åˆ·æ–°ç­–ç•¥ï¼ˆè‡ªåŠ¨æ›´æ–°ï¼‰
SELECT add_continuous_aggregate_policy(
    'sensor_hourly',
    start_offset => INTERVAL '3 hours',    -- ä¿ç•™3å°æ—¶æ•°æ®ä¸èšåˆï¼ˆå…è®¸ä¹±åºï¼‰
    end_offset => INTERVAL '1 hour',       -- æœ€æ–°1å°æ—¶ä¸èšåˆï¼ˆå®æ—¶æŸ¥è¯¢åŸè¡¨ï¼‰
    schedule_interval => INTERVAL '1 hour' -- æ¯å°æ—¶æ›´æ–°ä¸€æ¬¡
);

-- æŸ¥è¯¢è¿ç»­èšåˆï¼ˆåƒæ™®é€šè¡¨ä¸€æ ·ï¼‰
SELECT * FROM sensor_hourly
WHERE hour >= NOW() - INTERVAL '7 days'
  AND device_id = 123
ORDER BY hour DESC;
```

### 8.2 å®æ—¶èšåˆ

```sql
-- å®æ—¶èšåˆï¼šè‡ªåŠ¨åˆå¹¶ç‰©åŒ–æ•°æ®+æœ€æ–°å®æ—¶æ•°æ®
ALTER MATERIALIZED VIEW sensor_hourly SET (timescaledb.materialized_only = false);

-- æŸ¥è¯¢æ—¶è‡ªåŠ¨åˆå¹¶ï¼š
-- 1. å·²ç‰©åŒ–çš„å†å²æ•°æ®ï¼ˆå¿«ï¼‰
-- 2. æœ€æ–°1å°æ—¶çš„å®æ—¶æ•°æ®ï¼ˆå®æ—¶è®¡ç®—ï¼‰
SELECT * FROM sensor_hourly
WHERE hour >= NOW() - INTERVAL '7 days';
-- æ— ç¼æ•´åˆç‰©åŒ–+å®æ—¶æ•°æ®ï¼
```

### 8.3 å¤šçº§èšåˆ

```sql
-- åŸå§‹æ•°æ®ï¼šæ¯ç§’
-- ä¸€çº§èšåˆï¼šæ¯åˆ†é’Ÿ
CREATE MATERIALIZED VIEW sensor_minutely
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 minute', time) AS minute,
    device_id,
    AVG(temperature) AS avg_temp,
    MAX(temperature) AS max_temp,
    MIN(temperature) AS min_temp
FROM sensor_data
GROUP BY minute, device_id;

-- äºŒçº§èšåˆï¼šæ¯å°æ—¶ï¼ˆåŸºäºåˆ†é’Ÿèšåˆï¼‰
CREATE MATERIALIZED VIEW sensor_hourly_from_minutely
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 hour', minute) AS hour,
    device_id,
    AVG(avg_temp) AS avg_temp,  -- æ³¨æ„ï¼šavg of avgï¼ˆè¿‘ä¼¼ï¼‰
    MAX(max_temp) AS max_temp,
    MIN(min_temp) AS min_temp
FROM sensor_minutely
GROUP BY hour, device_id;

-- ä¸‰çº§èšåˆï¼šæ¯å¤©
CREATE MATERIALIZED VIEW sensor_daily
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 day', hour) AS day,
    device_id,
    AVG(avg_temp) AS avg_temp,
    MAX(max_temp) AS max_temp,
    MIN(min_temp) AS min_temp
FROM sensor_hourly_from_minutely
GROUP BY day, device_id;

-- æŸ¥è¯¢ç­–ç•¥ï¼šæ ¹æ®æ—¶é—´èŒƒå›´é€‰æ‹©åˆé€‚çš„èšåˆçº§åˆ«
-- < 1å°æ—¶    â†’ sensor_dataï¼ˆåŸå§‹ï¼‰
-- 1å°æ—¶-1å¤©  â†’ sensor_minutely
-- 1å¤©-30å¤©   â†’ sensor_hourly
-- > 30å¤©     â†’ sensor_daily
```

### 8.4 è¿ç»­èšåˆç®¡ç†

```sql
-- æŸ¥çœ‹æ‰€æœ‰è¿ç»­èšåˆ
SELECT * FROM timescaledb_information.continuous_aggregates;

-- æŸ¥çœ‹åˆ·æ–°ç­–ç•¥
SELECT * FROM timescaledb_information.jobs
WHERE proc_name = 'policy_refresh_continuous_aggregate';

-- æ‰‹åŠ¨åˆ·æ–°
CALL refresh_continuous_aggregate('sensor_hourly', '2025-01-01', '2025-01-02');

-- åˆ é™¤åˆ·æ–°ç­–ç•¥
SELECT remove_continuous_aggregate_policy('sensor_hourly');

-- åˆ é™¤è¿ç»­èšåˆ
DROP MATERIALIZED VIEW sensor_hourly;
```

---

## 9. æ•°æ®å‹ç¼©ä¸ä¿ç•™

### 9.1 æ•°æ®å‹ç¼©

#### å¯ç”¨å‹ç¼©

```sql
-- 1. åœ¨Hypertableä¸Šå¯ç”¨å‹ç¼©
ALTER TABLE sensor_data SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'device_id',  -- æŒ‰è®¾å¤‡åˆ†æ®µå‹ç¼©
    timescaledb.compress_orderby = 'time DESC'     -- æ—¶é—´é™åºæ’åˆ—
);

-- 2. æ·»åŠ è‡ªåŠ¨å‹ç¼©ç­–ç•¥
SELECT add_compression_policy(
    'sensor_data',
    compress_after => INTERVAL '7 days'  -- 7å¤©åå‹ç¼©
);

-- 3. æ‰‹åŠ¨å‹ç¼©ç‰¹å®šchunk
SELECT compress_chunk(i) FROM show_chunks('sensor_data', older_than => INTERVAL '8 days') i;

-- æŸ¥çœ‹å‹ç¼©ç‡
SELECT
    chunk_schema,
    chunk_name,
    compression_status,
    before_compression_total_bytes,
    after_compression_total_bytes,
    pg_size_pretty(before_compression_total_bytes) AS size_before,
    pg_size_pretty(after_compression_total_bytes) AS size_after,
    ROUND((1 - after_compression_total_bytes::numeric / before_compression_total_bytes) * 100, 2) AS compression_ratio
FROM timescaledb_information.compressed_chunk_stats
WHERE hypertable_name = 'sensor_data';

-- å…¸å‹å‹ç¼©ç‡ï¼š90-95%ï¼
```

#### å‹ç¼©åŸç†

```text
æœªå‹ç¼©Chunkï¼ˆè¡Œå¼å­˜å‚¨ï¼‰ï¼š
Row 1: time=2025-01-01 00:00:00, device_id=1, temp=25.3, humidity=60.5
Row 2: time=2025-01-01 00:01:00, device_id=1, temp=25.4, humidity=60.6
Row 3: time=2025-01-01 00:02:00, device_id=1, temp=25.3, humidity=60.7
...

å‹ç¼©Chunkï¼ˆåˆ—å¼å­˜å‚¨+ç®—æ³•å‹ç¼©ï¼‰ï¼š
Segment: device_id=1
  timeåˆ—: [2025-01-01 00:00:00, 00:01:00, 00:02:00, ...] â†’ Deltaç¼–ç 
  tempåˆ—: [25.3, 25.4, 25.3, ...] â†’ Gorillaå‹ç¼©ï¼ˆæ—¶åºä¸“ç”¨ï¼‰
  humidityåˆ—: [60.5, 60.6, 60.7, ...] â†’ Gorillaå‹ç¼©

ä¼˜åŠ¿ï¼š
âœ… 90-95%å‹ç¼©ç‡
âœ… æŸ¥è¯¢æ—¶æ— éœ€è§£å‹å…¨éƒ¨æ•°æ®ï¼ˆåˆ—å¼è®¿é—®ï¼‰
âœ… è‡ªåŠ¨å‹ç¼©ï¼Œæ— éœ€ç»´æŠ¤
```

### 9.2 æ•°æ®ä¿ç•™ç­–ç•¥

```sql
-- è‡ªåŠ¨åˆ é™¤è¶…è¿‡1å¹´çš„æ•°æ®
SELECT add_retention_policy(
    'sensor_data',
    drop_after => INTERVAL '1 year'
);

-- æŸ¥çœ‹ä¿ç•™ç­–ç•¥
SELECT * FROM timescaledb_information.jobs
WHERE proc_name = 'policy_retention';

-- ä¿®æ”¹ä¿ç•™ç­–ç•¥
SELECT remove_retention_policy('sensor_data');
SELECT add_retention_policy('sensor_data', drop_after => INTERVAL '2 years');

-- æ‰‹åŠ¨åˆ é™¤æ—§chunk
DROP TABLE _timescaledb_internal._hyper_1_1_chunk;
-- æˆ–ä½¿ç”¨å‡½æ•°
SELECT drop_chunks('sensor_data', older_than => INTERVAL '2 years');
```

### 9.3 åˆ†å±‚å­˜å‚¨

```sql
-- ç­–ç•¥ï¼šçƒ­æ•°æ®ï¼ˆæœªå‹ç¼©ï¼‰+ æ¸©æ•°æ®ï¼ˆå‹ç¼©ï¼‰+ å†·æ•°æ®ï¼ˆå½’æ¡£/åˆ é™¤ï¼‰

-- çƒ­æ•°æ®ï¼šæœ€è¿‘7å¤©ï¼Œæœªå‹ç¼©ï¼Œå¿«é€Ÿè¯»å†™
-- æ¸©æ•°æ®ï¼š7-90å¤©ï¼Œå‹ç¼©ï¼ŒèŠ‚çœç©ºé—´
-- å†·æ•°æ®ï¼š>90å¤©ï¼Œå½’æ¡£åˆ°S3æˆ–åˆ é™¤

-- å®ç°ï¼š
-- 1. 7å¤©åå‹ç¼©
SELECT add_compression_policy('sensor_data', compress_after => INTERVAL '7 days');

-- 2. 90å¤©åå½’æ¡£åˆ°S3ï¼ˆä½¿ç”¨pg_dump + cronï¼‰
-- è„šæœ¬ç¤ºä¾‹ï¼šarchive_old_data.sh
#!/bin/bash
DATE_90_DAYS_AGO=$(date -d '90 days ago' +%Y-%m-%d)

# å¯¼å‡º90å¤©å‰çš„æ•°æ®
pg_dump -h localhost -U postgres -d tsdb \
  -t sensor_data \
  --where="time < '$DATE_90_DAYS_AGO'" \
  | gzip > s3://my-bucket/archives/sensor_data_${DATE_90_DAYS_AGO}.sql.gz

# åˆ é™¤å·²å½’æ¡£çš„æ•°æ®
psql -h localhost -U postgres -d tsdb -c \
  "SELECT drop_chunks('sensor_data', older_than => INTERVAL '90 days');"

-- 3. Cronå®šæ—¶æ‰§è¡Œ
-- 0 2 * * * /path/to/archive_old_data.sh
```

---

## 10. é«˜çº§ç‰¹æ€§

### 10.1 Hyperfunctionsï¼ˆé«˜çº§æ—¶åºå‡½æ•°ï¼‰

```sql
-- éœ€è¦å®‰è£…timescaledb_toolkit
CREATE EXTENSION timescaledb_toolkit;

-- 1. Stats Aggï¼ˆç»Ÿè®¡èšåˆï¼‰
SELECT
    device_id,
    average(stats_agg(temperature)) AS avg_temp,
    stddev(stats_agg(temperature)) AS stddev_temp,
    skewness(stats_agg(temperature)) AS skew,
    kurtosis(stats_agg(temperature)) AS kurt
FROM sensor_data
WHERE time >= NOW() - INTERVAL '24 hours'
GROUP BY device_id;

-- 2. Time-Weighted Averageï¼ˆæ—¶é—´åŠ æƒå¹³å‡ï¼‰
SELECT
    device_id,
    average(time_weight('LOCF', time, temperature)) AS time_weighted_avg
FROM sensor_data
WHERE time >= NOW() - INTERVAL '24 hours'
GROUP BY device_id;

-- 3. Heartbeat Aggï¼ˆæ£€æµ‹è®¾å¤‡åœ¨çº¿çŠ¶æ€ï¼‰
SELECT
    device_id,
    live_ranges(heartbeat_agg(time, INTERVAL '5 minutes')) AS uptime_ranges,
    uptime(heartbeat_agg(time, INTERVAL '5 minutes')) AS uptime_ratio
FROM sensor_data
WHERE time >= NOW() - INTERVAL '24 hours'
GROUP BY device_id;

-- 4. Counter Aggï¼ˆå•è°ƒé€’å¢è®¡æ•°å™¨å¤„ç†ï¼Œå¦‚ç½‘ç»œå­—èŠ‚æ•°ï¼‰
SELECT
    device_id,
    delta(counter_agg(time, bytes_sent)) AS total_bytes,
    rate(counter_agg(time, bytes_sent)) AS avg_rate
FROM network_stats
WHERE time >= NOW() - INTERVAL '1 hour'
GROUP BY device_id;
```

### 10.2 æ•°æ®åˆ†å±‚æŸ¥è¯¢

```sql
-- é€æ˜æŸ¥è¯¢ï¼šè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ•°æ®æº
CREATE VIEW sensor_unified AS
SELECT time, device_id, temperature FROM sensor_data          -- æœ€è¿‘æ•°æ®
WHERE time >= NOW() - INTERVAL '1 day'
UNION ALL
SELECT hour AS time, device_id, avg_temp AS temperature      -- ä¸­ç­‰å†å²
FROM sensor_hourly
WHERE hour >= NOW() - INTERVAL '30 days'
  AND hour < NOW() - INTERVAL '1 day'
UNION ALL
SELECT day AS time, device_id, avg_temp AS temperature       -- è¿œæœŸå†å²
FROM sensor_daily
WHERE day < NOW() - INTERVAL '30 days';

-- åº”ç”¨æŸ¥è¯¢ç»Ÿä¸€è§†å›¾ï¼Œæ— éœ€å…³å¿ƒæ•°æ®åœ¨å“ª
SELECT * FROM sensor_unified
WHERE device_id = 123
  AND time >= NOW() - INTERVAL '60 days';
```

### 10.3 åˆ†å¸ƒå¼Hypertableï¼ˆå¤šèŠ‚ç‚¹ï¼‰

```sql
-- TimescaleDBå¤šèŠ‚ç‚¹ï¼ˆEnterpriseç‰¹æ€§ï¼‰
-- ç±»ä¼¼Citusï¼Œå°†æ•°æ®åˆ†å¸ƒåˆ°å¤šä¸ªèŠ‚ç‚¹

-- åœ¨Access Nodeä¸Šï¼š
SELECT add_data_node('data_node_1', host => 'dn1.example.com');
SELECT add_data_node('data_node_2', host => 'dn2.example.com');
SELECT add_data_node('data_node_3', host => 'dn3.example.com');

-- åˆ›å»ºåˆ†å¸ƒå¼Hypertable
CREATE TABLE sensor_data_distributed (
    time TIMESTAMPTZ NOT NULL,
    device_id INT NOT NULL,
    temperature DOUBLE PRECISION
);

SELECT create_distributed_hypertable(
    'sensor_data_distributed',
    'time',
    'device_id',  -- ç©ºé—´åˆ†åŒºé”®
    number_partitions => 3,  -- åˆ†å¸ƒåˆ°3ä¸ªèŠ‚ç‚¹
    replication_factor => 2  -- 2å‰¯æœ¬
);

-- æŸ¥è¯¢ï¼šè‡ªåŠ¨å¹¶è¡Œæ‰§è¡Œï¼Œä»å¤šä¸ªèŠ‚ç‚¹èšåˆç»“æœ
SELECT
    time_bucket('1 hour', time) AS hour,
    AVG(temperature) AS avg_temp
FROM sensor_data_distributed
WHERE time >= NOW() - INTERVAL '7 days'
GROUP BY hour;
-- Access Nodeè‡ªåŠ¨è·¯ç”±æŸ¥è¯¢åˆ°Data Nodesï¼Œå¹¶è¡Œæ‰§è¡Œåèšåˆ
```

---

## 11. æ€§èƒ½ä¼˜åŒ–

### 11.1 ç´¢å¼•ç­–ç•¥

```sql
-- æ—¶åºæ•°æ®ç´¢å¼•åŸåˆ™ï¼š

-- 1. æ—¶é—´ç´¢å¼•ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
-- Hypertableè‡ªåŠ¨ä¸ºæ—¶é—´åˆ—åˆ›å»ºç´¢å¼•

-- 2. å¤åˆç´¢å¼•ï¼ˆç»´åº¦+æ—¶é—´ï¼‰
CREATE INDEX sensor_data_device_time_idx
ON sensor_data (device_id, time DESC);

-- 3. éƒ¨åˆ†ç´¢å¼•ï¼ˆç‰¹å®šæ¡ä»¶ï¼‰
CREATE INDEX sensor_data_high_temp_idx
ON sensor_data (time DESC)
WHERE temperature > 100;

-- 4. é¿å…åœ¨é«˜åŸºæ•°åˆ—ä¸Šåˆ›å»ºç´¢å¼•
-- âŒ ä¸è¦
CREATE INDEX sensor_data_temp_idx ON sensor_data (temperature);
-- æ—¶åºæ•°æ®çš„å€¼é€šå¸¸é«˜åŸºæ•°ä¸”ä¸å¸¸ç”¨äºç²¾ç¡®åŒ¹é…

-- 5. GINç´¢å¼•ç”¨äºJSONBåˆ—
ALTER TABLE sensor_data ADD COLUMN metadata JSONB;
CREATE INDEX sensor_data_metadata_gin_idx
ON sensor_data USING GIN (metadata);
```

### 11.2 æŸ¥è¯¢ä¼˜åŒ–

```sql
-- 1. å§‹ç»ˆåŒ…å«æ—¶é—´èŒƒå›´è¿‡æ»¤
-- âœ… å¥½
SELECT * FROM sensor_data
WHERE time >= NOW() - INTERVAL '1 hour'  -- åˆ†åŒºè£å‰ª
  AND device_id = 123;

-- âŒ å
SELECT * FROM sensor_data
WHERE device_id = 123;  -- æ‰«ææ‰€æœ‰chunk

-- 2. ä½¿ç”¨è¿ç»­èšåˆæ›¿ä»£é‡å¤èšåˆæŸ¥è¯¢
-- âŒ åï¼šæ¯æ¬¡æŸ¥è¯¢éƒ½é‡æ–°è®¡ç®—
SELECT
    time_bucket('1 hour', time) AS hour,
    AVG(temperature)
FROM sensor_data
WHERE time >= NOW() - INTERVAL '30 days'
GROUP BY hour;

-- âœ… å¥½ï¼šæŸ¥è¯¢é¢„èšåˆçš„è§†å›¾
SELECT * FROM sensor_hourly
WHERE hour >= NOW() - INTERVAL '30 days';

-- 3. ä½¿ç”¨EXPLAIN ANALYZEæŸ¥çœ‹æ‰§è¡Œè®¡åˆ’
EXPLAIN (ANALYZE, BUFFERS)
SELECT * FROM sensor_data
WHERE time >= NOW() - INTERVAL '1 hour'
  AND device_id = 123;

-- æ£€æŸ¥ï¼š
-- - Chunks Excluded by Constraintï¼ˆåˆ†åŒºè£å‰ªï¼‰
-- - Index Scan vs Seq Scan
-- - Planning Time vs Execution Time
```

### 11.3 æ‰¹é‡æ“ä½œä¼˜åŒ–

```sql
-- 1. ç¦ç”¨è‡ªåŠ¨èšåˆåˆ·æ–°ï¼ˆå¤§æ‰¹é‡å¯¼å…¥æ—¶ï¼‰
SELECT alter_job(
    (SELECT job_id FROM timescaledb_information.jobs
     WHERE proc_name = 'policy_refresh_continuous_aggregate'),
    scheduled => false
);

-- 2. æ‰¹é‡å¯¼å…¥
\COPY sensor_data FROM 'large_file.csv' CSV HEADER;

-- 3. é‡æ–°å¯ç”¨è‡ªåŠ¨åˆ·æ–°
SELECT alter_job(
    (SELECT job_id FROM timescaledb_information.jobs
     WHERE proc_name = 'policy_refresh_continuous_aggregate'),
    scheduled => true
);

-- 4. æ‰‹åŠ¨åˆ·æ–°è¿ç»­èšåˆ
CALL refresh_continuous_aggregate('sensor_hourly', NULL, NULL);
```

### 11.4 ç›‘æ§æŸ¥è¯¢

```sql
-- æŸ¥çœ‹chunkæ•°é‡å’Œå¤§å°
SELECT
    hypertable_name,
    COUNT(*) AS chunk_count,
    pg_size_pretty(SUM(total_bytes)) AS total_size,
    pg_size_pretty(AVG(total_bytes)) AS avg_chunk_size
FROM timescaledb_information.chunks
GROUP BY hypertable_name;

-- æŸ¥çœ‹å‹ç¼©æ•ˆæœ
SELECT
    hypertable_name,
    COUNT(*) AS compressed_chunks,
    pg_size_pretty(SUM(before_compression_total_bytes)) AS size_before,
    pg_size_pretty(SUM(after_compression_total_bytes)) AS size_after,
    ROUND(AVG(1 - after_compression_total_bytes::numeric / before_compression_total_bytes) * 100, 2) AS avg_compression_ratio
FROM timescaledb_information.compressed_chunk_stats
GROUP BY hypertable_name;

-- æŸ¥çœ‹åå°ä»»åŠ¡çŠ¶æ€
SELECT * FROM timescaledb_information.jobs;

-- æŸ¥çœ‹ä»»åŠ¡æ‰§è¡Œå†å²
SELECT * FROM timescaledb_information.job_stats
ORDER BY last_run_started_at DESC;
```

---

## 12. ç”Ÿäº§å®æˆ˜æ¡ˆä¾‹

### 12.1 æ¡ˆä¾‹1ï¼šIoTè®¾å¤‡ç›‘æ§å¹³å°

#### éœ€æ±‚

- 100ä¸‡+è®¾å¤‡
- æ¯è®¾å¤‡æ¯ç§’1æ¡æ•°æ®
- 100ä¸‡ writes/ç§’
- ä¿ç•™1å¹´æ•°æ®
- å®æ—¶Dashboard

#### å®ç°

```sql
-- 1. æ ¸å¿ƒè¡¨è®¾è®¡
CREATE TABLE device_metrics (
    time TIMESTAMPTZ NOT NULL,
    device_id INT NOT NULL,
    metric_type VARCHAR(50) NOT NULL,
    value DOUBLE PRECISION,
    tags JSONB
);

SELECT create_hypertable(
    'device_metrics',
    'time',
    chunk_time_interval => INTERVAL '1 day',
    partitioning_column => 'device_id',
    number_partitions => 16  -- 16ä¸ªç©ºé—´åˆ†åŒºï¼Œå¹¶è¡Œå†™å…¥
);

-- 2. ç´¢å¼•
CREATE INDEX device_metrics_device_time_idx
ON device_metrics (device_id, time DESC);

CREATE INDEX device_metrics_type_time_idx
ON device_metrics (metric_type, time DESC);

-- 3. è¿ç»­èšåˆï¼ˆ5åˆ†é’Ÿï¼‰
CREATE MATERIALIZED VIEW device_metrics_5min
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('5 minutes', time) AS bucket,
    device_id,
    metric_type,
    AVG(value) AS avg_value,
    MAX(value) AS max_value,
    MIN(value) AS min_value,
    COUNT(*) AS sample_count
FROM device_metrics
GROUP BY bucket, device_id, metric_type
WITH NO DATA;

SELECT add_continuous_aggregate_policy(
    'device_metrics_5min',
    start_offset => INTERVAL '1 hour',
    end_offset => INTERVAL '5 minutes',
    schedule_interval => INTERVAL '5 minutes'
);

-- 4. å‹ç¼©ç­–ç•¥
ALTER TABLE device_metrics SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'device_id, metric_type',
    timescaledb.compress_orderby = 'time DESC'
);

SELECT add_compression_policy('device_metrics', compress_after => INTERVAL '7 days');

-- 5. ä¿ç•™ç­–ç•¥
SELECT add_retention_policy('device_metrics', drop_after => INTERVAL '1 year');

-- 6. DashboardæŸ¥è¯¢ï¼ˆå®æ—¶ï¼‰
SELECT
    bucket,
    device_id,
    metric_type,
    avg_value,
    max_value
FROM device_metrics_5min
WHERE bucket >= NOW() - INTERVAL '1 hour'
  AND device_id = ANY(ARRAY[123, 456, 789])  -- ç”¨æˆ·å…³æ³¨çš„è®¾å¤‡
ORDER BY bucket DESC, device_id, metric_type;
```

### 12.2 æ¡ˆä¾‹2ï¼šé‡‘èå¸‚åœºæ•°æ®

```sql
-- é«˜é¢‘äº¤æ˜“æ•°æ®ï¼ˆTick Dataï¼‰
CREATE TABLE market_ticks (
    time TIMESTAMPTZ NOT NULL,
    symbol VARCHAR(20) NOT NULL,
    exchange VARCHAR(10) NOT NULL,
    price DECIMAL(18, 8) NOT NULL,
    volume DECIMAL(20, 8) NOT NULL,
    bid_price DECIMAL(18, 8),
    ask_price DECIMAL(18, 8)
);

SELECT create_hypertable(
    'market_ticks',
    'time',
    chunk_time_interval => INTERVAL '1 day',
    partitioning_column => 'symbol',
    number_partitions => 32
);

-- Kçº¿èšåˆï¼ˆ1åˆ†é’Ÿï¼‰
CREATE MATERIALIZED VIEW market_klines_1min
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 minute', time) AS bucket,
    symbol,
    exchange,
    FIRST(price, time) AS open,
    MAX(price) AS high,
    MIN(price) AS low,
    LAST(price, time) AS close,
    SUM(volume) AS volume,
    COUNT(*) AS tick_count
FROM market_ticks
GROUP BY bucket, symbol, exchange;

-- å¤šçº§èšåˆï¼š5åˆ†é’Ÿã€15åˆ†é’Ÿã€1å°æ—¶ã€1å¤©
CREATE MATERIALIZED VIEW market_klines_5min
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('5 minutes', bucket) AS bucket,
    symbol,
    exchange,
    FIRST(open, bucket) AS open,
    MAX(high) AS high,
    MIN(low) AS low,
    LAST(close, bucket) AS close,
    SUM(volume) AS volume
FROM market_klines_1min
GROUP BY bucket, symbol, exchange;

-- æŸ¥è¯¢ï¼šè·å–BTC-USDçš„æœ€è¿‘24å°æ—¶1å°æ—¶Kçº¿
SELECT * FROM market_klines_1hour
WHERE symbol = 'BTC-USD'
  AND bucket >= NOW() - INTERVAL '24 hours'
ORDER BY bucket DESC;
```

### 12.3 æ¡ˆä¾‹3ï¼šAPMï¼ˆåº”ç”¨æ€§èƒ½ç›‘æ§ï¼‰

```sql
-- HTTPè¯·æ±‚è¿½è¸ª
CREATE TABLE http_requests (
    time TIMESTAMPTZ NOT NULL,
    request_id UUID NOT NULL,
    service_name VARCHAR(100) NOT NULL,
    endpoint VARCHAR(200) NOT NULL,
    method VARCHAR(10) NOT NULL,
    status_code INT NOT NULL,
    duration_ms DOUBLE PRECISION NOT NULL,
    user_id BIGINT,
    ip_address INET,
    user_agent TEXT,
    tags JSONB
);

SELECT create_hypertable('http_requests', 'time', chunk_time_interval => INTERVAL '6 hours');

CREATE INDEX http_requests_service_time_idx ON http_requests (service_name, time DESC);
CREATE INDEX http_requests_endpoint_time_idx ON http_requests (endpoint, time DESC);
CREATE INDEX http_requests_tags_gin_idx ON http_requests USING GIN (tags);

-- æ€§èƒ½åˆ†æèšåˆ
CREATE MATERIALIZED VIEW http_requests_stats_5min
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('5 minutes', time) AS bucket,
    service_name,
    endpoint,
    method,
    COUNT(*) AS request_count,
    COUNT(*) FILTER (WHERE status_code >= 500) AS error_count,
    AVG(duration_ms) AS avg_duration,
    percentile_cont(0.95) WITHIN GROUP (ORDER BY duration_ms) AS p95_duration,
    percentile_cont(0.99) WITHIN GROUP (ORDER BY duration_ms) AS p99_duration,
    MAX(duration_ms) AS max_duration
FROM http_requests
GROUP BY bucket, service_name, endpoint, method;

-- DashboardæŸ¥è¯¢ï¼šæœåŠ¡å¥åº·åº¦
SELECT
    service_name,
    SUM(request_count) AS total_requests,
    SUM(error_count) AS total_errors,
    ROUND(100.0 * SUM(error_count) / SUM(request_count), 2) AS error_rate,
    ROUND(AVG(avg_duration), 2) AS avg_latency,
    ROUND(AVG(p95_duration), 2) AS p95_latency
FROM http_requests_stats_5min
WHERE bucket >= NOW() - INTERVAL '1 hour'
GROUP BY service_name
ORDER BY error_rate DESC, avg_latency DESC;
```

---

## 13. æœ€ä½³å®è·µ

### 13.1 è®¾è®¡åŸåˆ™

#### âœ… æ¨èåšæ³•

1. **æ—¶é—´åˆ—ä½¿ç”¨TIMESTAMPTZ**

    ```sql
    -- âœ… å¥½ï¼šå¸¦æ—¶åŒº
    CREATE TABLE metrics (
        time TIMESTAMPTZ NOT NULL,
        ...
    );

    -- âŒ åï¼šä¸å¸¦æ—¶åŒº
    CREATE TABLE metrics (
        time TIMESTAMP NOT NULL,  -- å¯èƒ½å¯¼è‡´æ—¶åŒºæ··ä¹±
        ...
    );
    ```

2. **åˆç†é€‰æ‹©chunké—´éš”**

    ```text
    æ•°æ®é‡     |  å»ºè®®chunké—´éš”
    -------------------------------
    < 100GB   |  7 days
    100GB-1TB |  1 day
    1TB-10TB  |  6 hours
    > 10TB    |  1 hour

    åŸåˆ™ï¼šæ¯ä¸ªchunk 100MB-1GBæœ€ä½³
    ```

3. **ç©ºé—´åˆ†åŒºç”¨äºé«˜å¹¶å‘å†™å…¥**

    ```sql
    -- å•ä¸€æ—¶é—´åˆ†åŒºï¼šå†™å…¥çƒ­ç‚¹åœ¨æœ€æ–°chunk
    SELECT create_hypertable('metrics', 'time');

    -- å¤šç»´åˆ†åŒºï¼šåˆ†æ•£å†™å…¥åˆ°å¤šä¸ªchunk
    SELECT create_hypertable(
        'metrics',
        'time',
        partitioning_column => 'device_id',
        number_partitions => 4  -- 4ä¸ªå¹¶å‘å†™å…¥ç‚¹
    );
    ```

4. **ä½¿ç”¨è¿ç»­èšåˆæ›¿ä»£é‡å¤æŸ¥è¯¢**

5. **å¯ç”¨å‹ç¼©èŠ‚çœå­˜å‚¨**

6. **é…ç½®ä¿ç•™ç­–ç•¥è‡ªåŠ¨æ¸…ç†**

### 13.2 è¿ç»´Checklist

- [ ] ç›‘æ§chunkæ•°é‡ï¼ˆè¿‡å¤šå½±å“æ€§èƒ½ï¼‰
- [ ] ç›‘æ§å‹ç¼©ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€
- [ ] ç›‘æ§è¿ç»­èšåˆåˆ·æ–°å»¶è¿Ÿ
- [ ] å®šæœŸVACUUM ANALYZEï¼ˆå°¤å…¶æ˜¯æœªå‹ç¼©çš„chunkï¼‰
- [ ] ç›‘æ§ç£ç›˜ä½¿ç”¨
- [ ] æµ‹è¯•å¤‡ä»½æ¢å¤æµç¨‹
- [ ] ç›‘æ§åå°ä»»åŠ¡å¤±è´¥ï¼ˆtimescaledb_information.job_statsï¼‰

### 13.3 æ€§èƒ½è°ƒä¼˜Checklist

- [ ] chunké—´éš”é€‚å½“ï¼ˆä¸è¦å¤ªå°æˆ–å¤ªå¤§ï¼‰
- [ ] æŸ¥è¯¢åŒ…å«æ—¶é—´èŒƒå›´è¿‡æ»¤
- [ ] ä½¿ç”¨è¿ç»­èšåˆé¢„èšåˆ
- [ ] å¯ç”¨å‹ç¼©
- [ ] å¤šç»´åˆ†åŒºç”¨äºé«˜å¹¶å‘å†™å…¥
- [ ] ç´¢å¼•ç­–ç•¥åˆç†
- [ ] æ‰¹é‡å†™å…¥è€Œéé€è¡Œæ’å…¥
- [ ] PostgreSQLå‚æ•°è°ƒä¼˜ï¼ˆshared_buffers, work_memç­‰ï¼‰

---

## 14. FAQä¸ç–‘éš¾è§£ç­”

### Q1: Hypertable vs PostgreSQLåˆ†åŒºè¡¨ï¼Ÿ

| ç‰¹æ€§ | Hypertable | PostgreSQLåˆ†åŒº |
|------|-----------|----------------|
| **è‡ªåŠ¨åˆ†åŒº** | âœ… è‡ªåŠ¨åˆ›å»º | âŒ æ‰‹åŠ¨åˆ›å»º |
| **å‹ç¼©** | âœ… å†…ç½® | âŒ éœ€æ‰‹åŠ¨ |
| **ä¿ç•™ç­–ç•¥** | âœ… è‡ªåŠ¨ | âŒ éœ€è„šæœ¬ |
| **è¿ç»­èšåˆ** | âœ… å†…ç½® | âŒ éœ€æ‰‹åŠ¨ç»´æŠ¤ç‰©åŒ–è§†å›¾ |
| **æ—¶åºå‡½æ•°** | âœ… ä¸°å¯Œ | âŒ æ—  |
| **å­¦ä¹ æˆæœ¬** | ä½ | ä¸­ |

### Q2: chunkæ•°é‡è¿‡å¤šæ€ä¹ˆåŠï¼Ÿ

```sql
-- è¯Šæ–­ï¼šæŸ¥çœ‹chunkæ•°é‡
SELECT COUNT(*) FROM timescaledb_information.chunks
WHERE hypertable_name = 'sensor_data';

-- å¦‚æœè¶…è¿‡1000ä¸ªchunkï¼Œè€ƒè™‘ï¼š
-- 1. å¢å¤§chunké—´éš”
SELECT set_chunk_time_interval('sensor_data', INTERVAL '7 days');

-- 2. å¯ç”¨å‹ç¼©ï¼ˆå‡å°‘chunkæ•°é‡ï¼‰
ALTER TABLE sensor_data SET (timescaledb.compress);
SELECT add_compression_policy('sensor_data', compress_after => INTERVAL '7 days');

-- 3. åˆ é™¤æ—§æ•°æ®
SELECT add_retention_policy('sensor_data', drop_after => INTERVAL '180 days');
```

### Q3: å‹ç¼©åèƒ½æ›´æ–°æ•°æ®å—ï¼Ÿ

```sql
-- âŒ ä¸èƒ½ç›´æ¥æ›´æ–°å‹ç¼©chunk
UPDATE sensor_data SET temperature = 25.0
WHERE time = '2025-01-01 10:00:00';
-- ERROR: cannot update compressed chunk

-- è§£å†³æ–¹æ¡ˆ1ï¼šè§£å‹chunk
SELECT decompress_chunk('_timescaledb_internal._hyper_1_1_chunk');
-- æ‰§è¡Œæ›´æ–°
UPDATE sensor_data SET temperature = 25.0 WHERE ...;
-- é‡æ–°å‹ç¼©
SELECT compress_chunk('_timescaledb_internal._hyper_1_1_chunk');

-- è§£å†³æ–¹æ¡ˆ2ï¼šè®¾è®¡é¿å…æ›´æ–°
-- æ—¶åºæ•°æ®åº”è¯¥æ˜¯åªè¿½åŠ çš„ï¼Œé¿å…æ›´æ–°
```

### Q4: å¦‚ä½•è¿ç§»ç°æœ‰PostgreSQLæ—¶åºæ•°æ®åˆ°TimescaleDBï¼Ÿ

```sql
-- æ­¥éª¤1ï¼šå®‰è£…TimescaleDBæ‰©å±•
CREATE EXTENSION timescaledb;

-- æ­¥éª¤2ï¼šä¿ç•™åŸè¡¨ç»“æ„ï¼Œåˆ›å»ºæ–°Hypertable
ALTER TABLE sensor_data RENAME TO sensor_data_old;

CREATE TABLE sensor_data (LIKE sensor_data_old INCLUDING ALL);

SELECT create_hypertable('sensor_data', 'time',
    chunk_time_interval => INTERVAL '1 day',
    migrate_data => false
);

-- æ­¥éª¤3ï¼šè¿ç§»æ•°æ®
INSERT INTO sensor_data SELECT * FROM sensor_data_old;

-- æ­¥éª¤4ï¼šéªŒè¯
SELECT COUNT(*) FROM sensor_data;
SELECT COUNT(*) FROM sensor_data_old;

-- æ­¥éª¤5ï¼šåˆ é™¤æ—§è¡¨
DROP TABLE sensor_data_old;
```

### Q5: TimescaleDBå¯ä»¥ç”¨äºéæ—¶åºæ•°æ®å—ï¼Ÿ

**A**: å¯ä»¥ï¼Œä½†ä¸æ¨èã€‚

- Hypertableä»ç„¶æ˜¯PostgreSQLè¡¨ï¼Œå¯ä»¥å­˜å‚¨ä»»ä½•æ•°æ®
- ä½†TimescaleDBä¼˜åŒ–æ˜¯é’ˆå¯¹æ—¶åºæ•°æ®çš„ï¼ˆæ—¶é—´åˆ†åŒºã€å‹ç¼©ç­‰ï¼‰
- å¦‚æœæ•°æ®ä¸æ˜¯æ—¶åºçš„ï¼Œä½¿ç”¨æ™®é€šPostgreSQLè¡¨æ›´åˆé€‚

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

### å®˜æ–¹èµ„æº

- [TimescaleDB Documentation](https://docs.timescale.com/)
- [TimescaleDB GitHub](https://github.com/timescale/timescaledb)
- [Timescale Cloud](https://www.timescale.com/cloud)

### ç›¸å…³æŠ€æœ¯

- **InfluxDB**: çº¯æ—¶åºæ•°æ®åº“
- **Prometheus**: Metricsç›‘æ§ç³»ç»Ÿ
- **ClickHouse**: OLAPåˆ†ææ•°æ®åº“
- **Grafana**: å¯è§†åŒ–Dashboard

### æ¨èé˜…è¯»

- [Time-Series Data: Why and How to Use a Relational Database](https://blog.timescale.com/)
- [PostgreSQL Partitioning Best Practices](https://www.postgresql.org/docs/current/ddl-partitioning.html)

---

## âœ… å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£æ—¶åºæ•°æ®ç‰¹å¾å’ŒæŒ‘æˆ˜
- [ ] æŒæ¡Hypertableåˆ›å»ºå’Œç®¡ç†
- [ ] ç†Ÿç»ƒä½¿ç”¨time_bucketå’Œæ—¶åºå‡½æ•°
- [ ] ç†è§£è¿ç»­èšåˆåŸç†å’Œä½¿ç”¨
- [ ] æŒæ¡æ•°æ®å‹ç¼©å’Œä¿ç•™ç­–ç•¥
- [ ] èƒ½å¤Ÿè®¾è®¡é«˜æ€§èƒ½æ—¶åºæ•°æ®æ¶æ„
- [ ] ç†Ÿæ‚‰æ€§èƒ½ä¼˜åŒ–æŠ€å·§
- [ ] èƒ½å¤Ÿç›‘æ§å’Œè¿ç»´ç”Ÿäº§ç¯å¢ƒ

---

## ğŸ’¡ ä¸‹ä¸€æ­¥å­¦ä¹ 

1. **è¿›é˜¶ä¸»é¢˜**:
   - TimescaleDBå¤šèŠ‚ç‚¹ï¼ˆåˆ†å¸ƒå¼ï¼‰
   - ä¸Grafana/Prometheusé›†æˆ
   - å®æ—¶æµå¤„ç†ï¼ˆKafka + TimescaleDBï¼‰

2. **ç›¸å…³è¯¾ç¨‹**:
   - [Citusåˆ†å¸ƒå¼PostgreSQL](./ã€æ·±å…¥ã€‘Citusåˆ†å¸ƒå¼PostgreSQLå®Œæ•´å®æˆ˜æŒ‡å—.md)
   - [PostgreSQLæ€§èƒ½è°ƒä¼˜](../11-æ€§èƒ½è°ƒä¼˜/)
   - [PostGISç©ºé—´æ•°æ®åº“](./ã€æ·±å…¥ã€‘PostGISç©ºé—´æ•°æ®åº“å®Œæ•´å®æˆ˜æŒ‡å—.md)

---

**æ–‡æ¡£ç»´æŠ¤**: æœ¬æ–‡æ¡£æŒç»­æ›´æ–°ä»¥åæ˜ TimescaleDBæœ€æ–°ç‰¹æ€§ã€‚
**åé¦ˆ**: å¦‚å‘ç°é”™è¯¯æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œè¯·æäº¤issueã€‚

**ç‰ˆæœ¬å†å²**:

- v1.0 (2025-01): åˆå§‹ç‰ˆæœ¬ï¼Œè¦†ç›–TimescaleDB 2.13+æ ¸å¿ƒç‰¹æ€§
