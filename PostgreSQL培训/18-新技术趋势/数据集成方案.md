# 数据集成方案：PostgreSQL 数据集成最佳实践

> **更新时间**: 2025 年 1 月
> **技术版本**: PostgreSQL 17+ 数据集成方案
> **文档编号**: 03-03-TREND-27

## 📑 概述

本文档介绍 PostgreSQL 数据集成的最佳实践方案，包括 ETL 工具、数据同步、数据管道、数据迁移等场景的完整解决方案。

## 🎯 核心价值

- **ETL 工具集成**：与主流 ETL 工具的集成方案
- **数据同步**：实时和批量数据同步方案
- **数据管道**：构建可靠的数据管道
- **数据迁移**：从其他数据库迁移到 PostgreSQL
- **数据质量**：数据质量保证和验证

## 📚 目录

- [数据集成方案：PostgreSQL 数据集成最佳实践](#数据集成方案postgresql-数据集成最佳实践)
  - [📑 概述](#-概述)
  - [🎯 核心价值](#-核心价值)
  - [📚 目录](#-目录)
  - [1. 数据集成概述](#1-数据集成概述)
    - [1.1 数据集成场景](#11-数据集成场景)
    - [1.2 技术栈](#12-技术栈)
  - [2. ETL 工具集成](#2-etl-工具集成)
    - [2.1 Apache Airflow 集成](#21-apache-airflow-集成)
    - [2.2 dbt (Data Build Tool) 集成](#22-dbt-data-build-tool-集成)
    - [2.3 Talend / Informatica 集成](#23-talend--informatica-集成)
  - [3. 数据同步方案](#3-数据同步方案)
    - [3.1 逻辑复制同步](#31-逻辑复制同步)
    - [3.2 批量同步](#32-批量同步)
  - [4. 数据管道设计](#4-数据管道设计)
    - [4.1 数据管道架构](#41-数据管道架构)
    - [4.2 管道实现](#42-管道实现)
  - [5. 数据迁移方案](#5-数据迁移方案)
    - [5.1 从 MySQL 迁移](#51-从-mysql-迁移)
    - [5.2 从 Oracle 迁移](#52-从-oracle-迁移)
  - [6. 数据质量保证](#6-数据质量保证)
    - [6.1 数据验证](#61-数据验证)
  - [7. 实际案例](#7-实际案例)
    - [7.1 案例：多数据源集成](#71-案例多数据源集成)
  - [📊 总结](#-总结)

---

## 1. 数据集成概述

### 1.1 数据集成场景

- **ETL 处理**：提取、转换、加载数据
- **数据同步**：多数据源之间的数据同步
- **数据迁移**：从其他数据库迁移到 PostgreSQL
- **数据管道**：构建数据流处理管道
- **数据仓库**：构建数据仓库和数据集市

### 1.2 技术栈

```text
数据源
    ↓
ETL 工具 / 数据管道
    ↓
PostgreSQL
    ↓
数据仓库 / 分析系统
```

---

## 2. ETL 工具集成

### 2.1 Apache Airflow 集成

```python
# Airflow DAG 示例：PostgreSQL ETL
from airflow import DAG
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from datetime import datetime, timedelta

default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'postgres_etl',
    default_args=default_args,
    description='PostgreSQL ETL Pipeline',
    schedule_interval=timedelta(hours=1),
    catchup=False
)

# 提取数据
extract_task = PostgresOperator(
    task_id='extract_data',
    postgres_conn_id='postgres_source',
    sql='''
        SELECT * FROM source_table
        WHERE updated_at >= NOW() - INTERVAL '1 hour'
    ''',
    dag=dag
)

# 转换和加载数据
load_task = PostgresOperator(
    task_id='load_data',
    postgres_conn_id='postgres_target',
    sql='''
        INSERT INTO target_table (col1, col2, col3)
        SELECT col1, col2, col3
        FROM staging_table
        ON CONFLICT (id) DO UPDATE
        SET col1 = EXCLUDED.col1,
            col2 = EXCLUDED.col2,
            updated_at = NOW()
    ''',
    dag=dag
)

extract_task >> load_task
```

### 2.2 dbt (Data Build Tool) 集成

```sql
-- dbt 模型：转换数据
-- models/staging/stg_orders.sql
{{ config(materialized='view') }}

SELECT
    order_id,
    customer_id,
    order_date,
    total_amount,
    status
FROM {{ source('raw', 'orders') }}
WHERE order_date >= '2024-01-01'

-- models/marts/orders_summary.sql
{{ config(materialized='table') }}

SELECT
    DATE_TRUNC('month', order_date) AS month,
    COUNT(*) AS order_count,
    SUM(total_amount) AS total_revenue,
    AVG(total_amount) AS avg_order_value
FROM {{ ref('stg_orders') }}
GROUP BY DATE_TRUNC('month', order_date)
```

### 2.3 Talend / Informatica 集成

```sql
-- 使用外部表进行数据集成
-- 创建外部数据包装器
CREATE EXTENSION IF NOT EXISTS postgres_fdw;

-- 创建外部服务器
CREATE SERVER external_db
FOREIGN DATA WRAPPER postgres_fdw
OPTIONS (
    host 'external_host',
    port '5432',
    dbname 'external_db'
);

-- 创建用户映射
CREATE USER MAPPING FOR current_user
SERVER external_db
OPTIONS (
    user 'external_user',
    password 'external_password'
);

-- 创建外部表
CREATE FOREIGN TABLE external_orders (
    order_id INTEGER,
    customer_id INTEGER,
    order_date DATE,
    total_amount DECIMAL(10,2)
)
SERVER external_db
OPTIONS (
    schema_name 'public',
    table_name 'orders'
);

-- 从外部表同步数据
INSERT INTO local_orders
SELECT * FROM external_orders
WHERE order_date >= CURRENT_DATE - INTERVAL '1 day';
```

---

## 3. 数据同步方案

### 3.1 逻辑复制同步

```sql
-- 创建发布
CREATE PUBLICATION sync_publication FOR ALL TABLES;

-- 创建订阅
CREATE SUBSCRIPTION sync_subscription
CONNECTION 'host=target_host dbname=target_db user=replicator'
PUBLICATION sync_publication
WITH (
    copy_data = true,
    create_slot = true,
    enabled = true
);

-- 监控复制延迟
SELECT
    subname,
    pg_subscription_rel.srsubid,
    pg_stat_replication.lag
FROM pg_subscription
JOIN pg_subscription_rel ON pg_subscription.oid = pg_subscription_rel.srsubid
LEFT JOIN pg_stat_replication ON pg_subscription.subname = pg_stat_replication.application_name;
```

### 3.2 批量同步

```sql
-- 批量同步函数
CREATE OR REPLACE FUNCTION batch_sync_table(
    p_source_table TEXT,
    p_target_table TEXT,
    p_batch_size INTEGER DEFAULT 10000
)
RETURNS TABLE (
    batches_processed INTEGER,
    rows_synced BIGINT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_offset INTEGER := 0;
    v_rows_synced BIGINT := 0;
    v_batch_count INTEGER := 0;
BEGIN
    LOOP
        EXECUTE format('
            INSERT INTO %I
            SELECT * FROM %I
            ORDER BY id
            LIMIT %s OFFSET %s
            ON CONFLICT (id) DO UPDATE
            SET updated_at = EXCLUDED.updated_at
        ', p_target_table, p_source_table, p_batch_size, v_offset);

        GET DIAGNOSTICS v_rows_synced = ROW_COUNT;
        EXIT WHEN v_rows_synced = 0;

        v_batch_count := v_batch_count + 1;
        v_offset := v_offset + p_batch_size;
    END LOOP;

    RETURN QUERY SELECT v_batch_count, v_offset;
END;
$$;
```

---

## 4. 数据管道设计

### 4.1 数据管道架构

```text
数据源 → 提取 → 转换 → 加载 → 目标
         ↓       ↓       ↓
      验证    清洗    质量检查
```

### 4.2 管道实现

```sql
-- 创建数据管道表
CREATE TABLE data_pipeline (
    id SERIAL PRIMARY KEY,
    pipeline_name TEXT NOT NULL,
    source_table TEXT,
    target_table TEXT,
    status TEXT,
    started_at TIMESTAMPTZ,
    completed_at TIMESTAMPTZ,
    rows_processed BIGINT,
    error_message TEXT
);

-- 管道执行函数
CREATE OR REPLACE FUNCTION execute_pipeline(
    p_pipeline_name TEXT
)
RETURNS void
LANGUAGE plpgsql
AS $$
DECLARE
    v_pipeline_id INTEGER;
    v_rows_processed BIGINT;
BEGIN
    -- 记录管道开始
    INSERT INTO data_pipeline (pipeline_name, status, started_at)
    VALUES (p_pipeline_name, 'running', NOW())
    RETURNING id INTO v_pipeline_id;

    BEGIN
        -- 执行数据提取和转换
        -- ... 数据处理逻辑 ...

        GET DIAGNOSTICS v_rows_processed = ROW_COUNT;

        -- 更新管道状态
        UPDATE data_pipeline
        SET
            status = 'completed',
            completed_at = NOW(),
            rows_processed = v_rows_processed
        WHERE id = v_pipeline_id;

    EXCEPTION WHEN OTHERS THEN
        -- 记录错误
        UPDATE data_pipeline
        SET
            status = 'failed',
            completed_at = NOW(),
            error_message = SQLERRM
        WHERE id = v_pipeline_id;

        RAISE;
    END;
END;
$$;
```

---

## 5. 数据迁移方案

### 5.1 从 MySQL 迁移

```bash
# 使用 pgloader 迁移
pgloader mysql://user:password@mysql_host/dbname \
         postgresql://user:password@pg_host/dbname

# 使用 mysqldump + psql
mysqldump -u user -p dbname > dump.sql
# 转换 SQL 语法
sed -i 's/INTEGER AUTO_INCREMENT/SERIAL/g' dump.sql
psql -U user -d dbname -f dump.sql
```

### 5.2 从 Oracle 迁移

```sql
-- 使用 ora_fdw 迁移
CREATE EXTENSION IF NOT EXISTS oracle_fdw;

CREATE SERVER oracle_server
FOREIGN DATA WRAPPER oracle_fdw
OPTIONS (dbserver 'oracle_host:1521/orcl');

CREATE FOREIGN TABLE oracle_table (
    id INTEGER,
    name TEXT
)
SERVER oracle_server
OPTIONS (schema 'SCHEMA', table 'TABLE');

-- 迁移数据
INSERT INTO pg_table
SELECT * FROM oracle_table;
```

---

## 6. 数据质量保证

### 6.1 数据验证

```sql
-- 创建数据质量检查函数
CREATE OR REPLACE FUNCTION validate_data_quality(
    p_table_name TEXT
)
RETURNS TABLE (
    check_name TEXT,
    status TEXT,
    message TEXT
)
LANGUAGE plpgsql
AS $$
BEGIN
    -- 检查空值
    RETURN QUERY
    SELECT
        'null_check'::TEXT,
        CASE WHEN COUNT(*) = 0 THEN 'PASS' ELSE 'FAIL' END,
        format('Found %s null values in required columns', COUNT(*))
    FROM information_schema.columns
    WHERE table_name = p_table_name
      AND is_nullable = 'NO';

    -- 检查重复值
    RETURN QUERY
    SELECT
        'duplicate_check'::TEXT,
        CASE WHEN COUNT(*) = 0 THEN 'PASS' ELSE 'FAIL' END,
        format('Found %s duplicate rows', COUNT(*))
    FROM (
        SELECT id, COUNT(*)
        FROM p_table_name
        GROUP BY id
        HAVING COUNT(*) > 1
    ) duplicates;
END;
$$;
```

---

## 7. 实际案例

### 7.1 案例：多数据源集成

```sql
-- 场景：整合多个数据源到 PostgreSQL
-- 要求：实时同步，数据一致性

-- 创建统一数据模型
CREATE TABLE unified_orders (
    id SERIAL PRIMARY KEY,
    source_system TEXT,
    external_id TEXT,
    customer_id INTEGER,
    order_date DATE,
    total_amount DECIMAL(10,2),
    status TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE (source_system, external_id)
);

-- 同步函数
CREATE OR REPLACE FUNCTION sync_from_source(
    p_source_system TEXT,
    p_source_data JSONB
)
RETURNS void
LANGUAGE plpgsql
AS $$
BEGIN
    INSERT INTO unified_orders (
        source_system,
        external_id,
        customer_id,
        order_date,
        total_amount,
        status
    )
    VALUES (
        p_source_system,
        p_source_data->>'id',
        (p_source_data->>'customer_id')::INTEGER,
        (p_source_data->>'order_date')::DATE,
        (p_source_data->>'total_amount')::DECIMAL,
        p_source_data->>'status'
    )
    ON CONFLICT (source_system, external_id) DO UPDATE
    SET
        customer_id = EXCLUDED.customer_id,
        order_date = EXCLUDED.order_date,
        total_amount = EXCLUDED.total_amount,
        status = EXCLUDED.status,
        updated_at = NOW();
END;
$$;
```

---

## 📊 总结

PostgreSQL 数据集成方案提供了完整的数据集成解决方案，包括 ETL 工具集成、数据同步、数据管道、数据迁移等功能。通过合理使用这些方案，可以在生产环境中实现可靠的数据集成。建议根据实际场景选择合适的集成方案，并建立完善的数据质量保证机制。

---

**最后更新**: 2025 年 1 月
**维护者**: PostgreSQL Modern Team
**文档编号**: 03-03-TREND-27
