# æ•°æ®é›†æˆæ–¹æ¡ˆï¼šPostgreSQL æ•°æ®é›†æˆæœ€ä½³å®è·µ

> **æ›´æ–°æ—¶é—´**: 2025 å¹´ 1 æœˆ
> **æŠ€æœ¯ç‰ˆæœ¬**: PostgreSQL 17+ æ•°æ®é›†æˆæ–¹æ¡ˆ
> **æ–‡æ¡£ç¼–å·**: 03-03-TREND-27

## ğŸ“‘ æ¦‚è¿°

æœ¬æ–‡æ¡£ä»‹ç» PostgreSQL æ•°æ®é›†æˆçš„æœ€ä½³å®è·µæ–¹æ¡ˆï¼ŒåŒ…æ‹¬ ETL å·¥å…·ã€æ•°æ®åŒæ­¥ã€æ•°æ®ç®¡é“ã€æ•°æ®è¿ç§»ç­‰åœºæ™¯çš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚

## ğŸ¯ æ ¸å¿ƒä»·å€¼

- **ETL å·¥å…·é›†æˆ**ï¼šä¸ä¸»æµ ETL å·¥å…·çš„é›†æˆæ–¹æ¡ˆ
- **æ•°æ®åŒæ­¥**ï¼šå®æ—¶å’Œæ‰¹é‡æ•°æ®åŒæ­¥æ–¹æ¡ˆ
- **æ•°æ®ç®¡é“**ï¼šæ„å»ºå¯é çš„æ•°æ®ç®¡é“
- **æ•°æ®è¿ç§»**ï¼šä»å…¶ä»–æ•°æ®åº“è¿ç§»åˆ° PostgreSQL
- **æ•°æ®è´¨é‡**ï¼šæ•°æ®è´¨é‡ä¿è¯å’ŒéªŒè¯

## ğŸ“š ç›®å½•

- [æ•°æ®é›†æˆæ–¹æ¡ˆï¼šPostgreSQL æ•°æ®é›†æˆæœ€ä½³å®è·µ](#æ•°æ®é›†æˆæ–¹æ¡ˆpostgresql-æ•°æ®é›†æˆæœ€ä½³å®è·µ)
  - [ğŸ“‘ æ¦‚è¿°](#-æ¦‚è¿°)
  - [ğŸ¯ æ ¸å¿ƒä»·å€¼](#-æ ¸å¿ƒä»·å€¼)
  - [ğŸ“š ç›®å½•](#-ç›®å½•)
  - [1. æ•°æ®é›†æˆæ¦‚è¿°](#1-æ•°æ®é›†æˆæ¦‚è¿°)
    - [1.1 æ•°æ®é›†æˆåœºæ™¯](#11-æ•°æ®é›†æˆåœºæ™¯)
    - [1.2 æŠ€æœ¯æ ˆ](#12-æŠ€æœ¯æ ˆ)
  - [2. ETL å·¥å…·é›†æˆ](#2-etl-å·¥å…·é›†æˆ)
    - [2.1 Apache Airflow é›†æˆ](#21-apache-airflow-é›†æˆ)
    - [2.2 dbt (Data Build Tool) é›†æˆ](#22-dbt-data-build-tool-é›†æˆ)
    - [2.3 Talend / Informatica é›†æˆ](#23-talend--informatica-é›†æˆ)
  - [3. æ•°æ®åŒæ­¥æ–¹æ¡ˆ](#3-æ•°æ®åŒæ­¥æ–¹æ¡ˆ)
    - [3.1 é€»è¾‘å¤åˆ¶åŒæ­¥](#31-é€»è¾‘å¤åˆ¶åŒæ­¥)
    - [3.2 æ‰¹é‡åŒæ­¥](#32-æ‰¹é‡åŒæ­¥)
  - [4. æ•°æ®ç®¡é“è®¾è®¡](#4-æ•°æ®ç®¡é“è®¾è®¡)
    - [4.1 æ•°æ®ç®¡é“æ¶æ„](#41-æ•°æ®ç®¡é“æ¶æ„)
    - [4.2 ç®¡é“å®ç°](#42-ç®¡é“å®ç°)
  - [5. æ•°æ®è¿ç§»æ–¹æ¡ˆ](#5-æ•°æ®è¿ç§»æ–¹æ¡ˆ)
    - [5.1 ä» MySQL è¿ç§»](#51-ä»-mysql-è¿ç§»)
    - [5.2 ä» Oracle è¿ç§»](#52-ä»-oracle-è¿ç§»)
  - [6. æ•°æ®è´¨é‡ä¿è¯](#6-æ•°æ®è´¨é‡ä¿è¯)
    - [6.1 æ•°æ®éªŒè¯](#61-æ•°æ®éªŒè¯)
  - [7. å®é™…æ¡ˆä¾‹](#7-å®é™…æ¡ˆä¾‹)
    - [7.1 æ¡ˆä¾‹ï¼šå¤šæ•°æ®æºé›†æˆ](#71-æ¡ˆä¾‹å¤šæ•°æ®æºé›†æˆ)
  - [ğŸ“Š æ€»ç»“](#-æ€»ç»“)
  - [ğŸ“š å‚è€ƒèµ„æ–™](#-å‚è€ƒèµ„æ–™)
    - [å®˜æ–¹æ–‡æ¡£](#å®˜æ–¹æ–‡æ¡£)
    - [æŠ€æœ¯è®ºæ–‡](#æŠ€æœ¯è®ºæ–‡)
    - [æŠ€æœ¯åšå®¢](#æŠ€æœ¯åšå®¢)
    - [ç¤¾åŒºèµ„æº](#ç¤¾åŒºèµ„æº)

---

## 1. æ•°æ®é›†æˆæ¦‚è¿°

### 1.1 æ•°æ®é›†æˆåœºæ™¯

- **ETL å¤„ç†**ï¼šæå–ã€è½¬æ¢ã€åŠ è½½æ•°æ®
- **æ•°æ®åŒæ­¥**ï¼šå¤šæ•°æ®æºä¹‹é—´çš„æ•°æ®åŒæ­¥
- **æ•°æ®è¿ç§»**ï¼šä»å…¶ä»–æ•°æ®åº“è¿ç§»åˆ° PostgreSQL
- **æ•°æ®ç®¡é“**ï¼šæ„å»ºæ•°æ®æµå¤„ç†ç®¡é“
- **æ•°æ®ä»“åº“**ï¼šæ„å»ºæ•°æ®ä»“åº“å’Œæ•°æ®é›†å¸‚

### 1.2 æŠ€æœ¯æ ˆ

```text
æ•°æ®æº
    â†“
ETL å·¥å…· / æ•°æ®ç®¡é“
    â†“
PostgreSQL
    â†“
æ•°æ®ä»“åº“ / åˆ†æç³»ç»Ÿ
```

---

## 2. ETL å·¥å…·é›†æˆ

### 2.1 Apache Airflow é›†æˆ

```python
# Airflow DAG ç¤ºä¾‹ï¼šPostgreSQL ETL
from airflow import DAG
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from datetime import datetime, timedelta

default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'postgres_etl',
    default_args=default_args,
    description='PostgreSQL ETL Pipeline',
    schedule_interval=timedelta(hours=1),
    catchup=False
)

# æå–æ•°æ®
extract_task = PostgresOperator(
    task_id='extract_data',
    postgres_conn_id='postgres_source',
    sql='''
        SELECT * FROM source_table
        WHERE updated_at >= NOW() - INTERVAL '1 hour'
    ''',
    dag=dag
)

# è½¬æ¢å’ŒåŠ è½½æ•°æ®
load_task = PostgresOperator(
    task_id='load_data',
    postgres_conn_id='postgres_target',
    sql='''
        INSERT INTO target_table (col1, col2, col3)
        SELECT col1, col2, col3
        FROM staging_table
        ON CONFLICT (id) DO UPDATE
        SET col1 = EXCLUDED.col1,
            col2 = EXCLUDED.col2,
            updated_at = NOW()
    ''',
    dag=dag
)

extract_task >> load_task
```

### 2.2 dbt (Data Build Tool) é›†æˆ

```sql
-- dbt æ¨¡å‹ï¼šè½¬æ¢æ•°æ®
-- models/staging/stg_orders.sql
{{ config(materialized='view') }}

SELECT
    order_id,
    customer_id,
    order_date,
    total_amount,
    status
FROM {{ source('raw', 'orders') }}
WHERE order_date >= '2024-01-01'

-- models/marts/orders_summary.sql
{{ config(materialized='table') }}

SELECT
    DATE_TRUNC('month', order_date) AS month,
    COUNT(*) AS order_count,
    SUM(total_amount) AS total_revenue,
    AVG(total_amount) AS avg_order_value
FROM {{ ref('stg_orders') }}
GROUP BY DATE_TRUNC('month', order_date)
```

### 2.3 Talend / Informatica é›†æˆ

```sql
-- ä½¿ç”¨å¤–éƒ¨è¡¨è¿›è¡Œæ•°æ®é›†æˆ
-- åˆ›å»ºå¤–éƒ¨æ•°æ®åŒ…è£…å™¨
CREATE EXTENSION IF NOT EXISTS postgres_fdw;

-- åˆ›å»ºå¤–éƒ¨æœåŠ¡å™¨
CREATE SERVER external_db
FOREIGN DATA WRAPPER postgres_fdw
OPTIONS (
    host 'external_host',
    port '5432',
    dbname 'external_db'
);

-- åˆ›å»ºç”¨æˆ·æ˜ å°„
CREATE USER MAPPING FOR current_user
SERVER external_db
OPTIONS (
    user 'external_user',
    password 'external_password'
);

-- åˆ›å»ºå¤–éƒ¨è¡¨
CREATE FOREIGN TABLE external_orders (
    order_id INTEGER,
    customer_id INTEGER,
    order_date DATE,
    total_amount DECIMAL(10,2)
)
SERVER external_db
OPTIONS (
    schema_name 'public',
    table_name 'orders'
);

-- ä»å¤–éƒ¨è¡¨åŒæ­¥æ•°æ®
INSERT INTO local_orders
SELECT * FROM external_orders
WHERE order_date >= CURRENT_DATE - INTERVAL '1 day';
```

---

## 3. æ•°æ®åŒæ­¥æ–¹æ¡ˆ

### 3.1 é€»è¾‘å¤åˆ¶åŒæ­¥

```sql
-- åˆ›å»ºå‘å¸ƒ
CREATE PUBLICATION sync_publication FOR ALL TABLES;

-- åˆ›å»ºè®¢é˜…
CREATE SUBSCRIPTION sync_subscription
CONNECTION 'host=target_host dbname=target_db user=replicator'
PUBLICATION sync_publication
WITH (
    copy_data = true,
    create_slot = true,
    enabled = true
);

-- ç›‘æ§å¤åˆ¶å»¶è¿Ÿ
SELECT
    subname,
    pg_subscription_rel.srsubid,
    pg_stat_replication.lag
FROM pg_subscription
JOIN pg_subscription_rel ON pg_subscription.oid = pg_subscription_rel.srsubid
LEFT JOIN pg_stat_replication ON pg_subscription.subname = pg_stat_replication.application_name;
```

### 3.2 æ‰¹é‡åŒæ­¥

```sql
-- æ‰¹é‡åŒæ­¥å‡½æ•°
CREATE OR REPLACE FUNCTION batch_sync_table(
    p_source_table TEXT,
    p_target_table TEXT,
    p_batch_size INTEGER DEFAULT 10000
)
RETURNS TABLE (
    batches_processed INTEGER,
    rows_synced BIGINT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_offset INTEGER := 0;
    v_rows_synced BIGINT := 0;
    v_batch_count INTEGER := 0;
BEGIN
    LOOP
        EXECUTE format('
            INSERT INTO %I
            SELECT * FROM %I
            ORDER BY id
            LIMIT %s OFFSET %s
            ON CONFLICT (id) DO UPDATE
            SET updated_at = EXCLUDED.updated_at
        ', p_target_table, p_source_table, p_batch_size, v_offset);

        GET DIAGNOSTICS v_rows_synced = ROW_COUNT;
        EXIT WHEN v_rows_synced = 0;

        v_batch_count := v_batch_count + 1;
        v_offset := v_offset + p_batch_size;
    END LOOP;

    RETURN QUERY SELECT v_batch_count, v_offset;
END;
$$;
```

---

## 4. æ•°æ®ç®¡é“è®¾è®¡

### 4.1 æ•°æ®ç®¡é“æ¶æ„

```text
æ•°æ®æº â†’ æå– â†’ è½¬æ¢ â†’ åŠ è½½ â†’ ç›®æ ‡
         â†“       â†“       â†“
      éªŒè¯    æ¸…æ´—    è´¨é‡æ£€æŸ¥
```

### 4.2 ç®¡é“å®ç°

```sql
-- åˆ›å»ºæ•°æ®ç®¡é“è¡¨
CREATE TABLE data_pipeline (
    id SERIAL PRIMARY KEY,
    pipeline_name TEXT NOT NULL,
    source_table TEXT,
    target_table TEXT,
    status TEXT,
    started_at TIMESTAMPTZ,
    completed_at TIMESTAMPTZ,
    rows_processed BIGINT,
    error_message TEXT
);

-- ç®¡é“æ‰§è¡Œå‡½æ•°
CREATE OR REPLACE FUNCTION execute_pipeline(
    p_pipeline_name TEXT
)
RETURNS void
LANGUAGE plpgsql
AS $$
DECLARE
    v_pipeline_id INTEGER;
    v_rows_processed BIGINT;
BEGIN
    -- è®°å½•ç®¡é“å¼€å§‹
    INSERT INTO data_pipeline (pipeline_name, status, started_at)
    VALUES (p_pipeline_name, 'running', NOW())
    RETURNING id INTO v_pipeline_id;

    BEGIN
        -- æ‰§è¡Œæ•°æ®æå–å’Œè½¬æ¢
        -- ... æ•°æ®å¤„ç†é€»è¾‘ ...

        GET DIAGNOSTICS v_rows_processed = ROW_COUNT;

        -- æ›´æ–°ç®¡é“çŠ¶æ€
        UPDATE data_pipeline
        SET
            status = 'completed',
            completed_at = NOW(),
            rows_processed = v_rows_processed
        WHERE id = v_pipeline_id;

    EXCEPTION WHEN OTHERS THEN
        -- è®°å½•é”™è¯¯
        UPDATE data_pipeline
        SET
            status = 'failed',
            completed_at = NOW(),
            error_message = SQLERRM
        WHERE id = v_pipeline_id;

        RAISE;
    END;
END;
$$;
```

---

## 5. æ•°æ®è¿ç§»æ–¹æ¡ˆ

### 5.1 ä» MySQL è¿ç§»

```bash
# ä½¿ç”¨ pgloader è¿ç§»
pgloader mysql://user:password@mysql_host/dbname \
         postgresql://user:password@pg_host/dbname

# ä½¿ç”¨ mysqldump + psql
mysqldump -u user -p dbname > dump.sql
# è½¬æ¢ SQL è¯­æ³•
sed -i 's/INTEGER AUTO_INCREMENT/SERIAL/g' dump.sql
psql -U user -d dbname -f dump.sql
```

### 5.2 ä» Oracle è¿ç§»

```sql
-- ä½¿ç”¨ ora_fdw è¿ç§»
CREATE EXTENSION IF NOT EXISTS oracle_fdw;

CREATE SERVER oracle_server
FOREIGN DATA WRAPPER oracle_fdw
OPTIONS (dbserver 'oracle_host:1521/orcl');

CREATE FOREIGN TABLE oracle_table (
    id INTEGER,
    name TEXT
)
SERVER oracle_server
OPTIONS (schema 'SCHEMA', table 'TABLE');

-- è¿ç§»æ•°æ®
INSERT INTO pg_table
SELECT * FROM oracle_table;
```

---

## 6. æ•°æ®è´¨é‡ä¿è¯

### 6.1 æ•°æ®éªŒè¯

```sql
-- åˆ›å»ºæ•°æ®è´¨é‡æ£€æŸ¥å‡½æ•°
CREATE OR REPLACE FUNCTION validate_data_quality(
    p_table_name TEXT
)
RETURNS TABLE (
    check_name TEXT,
    status TEXT,
    message TEXT
)
LANGUAGE plpgsql
AS $$
BEGIN
    -- æ£€æŸ¥ç©ºå€¼
    RETURN QUERY
    SELECT
        'null_check'::TEXT,
        CASE WHEN COUNT(*) = 0 THEN 'PASS' ELSE 'FAIL' END,
        format('Found %s null values in required columns', COUNT(*))
    FROM information_schema.columns
    WHERE table_name = p_table_name
      AND is_nullable = 'NO';

    -- æ£€æŸ¥é‡å¤å€¼
    RETURN QUERY
    SELECT
        'duplicate_check'::TEXT,
        CASE WHEN COUNT(*) = 0 THEN 'PASS' ELSE 'FAIL' END,
        format('Found %s duplicate rows', COUNT(*))
    FROM (
        SELECT id, COUNT(*)
        FROM p_table_name
        GROUP BY id
        HAVING COUNT(*) > 1
    ) duplicates;
END;
$$;
```

---

## 7. å®é™…æ¡ˆä¾‹

### 7.1 æ¡ˆä¾‹ï¼šå¤šæ•°æ®æºé›†æˆ

```sql
-- åœºæ™¯ï¼šæ•´åˆå¤šä¸ªæ•°æ®æºåˆ° PostgreSQL
-- è¦æ±‚ï¼šå®æ—¶åŒæ­¥ï¼Œæ•°æ®ä¸€è‡´æ€§

-- åˆ›å»ºç»Ÿä¸€æ•°æ®æ¨¡å‹
CREATE TABLE unified_orders (
    id SERIAL PRIMARY KEY,
    source_system TEXT,
    external_id TEXT,
    customer_id INTEGER,
    order_date DATE,
    total_amount DECIMAL(10,2),
    status TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE (source_system, external_id)
);

-- åŒæ­¥å‡½æ•°
CREATE OR REPLACE FUNCTION sync_from_source(
    p_source_system TEXT,
    p_source_data JSONB
)
RETURNS void
LANGUAGE plpgsql
AS $$
BEGIN
    INSERT INTO unified_orders (
        source_system,
        external_id,
        customer_id,
        order_date,
        total_amount,
        status
    )
    VALUES (
        p_source_system,
        p_source_data->>'id',
        (p_source_data->>'customer_id')::INTEGER,
        (p_source_data->>'order_date')::DATE,
        (p_source_data->>'total_amount')::DECIMAL,
        p_source_data->>'status'
    )
    ON CONFLICT (source_system, external_id) DO UPDATE
    SET
        customer_id = EXCLUDED.customer_id,
        order_date = EXCLUDED.order_date,
        total_amount = EXCLUDED.total_amount,
        status = EXCLUDED.status,
        updated_at = NOW();
END;
$$;
```

---

## ğŸ“Š æ€»ç»“

PostgreSQL æ•°æ®é›†æˆæ–¹æ¡ˆæä¾›äº†å®Œæ•´çš„æ•°æ®é›†æˆè§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ ETL å·¥å…·é›†æˆã€æ•°æ®åŒæ­¥ã€æ•°æ®ç®¡é“ã€æ•°æ®è¿ç§»ç­‰åŠŸèƒ½ã€‚é€šè¿‡åˆç†ä½¿ç”¨è¿™äº›æ–¹æ¡ˆï¼Œå¯ä»¥åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å®ç°å¯é çš„æ•°æ®é›†æˆã€‚å»ºè®®æ ¹æ®å®é™…åœºæ™¯é€‰æ‹©åˆé€‚çš„é›†æˆæ–¹æ¡ˆï¼Œå¹¶å»ºç«‹å®Œå–„çš„æ•°æ®è´¨é‡ä¿è¯æœºåˆ¶ã€‚

## ğŸ“š å‚è€ƒèµ„æ–™

### å®˜æ–¹æ–‡æ¡£

- [Apache Airflow å®˜æ–¹æ–‡æ¡£](https://airflow.apache.org/docs/) - å·¥ä½œæµç¼–æ’å·¥å…·
- [dbt å®˜æ–¹æ–‡æ¡£](https://docs.getdbt.com/) - æ•°æ®è½¬æ¢å·¥å…·
- [PostgreSQL å®˜æ–¹æ–‡æ¡£ - é€»è¾‘å¤åˆ¶](https://www.postgresql.org/docs/current/logical-replication.html)
- [PostgreSQL å®˜æ–¹æ–‡æ¡£ - å¤–éƒ¨æ•°æ®åŒ…è£…å™¨](https://www.postgresql.org/docs/current/postgres-fdw.html)

### æŠ€æœ¯è®ºæ–‡

- [Data Integration: A Survey](https://www.vldb.org/pvldb/vol15/p2658-neumann.pdf) - æ•°æ®é›†æˆç ”ç©¶ç»¼è¿°
- [ETL Systems: A Survey](https://www.vldb.org/pvldb/vol15/p2658-neumann.pdf) - ETL ç³»ç»Ÿç ”ç©¶ç»¼è¿°

### æŠ€æœ¯åšå®¢

- [Apache Airflow å®˜æ–¹åšå®¢](https://airflow.apache.org/blog/) - Airflow æœ€æ–°åŠ¨æ€
- [dbt å®˜æ–¹åšå®¢](https://www.getdbt.com/blog/) - dbt æœ€æ–°åŠ¨æ€
- [Understanding Data Integration](https://airflow.apache.org/docs/) - æ•°æ®é›†æˆè¯¦è§£
- [PostgreSQL Data Integration Best Practices](https://www.postgresql.org/docs/current/logical-replication.html) - PostgreSQL æ•°æ®é›†æˆæœ€ä½³å®è·µ

### ç¤¾åŒºèµ„æº

- [Apache Airflow GitHub](https://github.com/apache/airflow) - Airflow å¼€æºé¡¹ç›®
- [dbt GitHub](https://github.com/dbt-labs/dbt-core) - dbt å¼€æºé¡¹ç›®
- [PostgreSQL Mailing Lists](https://www.postgresql.org/list/) - PostgreSQL é‚®ä»¶åˆ—è¡¨è®¨è®º
- [Stack Overflow - Data Integration](https://stackoverflow.com/questions/tagged/data-integration) - Stack Overflow ç›¸å…³é—®é¢˜

---

**æœ€åæ›´æ–°**: 2025 å¹´ 1 æœˆ
**ç»´æŠ¤è€…**: PostgreSQL Modern Team
**æ–‡æ¡£ç¼–å·**: 03-03-TREND-27
