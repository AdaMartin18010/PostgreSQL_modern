---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `docs\01-PostgreSQL18\22-ç›‘æ§ä¸å¯è§‚æµ‹æ€§å®Œæ•´ä½“ç³»æŒ‡å—.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# PostgreSQL 18 ç›‘æ§ä¸å¯è§‚æµ‹æ€§å®Œæ•´ä½“ç³»æŒ‡å—

> **ç‰ˆæœ¬**: PostgreSQL 18
> **æ›´æ–°æ—¶é—´**: 2025å¹´12æœˆ4æ—¥
> **æ–‡æ¡£ç¼–å·**: PG18-DOC-22
> **éš¾åº¦**: â­â­â­â­â­

---

## ğŸ“‘ ç›®å½•

- [PostgreSQL 18 ç›‘æ§ä¸å¯è§‚æµ‹æ€§å®Œæ•´ä½“ç³»æŒ‡å—](#postgresql-18-ç›‘æ§ä¸å¯è§‚æµ‹æ€§å®Œæ•´ä½“ç³»æŒ‡å—)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. PostgreSQL 18å¯è§‚æµ‹æ€§é©å‘½](#1-postgresql-18å¯è§‚æµ‹æ€§é©å‘½)
    - [1.1 æ ¸å¿ƒæ”¹è¿›æ¦‚è§ˆ](#11-æ ¸å¿ƒæ”¹è¿›æ¦‚è§ˆ)
    - [1.2 ä¸ä¼ ç»Ÿç›‘æ§çš„å¯¹æ¯”](#12-ä¸ä¼ ç»Ÿç›‘æ§çš„å¯¹æ¯”)
    - [1.3 å¯è§‚æµ‹æ€§ä¸‰æ”¯æŸ±](#13-å¯è§‚æµ‹æ€§ä¸‰æ”¯æŸ±)
  - [2. JSONç»“æ„åŒ–æ—¥å¿—æ·±åº¦åº”ç”¨](#2-jsonç»“æ„åŒ–æ—¥å¿—æ·±åº¦åº”ç”¨)
    - [2.1 å¯ç”¨JSONæ—¥å¿—](#21-å¯ç”¨jsonæ—¥å¿—)
    - [2.2 JSONæ—¥å¿—ç»“æ„è§£æ](#22-jsonæ—¥å¿—ç»“æ„è§£æ)
    - [2.3 æ—¥å¿—è§£æä¸åˆ†æ](#23-æ—¥å¿—è§£æä¸åˆ†æ)
      - [2.3.1 Pythonæ—¥å¿—è§£æå™¨](#231-pythonæ—¥å¿—è§£æå™¨)
      - [2.3.2 å®æ—¶æ—¥å¿—ç›‘æ§](#232-å®æ—¶æ—¥å¿—ç›‘æ§)
  - [3. pg\_stat\_ioå­—èŠ‚çº§ç›‘æ§](#3-pg_stat_ioå­—èŠ‚çº§ç›‘æ§)
    - [3.1 æ–°å¢å­—èŠ‚çº§æŒ‡æ ‡](#31-æ–°å¢å­—èŠ‚çº§æŒ‡æ ‡)
    - [3.2 I/Oæ€§èƒ½åˆ†æè§†å›¾](#32-ioæ€§èƒ½åˆ†æè§†å›¾)
    - [3.3 I/Oçƒ­ç‚¹åˆ†æ](#33-ioçƒ­ç‚¹åˆ†æ)
  - [4. è¿æ¥é˜¶æ®µè€—æ—¶åˆ†æ](#4-è¿æ¥é˜¶æ®µè€—æ—¶åˆ†æ)
    - [4.1 è¿æ¥ä¸‰é˜¶æ®µç›‘æ§](#41-è¿æ¥ä¸‰é˜¶æ®µç›‘æ§)
    - [4.2 è¿æ¥æ€§èƒ½ç›‘æ§æŸ¥è¯¢](#42-è¿æ¥æ€§èƒ½ç›‘æ§æŸ¥è¯¢)
    - [4.3 è¿æ¥æ€§èƒ½å¼‚å¸¸æ£€æµ‹](#43-è¿æ¥æ€§èƒ½å¼‚å¸¸æ£€æµ‹)
  - [5. AIOæ€§èƒ½ç›‘æ§](#5-aioæ€§èƒ½ç›‘æ§)
    - [5.1 pg\_aiosè§†å›¾è¯¦è§£](#51-pg_aiosè§†å›¾è¯¦è§£)
    - [5.2 AIOæ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿](#52-aioæ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿)
    - [5.3 AIOæ•ˆç‡åˆ†æ](#53-aioæ•ˆç‡åˆ†æ)
  - [6. é”å¤±è´¥è¯¦æƒ…è¯Šæ–­](#6-é”å¤±è´¥è¯¦æƒ…è¯Šæ–­)
    - [6.1 å¯ç”¨é”å¤±è´¥æ—¥å¿—](#61-å¯ç”¨é”å¤±è´¥æ—¥å¿—)
    - [6.2 é”å†²çªå®æ—¶ç›‘æ§](#62-é”å†²çªå®æ—¶ç›‘æ§)
    - [6.3 æ­»é”å†å²åˆ†æ](#63-æ­»é”å†å²åˆ†æ)
    - [6.4 é”è¶…æ—¶å‘Šè­¦](#64-é”è¶…æ—¶å‘Šè­¦)
  - [7. ç»Ÿè®¡ä¿¡æ¯ç›‘æ§å¢å¼º](#7-ç»Ÿè®¡ä¿¡æ¯ç›‘æ§å¢å¼º)
    - [7.1 pg\_stat\_statementså¢å¼º](#71-pg_stat_statementså¢å¼º)
    - [7.2 è¡¨çº§ç»´æŠ¤ç»Ÿè®¡](#72-è¡¨çº§ç»´æŠ¤ç»Ÿè®¡)
  - [8. Prometheus + Grafanaé›†æˆ](#8-prometheus--grafanaé›†æˆ)
    - [8.1 postgres\_exporteré…ç½®](#81-postgres_exporteré…ç½®)
    - [8.2 Grafanaä»ªè¡¨æ¿JSON](#82-grafanaä»ªè¡¨æ¿json)
    - [8.3 å®Œæ•´ç›‘æ§æ ˆéƒ¨ç½²](#83-å®Œæ•´ç›‘æ§æ ˆéƒ¨ç½²)
  - [9. å‘Šè­¦è§„åˆ™ä½“ç³»è®¾è®¡](#9-å‘Šè­¦è§„åˆ™ä½“ç³»è®¾è®¡)
    - [9.1 Prometheuså‘Šè­¦è§„åˆ™](#91-prometheuså‘Šè­¦è§„åˆ™)
    - [9.2 å‘Šè­¦é€šçŸ¥é…ç½®](#92-å‘Šè­¦é€šçŸ¥é…ç½®)
  - [10. ç”Ÿäº§ç¯å¢ƒç›‘æ§æ–¹æ¡ˆ](#10-ç”Ÿäº§ç¯å¢ƒç›‘æ§æ–¹æ¡ˆ)
    - [10.1 å®Œæ•´ç›‘æ§æ¶æ„](#101-å®Œæ•´ç›‘æ§æ¶æ„)
    - [10.2 ç›‘æ§æ£€æŸ¥æ¸…å•](#102-ç›‘æ§æ£€æŸ¥æ¸…å•)
    - [10.3 æ€§èƒ½åŸºçº¿å»ºç«‹](#103-æ€§èƒ½åŸºçº¿å»ºç«‹)
    - [10.4 è‡ªåŠ¨åŒ–å·¡æ£€è„šæœ¬](#104-è‡ªåŠ¨åŒ–å·¡æ£€è„šæœ¬)
  - [æ€»ç»“](#æ€»ç»“)
    - [PostgreSQL 18å¯è§‚æµ‹æ€§æ ¸å¿ƒä»·å€¼](#postgresql-18å¯è§‚æµ‹æ€§æ ¸å¿ƒä»·å€¼)
    - [ç›‘æ§æ¶æ„æœ€ä½³å®è·µ](#ç›‘æ§æ¶æ„æœ€ä½³å®è·µ)
    - [å…³é”®æŒ‡æ ‡é˜ˆå€¼](#å…³é”®æŒ‡æ ‡é˜ˆå€¼)
    - [PostgreSQL 18ç›‘æ§æ–°èŒƒå¼](#postgresql-18ç›‘æ§æ–°èŒƒå¼)

---

## 1. PostgreSQL 18å¯è§‚æµ‹æ€§é©å‘½

### 1.1 æ ¸å¿ƒæ”¹è¿›æ¦‚è§ˆ

PostgreSQL 18åœ¨å¯è§‚æµ‹æ€§æ–¹é¢å®ç°äº†**ä¸‰å¤§çªç ´**ï¼š

```mermaid
graph TB
    A[PostgreSQL 18<br/>å¯è§‚æµ‹æ€§å¢å¼º] --> B[ç»“æ„åŒ–æ—¥å¿—]
    A --> C[ç²¾ç»†åŒ–æŒ‡æ ‡]
    A --> D[å®æ—¶è¯Šæ–­]

    B --> B1[JSONæ ¼å¼æ—¥å¿—<br/>æœºå™¨å¯è§£æ]
    B --> B2[è¿æ¥é˜¶æ®µæ‹†è§£<br/>setup_durations]
    B --> B3[é”å¤±è´¥è¯¦æƒ…<br/>log_lock_failures]

    C --> C1[I/Oå­—èŠ‚çº§ç»Ÿè®¡<br/>pg_stat_io]
    C --> C2[AIOç›‘æ§è§†å›¾<br/>pg_aios]
    C --> C3[ç»´æŠ¤æ“ä½œè€—æ—¶<br/>total_vacuum_time]

    D --> D1[åç«¯çº§I/Oç»Ÿè®¡<br/>pg_stat_get_backend_io]
    D --> D2[æŸ¥è¯¢è¿›åº¦è·Ÿè¸ª<br/>pg_stat_progress_*]
    D --> D3[WALæ´»åŠ¨æ•´åˆ<br/>ç»Ÿä¸€æŒ‡æ ‡æ¨¡å‹]

    style A fill:#ff6b6b,color:#fff
    style B1 fill:#4ecdc4,color:#fff
    style C1 fill:#4ecdc4,color:#fff
    style D1 fill:#4ecdc4,color:#fff
```

### 1.2 ä¸ä¼ ç»Ÿç›‘æ§çš„å¯¹æ¯”

| ç›‘æ§ç»´åº¦ | PostgreSQL 17 | PostgreSQL 18 | æ”¹è¿›å¹…åº¦ |
|---------|--------------|--------------|---------|
| **æ—¥å¿—æ ¼å¼** | éç»“æ„åŒ–æ–‡æœ¬ | JSONç»“æ„åŒ– | **æœºå™¨å¯è§£æ** |
| **I/OæŒ‡æ ‡** | å—çº§ç»Ÿè®¡ | å­—èŠ‚çº§ç»Ÿè®¡ | **ç²¾åº¦æå‡8KB** |
| **è¿æ¥ç›‘æ§** | æ€»è€—æ—¶ | ä¸‰é˜¶æ®µæ‹†è§£ | **å®šä½ç²¾åº¦æå‡** |
| **é”ç›‘æ§** | ç­‰å¾…äº‹ä»¶ | è¯¦ç»†é˜»å¡ä¿¡æ¯ | **æ ¹å› åˆ†æèƒ½åŠ›** |
| **AIOç›‘æ§** | æ—  | ä¸“ç”¨pg_aiosè§†å›¾ | **æ–°å¢èƒ½åŠ›** |
| **ç»Ÿè®¡ä¿ç•™** | å‡çº§åä¸¢å¤± | pg_upgradeä¿ç•™ | **é›¶åœæœºå‡çº§** |

### 1.3 å¯è§‚æµ‹æ€§ä¸‰æ”¯æŸ±

```mermaid
graph LR
    A[å¯è§‚æµ‹æ€§] --> B[Metrics<br/>æŒ‡æ ‡]
    A --> C[Logs<br/>æ—¥å¿—]
    A --> D[Traces<br/>è¿½è¸ª]

    B --> B1[pg_stat_*è§†å›¾]
    B --> B2[pg_stat_statements]
    B --> B3[è‡ªå®šä¹‰æŒ‡æ ‡]

    C --> C1[JSONç»“æ„åŒ–æ—¥å¿—]
    C --> C2[æ…¢æŸ¥è¯¢æ—¥å¿—]
    C --> C3[é”™è¯¯æ—¥å¿—]

    D --> D1[æŸ¥è¯¢æ‰§è¡Œè¿½è¸ª]
    D --> D2[è¿æ¥ç”Ÿå‘½å‘¨æœŸ]
    D --> D3[äº‹åŠ¡è¿½è¸ª]

    style A fill:#ff6b6b,color:#fff
```

---

## 2. JSONç»“æ„åŒ–æ—¥å¿—æ·±åº¦åº”ç”¨

### 2.1 å¯ç”¨JSONæ—¥å¿—

```ini
# postgresql.conf

# === æ—¥å¿—åŸºç¡€é…ç½® ===
logging_collector = on
log_destination = 'jsonlog'  # PostgreSQL 18æ–°å¢æ ¼å¼
log_directory = '/var/log/postgresql'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.json'
log_rotation_age = 1d
log_rotation_size = 100MB
log_truncate_on_rotation = off

# === æ—¥å¿—å†…å®¹æ§åˆ¶ ===
log_min_messages = warning
log_min_error_statement = error
log_min_duration_statement = 1000  # è®°å½•>1ç§’çš„æŸ¥è¯¢

# === è¿æ¥æ—¥å¿—ï¼ˆæ–°å¢setup_durationsï¼‰ ===
log_connections = 'setup_durations'  # PostgreSQL 18å¢å¼º
log_disconnections = on

# === é”å¤±è´¥æ—¥å¿—ï¼ˆæ–°å¢ï¼‰ ===
log_lock_failures = on  # PostgreSQL 18æ–°å¢

# === è¯­å¥æ—¥å¿— ===
log_statement = 'ddl'  # è®°å½•DDLè¯­å¥
log_duration = on

# === è¡Œå‰ç¼€ï¼ˆJSONæ—¥å¿—ä¸­è‡ªåŠ¨åŒ…å«ï¼‰ ===
log_line_prefix = '%t [%p] %L %u@%d '  # %Lè¾“å‡ºå®¢æˆ·ç«¯IP
```

### 2.2 JSONæ—¥å¿—ç»“æ„è§£æ

**ç¤ºä¾‹æ—¥å¿—æ¡ç›®**ï¼š

```json
{
  "timestamp": "2025-12-04 10:25:30.123 UTC",
  "pid": 12345,
  "session_id": "657a2b3c.3039",
  "line_num": 1,
  "ps_display": "idle",
  "session_start": "2025-12-04 10:25:28 UTC",
  "vxid": "4/1234",
  "txid": 567890,
  "error_severity": "LOG",
  "state_code": "00000",
  "message": "duration: 1250.456 ms  statement: SELECT ...",
  "detail": null,
  "hint": null,
  "internal_query": null,
  "internal_position": null,
  "context": null,
  "query": "SELECT * FROM orders WHERE ...",
  "query_pos": 0,
  "location": null,
  "application_name": "my_app",
  "backend_type": "client backend",
  "leader_pid": null,
  "query_id": 123456789,

  // âœ… PostgreSQL 18æ–°å¢å­—æ®µ
  "remote_host": "192.168.1.100",  // %Lå®¢æˆ·ç«¯IP
  "setup_duration_ms": 45.2,  // è¿æ¥å»ºç«‹æ€»è€—æ—¶
  "fork_duration_ms": 12.3,  // è¿›ç¨‹forkè€—æ—¶
  "auth_duration_ms": 28.5,  // è®¤è¯è€—æ—¶
  "lock_failure_info": {  // é”å¤±è´¥è¯¦æƒ…
    "blocked_pid": 12345,
    "blocking_pid": 12344,
    "lock_type": "ExclusiveLock",
    "relation": "orders",
    "page": 1250,
    "tuple": 3
  }
}
```

### 2.3 æ—¥å¿—è§£æä¸åˆ†æ

#### 2.3.1 Pythonæ—¥å¿—è§£æå™¨

```python
#!/usr/bin/env python3
"""
PostgreSQL 18 JSONæ—¥å¿—è§£æå™¨
åŠŸèƒ½ï¼šè§£æã€è¿‡æ»¤ã€èšåˆã€å‘Šè­¦
"""

import json
import sys
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import re

class PG18LogAnalyzer:
    """PostgreSQL 18 JSONæ—¥å¿—åˆ†æå™¨"""

    def __init__(self, log_file):
        self.log_file = log_file
        self.slow_queries = []
        self.connection_stats = defaultdict(list)
        self.lock_failures = []
        self.error_stats = Counter()

    def parse_log(self):
        """è§£æJSONæ—¥å¿—æ–‡ä»¶"""
        with open(self.log_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    self.process_entry(entry)
                except json.JSONDecodeError as e:
                    print(f"JSONè§£æé”™è¯¯: {e}", file=sys.stderr)
                    continue

    def process_entry(self, entry):
        """å¤„ç†å•æ¡æ—¥å¿—"""
        # 1. æ…¢æŸ¥è¯¢åˆ†æ
        if 'duration' in entry.get('message', ''):
            duration = self._extract_duration(entry['message'])
            if duration and duration > 1000:  # >1ç§’
                self.slow_queries.append({
                    'timestamp': entry['timestamp'],
                    'duration_ms': duration,
                    'query': entry.get('query', ''),
                    'user': entry.get('user_name', ''),
                    'database': entry.get('database_name', ''),
                    'remote_host': entry.get('remote_host', '')
                })

        # 2. è¿æ¥æ€§èƒ½åˆ†æï¼ˆPostgreSQL 18æ–°å¢ï¼‰
        if entry.get('setup_duration_ms'):
            self.connection_stats['setup'].append(entry['setup_duration_ms'])
            self.connection_stats['fork'].append(entry.get('fork_duration_ms', 0))
            self.connection_stats['auth'].append(entry.get('auth_duration_ms', 0))

        # 3. é”å¤±è´¥åˆ†æï¼ˆPostgreSQL 18æ–°å¢ï¼‰
        if entry.get('lock_failure_info'):
            self.lock_failures.append({
                'timestamp': entry['timestamp'],
                'blocked_pid': entry['lock_failure_info']['blocked_pid'],
                'blocking_pid': entry['lock_failure_info']['blocking_pid'],
                'lock_type': entry['lock_failure_info']['lock_type'],
                'relation': entry['lock_failure_info'].get('relation', ''),
                'query': entry.get('query', '')
            })

        # 4. é”™è¯¯ç»Ÿè®¡
        if entry.get('error_severity') in ('ERROR', 'FATAL', 'PANIC'):
            error_type = entry.get('state_code', 'UNKNOWN')
            self.error_stats[error_type] += 1

    def _extract_duration(self, message):
        """ä»æ—¥å¿—æ¶ˆæ¯ä¸­æå–æ‰§è¡Œæ—¶é•¿"""
        match = re.search(r'duration:\s+([\d.]+)\s+ms', message)
        return float(match.group(1)) if match else None

    def generate_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("=" * 80)
        print("PostgreSQL 18 æ—¥å¿—åˆ†ææŠ¥å‘Š")
        print(f"åˆ†ææ—¶é—´: {datetime.now()}")
        print("=" * 80)

        # æ…¢æŸ¥è¯¢TOP 10
        print("\nã€æ…¢æŸ¥è¯¢TOP 10ã€‘")
        sorted_slow = sorted(self.slow_queries,
                            key=lambda x: x['duration_ms'],
                            reverse=True)[:10]
        for i, q in enumerate(sorted_slow, 1):
            print(f"{i}. {q['duration_ms']:.2f}ms - {q['query'][:60]}...")
            print(f"   ç”¨æˆ·: {q['user']}@{q['database']} from {q['remote_host']}")

        # è¿æ¥æ€§èƒ½ç»Ÿè®¡ï¼ˆPostgreSQL 18ç‰¹æ€§ï¼‰
        if self.connection_stats['setup']:
            print("\nã€è¿æ¥æ€§èƒ½ç»Ÿè®¡ã€‘ï¼ˆPostgreSQL 18æ–°å¢ï¼‰")
            avg_setup = sum(self.connection_stats['setup']) / len(self.connection_stats['setup'])
            avg_fork = sum(self.connection_stats['fork']) / len(self.connection_stats['fork'])
            avg_auth = sum(self.connection_stats['auth']) / len(self.connection_stats['auth'])

            print(f"å¹³å‡è¿æ¥å»ºç«‹è€—æ—¶: {avg_setup:.2f}ms")
            print(f"  - Forkè¿›ç¨‹: {avg_fork:.2f}ms ({avg_fork/avg_setup*100:.1f}%)")
            print(f"  - è®¤è¯: {avg_auth:.2f}ms ({avg_auth/avg_setup*100:.1f}%)")
            print(f"  - å…¶ä»–: {avg_setup-avg_fork-avg_auth:.2f}ms")

            # âš ï¸ å‘Šè­¦
            if avg_setup > 100:
                print(f"âš ï¸  è­¦å‘Š: å¹³å‡è¿æ¥è€—æ—¶è¿‡é«˜ ({avg_setup:.0f}ms > 100ms)")
                if avg_auth / avg_setup > 0.6:
                    print("    â†’ å»ºè®®: è®¤è¯è€—æ—¶å æ¯”è¿‡é«˜ï¼Œæ£€æŸ¥è®¤è¯é…ç½®")

        # é”å†²çªåˆ†æï¼ˆPostgreSQL 18ç‰¹æ€§ï¼‰
        if self.lock_failures:
            print(f"\nã€é”å¤±è´¥ç»Ÿè®¡ã€‘ï¼ˆPostgreSQL 18æ–°å¢ï¼‰")
            print(f"æ€»é”å¤±è´¥æ¬¡æ•°: {len(self.lock_failures)}")

            # æŒ‰è¡¨ç»Ÿè®¡
            relation_counter = Counter(
                lf['relation'] for lf in self.lock_failures if lf['relation']
            )
            print("\nTOP 5 é”å†²çªè¡¨:")
            for relation, count in relation_counter.most_common(5):
                print(f"  {relation}: {count}æ¬¡")

            # æœ€è¿‘5æ¬¡é”å¤±è´¥
            print("\næœ€è¿‘5æ¬¡é”å¤±è´¥:")
            for lf in self.lock_failures[-5:]:
                print(f"  {lf['timestamp']} - PID {lf['blocked_pid']} "
                      f"è¢« PID {lf['blocking_pid']} é˜»å¡")
                print(f"    è¡¨: {lf['relation']}, é”ç±»å‹: {lf['lock_type']}")

        # é”™è¯¯ç»Ÿè®¡
        if self.error_stats:
            print(f"\nã€é”™è¯¯ç»Ÿè®¡ã€‘")
            for error_code, count in self.error_stats.most_common(10):
                print(f"  {error_code}: {count}æ¬¡")

        print("\n" + "=" * 80)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python3 pg18_log_analyzer.py <æ—¥å¿—æ–‡ä»¶.json>")
        sys.exit(1)

    analyzer = PG18LogAnalyzer(sys.argv[1])
    analyzer.parse_log()
    analyzer.generate_report()
```

#### 2.3.2 å®æ—¶æ—¥å¿—ç›‘æ§

```bash
#!/bin/bash
# real_time_monitor.sh - å®æ—¶ç›‘æ§PostgreSQL 18 JSONæ—¥å¿—

LOG_FILE="/var/log/postgresql/postgresql-$(date +%Y-%m-%d)*.json"

echo "=== PostgreSQL 18 å®æ—¶æ—¥å¿—ç›‘æ§ ==="
echo "ç›‘æ§æ–‡ä»¶: $LOG_FILE"
echo "å¼€å§‹æ—¶é—´: $(date)"
echo "----------------------------------------"

# å®æ—¶è¿½è¸ªæ—¥å¿—
tail -f $LOG_FILE | while read -r line; do
    # è§£æJSON
    error_severity=$(echo "$line" | jq -r '.error_severity // empty')
    message=$(echo "$line" | jq -r '.message // empty')
    query=$(echo "$line" | jq -r '.query // empty')

    # âš ï¸ é”™è¯¯å‘Šè­¦
    if [[ "$error_severity" == "ERROR" || "$error_severity" == "FATAL" ]]; then
        echo "[$(date '+%H:%M:%S')] âŒ $error_severity: $message"

        # å‘é€å‘Šè­¦ï¼ˆç¤ºä¾‹ï¼‰
        # curl -X POST https://alert.example.com/webhook \
        #   -d "severity=$error_severity&message=$message"
    fi

    # ğŸŒ æ…¢æŸ¥è¯¢å‘Šè­¦
    if echo "$message" | grep -q "duration:"; then
        duration=$(echo "$message" | grep -oP 'duration: \K[\d.]+')
        if (( $(echo "$duration > 5000" | bc -l) )); then
            echo "[$(date '+%H:%M:%S')] ğŸŒ æ…¢æŸ¥è¯¢: ${duration}ms"
            echo "   SQL: ${query:0:80}..."
        fi
    fi

    # ğŸ”’ é”å¤±è´¥å‘Šè­¦ï¼ˆPostgreSQL 18æ–°å¢ï¼‰
    lock_failure=$(echo "$line" | jq -r '.lock_failure_info // empty')
    if [[ -n "$lock_failure" ]]; then
        blocked_pid=$(echo "$lock_failure" | jq -r '.blocked_pid')
        blocking_pid=$(echo "$lock_failure" | jq -r '.blocking_pid')
        relation=$(echo "$lock_failure" | jq -r '.relation')
        echo "[$(date '+%H:%M:%S')] ğŸ”’ é”å†²çª: PID $blocked_pid è¢« PID $blocking_pid é˜»å¡"
        echo "   è¡¨: $relation"
    fi
done
```

---

## 3. pg_stat_ioå­—èŠ‚çº§ç›‘æ§

### 3.1 æ–°å¢å­—èŠ‚çº§æŒ‡æ ‡

PostgreSQL 18å°†I/Oç»Ÿè®¡ä»**å—çº§ï¼ˆ8KBï¼‰**æå‡åˆ°**å­—èŠ‚çº§**ï¼Œç²¾åº¦æå‡æ˜¾è‘—ã€‚

```sql
-- æ€§èƒ½æµ‹è¯•ï¼šæŸ¥çœ‹pg_stat_ioç»“æ„ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½åˆ†æï¼‰
BEGIN;
EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT
    backend_type,
    object,
    context,

    -- âœ… æ–°å¢å­—èŠ‚çº§ç»Ÿè®¡
    reads,
    read_time,
    read_bytes,  -- æ–°å¢ï¼šè¯»å–å­—èŠ‚æ•°

    writes,
    write_time,
    write_bytes,  -- æ–°å¢ï¼šå†™å…¥å­—èŠ‚æ•°

    extends,
    extend_time,
    extend_bytes,  -- æ–°å¢ï¼šæ‰©å±•å­—èŠ‚æ•°

    -- åŒæ­¥ç»Ÿè®¡
    fsyncs,
    fsync_time,

    -- å‘½ä¸­ç‡
    hits,
    evictions,
    reuses

FROM pg_stat_io
ORDER BY backend_type, object;
COMMIT;
EXCEPTION
    WHEN undefined_table THEN
        RAISE NOTICE 'pg_stat_ioè§†å›¾ä¸å­˜åœ¨ï¼Œè¯·ç¡®è®¤PostgreSQLç‰ˆæœ¬ä¸º18+';
    WHEN OTHERS THEN
        RAISE NOTICE 'æŸ¥è¯¢pg_stat_ioå¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;
```

### 3.2 I/Oæ€§èƒ½åˆ†æè§†å›¾

```sql
-- æ€§èƒ½æµ‹è¯•ï¼šåˆ›å»ºI/Oæ€§èƒ½åˆ†æè§†å›¾ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
BEGIN;
CREATE OR REPLACE VIEW io_performance_analysis AS
SELECT
    backend_type,
    object,
    context,

    -- è¯»æ€§èƒ½
    reads AS read_operations,
    pg_size_pretty(read_bytes) AS read_data_size,
    CASE
        WHEN reads > 0 THEN ROUND((read_bytes::NUMERIC / reads) / 1024, 2)
        ELSE 0
    END AS avg_read_kb,
    CASE
        WHEN reads > 0 THEN ROUND(read_time / reads, 2)
        ELSE 0
    END AS avg_read_time_ms,

    -- å†™æ€§èƒ½
    writes AS write_operations,
    pg_size_pretty(write_bytes) AS write_data_size,
    CASE
        WHEN writes > 0 THEN ROUND((write_bytes::NUMERIC / writes) / 1024, 2)
        ELSE 0
    END AS avg_write_kb,
    CASE
        WHEN writes > 0 THEN ROUND(write_time / writes, 2)
        ELSE 0
    END AS avg_write_time_ms,

    -- ç¼“å­˜å‘½ä¸­ç‡
    CASE
        WHEN (reads + hits) > 0 THEN
            ROUND(hits * 100.0 / (reads + hits), 2)
        ELSE 0
    END AS cache_hit_ratio,

    -- I/Oååé‡ï¼ˆMB/sï¼‰
    ROUND((read_bytes + write_bytes)::NUMERIC / 1024 / 1024, 2) AS total_mb,

    -- æ€§èƒ½è¯„åˆ†
    CASE
        WHEN reads > 0 AND (read_time / reads) < 1 THEN 'ğŸŸ¢ ä¼˜ç§€'
        WHEN reads > 0 AND (read_time / reads) < 5 THEN 'ğŸŸ¡ è‰¯å¥½'
        WHEN reads > 0 THEN 'ğŸ”´ éœ€ä¼˜åŒ–'
        ELSE 'âšª æ— æ•°æ®'
    END AS performance_rating

FROM pg_stat_io
WHERE reads > 0 OR writes > 0
ORDER BY (read_bytes + write_bytes) DESC;
COMMIT;
EXCEPTION
    WHEN undefined_table THEN
        RAISE NOTICE 'pg_stat_ioè§†å›¾ä¸å­˜åœ¨ï¼Œè¯·ç¡®è®¤PostgreSQLç‰ˆæœ¬ä¸º18+';
    WHEN OTHERS THEN
        RAISE NOTICE 'åˆ›å»ºI/Oæ€§èƒ½åˆ†æè§†å›¾å¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;

-- æ€§èƒ½æµ‹è¯•ï¼šä½¿ç”¨ç¤ºä¾‹ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½åˆ†æï¼‰
BEGIN;
EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT * FROM io_performance_analysis
LIMIT 20;
COMMIT;
EXCEPTION
    WHEN undefined_table THEN
        RAISE NOTICE 'è§†å›¾io_performance_analysisä¸å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE NOTICE 'æŸ¥è¯¢I/Oæ€§èƒ½åˆ†æå¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;
```

**è¾“å‡ºç¤ºä¾‹**ï¼š

```text
 backend_type  | object | context | read_operations | read_data_size | avg_read_kb | avg_read_time_ms | cache_hit_ratio | total_mb | performance_rating
---------------+--------+---------+-----------------+----------------+-------------+------------------+-----------------+----------+-------------------
 client backend| relation| normal |      1250000    |    9.8 GB      |    8.00     |      0.85        |     92.50       | 10250.00 | ğŸŸ¢ ä¼˜ç§€
 autovacuum    | relation| vacuum |       350000    |    2.7 GB      |    8.00     |      1.25        |     85.30       |  2850.00 | ğŸŸ¡ è‰¯å¥½
 background writer| relation| normal|       120000    |  940 MB        |    8.00     |      2.10        |     78.20       |   980.00 | ğŸŸ¡ è‰¯å¥½
 checkpointer  | relation| normal |        85000    |  665 MB        |    8.00     |      4.85        |     65.40       |   720.00 | ğŸ”´ éœ€ä¼˜åŒ–
```

### 3.3 I/Oçƒ­ç‚¹åˆ†æ

```sql
-- æ€§èƒ½æµ‹è¯•ï¼šI/Oçƒ­ç‚¹è¡¨è¯†åˆ«ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½åˆ†æï¼‰
BEGIN;
EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT
    schemaname,
    tablename,

    -- å †è¡¨I/O
    heap_blks_read,
    heap_blks_hit,
    ROUND(heap_blks_hit * 100.0 / NULLIF(heap_blks_hit + heap_blks_read, 0), 2) AS heap_hit_ratio,

    -- âœ… PostgreSQL 18: è®¡ç®—å®é™…å­—èŠ‚æ•°
    pg_size_pretty((heap_blks_read + heap_blks_hit) * 8192) AS heap_io_size,

    -- ç´¢å¼•I/O
    idx_blks_read,
    idx_blks_hit,
    ROUND(idx_blks_hit * 100.0 / NULLIF(idx_blks_hit + idx_blks_read, 0), 2) AS idx_hit_ratio,

    pg_size_pretty((idx_blks_read + idx_blks_hit) * 8192) AS idx_io_size,

    -- æ€»I/O
    pg_size_pretty(
        (heap_blks_read + heap_blks_hit + idx_blks_read + idx_blks_hit) * 8192
    ) AS total_io_size,

    -- I/Oå¯†é›†åº¦è¯„åˆ†
    CASE
        WHEN heap_blks_read > 1000000 THEN 'ğŸ”¥ çƒ­ç‚¹è¡¨'
        WHEN heap_blks_read > 100000 THEN 'âš ï¸  é«˜I/O'
        ELSE 'âœ… æ­£å¸¸'
    END AS io_intensity

FROM pg_statio_user_tables
WHERE heap_blks_read + idx_blks_read > 1000
ORDER BY (heap_blks_read + idx_blks_read) DESC
LIMIT 20;
COMMIT;
EXCEPTION
    WHEN undefined_table THEN
        RAISE NOTICE 'pg_statio_user_tablesè§†å›¾ä¸å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE NOTICE 'I/Oçƒ­ç‚¹è¡¨è¯†åˆ«å¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;
```

---

## 4. è¿æ¥é˜¶æ®µè€—æ—¶åˆ†æ

### 4.1 è¿æ¥ä¸‰é˜¶æ®µç›‘æ§

PostgreSQL 18å°†è¿æ¥å»ºç«‹è¿‡ç¨‹æ‹†åˆ†ä¸º**ä¸‰ä¸ªå¯æµ‹é‡é˜¶æ®µ**ï¼š

```mermaid
sequenceDiagram
    participant Client as å®¢æˆ·ç«¯
    participant PG as PostgreSQL Master
    participant Backend as Backendè¿›ç¨‹
    participant Auth as è®¤è¯æ¨¡å—

    Note over Client,Auth: PostgreSQL 18è¿æ¥ä¸‰é˜¶æ®µ

    Client->>PG: 1. è¿æ¥è¯·æ±‚

    rect rgb(255, 200, 200)
        Note over PG: é˜¶æ®µ1: Forkè¿›ç¨‹<br/>fork_duration_ms
        PG->>Backend: fork()æ–°å»ºåç«¯è¿›ç¨‹
        Backend-->>PG: è¿›ç¨‹åˆ›å»ºå®Œæˆ
    end

    rect rgb(200, 255, 200)
        Note over Backend,Auth: é˜¶æ®µ2: è®¤è¯<br/>auth_duration_ms
        Backend->>Auth: éªŒè¯å‡­è¯
        Auth->>Auth: SCRAM-SHA-256
        Auth-->>Backend: è®¤è¯é€šè¿‡
    end

    rect rgb(200, 200, 255)
        Note over Backend: é˜¶æ®µ3: åˆå§‹åŒ–<br/>å…¶ä»–è€—æ—¶
        Backend->>Backend: åŠ è½½é…ç½®
        Backend->>Backend: åˆå§‹åŒ–ç¼“å­˜
        Backend-->>Client: è¿æ¥å°±ç»ª
    end

    Note over Client,Backend: æ€»è€—æ—¶ = setup_duration_ms
```

### 4.2 è¿æ¥æ€§èƒ½ç›‘æ§æŸ¥è¯¢

```sql
-- æ€§èƒ½æµ‹è¯•ï¼šä»JSONæ—¥å¿—ä¸­æå–è¿æ¥æ€§èƒ½ç»Ÿè®¡ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
BEGIN;
-- ï¼ˆéœ€è¦å…ˆå¯¼å…¥æ—¥å¿—åˆ°ä¸´æ—¶è¡¨ï¼‰
CREATE TEMP TABLE IF NOT EXISTS connection_logs (
    log_time TIMESTAMPTZ,
    session_id TEXT,
    remote_host TEXT,
    user_name TEXT,
    database_name TEXT,
    setup_duration_ms NUMERIC,
    fork_duration_ms NUMERIC,
    auth_duration_ms NUMERIC
);
COMMIT;
EXCEPTION
    WHEN OTHERS THEN
        RAISE NOTICE 'åˆ›å»ºä¸´æ—¶è¡¨å¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;

-- æ€§èƒ½æµ‹è¯•ï¼šå¯¼å…¥æ—¥å¿—æ•°æ®ï¼ˆç¤ºä¾‹ï¼‰ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
BEGIN;
-- \COPY connection_logs FROM PROGRAM 'jq -c ". | select(.setup_duration_ms != null) | {log_time: .timestamp, session_id, remote_host, user_name, database_name, setup_duration_ms, fork_duration_ms, auth_duration_ms}" /var/log/postgresql/*.json' WITH (FORMAT csv);
RAISE NOTICE 'æ—¥å¿—æ•°æ®å¯¼å…¥ï¼ˆç¤ºä¾‹ï¼Œå®é™…ä½¿ç”¨COPYå‘½ä»¤ï¼‰';
COMMIT;
EXCEPTION
    WHEN OTHERS THEN
        RAISE NOTICE 'å¯¼å…¥æ—¥å¿—æ•°æ®å¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;

-- æ€§èƒ½æµ‹è¯•ï¼šåˆ†æè¿æ¥æ€§èƒ½ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½åˆ†æï¼‰
BEGIN;
EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT
    -- æ€»ä½“ç»Ÿè®¡
    COUNT(*) AS total_connections,
    ROUND(AVG(setup_duration_ms), 2) AS avg_setup_ms,
    ROUND(AVG(fork_duration_ms), 2) AS avg_fork_ms,
    ROUND(AVG(auth_duration_ms), 2) AS avg_auth_ms,
    ROUND(AVG(setup_duration_ms - fork_duration_ms - auth_duration_ms), 2) AS avg_other_ms,

    -- ç™¾åˆ†ä½æ•°
    ROUND(PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY setup_duration_ms), 2) AS p50_setup_ms,
    ROUND(PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY setup_duration_ms), 2) AS p95_setup_ms,
    ROUND(PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY setup_duration_ms), 2) AS p99_setup_ms,

    -- æœ€å¤§å€¼![1766347347386](image/22-ç›‘æ§ä¸å¯è§‚æµ‹æ€§å®Œæ•´ä½“ç³»æŒ‡å—/1766347347386.pdf)
    ROUND(MAX(setup_duration_ms), 2) AS max_setup_ms,

    -- å„é˜¶æ®µå æ¯”
    ROUND(AVG(fork_duration_ms) * 100.0 / NULLIF(AVG(setup_duration_ms), 0), 1) AS fork_pct,
    ROUND(AVG(auth_duration_ms) * 100.0 / NULLIF(AVG(setup_duration_ms), 0), 1) AS auth_pct,

    -- æ€§èƒ½è¯„çº§
    CASE
        WHEN AVG(setup_duration_ms) < 10 THEN 'ğŸŸ¢ ä¼˜ç§€ (<10ms)'
        WHEN AVG(setup_duration_ms) < 50 THEN 'ğŸŸ¡ è‰¯å¥½ (<50ms)'
        WHEN AVG(setup_duration_ms) < 100 THEN 'ğŸŸ  ä¸€èˆ¬ (<100ms)'
        ELSE 'ğŸ”´ æ…¢ (>100ms)'
    END AS performance_rating

FROM connection_logs
WHERE log_time >= NOW() - INTERVAL '1 hour';
COMMIT;
EXCEPTION
    WHEN undefined_table THEN
        RAISE NOTICE 'è¡¨connection_logsä¸å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE NOTICE 'åˆ†æè¿æ¥æ€§èƒ½å¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;
```

**ç¤ºä¾‹è¾“å‡º**ï¼š

```text
 total_connections | avg_setup_ms | avg_fork_ms | avg_auth_ms | avg_other_ms | p50 | p95  | p99  | max   | fork_pct | auth_pct | performance_rating
-------------------+--------------+-------------+-------------+--------------+-----+------+------+-------+----------+----------+-------------------
       1250        |    45.2      |    12.3     |    28.5     |     4.4      | 42  | 85   | 120  | 250   |   27.2   |   63.1   | ğŸŸ¡ è‰¯å¥½ (<50ms)
```

### 4.3 è¿æ¥æ€§èƒ½å¼‚å¸¸æ£€æµ‹

```sql
-- æ€§èƒ½æµ‹è¯•ï¼šè¯†åˆ«æ…¢è¿æ¥ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½åˆ†æï¼‰
BEGIN;
EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT
    log_time,
    session_id,
    remote_host,
    user_name,
    database_name,
    setup_duration_ms,
    fork_duration_ms,
    auth_duration_ms,

    -- æ ‡è¯†ç“¶é¢ˆé˜¶æ®µ
    CASE
        WHEN auth_duration_ms > setup_duration_ms * 0.7 THEN 'ğŸ” è®¤è¯æ…¢'
        WHEN fork_duration_ms > setup_duration_ms * 0.5 THEN 'âš¡ Forkæ…¢'
        ELSE 'âš™ï¸  åˆå§‹åŒ–æ…¢'
    END AS bottleneck_stage,

    -- æ…¢çš„ç¨‹åº¦
    CASE
        WHEN setup_duration_ms > 200 THEN 'ğŸ”´ ä¸¥é‡æ…¢'
        WHEN setup_duration_ms > 100 THEN 'ğŸŸ  ä¸­ç­‰æ…¢'
        ELSE 'ğŸŸ¡ è½»å¾®æ…¢'
    END AS slowness_level

FROM connection_logs
WHERE setup_duration_ms > 50  -- è¶…è¿‡50msè§†ä¸ºæ…¢è¿æ¥
ORDER BY setup_duration_ms DESC
LIMIT 20;
COMMIT;
EXCEPTION
    WHEN undefined_table THEN
        RAISE NOTICE 'è¡¨connection_logsä¸å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE NOTICE 'è¯†åˆ«æ…¢è¿æ¥å¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;
```

**è¯Šæ–­å»ºè®®**ï¼š

```sql
-- æ€§èƒ½æµ‹è¯•ï¼šç”Ÿæˆä¼˜åŒ–å»ºè®®ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½åˆ†æï¼‰
BEGIN;
EXPLAIN (ANALYZE, BUFFERS, TIMING)
WITH connection_perf AS (
    SELECT
        AVG(setup_duration_ms) AS avg_setup,
        AVG(fork_duration_ms) AS avg_fork,
        AVG(auth_duration_ms) AS avg_auth
    FROM connection_logs
    WHERE log_time >= NOW() - INTERVAL '1 day'
)
SELECT
    CASE
        WHEN avg_auth / avg_setup > 0.6 THEN
            'å»ºè®®1: è®¤è¯è€—æ—¶å æ¯”' || ROUND(avg_auth * 100 / avg_setup) || '%ï¼Œæ£€æŸ¥ï¼š\n' ||
            '  - pg_hba.confé…ç½®æ˜¯å¦è¿‡äºå¤æ‚\n' ||
            '  - æ˜¯å¦ä½¿ç”¨äº†LDAP/RADIUSç­‰å¤–éƒ¨è®¤è¯\n' ||
            '  - ç½‘ç»œå»¶è¿Ÿæ˜¯å¦è¿‡é«˜'
        WHEN avg_fork / avg_setup > 0.4 THEN
            'å»ºè®®2: Forkè¿›ç¨‹è€—æ—¶å æ¯”' || ROUND(avg_fork * 100 / avg_setup) || '%ï¼Œæ£€æŸ¥ï¼š\n' ||
            '  - ç³»ç»Ÿè´Ÿè½½æ˜¯å¦è¿‡é«˜\n' ||
            '  - max_connectionsæ˜¯å¦è®¾ç½®è¿‡å¤§\n' ||
            '  - è€ƒè™‘ä½¿ç”¨è¿æ¥æ± ï¼ˆPgBouncerï¼‰'
        ELSE
            'å»ºè®®3: åˆå§‹åŒ–è€—æ—¶è¾ƒé«˜ï¼Œæ£€æŸ¥ï¼š\n' ||
            '  - shared_preload_librariesæ˜¯å¦åŠ è½½è¿‡å¤šæ‰©å±•\n' ||
            '  - é…ç½®æ–‡ä»¶æ˜¯å¦è¿‡å¤§\n' ||
            '  - æ˜¯å¦æœ‰è‡ªå®šä¹‰GUCå‚æ•°'
    END AS optimization_recommendation
FROM connection_perf;
COMMIT;
EXCEPTION
    WHEN undefined_table THEN
        RAISE NOTICE 'è¡¨connection_logsä¸å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE NOTICE 'ç”Ÿæˆä¼˜åŒ–å»ºè®®å¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;
```

---

## 5. AIOæ€§èƒ½ç›‘æ§

### 5.1 pg_aiosè§†å›¾è¯¦è§£

```sql
-- æ€§èƒ½æµ‹è¯•ï¼šæŸ¥çœ‹pg_aiosè§†å›¾ç»“æ„ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½åˆ†æï¼‰
-- \d pg_aios  -- éœ€è¦åœ¨psqlä¸­æ‰§è¡Œ

-- æ€§èƒ½æµ‹è¯•ï¼šæŸ¥è¯¢AIOçŠ¶æ€ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½åˆ†æï¼‰
BEGIN;
EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT
    backend_type,
    backend_pid,

    -- æ–‡ä»¶ä¿¡æ¯
    io_object,  -- 'relation', 'temp'ç­‰
    io_context,  -- 'normal', 'bulkread', 'vacuum'ç­‰

    -- âœ… AIOå¥æŸ„ç»Ÿè®¡
    allocated_aios,  -- å·²åˆ†é…çš„AIOå¥æŸ„æ•°
    max_aios,  -- æœ€å¤§AIOå¥æŸ„æ•°

    -- âœ… I/Oé˜Ÿåˆ—æ·±åº¦
    pending_reads,  -- å¾…å¤„ç†è¯»è¯·æ±‚
    pending_writes,  -- å¾…å¤„ç†å†™è¯·æ±‚ï¼ˆæœªæ¥ç‰ˆæœ¬ï¼‰

    -- âœ… æ€§èƒ½æŒ‡æ ‡
    completed_reads,
    completed_writes,

    -- âœ… èµ„æºä½¿ç”¨
    ROUND(allocated_aios * 100.0 / NULLIF(max_aios, 0), 2) AS aio_usage_pct

FROM pg_aios
WHERE allocated_aios > 0
ORDER BY allocated_aios DESC;
COMMIT;
EXCEPTION
    WHEN undefined_table THEN
        RAISE NOTICE 'pg_aiosè§†å›¾ä¸å­˜åœ¨ï¼Œè¯·ç¡®è®¤PostgreSQLç‰ˆæœ¬ä¸º18+';
    WHEN OTHERS THEN
        RAISE NOTICE 'æŸ¥è¯¢AIOçŠ¶æ€å¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;
```

### 5.2 AIOæ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿

```sql
-- æ€§èƒ½æµ‹è¯•ï¼šåˆ›å»ºAIOç›‘æ§è§†å›¾ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
BEGIN;
CREATE OR REPLACE VIEW aio_performance_dashboard AS
WITH aio_summary AS (
    SELECT
        COUNT(*) AS active_backends,
        SUM(allocated_aios) AS total_allocated,
        SUM(max_aios) AS total_capacity,
        SUM(pending_reads) AS total_pending,
        SUM(completed_reads) AS total_completed,
        AVG(allocated_aios) AS avg_per_backend,
        MAX(allocated_aios) AS max_per_backend
    FROM pg_aios
    WHERE backend_type = 'client backend'
)
SELECT
    *,
    -- ä½¿ç”¨ç‡
    ROUND(total_allocated * 100.0 / NULLIF(total_capacity, 0), 2) AS capacity_usage_pct,

    -- é˜Ÿåˆ—æ·±åº¦
    ROUND(total_pending * 1.0 / NULLIF(active_backends, 0), 2) AS avg_queue_depth,

    -- æ€§èƒ½è¯„åˆ†
    CASE
        WHEN total_allocated * 100.0 / NULLIF(total_capacity, 0) > 80 THEN 'ğŸ”´ é«˜è´Ÿè½½'
        WHEN total_allocated * 100.0 / NULLIF(total_capacity, 0) > 50 THEN 'ğŸŸ¡ ä¸­ç­‰è´Ÿè½½'
        ELSE 'ğŸŸ¢ æ­£å¸¸'
    END AS load_status,

    -- å»ºè®®
    CASE
        WHEN total_allocated * 100.0 / NULLIF(total_capacity, 0) > 80 THEN
            'å»ºè®®å¢åŠ maintenance_io_concurrencyå‚æ•°'
        WHEN avg_per_backend < max_aios * 0.3 THEN
            'å½“å‰AIOåˆ©ç”¨ç‡è¾ƒä½ï¼Œå¯èƒ½æœªå¯ç”¨æˆ–å·¥ä½œè´Ÿè½½ä¸é€‚åˆ'
        ELSE 'æ€§èƒ½æ­£å¸¸'
    END AS recommendation

FROM aio_summary;
COMMIT;
EXCEPTION
    WHEN undefined_table THEN
        RAISE NOTICE 'pg_aiosè§†å›¾ä¸å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE NOTICE 'åˆ›å»ºAIOç›‘æ§è§†å›¾å¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;

-- æ€§èƒ½æµ‹è¯•ï¼šæŸ¥çœ‹ä»ªè¡¨æ¿ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½åˆ†æï¼‰
BEGIN;
EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT * FROM aio_performance_dashboard;
COMMIT;
EXCEPTION
    WHEN undefined_table THEN
        RAISE NOTICE 'è§†å›¾aio_performance_dashboardä¸å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE NOTICE 'æŸ¥çœ‹ä»ªè¡¨æ¿å¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;
```

**è¾“å‡ºç¤ºä¾‹**ï¼š

```text
 active_backends | total_allocated | total_capacity | total_pending | total_completed | avg_per_backend | max_per_backend | capacity_usage_pct | avg_queue_depth | load_status | recommendation
-----------------+-----------------+----------------+---------------+-----------------+-----------------+-----------------+--------------------+-----------------+-------------+----------------
       8         |      128        |      256       |      45       |    12500000     |      16.0       |       32        |      50.00         |      5.63       | ğŸŸ¡ ä¸­ç­‰è´Ÿè½½  | æ€§èƒ½æ­£å¸¸
```

### 5.3 AIOæ•ˆç‡åˆ†æ

```sql
-- æ€§èƒ½æµ‹è¯•ï¼šå¯¹æ¯”AIOå¼€å¯å‰åçš„æ€§èƒ½ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
BEGIN;
-- ï¼ˆéœ€è¦å…ˆè®°å½•å†å²æ•°æ®åˆ°ç›‘æ§è¡¨ï¼‰
CREATE TABLE IF NOT EXISTS aio_performance_history (
    sample_time TIMESTAMPTZ DEFAULT now(),
    aio_enabled BOOLEAN,
    query_type TEXT,
    avg_duration_ms NUMERIC,
    io_read_bytes BIGINT,
    io_read_time_ms NUMERIC
);
COMMIT;
EXCEPTION
    WHEN duplicate_table THEN
        RAISE NOTICE 'è¡¨aio_performance_historyå·²å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE NOTICE 'åˆ›å»ºè¡¨aio_performance_historyå¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;

-- æ€§èƒ½æµ‹è¯•ï¼šæ’å…¥æµ‹è¯•æ•°æ®ï¼ˆç¤ºä¾‹ï¼‰ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
BEGIN;
-- AIOå…³é—­æ—¶
-- SET io_method = 'sync';  -- éœ€è¦åœ¨ä¼šè¯çº§åˆ«è®¾ç½®
INSERT INTO aio_performance_history (aio_enabled, query_type, avg_duration_ms, io_read_bytes, io_read_time_ms)
SELECT false, 'seq_scan', 5250, 2147483648, 4800
ON CONFLICT DO NOTHING;
COMMIT;
EXCEPTION
    WHEN undefined_table THEN
        RAISE NOTICE 'è¡¨aio_performance_historyä¸å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE NOTICE 'æ’å…¥AIOå…³é—­æ•°æ®å¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;

BEGIN;
-- AIOå¼€å¯æ—¶
-- SET io_method = 'io_uring';  -- éœ€è¦åœ¨ä¼šè¯çº§åˆ«è®¾ç½®
-- SET maintenance_io_concurrency = 32;  -- éœ€è¦åœ¨ä¼šè¯çº§åˆ«è®¾ç½®
INSERT INTO aio_performance_history (aio_enabled, query_type, avg_duration_ms, io_read_bytes, io_read_time_ms)
SELECT true, 'seq_scan', 1850, 2147483648, 1200
ON CONFLICT DO NOTHING;
COMMIT;
EXCEPTION
    WHEN undefined_table THEN
        RAISE NOTICE 'è¡¨aio_performance_historyä¸å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE NOTICE 'æ’å…¥AIOå¼€å¯æ•°æ®å¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;

-- æ€§èƒ½æµ‹è¯•ï¼šå¯¹æ¯”åˆ†æï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½åˆ†æï¼‰
BEGIN;
EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT
    query_type,

    -- AIOå…³é—­
    MAX(CASE WHEN NOT aio_enabled THEN avg_duration_ms END) AS duration_without_aio,
    MAX(CASE WHEN NOT aio_enabled THEN io_read_time_ms END) AS io_time_without_aio,

    -- AIOå¼€å¯
    MAX(CASE WHEN aio_enabled THEN avg_duration_ms END) AS duration_with_aio,
    MAX(CASE WHEN aio_enabled THEN io_read_time_ms END) AS io_time_with_aio,

    -- æ€§èƒ½æå‡
    ROUND(
        (1 - MAX(CASE WHEN aio_enabled THEN avg_duration_ms END) /
         NULLIF(MAX(CASE WHEN NOT aio_enabled THEN avg_duration_ms END), 0)) * 100,
        2
    ) AS duration_improvement_pct,

    ROUND(
        (1 - MAX(CASE WHEN aio_enabled THEN io_read_time_ms END) /
         NULLIF(MAX(CASE WHEN NOT aio_enabled THEN io_read_time_ms END), 0)) * 100,
        2
    ) AS io_improvement_pct,

    -- ç»“è®º
    CASE
        WHEN (1 - MAX(CASE WHEN aio_enabled THEN avg_duration_ms END) /
              NULLIF(MAX(CASE WHEN NOT aio_enabled THEN avg_duration_ms END), 0)) > 0.5
        THEN 'âœ… AIOæ˜¾è‘—æå‡æ€§èƒ½ (>50%)'
        WHEN (1 - MAX(CASE WHEN aio_enabled THEN avg_duration_ms END) /
              NULLIF(MAX(CASE WHEN NOT aio_enabled THEN avg_duration_ms END), 0)) > 0.2
        THEN 'ğŸŸ¡ AIOæœ‰æ•ˆæå‡æ€§èƒ½ (20-50%)'
        ELSE 'âš ï¸  AIOæå‡ä¸æ˜æ˜¾ (<20%)'
    END AS aio_effectiveness

FROM aio_performance_history
GROUP BY query_type;
COMMIT;
EXCEPTION
    WHEN undefined_table THEN
        RAISE NOTICE 'è¡¨aio_performance_historyä¸å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE NOTICE 'å¯¹æ¯”åˆ†æå¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;
```

---

## 6. é”å¤±è´¥è¯¦æƒ…è¯Šæ–­

### 6.1 å¯ç”¨é”å¤±è´¥æ—¥å¿—

```ini
# postgresql.conf
log_lock_failures = on  # PostgreSQL 18æ–°å¢

# é”ç­‰å¾…è¶…æ—¶
lock_timeout = 5000  # 5ç§’

# æ­»é”è¶…æ—¶
deadlock_timeout = 1000  # 1ç§’
```

### 6.2 é”å†²çªå®æ—¶ç›‘æ§

```sql
-- æ€§èƒ½æµ‹è¯•ï¼šæŸ¥çœ‹å½“å‰é”ç­‰å¾…æƒ…å†µï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½åˆ†æï¼‰
BEGIN;
EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT
    blocked_locks.pid AS blocked_pid,
    blocked_activity.usename AS blocked_user,
    blocking_locks.pid AS blocking_pid,
    blocking_activity.usename AS blocking_user,
    blocked_activity.query AS blocked_statement,
    blocking_activity.query AS blocking_statement,
    blocked_locks.mode AS blocked_lock_mode,
    blocking_locks.mode AS blocking_lock_mode,
    blocked_activity.application_name AS blocked_app,
    blocking_activity.application_name AS blocking_app,

    -- âœ… ç­‰å¾…æ—¶é•¿
    EXTRACT(EPOCH FROM (now() - blocked_activity.query_start)) AS wait_duration_seconds,

    -- âœ… é”å®šå¯¹è±¡
    blocked_locks.relation::regclass AS locked_relation,
    blocked_locks.page,
    blocked_locks.tuple,

    -- âœ… è¯Šæ–­å»ºè®®
    CASE
        WHEN EXTRACT(EPOCH FROM (now() - blocked_activity.query_start)) > 30 THEN
            'ğŸ”´ ä¸¥é‡: ç­‰å¾…è¶…è¿‡30ç§’ï¼Œå»ºè®®ç»ˆæ­¢é˜»å¡ä¼šè¯'
        WHEN EXTRACT(EPOCH FROM (now() - blocked_activity.query_start)) > 10 THEN
            'ğŸŸ¡ è­¦å‘Š: ç­‰å¾…è¶…è¿‡10ç§’'
        ELSE 'ğŸŸ¢ æ­£å¸¸'
    END AS severity

FROM pg_catalog.pg_locks blocked_locks
JOIN pg_catalog.pg_stat_activity blocked_activity ON blocked_activity.pid = blocked_locks.pid
JOIN pg_catalog.pg_locks blocking_locks
    ON blocking_locks.locktype = blocked_locks.locktype
    AND blocking_locks.database IS NOT DISTINCT FROM blocked_locks.database
    AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation
    AND blocking_locks.page IS NOT DISTINCT FROM blocked_locks.page
    AND blocking_locks.tuple IS NOT DISTINCT FROM blocked_locks.tuple
    AND blocking_locks.virtualxid IS NOT DISTINCT FROM blocked_locks.virtualxid
    AND blocking_locks.transactionid IS NOT DISTINCT FROM blocked_locks.transactionid
    AND blocking_locks.classid IS NOT DISTINCT FROM blocked_locks.classid
    AND blocking_locks.objid IS NOT DISTINCT FROM blocked_locks.objid
    AND blocking_locks.objsubid IS NOT DISTINCT FROM blocked_locks.objsubid
    AND blocking_locks.pid != blocked_locks.pid
JOIN pg_catalog.pg_stat_activity blocking_activity ON blocking_activity.pid = blocking_locks.pid
WHERE NOT blocked_locks.granted
ORDER BY wait_duration_seconds DESC;
COMMIT;
EXCEPTION
    WHEN undefined_table THEN
        RAISE NOTICE 'pg_locksæˆ–pg_stat_activityè§†å›¾ä¸å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE NOTICE 'æŸ¥çœ‹é”ç­‰å¾…æƒ…å†µå¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;
```

### 6.3 æ­»é”å†å²åˆ†æ

```sql
-- æ€§èƒ½æµ‹è¯•ï¼šä»JSONæ—¥å¿—ä¸­æå–æ­»é”ä¿¡æ¯ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
BEGIN;
CREATE TEMP TABLE IF NOT EXISTS deadlock_history (
    occurred_at TIMESTAMPTZ,
    victim_pid INT,
    victor_pid INT,
    victim_query TEXT,
    victor_query TEXT,
    locked_relation TEXT,
    lock_type TEXT
);
COMMIT;
EXCEPTION
    WHEN OTHERS THEN
        RAISE NOTICE 'åˆ›å»ºä¸´æ—¶è¡¨deadlock_historyå¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;

-- è§£ææ—¥å¿—å¹¶æ’å…¥ï¼ˆç¤ºä¾‹ï¼‰
-- å®é™…ç”Ÿäº§ä¸­ä½¿ç”¨æ—¥å¿—è§£æè„šæœ¬

-- æ€§èƒ½æµ‹è¯•ï¼šåˆ†ææ­»é”æ¨¡å¼ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½åˆ†æï¼‰
BEGIN;
EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT
    DATE_TRUNC('hour', occurred_at) AS hour,
    COUNT(*) AS deadlock_count,

    -- æœ€å¸¸è§çš„æ­»é”è¡¨
    MODE() WITHIN GROUP (ORDER BY locked_relation) AS most_common_relation,

    -- æœ€å¸¸è§çš„é”ç±»å‹
    MODE() WITHIN GROUP (ORDER BY lock_type) AS most_common_lock_type,

    -- å¹³å‡æ¯å°æ—¶æ­»é”æ•°
    ROUND(COUNT(*) * 1.0 /
          NULLIF(EXTRACT(EPOCH FROM (MAX(occurred_at) - MIN(occurred_at))) / 3600, 0),
          2) AS deadlocks_per_hour

FROM deadlock_history
WHERE occurred_at >= NOW() - INTERVAL '7 days'
GROUP BY DATE_TRUNC('hour', occurred_at)
HAVING COUNT(*) > 0
ORDER BY hour DESC
LIMIT 24;
COMMIT;
EXCEPTION
    WHEN undefined_table THEN
        RAISE NOTICE 'è¡¨deadlock_historyä¸å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE NOTICE 'åˆ†ææ­»é”æ¨¡å¼å¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;
```

### 6.4 é”è¶…æ—¶å‘Šè­¦

```sql
-- æ€§èƒ½æµ‹è¯•ï¼šåˆ›å»ºé”è¶…æ—¶å‘Šè­¦å‡½æ•°ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
BEGIN;
CREATE OR REPLACE FUNCTION check_long_running_locks()
RETURNS TABLE (
    alert_level TEXT,
    message TEXT,
    action_required TEXT
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        'ğŸ”´ CRITICAL' AS alert_level,
        'PID ' || blocked_locks.pid::TEXT || ' è¢« PID ' || blocking_locks.pid::TEXT ||
        ' é˜»å¡è¶…è¿‡ ' || ROUND(EXTRACT(EPOCH FROM (now() - blocked_activity.query_start)))::TEXT || ' ç§’' AS message,
        'EXECUTE: SELECT pg_terminate_backend(' || blocking_locks.pid::TEXT || ');' AS action_required
    FROM pg_catalog.pg_locks blocked_locks
    JOIN pg_catalog.pg_stat_activity blocked_activity ON blocked_activity.pid = blocked_locks.pid
    JOIN pg_catalog.pg_locks blocking_locks
        ON blocking_locks.locktype = blocked_locks.locktype
        AND blocking_locks.pid != blocked_locks.pid
    WHERE NOT blocked_locks.granted
      AND EXTRACT(EPOCH FROM (now() - blocked_activity.query_start)) > 60;  -- è¶…è¿‡60ç§’
EXCEPTION
    WHEN OTHERS THEN
        RAISE NOTICE 'check_long_running_locksæ‰§è¡Œå¤±è´¥: %', SQLERRM;
        RETURN;
END;
$$ LANGUAGE plpgsql;
COMMIT;
EXCEPTION
    WHEN OTHERS THEN
        RAISE NOTICE 'åˆ›å»ºé”è¶…æ—¶å‘Šè­¦å‡½æ•°å¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;

-- æ€§èƒ½æµ‹è¯•ï¼šå®šæœŸæ£€æŸ¥ï¼ˆå¯é…åˆpg_cronï¼‰ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½åˆ†æï¼‰
BEGIN;
EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT * FROM check_long_running_locks();
COMMIT;
EXCEPTION
    WHEN undefined_function THEN
        RAISE NOTICE 'å‡½æ•°check_long_running_locksä¸å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE NOTICE 'æ£€æŸ¥é•¿è¿è¡Œé”å¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;
```

---

## 7. ç»Ÿè®¡ä¿¡æ¯ç›‘æ§å¢å¼º

### 7.1 pg_stat_statementså¢å¼º

```sql
-- æ€§èƒ½æµ‹è¯•ï¼šPostgreSQL 18 pg_stat_statementsæ–°å¢å­—æ®µï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰

-- æ€§èƒ½æµ‹è¯•ï¼šå®‰è£…æ‰©å±•ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
BEGIN;
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;
COMMIT;
EXCEPTION
    WHEN duplicate_object THEN
        RAISE NOTICE 'æ‰©å±•pg_stat_statementså·²å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE NOTICE 'å®‰è£…æ‰©å±•pg_stat_statementså¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;

-- æ€§èƒ½æµ‹è¯•ï¼šæŸ¥è¯¢å¢å¼ºç»Ÿè®¡ä¿¡æ¯ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½åˆ†æï¼‰
BEGIN;
EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT
    queryid,
    query,
    calls,
    total_exec_time,
    mean_exec_time,
    stddev_exec_time,
    min_exec_time,
    max_exec_time,

    -- âœ… PG 18æ–°å¢ï¼šè¯¦ç»†I/Oç»Ÿè®¡
    shared_blks_hit,
    shared_blks_read,
    shared_blks_dirtied,
    shared_blks_written,

    -- âœ… æ–°å¢ï¼šä¸´æ—¶I/O
    temp_blks_read,
    temp_blks_written,

    -- âœ… æ–°å¢ï¼šWALç»Ÿè®¡
    wal_records,
    wal_fpi,
    wal_bytes,

    -- è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
    ROUND(
        shared_blks_hit * 100.0 /
        NULLIF(shared_blks_hit + shared_blks_read, 0),
        2
    ) AS cache_hit_ratio,

    -- è®¡ç®—I/Oæ•ˆç‡
    ROUND(
        (shared_blks_read + shared_blks_written) * 8192.0 /
        NULLIF(total_exec_time, 0) / 1024,
        2
    ) AS io_mb_per_sec

FROM pg_stat_statements
WHERE calls > 100  -- è‡³å°‘æ‰§è¡Œ100æ¬¡
ORDER BY total_exec_time DESC
LIMIT 20;
COMMIT;
EXCEPTION
    WHEN undefined_table THEN
        RAISE NOTICE 'pg_stat_statementsè§†å›¾ä¸å­˜åœ¨ï¼Œè¯·å…ˆå®‰è£…æ‰©å±•';
    WHEN OTHERS THEN
        RAISE NOTICE 'æŸ¥è¯¢å¢å¼ºç»Ÿè®¡ä¿¡æ¯å¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;
```

### 7.2 è¡¨çº§ç»´æŠ¤ç»Ÿè®¡

```sql
-- æ€§èƒ½æµ‹è¯•ï¼šPostgreSQL 18æ–°å¢ç»´æŠ¤è€—æ—¶ç»Ÿè®¡ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½åˆ†æï¼‰
BEGIN;
EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT
    schemaname,
    relname AS table_name,

    -- âœ… æ–°å¢ï¼šVACUUMè€—æ—¶ç»Ÿè®¡
    last_vacuum,
    vacuum_count,
    CASE
        WHEN total_vacuum_time_ms IS NOT NULL AND vacuum_count > 0 THEN
            ROUND(total_vacuum_time_ms / vacuum_count, 2)
        ELSE NULL
    END AS avg_vacuum_time_ms,

    pg_size_pretty(ROUND(total_vacuum_time_ms / 1000)::BIGINT * 1000000) AS total_vacuum_time,

    -- âœ… æ–°å¢ï¼šANALYZEè€—æ—¶ç»Ÿè®¡
    last_analyze,
    analyze_count,
    CASE
        WHEN total_analyze_time_ms IS NOT NULL AND analyze_count > 0 THEN
            ROUND(total_analyze_time_ms / analyze_count, 2)
        ELSE NULL
    END AS avg_analyze_time_ms,

    -- ç»´æŠ¤æ•ˆç‡è¯„åˆ†
    CASE
        WHEN total_vacuum_time_ms / vacuum_count > 60000 THEN 'ğŸ”´ æ…¢ (>1åˆ†é’Ÿ)'
        WHEN total_vacuum_time_ms / vacuum_count > 10000 THEN 'ğŸŸ¡ ä¸­ç­‰ (>10ç§’)'
        ELSE 'ğŸŸ¢ å¿« (<10ç§’)'
    END AS vacuum_performance

FROM pg_stat_user_tables
WHERE vacuum_count > 0 OR analyze_count > 0
ORDER BY total_vacuum_time_ms DESC NULLS LAST
LIMIT 20;
COMMIT;
EXCEPTION
    WHEN undefined_table THEN
        RAISE NOTICE 'pg_stat_user_tablesè§†å›¾ä¸å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE NOTICE 'æŸ¥è¯¢ç»´æŠ¤è€—æ—¶ç»Ÿè®¡å¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;
```

---

## 8. Prometheus + Grafanaé›†æˆ

### 8.1 postgres_exporteré…ç½®

```yaml
# postgres_exporteré…ç½®æ–‡ä»¶
# /etc/postgres_exporter/queries.yaml

# PostgreSQL 18ç‰¹å®šæŒ‡æ ‡

# 1. AIOæ€§èƒ½æŒ‡æ ‡
pg_aio_stats:
  query: |
    SELECT
      backend_type,
      io_context,
      SUM(allocated_aios) as allocated,
      SUM(max_aios) as max_capacity,
      SUM(pending_reads) as pending,
      SUM(completed_reads) as completed
    FROM pg_aios
    GROUP BY backend_type, io_context
  metrics:
    - backend_type:
        usage: "LABEL"
        description: "Backendç±»å‹"
    - io_context:
        usage: "LABEL"
        description: "I/Oä¸Šä¸‹æ–‡"
    - allocated:
        usage: "GAUGE"
        description: "å·²åˆ†é…AIOå¥æŸ„æ•°"
    - max_capacity:
        usage: "GAUGE"
        description: "æœ€å¤§AIOå®¹é‡"
    - pending:
        usage: "GAUGE"
        description: "å¾…å¤„ç†I/Oè¯·æ±‚"
    - completed:
        usage: "COUNTER"
        description: "å·²å®ŒæˆI/Oè¯·æ±‚æ•°"

# 2. I/Oå­—èŠ‚çº§ç»Ÿè®¡
pg_stat_io_bytes:
  query: |
    SELECT
      backend_type,
      object,
      context,
      read_bytes,
      write_bytes,
      extend_bytes,
      read_time,
      write_time
    FROM pg_stat_io
    WHERE reads > 0 OR writes > 0
  metrics:
    - backend_type:
        usage: "LABEL"
    - object:
        usage: "LABEL"
    - context:
        usage: "LABEL"
    - read_bytes:
        usage: "COUNTER"
        description: "è¯»å–å­—èŠ‚æ•°"
    - write_bytes:
        usage: "COUNTER"
        description: "å†™å…¥å­—èŠ‚æ•°"
    - read_time:
        usage: "COUNTER"
        description: "è¯»å–è€—æ—¶(ms)"

# 3. è¿æ¥æ€§èƒ½ç»Ÿè®¡ï¼ˆä»JSONæ—¥å¿—é‡‡é›†ï¼‰
pg_connection_performance:
  query: |
    SELECT
      COUNT(*) FILTER (WHERE setup_duration_ms < 10) as fast_connections,
      COUNT(*) FILTER (WHERE setup_duration_ms BETWEEN 10 AND 50) as normal_connections,
      COUNT(*) FILTER (WHERE setup_duration_ms > 50) as slow_connections,
      AVG(setup_duration_ms) as avg_setup_ms,
      AVG(fork_duration_ms) as avg_fork_ms,
      AVG(auth_duration_ms) as avg_auth_ms
    FROM connection_logs_last_hour
  metrics:
    - fast_connections:
        usage: "GAUGE"
        description: "å¿«é€Ÿè¿æ¥æ•°(<10ms)"
    - avg_setup_ms:
        usage: "GAUGE"
        description: "å¹³å‡è¿æ¥å»ºç«‹è€—æ—¶"

# 4. VACUUMæ•ˆç‡ç»Ÿè®¡
pg_vacuum_efficiency:
  query: |
    SELECT
      schemaname,
      relname,
      vacuum_count,
      autovacuum_count,
      EXTRACT(EPOCH FROM (now() - last_autovacuum)) as seconds_since_vacuum,
      n_dead_tup,
      n_live_tup,
      ROUND(n_dead_tup * 100.0 / NULLIF(n_live_tup + n_dead_tup, 0), 2) as dead_tuple_ratio
    FROM pg_stat_user_tables
    WHERE n_dead_tup > 1000
  metrics:
    - schemaname:
        usage: "LABEL"
    - relname:
        usage: "LABEL"
    - vacuum_count:
        usage: "COUNTER"
    - seconds_since_vacuum:
        usage: "GAUGE"
    - dead_tuple_ratio:
        usage: "GAUGE"
        description: "æ­»å…ƒç»„æ¯”ä¾‹(%)"
```

### 8.2 Grafanaä»ªè¡¨æ¿JSON

```json
{
  "dashboard": {
    "title": "PostgreSQL 18 ç›‘æ§ä»ªè¡¨æ¿",
    "tags": ["postgresql", "pg18"],
    "timezone": "browser",
    "panels": [
      {
        "title": "AIOæ€§èƒ½ç›‘æ§ï¼ˆPG18æ–°å¢ï¼‰",
        "type": "graph",
        "targets": [
          {
            "expr": "pg_aio_stats_allocated",
            "legendFormat": "å·²åˆ†é…AIO: {{backend_type}}"
          },
          {
            "expr": "pg_aio_stats_pending",
            "legendFormat": "å¾…å¤„ç†I/O: {{backend_type}}"
          }
        ],
        "yaxes": [
          {"format": "short", "label": "å¥æŸ„æ•°"}
        ]
      },
      {
        "title": "I/Oå­—èŠ‚çº§ç»Ÿè®¡ï¼ˆPG18å¢å¼ºï¼‰",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(pg_stat_io_bytes_read_bytes[5m])",
            "legendFormat": "è¯»å–é€Ÿç‡: {{context}}"
          },
          {
            "expr": "rate(pg_stat_io_bytes_write_bytes[5m])",
            "legendFormat": "å†™å…¥é€Ÿç‡: {{context}}"
          }
        ],
        "yaxes": [
          {"format": "Bps", "label": "å­—èŠ‚/ç§’"}
        ]
      },
      {
        "title": "è¿æ¥æ€§èƒ½åˆ†å¸ƒï¼ˆPG18æ–°å¢ï¼‰",
        "type": "heatmap",
        "targets": [
          {
            "expr": "pg_connection_performance_avg_setup_ms",
            "legendFormat": "è¿æ¥è€—æ—¶"
          }
        ]
      },
      {
        "title": "æ­»å…ƒç»„æ¯”ä¾‹TOP10",
        "type": "table",
        "targets": [
          {
            "expr": "topk(10, pg_vacuum_efficiency_dead_tuple_ratio)",
            "format": "table"
          }
        ]
      }
    ]
  }
}
```

### 8.3 å®Œæ•´ç›‘æ§æ ˆéƒ¨ç½²

```yaml
# docker-compose.yml - å®Œæ•´ç›‘æ§æ ˆ

version: '3.8'

services:
  # PostgreSQL 18
  postgres:
    image: postgres:18
    environment:
      POSTGRES_PASSWORD: secret
      POSTGRES_DB: monitoring_demo
    volumes:
      - ./postgresql.conf:/etc/postgresql/postgresql.conf
      - pgdata:/var/lib/postgresql/data
      - pglogs:/var/log/postgresql
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    ports:
      - "5432:5432"

  # Postgres Exporter
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    environment:
      DATA_SOURCE_NAME: "postgresql://monitor:password@postgres:5432/monitoring_demo?sslmode=disable"
      PG_EXPORTER_EXTEND_QUERY_PATH: /etc/postgres_exporter/queries.yaml
    volumes:
      - ./queries.yaml:/etc/postgres_exporter/queries.yaml:ro
    ports:
      - "9187:9187"
    depends_on:
      - postgres

  # JSONæ—¥å¿—é‡‡é›†å™¨
  promtail:
    image: grafana/promtail:latest
    volumes:
      - pglogs:/var/log/postgresql:ro
      - ./promtail-config.yaml:/etc/promtail/config.yaml:ro
    command: -config.file=/etc/promtail/config.yaml
    depends_on:
      - loki

  # Prometheus
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
    ports:
      - "9090:9090"

  # Loki (æ—¥å¿—èšåˆ)
  loki:
    image: grafana/loki:latest
    ports:
      - "3100:3100"
    volumes:
      - loki-data:/loki

  # Grafana
  grafana:
    image: grafana/grafana:latest
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_INSTALL_PLUGINS=grafana-clock-panel
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana-dashboards:/etc/grafana/provisioning/dashboards
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
      - loki

volumes:
  pgdata:
  pglogs:
  prometheus-data:
  loki-data:
  grafana-data:
```

---

## 9. å‘Šè­¦è§„åˆ™ä½“ç³»è®¾è®¡

### 9.1 Prometheuså‘Šè­¦è§„åˆ™

```yaml
# prometheus-alerts.yml

groups:
  - name: postgresql18_critical
    interval: 30s
    rules:
      # === æ€§èƒ½å‘Šè­¦ ===

      - alert: PostgreSQL_QueryTooSlow
        expr: pg_stat_statements_mean_exec_time_seconds > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "æŸ¥è¯¢ {{$labels.query}} å¹³å‡æ‰§è¡Œæ—¶é—´è¶…è¿‡5ç§’"
          description: "å½“å‰å¹³å‡è€—æ—¶: {{ $value }}ç§’"

      - alert: PostgreSQL_CacheHitRatioLow
        expr: |
          (
            sum(pg_stat_database_blks_hit) /
            (sum(pg_stat_database_blks_hit) + sum(pg_stat_database_blks_read))
          ) < 0.90
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "æ•°æ®åº“ {{$labels.datname}} ç¼“å­˜å‘½ä¸­ç‡ä½äº90%"
          description: "å½“å‰å‘½ä¸­ç‡: {{ $value | humanizePercentage }}"
          action: "è€ƒè™‘å¢åŠ shared_bufferså‚æ•°"

      # === AIOç›‘æ§ï¼ˆPG18ç‰¹æœ‰ï¼‰ ===

      - alert: PostgreSQL18_AIO_HighUsage
        expr: |
          (pg_aio_stats_allocated / pg_aio_stats_max_capacity) > 0.85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "AIOå¥æŸ„ä½¿ç”¨ç‡è¶…è¿‡85%"
          description: "å½“å‰ä½¿ç”¨: {{$value | humanizePercentage}}"
          action: "å¢åŠ maintenance_io_concurrencyå‚æ•°"

      - alert: PostgreSQL18_AIO_QueueDeep
        expr: pg_aio_stats_pending > 1000
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "AIOé˜Ÿåˆ—æ·±åº¦è¿‡å¤§"
          description: "å½“å‰é˜Ÿåˆ—: {{ $value }}ä¸ªå¾…å¤„ç†è¯·æ±‚"

      # === è¿æ¥æ€§èƒ½ï¼ˆPG18ç‰¹æœ‰ï¼‰ ===

      - alert: PostgreSQL18_SlowConnections
        expr: pg_connection_performance_avg_setup_ms > 100
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "è¿æ¥å»ºç«‹å¹³å‡è€—æ—¶è¶…è¿‡100ms"
          description: |
            å¹³å‡è¿æ¥è€—æ—¶: {{ $value }}ms
            - Fork: {{ $labels.avg_fork_ms }}ms
            - è®¤è¯: {{ $labels.avg_auth_ms }}ms
          action: "æ£€æŸ¥è®¤è¯é…ç½®å’Œç³»ç»Ÿè´Ÿè½½"

      # === VACUUMç›‘æ§ ===

      - alert: PostgreSQL_VacuumNotRunning
        expr: |
          (time() - pg_stat_user_tables_last_autovacuum) > 86400 * 7
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "è¡¨ {{$labels.relname}} è¶…è¿‡7å¤©æœªVACUUM"
          description: "æœ€åVACUUM: {{ $value | humanizeDuration }}å‰"

      - alert: PostgreSQL_DeadTuplesHigh
        expr: pg_vacuum_efficiency_dead_tuple_ratio > 30
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "è¡¨ {{$labels.relname}} æ­»å…ƒç»„æ¯”ä¾‹è¶…è¿‡30%"
          description: "æ­»å…ƒç»„æ¯”ä¾‹: {{ $value }}%"
          action: "ç«‹å³æ‰§è¡ŒVACUUM"

      # === XIDé£é™© ===

      - alert: PostgreSQL_XID_AgeCritical
        expr: |
          (pg_database_age / 2000000000) > 0.90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "æ•°æ®åº“ {{$labels.datname}} XIDå¹´é¾„è¶…è¿‡90%"
          description: "XIDå¹´é¾„: {{ $value }}ï¼Œå‰©ä½™: {{ 2000000000 - $value }}"
          action: "ç´§æ€¥æ‰§è¡ŒVACUUM FREEZEï¼"

      # === é”ç›‘æ§ï¼ˆPG18å¢å¼ºï¼‰ ===

      - alert: PostgreSQL18_LockTimeout
        expr: pg_locks_blocked_duration_seconds > 30
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "æ£€æµ‹åˆ°é•¿æ—¶é—´é”ç­‰å¾…"
          description: |
            è¢«é˜»å¡PID: {{$labels.blocked_pid}}
            é˜»å¡PID: {{$labels.blocking_pid}}
            ç­‰å¾…æ—¶é•¿: {{ $value }}ç§’
          action: "è€ƒè™‘ç»ˆæ­¢é˜»å¡ä¼šè¯: SELECT pg_terminate_backend({{$labels.blocking_pid}});"

      # === ç£ç›˜ç©ºé—´ ===

      - alert: PostgreSQL_DiskSpaceLow
        expr: |
          (
            pg_database_size_bytes /
            (node_filesystem_size_bytes{mountpoint="/var/lib/postgresql"})
          ) > 0.80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "æ•°æ®åº“ç£ç›˜ä½¿ç”¨è¶…è¿‡80%"
          description: "å½“å‰ä½¿ç”¨: {{ $value | humanizePercentage }}"
          action: "æ¸…ç†å½’æ¡£æ—¥å¿—æˆ–æ‰©å®¹å­˜å‚¨"

      # === å¤åˆ¶å»¶è¿Ÿ ===

      - alert: PostgreSQL_ReplicationLag
        expr: |
          (pg_replication_lag_bytes / (1024 * 1024)) > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "å¤åˆ¶å»¶è¿Ÿè¶…è¿‡100MB"
          description: "å½“å‰å»¶è¿Ÿ: {{ $value }}MB"
```

### 9.2 å‘Šè­¦é€šçŸ¥é…ç½®

```yaml
# alertmanager.yml

global:
  resolve_timeout: 5m
  smtp_smarthost: 'smtp.example.com:587'
  smtp_from: 'postgres-alerts@example.com'
  smtp_auth_username: 'alerts'
  smtp_auth_password: 'secret'

route:
  group_by: ['alertname', 'cluster']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'team-db-ops'

  routes:
    # ä¸¥é‡å‘Šè­¦ç«‹å³é€šçŸ¥
    - match:
        severity: critical
      receiver: 'team-db-ops-critical'
      group_wait: 0s
      repeat_interval: 5m

    # æ™®é€šå‘Šè­¦æ‰¹é‡é€šçŸ¥
    - match:
        severity: warning
      receiver: 'team-db-ops'
      group_wait: 5m
      repeat_interval: 4h

receivers:
  - name: 'team-db-ops-critical'
    email_configs:
      - to: 'dba-oncall@example.com'
        headers:
          Subject: 'ğŸš¨ã€ç´§æ€¥ã€‘PostgreSQLå‘Šè­¦ - {{ .GroupLabels.alertname }}'
    webhook_configs:
      - url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
        send_resolved: true

  - name: 'team-db-ops'
    email_configs:
      - to: 'dba-team@example.com'
        headers:
          Subject: 'âš ï¸ PostgreSQLå‘Šè­¦ - {{ .GroupLabels.alertname }}'

inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'instance']
```

---

## 10. ç”Ÿäº§ç¯å¢ƒç›‘æ§æ–¹æ¡ˆ

### 10.1 å®Œæ•´ç›‘æ§æ¶æ„

```mermaid
graph TB
    subgraph "PostgreSQL 18é›†ç¾¤"
        PG1[Primary<br/>JSONæ—¥å¿— + ç»Ÿè®¡è§†å›¾]
        PG2[Replica 1<br/>JSONæ—¥å¿— + ç»Ÿè®¡è§†å›¾]
        PG3[Replica 2<br/>JSONæ—¥å¿— + ç»Ÿè®¡è§†å›¾]
    end

    subgraph "é‡‡é›†å±‚"
        PE1[postgres_exporter<br/>æŒ‡æ ‡é‡‡é›†]
        PE2[postgres_exporter<br/>æŒ‡æ ‡é‡‡é›†]
        PE3[postgres_exporter<br/>æŒ‡æ ‡é‡‡é›†]

        PL1[promtail<br/>æ—¥å¿—é‡‡é›†]
        PL2[promtail<br/>æ—¥å¿—é‡‡é›†]
        PL3[promtail<br/>æ—¥å¿—é‡‡é›†]
    end

    subgraph "å­˜å‚¨å±‚"
        Prom[Prometheus<br/>æ—¶åºæ•°æ®åº“<br/>30å¤©ä¿ç•™]
        Loki[Loki<br/>æ—¥å¿—èšåˆ<br/>7å¤©ä¿ç•™]
    end

    subgraph "åˆ†æå±‚"
        Grafana[Grafana<br/>å¯è§†åŒ–ä»ªè¡¨æ¿]
        AM[AlertManager<br/>å‘Šè­¦è·¯ç”±]
    end

    subgraph "é€šçŸ¥å±‚"
        Email[Email<br/>é‚®ä»¶é€šçŸ¥]
        Slack[Slack<br/>å³æ—¶é€šçŸ¥]
        PD[PagerDuty<br/>Oncallè½®å€¼]
    end

    PG1 -->|æŒ‡æ ‡| PE1
    PG2 -->|æŒ‡æ ‡| PE2
    PG3 -->|æŒ‡æ ‡| PE3

    PG1 -->|æ—¥å¿—| PL1
    PG2 -->|æ—¥å¿—| PL2
    PG3 -->|æ—¥å¿—| PL3

    PE1 -->|push| Prom
    PE2 -->|push| Prom
    PE3 -->|push| Prom

    PL1 -->|push| Loki
    PL2 -->|push| Loki
    PL3 -->|push| Loki

    Prom --> Grafana
    Loki --> Grafana
    Prom --> AM

    AM --> Email
    AM --> Slack
    AM --> PD

    style PG1 fill:#4ecdc4,color:#fff
    style Prom fill:#ff6b6b,color:#fff
    style Grafana fill:#95e1d3,color:#000
```

### 10.2 ç›‘æ§æ£€æŸ¥æ¸…å•

```bash
#!/bin/bash
# monitoring_health_check.sh - ç›‘æ§ç³»ç»Ÿå¥åº·æ£€æŸ¥

echo "=== PostgreSQL 18 ç›‘æ§ç³»ç»Ÿå¥åº·æ£€æŸ¥ ==="
echo "æ£€æŸ¥æ—¶é—´: $(date)"
echo ""

# 1. æ£€æŸ¥postgres_exporterçŠ¶æ€
echo "ã€1/8ã€‘æ£€æŸ¥postgres_exporter..."
if curl -s http://localhost:9187/metrics | grep -q "pg_up 1"; then
    echo "âœ… postgres_exporteræ­£å¸¸è¿è¡Œ"
else
    echo "âŒ postgres_exporterä¸å¯ç”¨"
fi

# 2. æ£€æŸ¥PrometheusçŠ¶æ€
echo -e "\nã€2/8ã€‘æ£€æŸ¥Prometheus..."
if curl -s http://localhost:9090/-/healthy | grep -q "Prometheus is Healthy"; then
    echo "âœ… Prometheusæ­£å¸¸è¿è¡Œ"

    # æ£€æŸ¥ç›®æ ‡çŠ¶æ€
    targets=$(curl -s http://localhost:9090/api/v1/targets | jq -r '.data.activeTargets[] | select(.health=="down") | .labels.instance')
    if [ -z "$targets" ]; then
        echo "âœ… æ‰€æœ‰ç›‘æ§ç›®æ ‡æ­£å¸¸"
    else
        echo "âŒ ä»¥ä¸‹ç›®æ ‡ä¸å¯ç”¨: $targets"
    fi
else
    echo "âŒ Prometheusä¸å¯ç”¨"
fi

# 3. æ£€æŸ¥GrafanaçŠ¶æ€
echo -e "\nã€3/8ã€‘æ£€æŸ¥Grafana..."
if curl -s http://localhost:3000/api/health | grep -q "ok"; then
    echo "âœ… Grafanaæ­£å¸¸è¿è¡Œ"
else
    echo "âŒ Grafanaä¸å¯ç”¨"
fi

# 4. æ£€æŸ¥AlertManagerçŠ¶æ€
echo -e "\nã€4/8ã€‘æ£€æŸ¥AlertManager..."
active_alerts=$(curl -s http://localhost:9093/api/v2/alerts | jq -r '. | length')
echo "âœ… AlertManagerè¿è¡Œä¸­ï¼Œå½“å‰å‘Šè­¦: $active_alerts æ¡"

# 5. æ£€æŸ¥PostgreSQL JSONæ—¥å¿—
echo -e "\nã€5/8ã€‘æ£€æŸ¥JSONæ—¥å¿—..."
latest_log=$(ls -t /var/log/postgresql/*.json 2>/dev/null | head -1)
if [ -n "$latest_log" ]; then
    log_size=$(du -h "$latest_log" | cut -f1)
    log_lines=$(wc -l < "$latest_log")
    echo "âœ… æœ€æ–°æ—¥å¿—: $latest_log ($log_size, $log_lines è¡Œ)"

    # éªŒè¯JSONæ ¼å¼
    if tail -1 "$latest_log" | jq . >/dev/null 2>&1; then
        echo "âœ… JSONæ ¼å¼æ­£ç¡®"
    else
        echo "âŒ JSONæ ¼å¼é”™è¯¯"
    fi
else
    echo "âŒ æœªæ‰¾åˆ°JSONæ—¥å¿—æ–‡ä»¶"
fi

# 6. æ£€æŸ¥pg_stat_statements
echo -e "\nã€6/8ã€‘æ£€æŸ¥pg_stat_statements..."
stmt_count=$(psql -t -A -c "SELECT count(*) FROM pg_stat_statements;" 2>/dev/null)
if [ -n "$stmt_count" ]; then
    echo "âœ… pg_stat_statementså¯ç”¨ï¼Œè®°å½• $stmt_count æ¡æŸ¥è¯¢"
else
    echo "âŒ pg_stat_statementsä¸å¯ç”¨"
fi

# 7. æ£€æŸ¥AIOç›‘æ§ï¼ˆPG18ç‰¹æœ‰ï¼‰
echo -e "\nã€7/8ã€‘æ£€æŸ¥AIOç›‘æ§..."
aio_count=$(psql -t -A -c "SELECT count(*) FROM pg_aios WHERE allocated_aios > 0;" 2>/dev/null)
if [ -n "$aio_count" ] && [ "$aio_count" -gt 0 ]; then
    echo "âœ… AIOç›‘æ§å¯ç”¨ï¼Œæ´»è·ƒåç«¯: $aio_count"
else
    echo "âš ï¸  AIOç›‘æ§æ— æ•°æ®ï¼ˆå¯èƒ½æœªå¯ç”¨AIOï¼‰"
fi

# 8. æ£€æŸ¥ç£ç›˜ç©ºé—´
echo -e "\nã€8/8ã€‘æ£€æŸ¥ç£ç›˜ç©ºé—´..."
disk_usage=$(df -h /var/lib/postgresql | tail -1 | awk '{print $5}' | sed 's/%//')
if [ "$disk_usage" -lt 80 ]; then
    echo "âœ… ç£ç›˜ç©ºé—´å……è¶³ (ä½¿ç”¨${disk_usage}%)"
elif [ "$disk_usage" -lt 90 ]; then
    echo "ğŸŸ¡ ç£ç›˜ç©ºé—´ç´§å¼  (ä½¿ç”¨${disk_usage}%)"
else
    echo "ğŸ”´ ç£ç›˜ç©ºé—´ä¸¥é‡ä¸è¶³ (ä½¿ç”¨${disk_usage}%)"
fi

echo -e "\n=== å¥åº·æ£€æŸ¥å®Œæˆ ==="
```

### 10.3 æ€§èƒ½åŸºçº¿å»ºç«‹

```sql
-- æ€§èƒ½æµ‹è¯•ï¼šåˆ›å»ºæ€§èƒ½åŸºçº¿è¡¨ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
BEGIN;
CREATE TABLE IF NOT EXISTS performance_baseline (
    metric_name TEXT PRIMARY KEY,
    baseline_value NUMERIC,
    unit TEXT,
    threshold_warning NUMERIC,
    threshold_critical NUMERIC,
    last_updated TIMESTAMPTZ DEFAULT now()
);
COMMIT;
EXCEPTION
    WHEN duplicate_table THEN
        RAISE NOTICE 'è¡¨performance_baselineå·²å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE NOTICE 'åˆ›å»ºæ€§èƒ½åŸºçº¿è¡¨å¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;

-- æ€§èƒ½æµ‹è¯•ï¼šæ’å…¥åŸºçº¿æ•°æ®ï¼ˆæ ¹æ®å®é™…ç¯å¢ƒè°ƒæ•´ï¼‰ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
BEGIN;
INSERT INTO performance_baseline VALUES
('avg_query_time_ms', 50, 'milliseconds', 100, 500, now()),
('cache_hit_ratio', 95, 'percent', 90, 80, now()),
('connections_per_second', 20, 'count', 100, 500, now()),
('tps', 5000, 'transactions', 3000, 1000, now()),
('vacuum_frequency_hours', 24, 'hours', 48, 168, now()),
('dead_tuple_ratio', 5, 'percent', 15, 30, now()),
('replication_lag_mb', 10, 'megabytes', 100, 500, now()),
-- âœ… PostgreSQL 18ç‰¹æœ‰
('aio_usage_pct', 40, 'percent', 70, 85, now()),
('avg_connection_setup_ms', 25, 'milliseconds', 50, 100, now()),
('io_bytes_per_second', 104857600, 'bytes', 209715200, 524288000, now())
ON CONFLICT (metric_name) DO UPDATE SET
    baseline_value = EXCLUDED.baseline_value,
    threshold_warning = EXCLUDED.threshold_warning,
    threshold_critical = EXCLUDED.threshold_critical,
    last_updated = now();
COMMIT;
EXCEPTION
    WHEN undefined_table THEN
        RAISE NOTICE 'è¡¨performance_baselineä¸å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE NOTICE 'æ’å…¥åŸºçº¿æ•°æ®å¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;

-- æ€§èƒ½æµ‹è¯•ï¼šå¯¹æ¯”å½“å‰æ€§èƒ½ä¸åŸºçº¿ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½åˆ†æï¼‰
BEGIN;
EXPLAIN (ANALYZE, BUFFERS, TIMING)
WITH current_metrics AS (
    SELECT
        'avg_query_time_ms' AS metric_name,
        (SELECT mean_exec_time FROM pg_stat_statements ORDER BY mean_exec_time DESC LIMIT 1) AS current_value
    UNION ALL
    SELECT
        'cache_hit_ratio',
        ROUND(SUM(blks_hit) *100.0 / NULLIF(SUM(blks_hit + blks_read), 0), 2)
    FROM pg_stat_database
    UNION ALL
    SELECT
        'tps',
        (SELECT sum(xact_commit + xact_rollback) FROM pg_stat_database)
)
SELECT
    pb.metric_name,
    pb.baseline_value,
    cm.current_value,
    pb.unit,
    ROUND((cm.current_value - pb.baseline_value)* 100.0 / NULLIF(pb.baseline_value, 0), 2) AS deviation_pct,

    -- çŠ¶æ€åˆ¤æ–­
    CASE
        WHEN cm.current_value > pb.threshold_critical THEN 'ğŸ”´ ä¸¥é‡åç¦»'
        WHEN cm.current_value > pb.threshold_warning THEN 'ğŸŸ¡ åç¦»åŸºçº¿'
        WHEN ABS(cm.current_value - pb.baseline_value) / pb.baseline_value < 0.1 THEN 'ğŸŸ¢ æ­£å¸¸'
        ELSE 'ğŸŸ¢ æ­£å¸¸'
    END AS status

FROM performance_baseline pb
LEFT JOIN current_metrics cm ON pb.metric_name = cm.metric_name;
COMMIT;
EXCEPTION
    WHEN undefined_table THEN
        RAISE NOTICE 'è¡¨performance_baselineæˆ–ç›¸å…³è§†å›¾ä¸å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE NOTICE 'å¯¹æ¯”æ€§èƒ½åŸºçº¿å¤±è´¥: %', SQLERRM;
        ROLLBACK;
        RAISE;

```

### 10.4 è‡ªåŠ¨åŒ–å·¡æ£€è„šæœ¬

```python
#!/usr/bin/env python3
"""
PostgreSQL 18 è‡ªåŠ¨åŒ–å·¡æ£€è„šæœ¬
æ¯æ—¥æ‰§è¡Œï¼Œç”Ÿæˆå¥åº·æŠ¥å‘Š
"""

import psycopg2
import json
from datetime import datetime, timedelta
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

class PG18HealthCheck:
    """PostgreSQL 18å¥åº·æ£€æŸ¥"""

    def __init__(self, dsn):
        self.conn = psycopg2.connect(dsn)
        self.cursor = self.conn.cursor()
        self.issues = []
        self.warnings = []
        self.info = []

    def check_all(self):
        """æ‰§è¡Œæ‰€æœ‰æ£€æŸ¥"""
        print(f"å¼€å§‹å¥åº·æ£€æŸ¥: {datetime.now()}")

        self.check_connections()
        self.check_cache_hit_ratio()
        self.check_vacuum_status()
        self.check_xid_age()
        self.check_replication_lag()
        self.check_locks()
        self.check_aio_performance()  # PG18ç‰¹æœ‰
        self.check_table_bloat()

        return self.generate_report()

    def check_aio_performance(self):
        """æ£€æŸ¥AIOæ€§èƒ½ï¼ˆPostgreSQL 18ç‰¹æœ‰ï¼‰"""
        self.cursor.execute("""
            SELECT
                COUNT(*) as active_backends,
                SUM(allocated_aios) as total_allocated,
                SUM(max_aios) as total_capacity,
                ROUND(SUM(allocated_aios) * 100.0 / NULLIF(SUM(max_aios), 0), 2) as usage_pct
            FROM pg_aios
            WHERE backend_type = 'client backend'
        """)

        result = self.cursor.fetchone()
        if result and result[0] > 0:
            active, allocated, capacity, usage = result

            if usage > 80:
                self.issues.append(
                    f"ğŸ”´ AIOä½¿ç”¨ç‡è¿‡é«˜: {usage}% ({allocated}/{capacity})"
                )
            elif usage > 60:
                self.warnings.append(
                    f"ğŸŸ¡ AIOä½¿ç”¨ç‡è¾ƒé«˜: {usage}%"
                )
            else:
                self.info.append(
                    f"âœ… AIOä½¿ç”¨ç‡æ­£å¸¸: {usage}% ({active}ä¸ªæ´»è·ƒåç«¯)"
                )
        else:
            self.info.append("â„¹ï¸  AIOæœªå¯ç”¨æˆ–æ— æ´»åŠ¨")

    def check_connections(self):
        """æ£€æŸ¥è¿æ¥æ€§èƒ½"""
        # ä»JSONæ—¥å¿—æˆ–ç»Ÿè®¡è¡¨æŸ¥è¯¢
        # ç®€åŒ–ç¤ºä¾‹
        self.cursor.execute("""
            SELECT
                count(*) as total,
                count(*) FILTER (WHERE state = 'active') as active,
                count(*) FILTER (WHERE state = 'idle in transaction') as idle_in_tx
            FROM pg_stat_activity
            WHERE pid != pg_backend_pid()
        """)

        total, active, idle_in_tx = self.cursor.fetchone()

        if idle_in_tx > 10:
            self.warnings.append(
                f"ğŸŸ¡ æ£€æµ‹åˆ° {idle_in_tx} ä¸ªç©ºé—²äº‹åŠ¡ï¼ˆidle in transactionï¼‰"
            )

        self.info.append(f"âœ… è¿æ¥æ•°: {total} (æ´»è·ƒ: {active})")

    def check_cache_hit_ratio(self):
        """æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡"""
        self.cursor.execute("""
            SELECT
                ROUND(
                    SUM(blks_hit) * 100.0 /
                    NULLIF(SUM(blks_hit + blks_read), 0),
                    2
                ) as hit_ratio
            FROM pg_stat_database
        """)

        hit_ratio = self.cursor.fetchone()[0]

        if hit_ratio < 90:
            self.issues.append(
                f"ğŸ”´ ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½: {hit_ratio}% (å»ºè®®>95%)"
            )
        elif hit_ratio < 95:
            self.warnings.append(
                f"ğŸŸ¡ ç¼“å­˜å‘½ä¸­ç‡åä½: {hit_ratio}%"
            )
        else:
            self.info.append(f"âœ… ç¼“å­˜å‘½ä¸­ç‡æ­£å¸¸: {hit_ratio}%")

    def check_vacuum_status(self):
        """æ£€æŸ¥VACUUMçŠ¶æ€"""
        self.cursor.execute("""
            SELECT
                count(*) as tables_need_vacuum
            FROM pg_stat_user_tables
            WHERE (last_autovacuum IS NULL AND last_vacuum IS NULL)
               OR last_autovacuum < now() - INTERVAL '7 days'
               OR n_dead_tup > n_live_tup * 0.2
        """)

        count = self.cursor.fetchone()[0]

        if count > 0:
            self.warnings.append(
                f"ğŸŸ¡ {count} ä¸ªè¡¨éœ€è¦VACUUM"
            )

    def check_xid_age(self):
        """æ£€æŸ¥XIDå¹´é¾„"""
        self.cursor.execute("""
            SELECT
                datname,
                age(datfrozenxid) as xid_age,
                current_setting('autovacuum_freeze_max_age')::INT - age(datfrozenxid) as remaining
            FROM pg_database
            WHERE datallowconn
            ORDER BY age(datfrozenxid) DESC
            LIMIT 1
        """)

        datname, xid_age, remaining = self.cursor.fetchone()

        if remaining < 20000000:  # å‰©ä½™<2000ä¸‡
            self.issues.append(
                f"ğŸ”´ {datname} XIDå¹´é¾„ä¸¥é‡: {xid_age} (å‰©ä½™{remaining})"
            )
        elif remaining < 50000000:
            self.warnings.append(
                f"ğŸŸ¡ {datname} XIDå¹´é¾„è¾ƒé«˜: {xid_age}"
            )
        else:
            self.info.append(f"âœ… XIDå¹´é¾„æ­£å¸¸: {xid_age}")

    def check_replication_lag(self):
        """æ£€æŸ¥å¤åˆ¶å»¶è¿Ÿ"""
        self.cursor.execute("""
            SELECT
                client_addr,
                application_name,
                state,
                pg_wal_lsn_diff(pg_current_wal_lsn(), replay_lsn) / 1024 / 1024 as lag_mb
            FROM pg_stat_replication
        """)

        replicas = self.cursor.fetchall()
        if replicas:
            for replica in replicas:
                addr, app_name, state, lag_mb = replica
                if lag_mb > 100:
                    self.issues.append(
                        f"ğŸ”´ å‰¯æœ¬ {app_name} å»¶è¿Ÿä¸¥é‡: {lag_mb:.2f}MB"
                    )
                elif lag_mb > 10:
                    self.warnings.append(
                        f"ğŸŸ¡ å‰¯æœ¬ {app_name} å»¶è¿Ÿ: {lag_mb:.2f}MB"
                    )
                else:
                    self.info.append(f"âœ… å‰¯æœ¬ {app_name} å»¶è¿Ÿæ­£å¸¸: {lag_mb:.2f}MB")
        else:
            self.info.append("â„¹ï¸  æ— å¤åˆ¶å‰¯æœ¬")

    def check_locks(self):
        """æ£€æŸ¥é”ç­‰å¾…"""
        self.cursor.execute("""
            SELECT count(*)
            FROM pg_stat_activity
            WHERE wait_event_type = 'Lock'
        """)

        locked = self.cursor.fetchone()[0]
        if locked > 0:
            self.warnings.append(f"ğŸŸ¡ {locked} ä¸ªä¼šè¯æ­£åœ¨ç­‰å¾…é”")

    def check_table_bloat(self):
        """æ£€æŸ¥è¡¨è†¨èƒ€"""
        self.cursor.execute("""
            SELECT count(*)
            FROM pg_stat_user_tables
            WHERE n_dead_tup > n_live_tup * 0.3
        """)

        bloated = self.cursor.fetchone()[0]
        if bloated > 0:
            self.warnings.append(f"ğŸŸ¡ {bloated} ä¸ªè¡¨è†¨èƒ€ç‡è¶…è¿‡30%")

    def generate_report(self):
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        report = f"""
<!DOCTYPE html>
<html>
<head>
    <title>PostgreSQL 18 å¥åº·æ£€æŸ¥æŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .critical {{ color: #d32f2f; font-weight: bold; }}
        .warning {{ color: #f57c00; }}
        .info {{ color: #388e3c; }}
        .section {{ margin: 20px 0; padding: 15px; border-left: 4px solid #ccc; }}
    </style>
</head>
<body>
    <h1>PostgreSQL 18 å¥åº·æ£€æŸ¥æŠ¥å‘Š</h1>
    <p>ç”Ÿæˆæ—¶é—´: {datetime.now()}</p>

    <div class="section" style="border-color: #d32f2f;">
        <h2>ğŸ”´ ä¸¥é‡é—®é¢˜ ({len(self.issues)})</h2>
        <ul>
            {''.join(f'<li class="critical">{issue}</li>' for issue in self.issues) or '<li>æ— </li>'}
        </ul>
    </div>

    <div class="section" style="border-color: #f57c00;">
        <h2>ğŸŸ¡ è­¦å‘Š ({len(self.warnings)})</h2>
        <ul>
            {''.join(f'<li class="warning">{warning}</li>' for warning in self.warnings) or '<li>æ— </li>'}
        </ul>
    </div>

    <div class="section" style="border-color: #388e3c;">
        <h2>âœ… æ­£å¸¸é¡¹ ({len(self.info)})</h2>
        <ul>
            {''.join(f'<li class="info">{info}</li>' for info in self.info)}
        </ul>
    </div>
</body>
</html>
"""
        return report

    def send_email(self, report_html):
        """å‘é€é‚®ä»¶æŠ¥å‘Š"""
        msg = MIMEMultipart('alternative')
        msg['Subject'] = f'PostgreSQL 18 æ¯æ—¥å¥åº·æŠ¥å‘Š - {datetime.now().strftime("%Y-%m-%d")}'
        msg['From'] = 'postgres-monitor@example.com'
        msg['To'] = 'dba-team@example.com'

        html_part = MIMEText(report_html, 'html')
        msg.attach(html_part)

        # å‘é€é‚®ä»¶
        with smtplib.SMTP('smtp.example.com', 587) as server:
            server.starttls()
            server.login('user', 'password')
            server.send_message(msg)

    def close(self):
        """å…³é—­è¿æ¥"""
        self.cursor.close()
        self.conn.close()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    dsn = "host=localhost dbname=postgres user=monitor password=secret"

    checker = PG18HealthCheck(dsn)
    report = checker.check_all()

    # è¾“å‡ºåˆ°æ–‡ä»¶
    with open(f'/tmp/pg18_health_{datetime.now().strftime("%Y%m%d")}.html', 'w') as f:
        f.write(report)

    # å‘é€é‚®ä»¶ï¼ˆå¯é€‰ï¼‰
    # checker.send_email(report)

    checker.close()
    print("å¥åº·æ£€æŸ¥å®Œæˆï¼ŒæŠ¥å‘Šå·²ç”Ÿæˆ")
```

---

## æ€»ç»“

### PostgreSQL 18å¯è§‚æµ‹æ€§æ ¸å¿ƒä»·å€¼

1. **ç»“æ„åŒ–æ—¥å¿—**ï¼šJSONæ ¼å¼ï¼Œè§£ææ•ˆç‡æå‡10å€
2. **ç²¾ç»†åŒ–æŒ‡æ ‡**ï¼šå­—èŠ‚çº§I/Oç»Ÿè®¡ï¼Œç²¾åº¦æå‡8KB
3. **è¿æ¥è¯Šæ–­**ï¼šä¸‰é˜¶æ®µæ‹†è§£ï¼Œå®šä½ç²¾å‡†åº¦100%
4. **AIOç›‘æ§**ï¼šæ–°å¢pg_aiosè§†å›¾ï¼Œå®æ—¶æŒæ¡å¼‚æ­¥I/OçŠ¶æ€
5. **é”è¯Šæ–­**ï¼šlog_lock_failuresæä¾›è¯¦ç»†é˜»å¡ä¿¡æ¯
6. **ç»Ÿè®¡ä¿ç•™**ï¼špg_upgradeæ— éœ€é‡å»ºç»Ÿè®¡ï¼Œé›¶åœæœºå‡çº§

### ç›‘æ§æ¶æ„æœ€ä½³å®è·µ

```text
é‡‡é›†å±‚ï¼špostgres_exporter + promtail
å­˜å‚¨å±‚ï¼šPrometheus (30å¤©) + Loki (7å¤©)
åˆ†æå±‚ï¼šGrafana + AlertManager
é€šçŸ¥å±‚ï¼šEmail + Slack + PagerDuty
```

### å…³é”®æŒ‡æ ‡é˜ˆå€¼

| æŒ‡æ ‡ | æ­£å¸¸ | è­¦å‘Š | ä¸¥é‡ |
|-----|------|------|------|
| **ç¼“å­˜å‘½ä¸­ç‡** | >95% | 90-95% | <90% |
| **è¿æ¥è€—æ—¶** | <10ms | 10-50ms | >50ms |
| **æ­»å…ƒç»„æ¯”ä¾‹** | <10% | 10-20% | >20% |
| **XIDå¹´é¾„** | <1.5äº¿ | 1.5-1.8äº¿ | >1.8äº¿ |
| **AIOä½¿ç”¨ç‡** | <60% | 60-80% | >80% |
| **å¤åˆ¶å»¶è¿Ÿ** | <10MB | 10-100MB | >100MB |

### PostgreSQL 18ç›‘æ§æ–°èŒƒå¼

ä¼ ç»Ÿç›‘æ§èšç„¦"**å‘ç°é—®é¢˜**"ï¼ŒPostgreSQL 18ç›‘æ§å‡çº§ä¸º"**é¢„é˜²é—®é¢˜**"ï¼š

- âœ… **è¿æ¥é˜¶æ®µç›‘æ§** â†’ æå‰è¯†åˆ«è®¤è¯ç“¶é¢ˆ
- âœ… **AIOå®æ—¶ç›‘æ§** â†’ åŠ¨æ€è°ƒæ•´å¹¶å‘åº¦
- âœ… **é”å¤±è´¥æ—¥å¿—** â†’ å¿«é€Ÿå®šä½é˜»å¡æº
- âœ… **ç»´æŠ¤è€—æ—¶ç»Ÿè®¡** â†’ ä¼˜åŒ–VACUUMçª—å£

PostgreSQL 18å¯è§‚æµ‹æ€§å·²è¾¾åˆ°**äº‘åŸç”Ÿç›‘æ§æ ‡å‡†**ï¼Œä¸ºè¿ç»´è‡ªåŠ¨åŒ–å¥ å®šåŸºç¡€ï¼

---

**æ–‡æ¡£å®Œæˆæ—¶é—´**: 2025å¹´12æœˆ4æ—¥
**æ€»å­—æ•°**: çº¦40,000å­—
**ä»£ç ç¤ºä¾‹**: 90+
**ç›‘æ§è„šæœ¬**: 15ä¸ª
**å‘Šè­¦è§„åˆ™**: 20+æ¡
