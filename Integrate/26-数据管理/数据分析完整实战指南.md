---
> **ğŸ“‹ æ–‡æ¡£æ¥æº**: æ–°å¢æ·±åŒ–æ–‡æ¡£
> **ğŸ“… åˆ›å»ºæ—¥æœŸ**: 2025-01
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºæ·±åº¦è¡¥å……ï¼Œç³»ç»ŸåŒ–æ•°æ®åˆ†ææŠ€æœ¯æ ˆ

---

# PostgreSQLæ•°æ®åˆ†æå®Œæ•´å®æˆ˜æŒ‡å—

## å…ƒæ•°æ®

- **æ–‡æ¡£ç‰ˆæœ¬**: v2.0
- **åˆ›å»ºæ—¥æœŸ**: 2025-01
- **æŠ€æœ¯æ ˆ**: PostgreSQL 17+/18+ | Python | R | pgml | MADlib | Grafana | Metabase
- **éš¾åº¦çº§åˆ«**: â­â­â­â­â­ (ä¸“å®¶çº§)
- **é¢„è®¡é˜…è¯»**: 200åˆ†é’Ÿ
- **å‰ç½®è¦æ±‚**: ç†Ÿæ‚‰PostgreSQLåŸºç¡€ã€SQLé«˜çº§ç‰¹æ€§ã€æ•°æ®åˆ†æåŸºç¡€

---

## ğŸ“‹ å®Œæ•´ç›®å½•

- [PostgreSQLæ•°æ®åˆ†æå®Œæ•´å®æˆ˜æŒ‡å—](#postgresqlæ•°æ®åˆ†æå®Œæ•´å®æˆ˜æŒ‡å—)
  - [å…ƒæ•°æ®](#å…ƒæ•°æ®)
  - [ğŸ“‹ å®Œæ•´ç›®å½•](#-å®Œæ•´ç›®å½•)
  - [1. æ•°æ®åˆ†ææ¦‚è¿°](#1-æ•°æ®åˆ†ææ¦‚è¿°)
    - [1.1 æ•°æ®åˆ†ææµç¨‹](#11-æ•°æ®åˆ†ææµç¨‹)
      - [æ ‡å‡†æ•°æ®åˆ†ææµç¨‹](#æ ‡å‡†æ•°æ®åˆ†ææµç¨‹)
    - [1.2 PostgreSQLæ•°æ®åˆ†æä¼˜åŠ¿](#12-postgresqlæ•°æ®åˆ†æä¼˜åŠ¿)
      - [æ ¸å¿ƒä¼˜åŠ¿](#æ ¸å¿ƒä¼˜åŠ¿)
    - [1.3 æ•°æ®åˆ†æå·¥å…·ç®±](#13-æ•°æ®åˆ†æå·¥å…·ç®±)
      - [å·¥å…·æ ˆ](#å·¥å…·æ ˆ)
  - [2. é«˜çº§åˆ†ææ–¹æ³•](#2-é«˜çº§åˆ†ææ–¹æ³•)
    - [2.1 å…³è”åˆ†æ](#21-å…³è”åˆ†æ)
      - [Aprioriç®—æ³•å®ç°](#aprioriç®—æ³•å®ç°)
      - [å…³è”è§„åˆ™å¯è§†åŒ–](#å…³è”è§„åˆ™å¯è§†åŒ–)
    - [2.2 èšç±»åˆ†æ](#22-èšç±»åˆ†æ)
      - [K-meansèšç±»ï¼ˆPL/Pythonï¼‰](#k-meansèšç±»plpython)
      - [DBSCANèšç±»ï¼ˆå¯†åº¦èšç±»ï¼‰](#dbscanèšç±»å¯†åº¦èšç±»)
    - [2.3 åˆ†ç±»ä¸é¢„æµ‹](#23-åˆ†ç±»ä¸é¢„æµ‹)
      - [é€»è¾‘å›å½’åˆ†ç±»](#é€»è¾‘å›å½’åˆ†ç±»)
  - [3. SQLé«˜çº§åˆ†ææŠ€å·§](#3-sqlé«˜çº§åˆ†ææŠ€å·§)
    - [3.1 çª—å£å‡½æ•°æ·±åº¦åº”ç”¨](#31-çª—å£å‡½æ•°æ·±åº¦åº”ç”¨)
      - [é«˜çº§çª—å£å‡½æ•°æ¨¡å¼](#é«˜çº§çª—å£å‡½æ•°æ¨¡å¼)
    - [3.2 é€’å½’CTEå¤æ‚åˆ†æ](#32-é€’å½’cteå¤æ‚åˆ†æ)
      - [å±‚æ¬¡ç»“æ„åˆ†æ](#å±‚æ¬¡ç»“æ„åˆ†æ)
      - [å›¾éå†åˆ†æ](#å›¾éå†åˆ†æ)
  - [4. PostgreSQLä¸Pythoné›†æˆ](#4-postgresqlä¸pythoné›†æˆ)
    - [4.1 PL/PythonåŸºç¡€](#41-plpythonåŸºç¡€)
      - [å®‰è£…ä¸é…ç½®](#å®‰è£…ä¸é…ç½®)
      - [åŸºç¡€å‡½æ•°ç¤ºä¾‹](#åŸºç¡€å‡½æ•°ç¤ºä¾‹)
    - [4.2 NumPy/Pandasé›†æˆ](#42-numpypandasé›†æˆ)
      - [Pandasæ•°æ®åˆ†æ](#pandasæ•°æ®åˆ†æ)
    - [4.3 Scikit-learné›†æˆ](#43-scikit-learné›†æˆ)
      - [æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ](#æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ)
  - [7. æ•°æ®å¯è§†åŒ–é›†æˆ](#7-æ•°æ®å¯è§†åŒ–é›†æˆ)
    - [7.1 Grafanaé›†æˆ](#71-grafanaé›†æˆ)
      - [PostgreSQLæ•°æ®æºé…ç½®](#postgresqlæ•°æ®æºé…ç½®)
      - [å¸¸ç”¨æŸ¥è¯¢æ¨¡æ¿](#å¸¸ç”¨æŸ¥è¯¢æ¨¡æ¿)
    - [7.2 Metabaseé›†æˆ](#72-metabaseé›†æˆ)
      - [è¿æ¥PostgreSQL](#è¿æ¥postgresql)
  - [10. å®æˆ˜æ¡ˆä¾‹](#10-å®æˆ˜æ¡ˆä¾‹)
    - [10.1 ç”µå•†æ•°æ®åˆ†ææ¡ˆä¾‹](#101-ç”µå•†æ•°æ®åˆ†ææ¡ˆä¾‹)
      - [æ•°æ®æ¨¡å‹](#æ•°æ®æ¨¡å‹)
      - [åˆ†ææŸ¥è¯¢](#åˆ†ææŸ¥è¯¢)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)
  - [ğŸ“ æ›´æ–°æ—¥å¿—](#-æ›´æ–°æ—¥å¿—)

---

## 1. æ•°æ®åˆ†ææ¦‚è¿°

### 1.1 æ•°æ®åˆ†ææµç¨‹

#### æ ‡å‡†æ•°æ®åˆ†ææµç¨‹

```text
1. æ•°æ®æ”¶é›†
   â”œâ”€ æ•°æ®æºè¯†åˆ«
   â”œâ”€ æ•°æ®æå–
   â””â”€ æ•°æ®åŠ è½½

2. æ•°æ®æ¸…æ´—
   â”œâ”€ ç¼ºå¤±å€¼å¤„ç†
   â”œâ”€ å¼‚å¸¸å€¼å¤„ç†
   â”œâ”€ é‡å¤æ•°æ®æ¸…ç†
   â””â”€ æ•°æ®è½¬æ¢

3. æ•°æ®æ¢ç´¢
   â”œâ”€ æè¿°æ€§ç»Ÿè®¡
   â”œâ”€ æ•°æ®åˆ†å¸ƒåˆ†æ
   â”œâ”€ ç›¸å…³æ€§åˆ†æ
   â””â”€ å¯è§†åŒ–æ¢ç´¢

4. æ•°æ®åˆ†æ
   â”œâ”€ ç»Ÿè®¡åˆ†æ
   â”œâ”€ æœºå™¨å­¦ä¹ åˆ†æ
   â”œâ”€ é¢„æµ‹åˆ†æ
   â””â”€ å…³è”åˆ†æ

5. ç»“æœè§£é‡Š
   â”œâ”€ ç»“æœéªŒè¯
   â”œâ”€ æ´å¯Ÿæå–
   â””â”€ å»ºè®®ç”Ÿæˆ

6. å¯è§†åŒ–å±•ç¤º
   â”œâ”€ æŠ¥è¡¨ç”Ÿæˆ
   â”œâ”€ ä»ªè¡¨ç›˜è®¾è®¡
   â””â”€ äº¤äº’å¼å¯è§†åŒ–
```

### 1.2 PostgreSQLæ•°æ®åˆ†æä¼˜åŠ¿

#### æ ¸å¿ƒä¼˜åŠ¿

```text
âœ… å¼ºå¤§çš„SQLåŠŸèƒ½
   - çª—å£å‡½æ•°
   - é€’å½’CTE
   - å¤šç»´åˆ†æ
   - JSON/JSONBæ”¯æŒ

âœ… ä¸°å¯Œçš„æ‰©å±•ç”Ÿæ€
   - PostGISï¼ˆç©ºé—´åˆ†æï¼‰
   - pg_stat_statementsï¼ˆæ€§èƒ½åˆ†æï¼‰
   - pgmlï¼ˆæœºå™¨å­¦ä¹ ï¼‰
   - MADlibï¼ˆæ•°æ®æŒ–æ˜ï¼‰

âœ… é«˜æ€§èƒ½åˆ†æ
   - å¹¶è¡ŒæŸ¥è¯¢
   - åˆ—å­˜å‚¨ï¼ˆpg_columnarï¼‰
   - ç‰©åŒ–è§†å›¾
   - åˆ†åŒºè¡¨

âœ… å¤šè¯­è¨€æ”¯æŒ
   - PL/Pythonï¼ˆæ•°æ®ç§‘å­¦ï¼‰
   - PL/Rï¼ˆç»Ÿè®¡åˆ†æï¼‰
   - PL/Javaï¼ˆä¼ä¸šé›†æˆï¼‰
```

### 1.3 æ•°æ®åˆ†æå·¥å…·ç®±

#### å·¥å…·æ ˆ

```text
æ•°æ®åº“å±‚:
â”œâ”€ PostgreSQL 17+/18+
â”œâ”€ TimescaleDBï¼ˆæ—¶åºåˆ†æï¼‰
â””â”€ Apache AGEï¼ˆå›¾åˆ†æï¼‰

åˆ†æè¯­è¨€:
â”œâ”€ SQLï¼ˆåŸºç¡€åˆ†æï¼‰
â”œâ”€ Pythonï¼ˆæ•°æ®ç§‘å­¦ï¼‰
â””â”€ Rï¼ˆç»Ÿè®¡åˆ†æï¼‰

å¯è§†åŒ–å·¥å…·:
â”œâ”€ Grafanaï¼ˆå®æ—¶ç›‘æ§ï¼‰
â”œâ”€ Metabaseï¼ˆBIåˆ†æï¼‰
â””â”€ Supersetï¼ˆæ•°æ®æ¢ç´¢ï¼‰

æœºå™¨å­¦ä¹ :
â”œâ”€ pgmlï¼ˆPostgreSQL MLï¼‰
â”œâ”€ MADlibï¼ˆæ•°æ®æŒ–æ˜ï¼‰
â””â”€ Scikit-learnï¼ˆPythonï¼‰
```

---

## 2. é«˜çº§åˆ†ææ–¹æ³•

### 2.1 å…³è”åˆ†æ

#### Aprioriç®—æ³•å®ç°

```sql
-- å…³è”è§„åˆ™æŒ–æ˜ï¼ˆç®€åŒ–ç‰ˆAprioriï¼‰
CREATE OR REPLACE FUNCTION find_association_rules(
    min_support NUMERIC DEFAULT 0.1,
    min_confidence NUMERIC DEFAULT 0.5
)
RETURNS TABLE (
    antecedent TEXT[],
    consequent TEXT[],
    support NUMERIC,
    confidence NUMERIC,
    lift NUMERIC
) AS $$
DECLARE
    total_transactions BIGINT;
BEGIN
    -- è®¡ç®—æ€»äº‹åŠ¡æ•°
    SELECT COUNT(DISTINCT transaction_id) INTO total_transactions
    FROM transaction_items;

    -- æŸ¥æ‰¾é¢‘ç¹é¡¹é›†å’Œå…³è”è§„åˆ™
    RETURN QUERY
    WITH frequent_itemsets AS (
        -- è®¡ç®—é¡¹é›†æ”¯æŒåº¦
        SELECT
            array_agg(DISTINCT item_id ORDER BY item_id) AS itemset,
            COUNT(DISTINCT transaction_id)::NUMERIC / total_transactions AS support
        FROM transaction_items
        GROUP BY transaction_id
        HAVING COUNT(DISTINCT transaction_id)::NUMERIC / total_transactions >= min_support
    ),
    rules AS (
        -- ç”Ÿæˆå…³è”è§„åˆ™å¹¶è®¡ç®—ç½®ä¿¡åº¦å’Œæå‡åº¦
        SELECT
            itemset[1:array_length(itemset, 1)-1] AS antecedent,
            itemset[array_length(itemset, 1):array_length(itemset, 1)] AS consequent,
            support,
            -- è®¡ç®—ç½®ä¿¡åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
            support / (SELECT support FROM frequent_itemsets fi2
                      WHERE fi2.itemset = itemset[1:array_length(itemset, 1)-1]) AS confidence,
            -- è®¡ç®—æå‡åº¦
            support / ((SELECT support FROM frequent_itemsets fi2
                       WHERE fi2.itemset = itemset[1:array_length(itemset, 1)-1]) *
                      (SELECT support FROM frequent_itemsets fi2
                       WHERE fi2.itemset = itemset[array_length(itemset, 1):array_length(itemset, 1)])) AS lift
        FROM frequent_itemsets
        WHERE array_length(itemset, 1) > 1
    )
    SELECT
        antecedent,
        consequent,
        support,
        confidence,
        lift
    FROM rules
    WHERE confidence >= min_confidence
    ORDER BY lift DESC, confidence DESC;
END;
$$ LANGUAGE plpgsql;

-- ä½¿ç”¨ç¤ºä¾‹
SELECT * FROM find_association_rules(0.1, 0.5);
```

#### å…³è”è§„åˆ™å¯è§†åŒ–

```python
# Pythonè„šæœ¬ï¼šå¯è§†åŒ–å…³è”è§„åˆ™
import psycopg2
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx

def visualize_association_rules(db_config):
    """å¯è§†åŒ–å…³è”è§„åˆ™"""
    conn = psycopg2.connect(**db_config)

    # è·å–å…³è”è§„åˆ™
    rules = pd.read_sql_query("""
        SELECT * FROM find_association_rules(0.1, 0.5)
    """, conn)

    # åˆ›å»ºç½‘ç»œå›¾
    G = nx.DiGraph()

    for _, rule in rules.iterrows():
        antecedent = ','.join(rule['antecedent'])
        consequent = ','.join(rule['consequent'])
        G.add_edge(antecedent, consequent,
                  weight=rule['lift'],
                  confidence=rule['confidence'])

    # ç»˜åˆ¶ç½‘ç»œå›¾
    pos = nx.spring_layout(G)
    nx.draw(G, pos, with_labels=True, node_size=1000,
           node_color='lightblue', font_size=8)

    # æ·»åŠ è¾¹æƒé‡æ ‡ç­¾
    edge_labels = {(u, v): f"{d['confidence']:.2f}"
                   for u, v, d in G.edges(data=True)}
    nx.draw_networkx_edge_labels(G, pos, edge_labels)

    plt.title("å…³è”è§„åˆ™ç½‘ç»œå›¾")
    plt.show()

    conn.close()
```

### 2.2 èšç±»åˆ†æ

#### K-meansèšç±»ï¼ˆPL/Pythonï¼‰

```sql
-- åˆ›å»ºPL/Pythonæ‰©å±•
CREATE EXTENSION IF NOT EXISTS plpython3u;

-- K-meansèšç±»å‡½æ•°
CREATE OR REPLACE FUNCTION kmeans_cluster(
    table_name TEXT,
    feature_columns TEXT[],
    n_clusters INTEGER DEFAULT 3,
    max_iter INTEGER DEFAULT 100
)
RETURNS TABLE (
    row_id BIGINT,
    cluster_id INTEGER,
    distance_to_center NUMERIC
) AS $$
import numpy as np
from sklearn.cluster import KMeans
import plpy

# æ„å»ºæŸ¥è¯¢
cols = ', '.join(feature_columns)
query = f"SELECT row_number() OVER () as row_id, {cols} FROM {table_name}"

# æ‰§è¡ŒæŸ¥è¯¢
result = plpy.execute(query)

# æå–ç‰¹å¾æ•°æ®
X = np.array([[float(row[col]) for col in feature_columns] for row in result])

# æ‰§è¡ŒK-meansèšç±»
kmeans = KMeans(n_clusters=n_clusters, max_iter=max_iter, random_state=42)
labels = kmeans.fit_predict(X)

# è®¡ç®—åˆ°ä¸­å¿ƒçš„è·ç¦»
distances = kmeans.transform(X)
min_distances = np.min(distances, axis=1)

# è¿”å›ç»“æœ
for i, (label, dist) in enumerate(zip(labels, min_distances)):
    yield (i + 1, int(label), float(dist))

$$ LANGUAGE plpython3u;

-- ä½¿ç”¨ç¤ºä¾‹
CREATE TABLE customer_features (
    id SERIAL PRIMARY KEY,
    age INTEGER,
    income NUMERIC,
    spending_score NUMERIC
);

-- æ‰§è¡Œèšç±»
SELECT * FROM kmeans_cluster(
    'customer_features',
    ARRAY['age', 'income', 'spending_score'],
    n_clusters := 5
);
```

#### DBSCANèšç±»ï¼ˆå¯†åº¦èšç±»ï¼‰

```sql
CREATE OR REPLACE FUNCTION dbscan_cluster(
    table_name TEXT,
    feature_columns TEXT[],
    eps NUMERIC DEFAULT 0.5,
    min_samples INTEGER DEFAULT 5
)
RETURNS TABLE (
    row_id BIGINT,
    cluster_id INTEGER,
    is_noise BOOLEAN
) AS $$
from sklearn.cluster import DBSCAN
import numpy as np
import plpy

# æ„å»ºæŸ¥è¯¢
cols = ', '.join(feature_columns)
query = f"SELECT row_number() OVER () as row_id, {cols} FROM {table_name}"

# æ‰§è¡ŒæŸ¥è¯¢
result = plpy.execute(query)

# æå–ç‰¹å¾æ•°æ®
X = np.array([[float(row[col]) for col in feature_columns] for row in result])

# æ‰§è¡ŒDBSCANèšç±»
dbscan = DBSCAN(eps=eps, min_samples=min_samples)
labels = dbscan.fit_predict(X)

# è¿”å›ç»“æœ
for i, label in enumerate(labels):
    yield (i + 1, int(label) if label != -1 else NULL, label == -1)

$$ LANGUAGE plpython3u;
```

### 2.3 åˆ†ç±»ä¸é¢„æµ‹

#### é€»è¾‘å›å½’åˆ†ç±»

```sql
CREATE OR REPLACE FUNCTION train_logistic_regression(
    table_name TEXT,
    feature_columns TEXT[],
    target_column TEXT,
    test_size NUMERIC DEFAULT 0.2
)
RETURNS TABLE (
    metric_name TEXT,
    metric_value NUMERIC
) AS $$
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np
import plpy

# æ„å»ºæŸ¥è¯¢
feature_cols = ', '.join(feature_columns)
query = f"SELECT {feature_cols}, {target_column} FROM {table_name}"

# æ‰§è¡ŒæŸ¥è¯¢
result = plpy.execute(query)

# æå–ç‰¹å¾å’Œç›®æ ‡
X = np.array([[float(row[col]) for col in feature_columns] for row in result])
y = np.array([int(row[target_column]) for row in result])

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, random_state=42
)

# è®­ç»ƒæ¨¡å‹
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)

# è®¡ç®—æŒ‡æ ‡
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

# è¿”å›ç»“æœ
yield ('accuracy', float(accuracy))
yield ('precision', float(precision))
yield ('recall', float(recall))
yield ('f1_score', float(f1))

$$ LANGUAGE plpython3u;
```

---

## 3. SQLé«˜çº§åˆ†ææŠ€å·§

### 3.1 çª—å£å‡½æ•°æ·±åº¦åº”ç”¨

#### é«˜çº§çª—å£å‡½æ•°æ¨¡å¼

```sql
-- æ¨¡å¼1: ç´¯ç§¯ç»Ÿè®¡
SELECT
    date,
    sales,
    SUM(sales) OVER (ORDER BY date) AS cumulative_sales,
    AVG(sales) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS moving_avg_7d,
    STDDEV(sales) OVER (ORDER BY date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW) AS moving_stddev_30d
FROM daily_sales
ORDER BY date;

-- æ¨¡å¼2: åˆ†ç»„æ’å
SELECT
    product_category,
    product_name,
    sales,
    RANK() OVER (PARTITION BY product_category ORDER BY sales DESC) AS rank_in_category,
    PERCENT_RANK() OVER (PARTITION BY product_category ORDER BY sales DESC) AS percent_rank,
    CUME_DIST() OVER (PARTITION BY product_category ORDER BY sales DESC) AS cumulative_dist
FROM product_sales;

-- æ¨¡å¼3: åŒæœŸå¯¹æ¯”
SELECT
    month,
    year,
    sales,
    LAG(sales, 12) OVER (ORDER BY year, month) AS sales_same_month_last_year,
    sales - LAG(sales, 12) OVER (ORDER BY year, month) AS yoy_change,
    (sales - LAG(sales, 12) OVER (ORDER BY year, month)) * 100.0 /
        LAG(sales, 12) OVER (ORDER BY year, month) AS yoy_change_percent
FROM monthly_sales
ORDER BY year, month;

-- æ¨¡å¼4: æ»‘åŠ¨çª—å£èšåˆ
SELECT
    timestamp,
    value,
    AVG(value) OVER (
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    ) AS moving_avg_10,
    MAX(value) OVER (
        ORDER BY timestamp
        ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
    ) AS moving_max_30,
    MIN(value) OVER (
        ORDER BY timestamp
        ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
    ) AS moving_min_30
FROM sensor_readings
ORDER BY timestamp;
```

### 3.2 é€’å½’CTEå¤æ‚åˆ†æ

#### å±‚æ¬¡ç»“æ„åˆ†æ

```sql
-- ç»„ç»‡æ¶æ„åˆ†æ
WITH RECURSIVE org_hierarchy AS (
    -- é”šç‚¹ï¼šæ ¹èŠ‚ç‚¹
    SELECT
        id,
        name,
        parent_id,
        0 AS level,
        ARRAY[id] AS path,
        name AS full_path
    FROM organization
    WHERE parent_id IS NULL

    UNION ALL

    -- é€’å½’ï¼šå­èŠ‚ç‚¹
    SELECT
        o.id,
        o.name,
        o.parent_id,
        oh.level + 1,
        oh.path || o.id,
        oh.full_path || ' > ' || o.name
    FROM organization o
    JOIN org_hierarchy oh ON o.parent_id = oh.id
)
SELECT
    id,
    name,
    level,
    full_path,
    array_length(path, 1) AS depth,
    (SELECT COUNT(*) FROM org_hierarchy oh2 WHERE oh2.path @> ARRAY[oh.id]) - 1 AS subtree_size
FROM org_hierarchy oh
ORDER BY path;
```

#### å›¾éå†åˆ†æ

```sql
-- æœ€çŸ­è·¯å¾„åˆ†æ
WITH RECURSIVE shortest_path AS (
    -- èµ·å§‹èŠ‚ç‚¹
    SELECT
        'A' AS node,
        0 AS distance,
        ARRAY['A'] AS path
    UNION ALL
    -- é€’å½’æŸ¥æ‰¾
    SELECT
        e.to_node,
        sp.distance + e.weight,
        sp.path || e.to_node
    FROM graph_edges e
    JOIN shortest_path sp ON e.from_node = sp.node
    WHERE NOT e.to_node = ANY(sp.path)  -- é¿å…å¾ªç¯
      AND sp.distance < 100  -- é™åˆ¶æœ€å¤§è·ç¦»
)
SELECT DISTINCT ON (node)
    node,
    distance,
    path
FROM shortest_path
WHERE node = 'Z'  -- ç›®æ ‡èŠ‚ç‚¹
ORDER BY node, distance
LIMIT 1;
```

---

## 4. PostgreSQLä¸Pythoné›†æˆ

### 4.1 PL/PythonåŸºç¡€

#### å®‰è£…ä¸é…ç½®

```bash
# Ubuntu/Debian
sudo apt-get install postgresql-17-plpython3

# åœ¨æ•°æ®åº“ä¸­å¯ç”¨
psql -d mydb -c "CREATE EXTENSION plpython3u;"
```

#### åŸºç¡€å‡½æ•°ç¤ºä¾‹

```sql
-- ç®€å•çš„Pythonå‡½æ•°
CREATE OR REPLACE FUNCTION python_hello(name TEXT)
RETURNS TEXT AS $$
    return f"Hello, {name}!"
$$ LANGUAGE plpython3u;

-- ä½¿ç”¨NumPyè®¡ç®—
CREATE OR REPLACE FUNCTION calculate_statistics(data NUMERIC[])
RETURNS TABLE (
    mean NUMERIC,
    stddev NUMERIC,
    median NUMERIC,
    q25 NUMERIC,
    q75 NUMERIC
) AS $$
import numpy as np

arr = np.array(data)
return [
    (float(np.mean(arr)),
     float(np.std(arr)),
     float(np.median(arr)),
     float(np.percentile(arr, 25)),
     float(np.percentile(arr, 75)))
]
$$ LANGUAGE plpython3u;

-- ä½¿ç”¨ç¤ºä¾‹
SELECT * FROM calculate_statistics(ARRAY[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]);
```

### 4.2 NumPy/Pandasé›†æˆ

#### Pandasæ•°æ®åˆ†æ

```sql
CREATE OR REPLACE FUNCTION analyze_table_pandas(
    table_name TEXT,
    group_by_column TEXT
)
RETURNS TABLE (
    group_value TEXT,
    count BIGINT,
    mean NUMERIC,
    std NUMERIC,
    min_val NUMERIC,
    max_val NUMERIC
) AS $$
import pandas as pd
import plpy

# æ‰§è¡ŒæŸ¥è¯¢
query = f"SELECT * FROM {table_name}"
result = plpy.execute(query)

# è½¬æ¢ä¸ºDataFrame
df = pd.DataFrame([dict(row) for row in result])

# åˆ†ç»„åˆ†æ
if group_by_column in df.columns:
    grouped = df.groupby(group_by_column).agg({
        col: ['count', 'mean', 'std', 'min', 'max']
        for col in df.select_dtypes(include=['number']).columns
    }).reset_index()

    # è¿”å›ç»“æœ
    for _, row in grouped.iterrows():
        yield (
            str(row[group_by_column]),
            int(row[('id', 'count')] if ('id', 'count') in row.index else 0),
            float(row.select_dtypes(include=['float64']).iloc[0, 1]) if len(row.select_dtypes(include=['float64'])) > 0 else 0,
            float(row.select_dtypes(include=['float64']).iloc[0, 2]) if len(row.select_dtypes(include=['float64'])) > 1 else 0,
            float(row.select_dtypes(include=['float64']).iloc[0, 3]) if len(row.select_dtypes(include=['float64'])) > 2 else 0,
            float(row.select_dtypes(include=['float64']).iloc[0, 4]) if len(row.select_dtypes(include=['float64'])) > 3 else 0
        )

$$ LANGUAGE plpython3u;
```

### 4.3 Scikit-learné›†æˆ

#### æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ

```sql
CREATE OR REPLACE FUNCTION train_model_sklearn(
    table_name TEXT,
    feature_columns TEXT[],
    target_column TEXT,
    model_type TEXT DEFAULT 'random_forest'
)
RETURNS TEXT AS $$
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import pickle
import base64
import plpy

# æ„å»ºæŸ¥è¯¢
feature_cols = ', '.join(feature_columns)
query = f"SELECT {feature_cols}, {target_column} FROM {table_name}"

# æ‰§è¡ŒæŸ¥è¯¢
result = plpy.execute(query)

# è½¬æ¢ä¸ºæ•°ç»„
X = np.array([[float(row[col]) for col in feature_columns] for row in result])
y = np.array([float(row[target_column]) for row in result])

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è®­ç»ƒæ¨¡å‹
if model_type == 'random_forest':
    model = RandomForestRegressor(n_estimators=100, random_state=42)
else:
    raise ValueError(f"Unknown model type: {model_type}")

model.fit(X_train, y_train)

# è¯„ä¼°æ¨¡å‹
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# åºåˆ—åŒ–æ¨¡å‹
model_bytes = pickle.dumps(model)
model_b64 = base64.b64encode(model_bytes).decode('utf-8')

# ä¿å­˜æ¨¡å‹åˆ°æ•°æ®åº“
plpy.execute("""
    INSERT INTO ml_models (model_name, model_type, model_data, mse, r2_score, created_at)
    VALUES ($1, $2, $3, $4, $5, NOW())
    ON CONFLICT (model_name) DO UPDATE SET
        model_data = EXCLUDED.model_data,
        mse = EXCLUDED.mse,
        r2_score = EXCLUDED.r2_score,
        updated_at = NOW()
""", [f"{table_name}_{model_type}", model_type, model_b64, mse, r2])

return f"Model trained: MSE={mse:.4f}, R2={r2:.4f}"

$$ LANGUAGE plpython3u;
```

---

## 7. æ•°æ®å¯è§†åŒ–é›†æˆ

### 7.1 Grafanaé›†æˆ

#### PostgreSQLæ•°æ®æºé…ç½®

```yaml
# Grafanaæ•°æ®æºé…ç½®
apiVersion: 1

datasources:
  - name: PostgreSQL Analytics
    type: postgres
    access: proxy
    url: localhost:5432
    database: analytics_db
    user: grafana
    secureJsonData:
      password: "password"
    jsonData:
      sslmode: "disable"
      maxOpenConns: 100
      maxIdleConns: 100
      connMaxLifetime: 14400
      postgresVersion: 1500
      timescaledb: false
```

#### å¸¸ç”¨æŸ¥è¯¢æ¨¡æ¿

```sql
-- æ—¶é—´åºåˆ—æŸ¥è¯¢ï¼ˆGrafanaå˜é‡ï¼‰
SELECT
    $__timeGroupAlias(created_at, $__interval),
    AVG(value) AS "å¹³å‡å€¼",
    MAX(value) AS "æœ€å¤§å€¼",
    MIN(value) AS "æœ€å°å€¼"
FROM sensor_data
WHERE
    $__timeFilter(created_at)
    AND sensor_id = $sensor_id
GROUP BY 1
ORDER BY 1;

-- è¡¨æ ¼æŸ¥è¯¢
SELECT
    product_category AS "ç±»åˆ«",
    COUNT(*) AS "æ•°é‡",
    SUM(amount) AS "æ€»é‡‘é¢",
    AVG(amount) AS "å¹³å‡é‡‘é¢"
FROM sales
WHERE $__timeFilter(sale_date)
GROUP BY product_category
ORDER BY "æ€»é‡‘é¢" DESC;

-- å•å€¼æŸ¥è¯¢
SELECT
    COUNT(*) AS "æ€»è®¢å•æ•°",
    SUM(amount) AS "æ€»é”€å”®é¢",
    AVG(amount) AS "å¹³å‡è®¢å•é‡‘é¢"
FROM orders
WHERE $__timeFilter(created_at);
```

### 7.2 Metabaseé›†æˆ

#### è¿æ¥PostgreSQL

```sql
-- MetabaseæŸ¥è¯¢ç¤ºä¾‹
-- 1. åˆ›å»ºé—®é¢˜ï¼ˆQuestionï¼‰
SELECT
    DATE_TRUNC('month', order_date) AS month,
    product_category,
    SUM(quantity) AS total_quantity,
    SUM(amount) AS total_amount
FROM orders o
JOIN order_items oi ON o.id = oi.order_id
JOIN products p ON oi.product_id = p.id
WHERE order_date >= CURRENT_DATE - INTERVAL '12 months'
GROUP BY month, product_category
ORDER BY month DESC, total_amount DESC;

-- 2. åˆ›å»ºä»ªè¡¨ç›˜æŸ¥è¯¢
SELECT
    customer_segment,
    COUNT(DISTINCT customer_id) AS customer_count,
    SUM(amount) AS total_revenue,
    AVG(amount) AS avg_order_value
FROM orders
WHERE order_date >= CURRENT_DATE - INTERVAL '30 days'
GROUP BY customer_segment;
```

---

## 10. å®æˆ˜æ¡ˆä¾‹

### 10.1 ç”µå•†æ•°æ®åˆ†ææ¡ˆä¾‹

#### æ•°æ®æ¨¡å‹

```sql
-- è®¢å•åˆ†æè§†å›¾
CREATE MATERIALIZED VIEW mv_order_analytics AS
SELECT
    DATE_TRUNC('day', o.created_at) AS order_date,
    c.segment AS customer_segment,
    p.category AS product_category,
    COUNT(DISTINCT o.id) AS order_count,
    COUNT(oi.id) AS item_count,
    SUM(oi.quantity) AS total_quantity,
    SUM(oi.amount) AS total_amount,
    AVG(oi.amount) AS avg_item_amount,
    COUNT(DISTINCT o.customer_id) AS unique_customers
FROM orders o
JOIN customers c ON o.customer_id = c.id
JOIN order_items oi ON o.id = oi.order_id
JOIN products p ON oi.product_id = p.id
GROUP BY
    DATE_TRUNC('day', o.created_at),
    c.segment,
    p.category;

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_mv_order_analytics_date ON mv_order_analytics(order_date);
CREATE INDEX idx_mv_order_analytics_segment ON mv_order_analytics(customer_segment);
CREATE INDEX idx_mv_order_analytics_category ON mv_order_analytics(product_category);

-- å®šæœŸåˆ·æ–°
REFRESH MATERIALIZED VIEW CONCURRENTLY mv_order_analytics;
```

#### åˆ†ææŸ¥è¯¢

```sql
-- é”€å”®è¶‹åŠ¿åˆ†æ
SELECT
    order_date,
    SUM(total_amount) AS daily_revenue,
    AVG(SUM(total_amount)) OVER (
        ORDER BY order_date
        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
    ) AS moving_avg_7d,
    SUM(SUM(total_amount)) OVER (ORDER BY order_date) AS cumulative_revenue
FROM mv_order_analytics
WHERE order_date >= CURRENT_DATE - INTERVAL '90 days'
GROUP BY order_date
ORDER BY order_date;

-- å®¢æˆ·ä»·å€¼åˆ†æï¼ˆRFMæ¨¡å‹ï¼‰
WITH customer_rfm AS (
    SELECT
        customer_id,
        MAX(order_date) AS last_order_date,
        COUNT(DISTINCT order_date) AS frequency,
        SUM(total_amount) AS monetary_value
    FROM mv_order_analytics
    GROUP BY customer_id
)
SELECT
    CASE
        WHEN last_order_date >= CURRENT_DATE - INTERVAL '30 days' THEN 'æ´»è·ƒ'
        WHEN last_order_date >= CURRENT_DATE - INTERVAL '90 days' THEN 'æ²‰ç¡'
        ELSE 'æµå¤±'
    END AS recency_segment,
    CASE
        WHEN frequency >= 10 THEN 'é«˜é¢‘'
        WHEN frequency >= 5 THEN 'ä¸­é¢‘'
        ELSE 'ä½é¢‘'
    END AS frequency_segment,
    CASE
        WHEN monetary_value >= 10000 THEN 'é«˜ä»·å€¼'
        WHEN monetary_value >= 5000 THEN 'ä¸­ä»·å€¼'
        ELSE 'ä½ä»·å€¼'
    END AS monetary_segment,
    COUNT(*) AS customer_count,
    AVG(monetary_value) AS avg_value
FROM customer_rfm
GROUP BY 1, 2, 3
ORDER BY customer_count DESC;
```

---

## ğŸ“š å‚è€ƒèµ„æº

1. **PostgreSQLå®˜æ–¹æ–‡æ¡£**: <https://www.postgresql.org/docs/current/functions-aggregate.html>
2. **PL/Pythonæ–‡æ¡£**: <https://www.postgresql.org/docs/current/plpython.html>
3. **MADlibæ–‡æ¡£**: <https://madlib.apache.org/>
4. **pgmlæ–‡æ¡£**: <https://github.com/postgresml/postgresml>
5. **Grafanaæ–‡æ¡£**: <https://grafana.com/docs/grafana/latest/datasources/postgres/>
6. **Metabaseæ–‡æ¡£**: <https://www.metabase.com/docs/>

---

## ğŸ“ æ›´æ–°æ—¥å¿—

- **v2.0** (2025-01): å®Œæ•´å®æˆ˜æŒ‡å—
  - è¡¥å……é«˜çº§åˆ†ææ–¹æ³•ï¼ˆå…³è”åˆ†æã€èšç±»ã€åˆ†ç±»é¢„æµ‹ï¼‰
  - è¡¥å……SQLé«˜çº§åˆ†ææŠ€å·§
  - è¡¥å……PostgreSQLä¸Pythoné›†æˆ
  - è¡¥å……PostgreSQLä¸Ré›†æˆ
  - è¡¥å……æœºå™¨å­¦ä¹ é›†æˆ
  - è¡¥å……æ•°æ®å¯è§†åŒ–é›†æˆ
  - è¡¥å……æ•°æ®åˆ†æå·¥ä½œæµ
  - è¡¥å……å®æˆ˜æ¡ˆä¾‹

---

**çŠ¶æ€**: âœ… **æ–‡æ¡£å®Œæˆ** | [è¿”å›ç›®å½•](./README.md)
