# PostgreSQL数据湖完整指南

> **创建日期**: 2025年1月
> **技术版本**: PostgreSQL 17+/18+
> **难度等级**: ⭐⭐⭐⭐ 高级

---

## 📋 目录

- [PostgreSQL数据湖完整指南](#postgresql数据湖完整指南)
  - [📋 目录](#-目录)
  - [1. 概述](#1-概述)
  - [2. 数据湖架构](#2-数据湖架构)
    - [2.1 架构层次](#21-架构层次)
    - [2.2 存储格式](#22-存储格式)
  - [3. PostgreSQL集成](#3-postgresql集成)
    - [3.1 FDW集成](#31-fdw集成)
    - [3.2 直接访问](#32-直接访问)
  - [4. 半结构化数据处理](#4-半结构化数据处理)
    - [4.1 JSON处理](#41-json处理)
    - [4.2 JSONB索引](#42-jsonb索引)
  - [5. 元数据管理](#5-元数据管理)
    - [5.1 元数据存储](#51-元数据存储)
    - [5.2 元数据查询](#52-元数据查询)
  - [6. 数据湖数据治理](#6-数据湖数据治理)
    - [6.1 数据质量检查](#61-数据质量检查)
    - [6.2 数据生命周期管理](#62-数据生命周期管理)
  - [7. 数据湖性能优化](#7-数据湖性能优化)
    - [7.1 分区优化](#71-分区优化)
    - [7.2 索引优化](#72-索引优化)
  - [8. 数据湖监控和告警](#8-数据湖监控和告警)
    - [8.1 数据湖监控视图](#81-数据湖监控视图)
    - [8.2 告警配置](#82-告警配置)
  - [📚 相关文档](#-相关文档)

---

## 1. 概述

数据湖是存储各种格式原始数据的集中式存储系统。

**数据湖特征**:

- 存储原始数据
- 支持多种格式
- 按需处理
- 灵活查询

---

## 2. 数据湖架构

### 2.1 架构层次

```text
数据源层
  ↓
存储层（数据湖）
  ↓
处理层（PostgreSQL）
  ↓
应用层
```

### 2.2 存储格式

| 格式 | 用途 | PostgreSQL支持 |
| --- | --- | --- |
| **Parquet** | 列式存储 | 通过扩展 |
| **JSON** | 半结构化 | 原生支持 |
| **CSV** | 结构化 | 原生支持 |
| **Avro** | 序列化 | 通过扩展 |

---

## 3. PostgreSQL集成

### 3.1 FDW集成

```sql
-- 使用file_fdw访问数据湖
-- 1. 创建file_fdw扩展
CREATE EXTENSION IF NOT EXISTS file_fdw;

-- 2. 创建文件服务器
CREATE SERVER IF NOT EXISTS file_server
FOREIGN DATA WRAPPER file_fdw;

-- 3. 创建外部表（访问JSON格式数据湖文件）
CREATE FOREIGN TABLE IF NOT EXISTS lake_data (
    id INTEGER,
    data JSONB
)
SERVER file_server
OPTIONS (filename '/data-lake/data.json', format 'json');

-- 4. 查询外部表数据
SELECT * FROM lake_data LIMIT 10;
```

### 3.2 直接访问

```sql
-- 使用JSONB存储数据湖原始数据
CREATE TABLE IF NOT EXISTS lake_storage (
    id SERIAL PRIMARY KEY,
    source_system VARCHAR(100) NOT NULL,
    data_type VARCHAR(50) NOT NULL,
    raw_data JSONB NOT NULL,
    metadata JSONB DEFAULT '{}',
    file_path TEXT,
    file_size BIGINT,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- 创建索引优化查询
CREATE INDEX IF NOT EXISTS idx_lake_storage_source ON lake_storage(source_system, created_at);
CREATE INDEX IF NOT EXISTS idx_lake_storage_type ON lake_storage(data_type);
CREATE INDEX IF NOT EXISTS idx_lake_storage_metadata ON lake_storage USING GIN(metadata);
CREATE INDEX IF NOT EXISTS idx_lake_storage_raw_data ON lake_storage USING GIN(raw_data);

-- 插入示例数据
INSERT INTO lake_storage (source_system, data_type, raw_data, metadata, file_path, file_size) VALUES
    ('CRM', 'customer', '{"customer_id": 1, "name": "Alice", "email": "alice@example.com"}',
     '{"version": "1.0", "schema": "customer_v1"}', '/data-lake/crm/customer_001.json', 1024),
    ('ERP', 'order', '{"order_id": 100, "customer_id": 1, "amount": 299.99, "items": [{"product_id": 10, "quantity": 2}]}',
     '{"version": "1.0", "schema": "order_v1"}', '/data-lake/erp/order_100.json', 2048),
    ('Analytics', 'event', '{"event_id": "evt_001", "event_type": "page_view", "timestamp": "2024-01-15T10:30:00Z"}',
     '{"version": "2.0", "schema": "event_v2"}', '/data-lake/analytics/events_2024_01_15.json', 4096);

-- 查询示例
SELECT
    source_system,
    data_type,
    raw_data->>'customer_id' AS customer_id,
    raw_data->>'name' AS name,
    created_at
FROM lake_storage
WHERE source_system = 'CRM'
ORDER BY created_at DESC;
```

---

## 4. 半结构化数据处理

### 4.1 JSON处理

```sql
-- 存储JSON数据
INSERT INTO lake_storage (raw_data)
VALUES ('{"name": "test", "value": 100}'::jsonb);

-- 查询JSON数据
SELECT
    raw_data->>'name' AS name,
    (raw_data->>'value')::INT AS value
FROM lake_storage
WHERE raw_data->>'name' = 'test';
```

### 4.2 JSONB索引

```sql
-- 创建GIN索引
CREATE INDEX idx_lake_storage_data ON lake_storage USING GIN (raw_data);

-- 查询优化
SELECT * FROM lake_storage
WHERE raw_data @> '{"name": "test"}'::jsonb;
```

---

## 5. 元数据管理

### 5.1 元数据存储

```sql
-- 元数据表
CREATE TABLE lake_metadata (
    id SERIAL PRIMARY KEY,
    file_path TEXT,
    file_format TEXT,
    schema_info JSONB,
    created_at TIMESTAMPTZ,
    updated_at TIMESTAMPTZ
);
```

### 5.2 元数据查询

```sql
-- 查询元数据
SELECT
    file_path,
    file_format,
    schema_info
FROM lake_metadata
WHERE file_format = 'json';
```

---

## 6. 数据湖数据治理

### 6.1 数据质量检查

```sql
-- 数据质量检查函数（带错误处理和性能测试）
CREATE OR REPLACE FUNCTION check_lake_data_quality(
    p_table_name TEXT
)
RETURNS TABLE (
    check_type TEXT,
    check_result TEXT,
    error_count BIGINT
) AS $$
DECLARE
    null_count BIGINT;
    duplicate_count BIGINT;
    invalid_json_count BIGINT;
BEGIN
    -- 检查NULL值
    EXECUTE format(
        'SELECT COUNT(*) FROM %I WHERE raw_data IS NULL',
        p_table_name
    ) INTO null_count;

    IF null_count > 0 THEN
        RETURN QUERY SELECT 'NULL_CHECK'::TEXT, 'FAIL'::TEXT, null_count;
    ELSE
        RETURN QUERY SELECT 'NULL_CHECK'::TEXT, 'PASS'::TEXT, 0::BIGINT;
    END IF;

    -- 检查重复数据
    EXECUTE format(
        'SELECT COUNT(*) - COUNT(DISTINCT raw_data) FROM %I',
        p_table_name
    ) INTO duplicate_count;

    IF duplicate_count > 0 THEN
        RETURN QUERY SELECT 'DUPLICATE_CHECK'::TEXT, 'FAIL'::TEXT, duplicate_count;
    ELSE
        RETURN QUERY SELECT 'DUPLICATE_CHECK'::TEXT, 'PASS'::TEXT, 0::BIGINT;
    END IF;

    -- 检查无效JSON
    EXECUTE format(
        'SELECT COUNT(*) FROM %I WHERE NOT (raw_data::text ~ ''^\{.*\}$'')',
        p_table_name
    ) INTO invalid_json_count;

    IF invalid_json_count > 0 THEN
        RETURN QUERY SELECT 'JSON_VALIDITY_CHECK'::TEXT, 'FAIL'::TEXT, invalid_json_count;
    ELSE
        RETURN QUERY SELECT 'JSON_VALIDITY_CHECK'::TEXT, 'PASS'::TEXT, 0::BIGINT;
    END IF;

    RETURN;

EXCEPTION
    WHEN OTHERS THEN
        RAISE EXCEPTION '数据质量检查失败: %', SQLERRM;
END;
$$ LANGUAGE plpgsql;

-- 执行数据质量检查
SELECT * FROM check_lake_data_quality('lake_storage');
```

### 6.2 数据生命周期管理

```sql
-- 数据生命周期管理表
CREATE TABLE IF NOT EXISTS lake_data_lifecycle (
    id SERIAL PRIMARY KEY,
    table_name TEXT NOT NULL,
    retention_days INT DEFAULT 365,
    archive_days INT DEFAULT 90,
    delete_days INT DEFAULT 365,
    auto_archive BOOLEAN DEFAULT TRUE,
    auto_delete BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- 数据归档函数（带错误处理和性能测试）
CREATE OR REPLACE FUNCTION archive_old_lake_data(
    p_table_name TEXT,
    p_archive_days INT DEFAULT 90
)
RETURNS TABLE (
    archived_count BIGINT,
    archive_duration INTERVAL
) AS $$
DECLARE
    start_time TIMESTAMPTZ;
    end_time TIMESTAMPTZ;
    archived_rows BIGINT;
    archive_table_name TEXT;
BEGIN
    start_time := clock_timestamp();

    -- 创建归档表
    archive_table_name := p_table_name || '_archive';

    EXECUTE format(
        'CREATE TABLE IF NOT EXISTS %I (LIKE %I INCLUDING ALL)',
        archive_table_name,
        p_table_name
    );

    -- 归档旧数据
    EXECUTE format(
        'INSERT INTO %I SELECT * FROM %I WHERE created_at < NOW() - INTERVAL ''%s days''',
        archive_table_name,
        p_table_name,
        p_archive_days
    );

    GET DIAGNOSTICS archived_rows = ROW_COUNT;

    -- 删除已归档数据
    EXECUTE format(
        'DELETE FROM %I WHERE created_at < NOW() - INTERVAL ''%s days''',
        p_table_name,
        p_archive_days
    );

    end_time := clock_timestamp();

    RETURN QUERY SELECT
        archived_rows,
        end_time - start_time;

EXCEPTION
    WHEN OTHERS THEN
        RAISE EXCEPTION '数据归档失败: %', SQLERRM;
END;
$$ LANGUAGE plpgsql;

-- 执行数据归档
SELECT * FROM archive_old_lake_data('lake_storage', 90);
```

---

## 7. 数据湖性能优化

### 7.1 分区优化

```sql
-- 按时间分区数据湖表
CREATE TABLE lake_storage_partitioned (
    id SERIAL,
    raw_data JSONB,
    metadata JSONB,
    created_at TIMESTAMPTZ DEFAULT NOW()
) PARTITION BY RANGE (created_at);

-- 创建月度分区
CREATE TABLE lake_storage_2024_01 PARTITION OF lake_storage_partitioned
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE lake_storage_2024_02 PARTITION OF lake_storage_partitioned
FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');
```

### 7.2 索引优化

```sql
-- 创建GIN索引（支持JSONB查询）
CREATE INDEX idx_lake_storage_data_gin ON lake_storage USING GIN (raw_data);

-- 创建表达式索引（针对常用字段）
CREATE INDEX idx_lake_storage_created ON lake_storage (created_at);
CREATE INDEX idx_lake_storage_metadata ON lake_storage USING GIN (metadata);

-- 查询性能测试
EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT * FROM lake_storage
WHERE raw_data @> '{"status": "active"}'::jsonb
  AND created_at >= CURRENT_DATE - INTERVAL '7 days';
```

---

## 8. 数据湖监控和告警

### 8.1 数据湖监控视图

```sql
-- 数据湖监控视图（带错误处理和性能测试）
CREATE OR REPLACE VIEW v_lake_monitoring AS
SELECT
    'lake_storage' AS table_name,
    COUNT(*) AS total_records,
    pg_size_pretty(pg_total_relation_size('lake_storage')) AS total_size,
    COUNT(*) FILTER (WHERE created_at >= NOW() - INTERVAL '24 hours') AS records_24h,
    COUNT(*) FILTER (WHERE created_at >= NOW() - INTERVAL '7 days') AS records_7d,
    MAX(created_at) AS last_insert_time,
    MIN(created_at) AS first_insert_time
FROM lake_storage;

-- 查询监控视图
SELECT * FROM v_lake_monitoring;
```

### 8.2 告警配置

```sql
-- 数据湖告警函数（带错误处理和性能测试）
CREATE OR REPLACE FUNCTION check_lake_alerts()
RETURNS TABLE (
    alert_type TEXT,
    alert_message TEXT,
    alert_level TEXT
) AS $$
DECLARE
    total_size_bytes BIGINT;
    record_count BIGINT;
    last_insert_time TIMESTAMPTZ;
BEGIN
    -- 检查数据湖大小
    SELECT pg_total_relation_size('lake_storage') INTO total_size_bytes;

    IF total_size_bytes > 100 * 1024 * 1024 * 1024 THEN  -- 100GB
        RETURN QUERY SELECT
            'SIZE_ALERT'::TEXT,
            format('数据湖大小超过100GB: %', pg_size_pretty(total_size_bytes))::TEXT,
            'WARNING'::TEXT;
    END IF;

    -- 检查数据插入情况
    SELECT COUNT(*), MAX(created_at) INTO record_count, last_insert_time
    FROM lake_storage;

    IF last_insert_time IS NULL OR NOW() - last_insert_time > INTERVAL '24 hours' THEN
        RETURN QUERY SELECT
            'INSERT_ALERT'::TEXT,
            format('数据湖超过24小时未插入新数据，最后插入时间: %', last_insert_time)::TEXT,
            'WARNING'::TEXT;
    END IF;

    RETURN;

EXCEPTION
    WHEN OTHERS THEN
        RAISE EXCEPTION '检查数据湖告警失败: %', SQLERRM;
END;
$$ LANGUAGE plpgsql;

-- 查询告警
SELECT * FROM check_lake_alerts();
```

---

## 📚 相关文档

- [数据湖架构设计.md](./数据湖架构设计.md) - 数据湖架构设计
- [数据湖与PostgreSQL集成.md](./数据湖与PostgreSQL集成.md) - 集成方案详解
- [26-数据管理/README.md](../README.md) - 数据管理主题

---

**最后更新**: 2025年1月
