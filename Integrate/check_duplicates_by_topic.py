#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†ä¸»é¢˜å»é‡æ£€æŸ¥å·¥å…·
é’ˆå¯¹Integrateç›®å½•ï¼ŒæŒ‰ä¸»é¢˜è¿›è¡Œå»é‡æ£€æŸ¥ï¼Œæé«˜æ•ˆç‡
"""

import os
import re
from pathlib import Path
from typing import List, Dict, Tuple
from difflib import SequenceMatcher
import argparse

class TopicDeduplicationChecker:
    """åˆ†ä¸»é¢˜å»é‡æ£€æŸ¥å™¨"""

    def __init__(self, root_dir: str, threshold: float = 0.8):
        self.root_dir = Path(root_dir)
        self.threshold = threshold
        self.similarities: List[Tuple[str, str, float]] = []

    def extract_text(self, content: str) -> str:
        """ä»Markdownå†…å®¹ä¸­æå–çº¯æ–‡æœ¬"""
        # ç§»é™¤ä»£ç å—
        content = re.sub(r'```[\s\S]*?```', '', content)
        # ç§»é™¤è¡Œå†…ä»£ç 
        content = re.sub(r'`[^`]+`', '', content)
        # ç§»é™¤é“¾æ¥
        content = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', content)
        # ç§»é™¤å›¾ç‰‡
        content = re.sub(r'!\[([^\]]*)\]\([^\)]+\)', '', content)
        # ç§»é™¤æ ‡é¢˜æ ‡è®°
        content = re.sub(r'^#+\s+', '', content, flags=re.MULTILINE)
        # ç§»é™¤æ¥æºä¿¡æ¯ï¼ˆIntegrateç›®å½•ç‰¹æœ‰çš„ï¼‰
        content = re.sub(r'^> \*\*ğŸ“‹ æ–‡æ¡£æ¥æº.*?\n', '', content, flags=re.MULTILINE)
        # ç§»é™¤å¤šä½™ç©ºç™½
        content = re.sub(r'\s+', ' ', content)
        return content.strip()

    def calculate_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
        if not text1 or not text2:
            return 0.0
        return SequenceMatcher(None, text1, text2).ratio()

    def check_topic(self, topic_dir: Path) -> List[Tuple[str, str, float]]:
        """æ£€æŸ¥å•ä¸ªä¸»é¢˜ç›®å½•ä¸­çš„é‡å¤æ–‡æ¡£"""
        topic_similarities = []

        if not topic_dir.exists() or not topic_dir.is_dir():
            return topic_similarities

        # è·å–è¯¥ä¸»é¢˜ä¸‹çš„æ‰€æœ‰mdæ–‡ä»¶
        md_files = list(topic_dir.rglob("*.md"))
        md_files = [f for f in md_files if f.name != "README.md"]  # è·³è¿‡README

        if len(md_files) < 2:
            return topic_similarities

        print(f"\næ£€æŸ¥ä¸»é¢˜: {topic_dir.name} ({len(md_files)} ä¸ªæ–‡æ¡£)")

        # è¯»å–æ‰€æœ‰æ–‡æ¡£
        documents = {}
        for md_file in md_files:
            try:
                content = md_file.read_text(encoding='utf-8')
                text_content = self.extract_text(content)
                if len(text_content) > 100:  # åªå¤„ç†æœ‰å®è´¨å†…å®¹çš„æ–‡æ¡£
                    rel_path = str(md_file.relative_to(self.root_dir))
                    documents[rel_path] = text_content
            except Exception as e:
                print(f"  è­¦å‘Š: æ— æ³•è¯»å– {md_file.name}: {e}")

        # æ¯”è¾ƒæ–‡æ¡£å¯¹
        doc_list = list(documents.items())
        for i, (path1, content1) in enumerate(doc_list):
            for j, (path2, content2) in enumerate(doc_list[i+1:], start=i+1):
                similarity = self.calculate_similarity(content1, content2)
                if similarity >= self.threshold:
                    topic_similarities.append((path1, path2, similarity))
                    print(f"  âš ï¸  å‘ç°ç›¸ä¼¼æ–‡æ¡£: {Path(path1).name} <-> {Path(path2).name} (ç›¸ä¼¼åº¦: {similarity:.2%})")

        return topic_similarities

    def check_all_topics(self):
        """æ£€æŸ¥æ‰€æœ‰ä¸»é¢˜"""
        print(f"å¼€å§‹æ£€æŸ¥ Integrate ç›®å½•...")
        print(f"ç›¸ä¼¼åº¦é˜ˆå€¼: {self.threshold}")

        # è·å–æ‰€æœ‰ä¸»é¢˜ç›®å½•
        topic_dirs = [d for d in self.root_dir.iterdir()
                     if d.is_dir() and d.name.startswith(('0', '1', '2'))
                     and not d.name.startswith('00-')]

        topic_dirs.sort()

        print(f"\næ‰¾åˆ° {len(topic_dirs)} ä¸ªä¸»é¢˜ç›®å½•")

        for topic_dir in topic_dirs:
            similarities = self.check_topic(topic_dir)
            self.similarities.extend(similarities)

        print(f"\nğŸ“Š æ£€æŸ¥å®Œæˆ:")
        print(f"  æ€»ç›¸ä¼¼æ–‡æ¡£å¯¹: {len(self.similarities)}")
        print(f"  é«˜ç›¸ä¼¼åº¦ (>=0.9): {len([s for s in self.similarities if s[2] >= 0.9])}")
        print(f"  ä¸­ç­‰ç›¸ä¼¼åº¦ (0.8-0.9): {len([s for s in self.similarities if 0.8 <= s[2] < 0.9])}")

    def generate_report(self, output_file: str = "00-é‡å¤å†…å®¹æŠ¥å‘Š.md"):
        """ç”Ÿæˆå»é‡æŠ¥å‘Š"""
        output_path = self.root_dir / output_file

        report = f"""# Integrate ç›®å½•é‡å¤å†…å®¹æŠ¥å‘Š

> **ç”Ÿæˆæ—¶é—´**: {Path(__file__).stat().st_mtime}
> **æ£€æŸ¥ç›®å½•**: Integrate
> **ç›¸ä¼¼åº¦é˜ˆå€¼**: {self.threshold}
> **å‘ç°é‡å¤æ–‡æ¡£å¯¹**: {len(self.similarities)}

---

## ğŸ“Š ç»Ÿè®¡æ‘˜è¦

- æ€»ç›¸ä¼¼æ–‡æ¡£å¯¹: {len(self.similarities)}
- é«˜ç›¸ä¼¼åº¦æ–‡æ¡£å¯¹ (>=0.9): {len([s for s in self.similarities if s[2] >= 0.9])}
- ä¸­ç­‰ç›¸ä¼¼åº¦æ–‡æ¡£å¯¹ (0.8-0.9): {len([s for s in self.similarities if 0.8 <= s[2] < 0.9])}

---

## ğŸ” é‡å¤æ–‡æ¡£è¯¦æƒ…

"""

        if self.similarities:
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            sorted_similarities = sorted(self.similarities, key=lambda x: x[2], reverse=True)

            report += "### é«˜ç›¸ä¼¼åº¦æ–‡æ¡£å¯¹ (>=0.9)\n\n"
            high_sim = [s for s in sorted_similarities if s[2] >= 0.9]
            if high_sim:
                for path1, path2, sim in high_sim:
                    report += f"- **ç›¸ä¼¼åº¦: {sim:.2%}**\n"
                    report += f"  - `{path1}`\n"
                    report += f"  - `{path2}`\n\n"
            else:
                report += "æ— \n\n"

            report += "### ä¸­ç­‰ç›¸ä¼¼åº¦æ–‡æ¡£å¯¹ (0.8-0.9)\n\n"
            med_sim = [s for s in sorted_similarities if 0.8 <= s[2] < 0.9]
            if med_sim:
                for path1, path2, sim in med_sim:
                    report += f"- **ç›¸ä¼¼åº¦: {sim:.2%}**\n"
                    report += f"  - `{path1}`\n"
                    report += f"  - `{path2}`\n\n"
            else:
                report += "æ— \n\n"
        else:
            report += "æœªå‘ç°é‡å¤æ–‡æ¡£ã€‚\n\n"

        report += """---

## ğŸ“‹ å»ºè®®çš„åˆå¹¶æ¸…å•

### é«˜ä¼˜å…ˆçº§åˆå¹¶ï¼ˆç›¸ä¼¼åº¦ >= 0.9ï¼‰

å»ºè®®åˆå¹¶è¿™äº›é«˜åº¦ç›¸ä¼¼çš„æ–‡æ¡£ï¼Œä¿ç•™å†…å®¹æœ€å®Œæ•´çš„ç‰ˆæœ¬ã€‚

### ä¸­ç­‰ä¼˜å…ˆçº§åˆå¹¶ï¼ˆç›¸ä¼¼åº¦ 0.8-0.9ï¼‰

å»ºè®®å®¡æŸ¥è¿™äº›æ–‡æ¡£ï¼Œå†³å®šæ˜¯å¦éœ€è¦åˆå¹¶ã€‚

---

## ğŸ”§ ä½¿ç”¨è¯´æ˜

1. å®¡æŸ¥é«˜ç›¸ä¼¼åº¦æ–‡æ¡£å¯¹
2. å†³å®šä¿ç•™å“ªä¸ªæ–‡æ¡£
3. åˆå¹¶å†…å®¹åˆ°ä¿ç•™çš„æ–‡æ¡£
4. æ›´æ–°æ‰€æœ‰å¼•ç”¨é“¾æ¥
5. åˆ é™¤é‡å¤çš„æ–‡æ¡£ï¼ˆåœ¨Integrateç›®å½•ä¸­ï¼‰

---

**æŠ¥å‘Šç”Ÿæˆå·¥å…·**: `Integrate/check_duplicates_by_topic.py`
"""

        output_path.write_text(report, encoding='utf-8')
        print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")

def main():
    parser = argparse.ArgumentParser(description='åˆ†ä¸»é¢˜å»é‡æ£€æŸ¥å·¥å…·')
    parser.add_argument('--root', type=str, default='Integrate', help='Integrateç›®å½•è·¯å¾„')
    parser.add_argument('--threshold', type=float, default=0.8, help='ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1)')
    parser.add_argument('--output', type=str, default='00-é‡å¤å†…å®¹æŠ¥å‘Š.md', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶å')

    args = parser.parse_args()

    checker = TopicDeduplicationChecker(args.root, args.threshold)
    checker.check_all_topics()
    checker.generate_report(args.output)

    print("\nâœ… æ£€æŸ¥å®Œæˆï¼")

if __name__ == '__main__':
    main()
