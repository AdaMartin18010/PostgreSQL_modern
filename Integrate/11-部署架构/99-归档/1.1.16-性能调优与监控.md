---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `PostgreSQL\04-éƒ¨ç½²è¿ç»´\99-å½’æ¡£\1.1.16-æ€§èƒ½è°ƒä¼˜ä¸ç›‘æ§.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# 1.1.16 PostgreSQLæ€§èƒ½è°ƒä¼˜ä¸ç›‘æ§æ·±åº¦æŒ‡å—

## ç›®å½•

- [1.1.16 PostgreSQLæ€§èƒ½è°ƒä¼˜ä¸ç›‘æ§æ·±åº¦æŒ‡å—](#1116-postgresqlæ€§èƒ½è°ƒä¼˜ä¸ç›‘æ§æ·±åº¦æŒ‡å—)
  - [ç›®å½•](#ç›®å½•)
  - [1. æ¦‚è¿°](#1-æ¦‚è¿°)
    - [1.1 æ€§èƒ½æ¡†æ¶](#11-æ€§èƒ½æ¡†æ¶)
    - [1.2 æ€§èƒ½æŒ‡æ ‡æ¨¡å‹](#12-æ€§èƒ½æŒ‡æ ‡æ¨¡å‹)
  - [2. æ€§èƒ½ç†è®ºåŸºç¡€](#2-æ€§èƒ½ç†è®ºåŸºç¡€)
    - [2.1 æ€§èƒ½ç“¶é¢ˆåˆ†æ](#21-æ€§èƒ½ç“¶é¢ˆåˆ†æ)
    - [2.2 æ€§èƒ½ä¼˜åŒ–åŸåˆ™](#22-æ€§èƒ½ä¼˜åŒ–åŸåˆ™)
  - [3. å‚æ•°è°ƒä¼˜](#3-å‚æ•°è°ƒä¼˜)
    - [3.1 å†…å­˜å‚æ•°é…ç½®](#31-å†…å­˜å‚æ•°é…ç½®)
    - [3.2 æŸ¥è¯¢ä¼˜åŒ–å‚æ•°](#32-æŸ¥è¯¢ä¼˜åŒ–å‚æ•°)
    - [3.3 è‡ªåŠ¨è°ƒä¼˜å·¥å…·](#33-è‡ªåŠ¨è°ƒä¼˜å·¥å…·)
  - [4. ç´¢å¼•ä¼˜åŒ–](#4-ç´¢å¼•ä¼˜åŒ–)
    - [4.1 ç´¢å¼•ç­–ç•¥](#41-ç´¢å¼•ç­–ç•¥)
    - [4.2 ç´¢å¼•åˆ†æå·¥å…·](#42-ç´¢å¼•åˆ†æå·¥å…·)
  - [5. æŸ¥è¯¢ä¼˜åŒ–](#5-æŸ¥è¯¢ä¼˜åŒ–)
    - [5.1 æ…¢æŸ¥è¯¢åˆ†æ](#51-æ…¢æŸ¥è¯¢åˆ†æ)
    - [5.2 æŸ¥è¯¢é‡å†™ä¼˜åŒ–](#52-æŸ¥è¯¢é‡å†™ä¼˜åŒ–)
  - [6. ç›‘æ§ç³»ç»Ÿ](#6-ç›‘æ§ç³»ç»Ÿ)
    - [6.1 ç›‘æ§æŒ‡æ ‡](#61-ç›‘æ§æŒ‡æ ‡)
  - [7. æ€§èƒ½è¯Šæ–­](#7-æ€§èƒ½è¯Šæ–­)
    - [7.1 æ€§èƒ½é—®é¢˜è¯Šæ–­](#71-æ€§èƒ½é—®é¢˜è¯Šæ–­)
  - [8. è¡Œä¸šåº”ç”¨](#8-è¡Œä¸šåº”ç”¨)
    - [8.1 äº’è”ç½‘è¡Œä¸šæ€§èƒ½ç›‘æ§](#81-äº’è”ç½‘è¡Œä¸šæ€§èƒ½ç›‘æ§)
    - [8.2 é‡‘èè¡Œä¸šæ€§èƒ½è°ƒä¼˜](#82-é‡‘èè¡Œä¸šæ€§èƒ½è°ƒä¼˜)
  - [9. æœ€ä½³å®è·µ](#9-æœ€ä½³å®è·µ)
    - [9.1 æ€§èƒ½è°ƒä¼˜æ£€æŸ¥æ¸…å•](#91-æ€§èƒ½è°ƒä¼˜æ£€æŸ¥æ¸…å•)
    - [9.2 ç›‘æ§æŒ‡æ ‡é˜ˆå€¼](#92-ç›‘æ§æŒ‡æ ‡é˜ˆå€¼)
    - [9.3 æ€§èƒ½ä¼˜åŒ–æµç¨‹](#93-æ€§èƒ½ä¼˜åŒ–æµç¨‹)
  - [10. ç›¸å…³é“¾æ¥](#10-ç›¸å…³é“¾æ¥)
    - [10.1 å†…éƒ¨é“¾æ¥](#101-å†…éƒ¨é“¾æ¥)
    - [10.2 å¤–éƒ¨èµ„æº](#102-å¤–éƒ¨èµ„æº)
    - [10.3 ç›‘æ§å·¥å…·](#103-ç›‘æ§å·¥å…·)

## 1. æ¦‚è¿°

PostgreSQLæ€§èƒ½è°ƒä¼˜ä¸ç›‘æ§æ˜¯æ•°æ®åº“è¿ç»´çš„æ ¸å¿ƒæŠ€èƒ½ï¼Œæ¶‰åŠå‚æ•°é…ç½®ã€ç´¢å¼•è®¾è®¡ã€æŸ¥è¯¢ä¼˜åŒ–å’Œå®æ—¶ç›‘æ§ç­‰å¤šä¸ªæ–¹é¢ã€‚

### 1.1 æ€§èƒ½æ¡†æ¶

**å®šä¹‰ 1.1.1** (æ€§èƒ½ä¼˜åŒ–æ¡†æ¶)ï¼šPostgreSQLæ€§èƒ½ä¼˜åŒ–æ¡†æ¶æ˜¯ä¸€ä¸ªäº”å…ƒç»„ $PF = (P, I, Q, M, D)$ï¼Œå…¶ä¸­ï¼š

- $P$ æ˜¯å‚æ•°è°ƒä¼˜ç³»ç»Ÿ
- $I$ æ˜¯ç´¢å¼•ä¼˜åŒ–ç³»ç»Ÿ
- $Q$ æ˜¯æŸ¥è¯¢ä¼˜åŒ–ç³»ç»Ÿ
- $M$ æ˜¯ç›‘æ§ç³»ç»Ÿ
- $D$ æ˜¯è¯Šæ–­ç³»ç»Ÿ

### 1.2 æ€§èƒ½æŒ‡æ ‡æ¨¡å‹

$$
\text{æ€§èƒ½æå‡} = \alpha \cdot \frac{\text{ä¼˜åŒ–åTPS}}{\text{ä¼˜åŒ–å‰TPS}} + \beta \cdot \frac{\text{ä¼˜åŒ–å‰å»¶è¿Ÿ}}{\text{ä¼˜åŒ–åå»¶è¿Ÿ}} + \gamma \cdot \frac{\text{ä¼˜åŒ–åQPS}}{\text{ä¼˜åŒ–å‰QPS}}
$$

å…¶ä¸­ $\alpha, \beta, \gamma$ æ˜¯æƒé‡ç³»æ•°ï¼Œæ»¡è¶³ $\alpha + \beta + \gamma = 1$ã€‚

## 2. æ€§èƒ½ç†è®ºåŸºç¡€

### 2.1 æ€§èƒ½ç“¶é¢ˆåˆ†æ

**å®šä¹‰ 2.1.1** (æ€§èƒ½ç“¶é¢ˆ)ï¼šæ€§èƒ½ç“¶é¢ˆæ˜¯æŒ‡ç³»ç»Ÿä¸­é™åˆ¶æ•´ä½“æ€§èƒ½çš„å…³é”®èµ„æºæˆ–ç»„ä»¶ã€‚

å¸¸è§çš„æ€§èƒ½ç“¶é¢ˆç±»å‹ï¼š

- **CPUç“¶é¢ˆ**ï¼šè®¡ç®—å¯†é›†å‹æ“ä½œ
- **å†…å­˜ç“¶é¢ˆ**ï¼šç¼“å†²åŒºä¸è¶³
- **I/Oç“¶é¢ˆ**ï¼šç£ç›˜è¯»å†™é™åˆ¶
- **ç½‘ç»œç“¶é¢ˆ**ï¼šç½‘ç»œä¼ è¾“é™åˆ¶
- **é”ç«äº‰**ï¼šå¹¶å‘è®¿é—®å†²çª

### 2.2 æ€§èƒ½ä¼˜åŒ–åŸåˆ™

1. **æµ‹é‡ä¼˜å…ˆ**ï¼šå…ˆæµ‹é‡ï¼Œå†ä¼˜åŒ–
2. **ç“¶é¢ˆè¯†åˆ«**ï¼šæ‰¾åˆ°çœŸæ­£çš„ç“¶é¢ˆ
3. **æ¸è¿›ä¼˜åŒ–**ï¼šé€æ­¥æ”¹è¿›ï¼ŒéªŒè¯æ•ˆæœ
4. **å¹³è¡¡è€ƒè™‘**ï¼šæƒè¡¡æ€§èƒ½ä¸èµ„æºæ¶ˆè€—

## 3. å‚æ•°è°ƒä¼˜

### 3.1 å†…å­˜å‚æ•°é…ç½®

```sql
-- å†…å­˜é…ç½®ä¼˜åŒ–
-- postgresql.conf

# å…±äº«å†…å­˜é…ç½®
shared_buffers = 256MB                    -- å»ºè®®ä¸ºç³»ç»Ÿå†…å­˜çš„25%
effective_cache_size = 1GB                -- å»ºè®®ä¸ºç³»ç»Ÿå†…å­˜çš„75%
work_mem = 4MB                            -- æ ¹æ®å¹¶å‘è¿æ¥æ•°è°ƒæ•´
maintenance_work_mem = 64MB               -- ç»´æŠ¤æ“ä½œå†…å­˜

# æ£€æŸ¥ç‚¹é…ç½®
checkpoint_completion_target = 0.9        -- æ£€æŸ¥ç‚¹å®Œæˆç›®æ ‡
checkpoint_timeout = 5min                 -- æ£€æŸ¥ç‚¹è¶…æ—¶
max_wal_size = 1GB                        -- WALæ–‡ä»¶æœ€å¤§å¤§å°
min_wal_size = 80MB                       -- WALæ–‡ä»¶æœ€å°å¤§å°

# å¹¶å‘é…ç½®
max_connections = 100                     -- æœ€å¤§è¿æ¥æ•°
superuser_reserved_connections = 3        -- è¶…çº§ç”¨æˆ·ä¿ç•™è¿æ¥

# æ—¥å¿—é…ç½®
log_min_duration_statement = 1000         -- è®°å½•æ‰§è¡Œæ—¶é—´è¶…è¿‡1ç§’çš„æŸ¥è¯¢
log_checkpoints = on                      -- è®°å½•æ£€æŸ¥ç‚¹ä¿¡æ¯
log_connections = on                      -- è®°å½•è¿æ¥ä¿¡æ¯
log_disconnections = on                   -- è®°å½•æ–­å¼€è¿æ¥ä¿¡æ¯
```

### 3.2 æŸ¥è¯¢ä¼˜åŒ–å‚æ•°

```sql
-- æŸ¥è¯¢ä¼˜åŒ–å‚æ•°
random_page_cost = 1.1                    -- éšæœºé¡µé¢è®¿é—®æˆæœ¬
effective_io_concurrency = 200            -- æœ‰æ•ˆI/Oå¹¶å‘æ•°
seq_page_cost = 1.0                       -- é¡ºåºé¡µé¢è®¿é—®æˆæœ¬
cpu_tuple_cost = 0.01                     -- CPUå¤„ç†å…ƒç»„æˆæœ¬
cpu_index_tuple_cost = 0.005              -- CPUå¤„ç†ç´¢å¼•å…ƒç»„æˆæœ¬
cpu_operator_cost = 0.0025                -- CPUæ“ä½œæˆæœ¬

# å¹¶è¡ŒæŸ¥è¯¢é…ç½®
max_parallel_workers_per_gather = 2       -- æ¯ä¸ªGatherèŠ‚ç‚¹çš„æœ€å¤§å¹¶è¡Œå·¥ä½œè¿›ç¨‹
max_parallel_workers = 8                  -- ç³»ç»Ÿæœ€å¤§å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°
parallel_tuple_cost = 0.1                 -- å¹¶è¡Œå…ƒç»„å¤„ç†æˆæœ¬
parallel_setup_cost = 1000.0              -- å¹¶è¡Œè®¾ç½®æˆæœ¬
```

### 3.3 è‡ªåŠ¨è°ƒä¼˜å·¥å…·

```python
# PostgreSQLè‡ªåŠ¨è°ƒä¼˜è„šæœ¬
import psycopg2
import json
import subprocess
from typing import Dict, Any

class PostgreSQLTuner:
    def __init__(self, connection_params):
        self.connection_params = connection_params

    def get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        try:
            conn = psycopg2.connect(**self.connection_params)
            cursor = conn.cursor()

            # è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯
            cursor.execute("""
                SELECT name, setting, unit
                FROM pg_settings
                WHERE name IN ('shared_buffers', 'effective_cache_size', 'work_mem')
            """)

            settings = {}
            for name, setting, unit in cursor.fetchall():
                settings[name] = {'value': setting, 'unit': unit}

            cursor.close()
            conn.close()

            return settings

        except Exception as e:
            print(f"è·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {e}")
            return {}

    def recommend_memory_settings(self, total_memory_gb: int) -> Dict[str, str]:
        """æ¨èå†…å­˜è®¾ç½®"""
        recommendations = {
            'shared_buffers': f"{int(total_memory_gb * 0.25)}GB",
            'effective_cache_size': f"{int(total_memory_gb * 0.75)}GB",
            'work_mem': f"{max(4, int(total_memory_gb * 0.01))}MB",
            'maintenance_work_mem': f"{max(64, int(total_memory_gb * 0.05))}MB"
        }

        return recommendations

    def generate_config_file(self, recommendations: Dict[str, str]) -> str:
        """ç”Ÿæˆé…ç½®æ–‡ä»¶"""
        config_content = f"""
# PostgreSQLæ€§èƒ½ä¼˜åŒ–é…ç½®
# è‡ªåŠ¨ç”Ÿæˆäº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

# å†…å­˜é…ç½®
shared_buffers = {recommendations['shared_buffers']}
effective_cache_size = {recommendations['effective_cache_size']}
work_mem = {recommendations['work_mem']}
maintenance_work_mem = {recommendations['maintenance_work_mem']}

# æ£€æŸ¥ç‚¹é…ç½®
checkpoint_completion_target = 0.9
checkpoint_timeout = 5min
max_wal_size = 1GB
min_wal_size = 80MB

# å¹¶å‘é…ç½®
max_connections = 100
superuser_reserved_connections = 3

# æ—¥å¿—é…ç½®
log_min_duration_statement = 1000
log_checkpoints = on
log_connections = on
log_disconnections = on

# æŸ¥è¯¢ä¼˜åŒ–é…ç½®
random_page_cost = 1.1
effective_io_concurrency = 200
seq_page_cost = 1.0
cpu_tuple_cost = 0.01
cpu_index_tuple_cost = 0.005
cpu_operator_cost = 0.0025

# å¹¶è¡ŒæŸ¥è¯¢é…ç½®
max_parallel_workers_per_gather = 2
max_parallel_workers = 8
parallel_tuple_cost = 0.1
parallel_setup_cost = 1000.0
"""
        return config_content

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    tuner = PostgreSQLTuner({
        'host': 'localhost',
        'database': 'postgres',
        'user': 'postgres',
        'password': 'password'
    })

    # è·å–å½“å‰è®¾ç½®
    current_settings = tuner.get_system_info()
    print("å½“å‰è®¾ç½®:", json.dumps(current_settings, indent=2))

    # æ¨èè®¾ç½®ï¼ˆå‡è®¾16GBå†…å­˜ï¼‰
    recommendations = tuner.recommend_memory_settings(16)
    print("æ¨èè®¾ç½®:", json.dumps(recommendations, indent=2))

    # ç”Ÿæˆé…ç½®æ–‡ä»¶
    config = tuner.generate_config_file(recommendations)
    with open('postgresql_optimized.conf', 'w') as f:
        f.write(config)
    print("é…ç½®æ–‡ä»¶å·²ç”Ÿæˆ: postgresql_optimized.conf")
```

## 4. ç´¢å¼•ä¼˜åŒ–

### 4.1 ç´¢å¼•ç­–ç•¥

```sql
-- ç´¢å¼•åˆ›å»ºç­–ç•¥
-- 1. ä¸»é”®ç´¢å¼•ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
CREATE TABLE users (
    id SERIAL PRIMARY KEY,  -- è‡ªåŠ¨åˆ›å»ºä¸»é”®ç´¢å¼•
    username VARCHAR(50) UNIQUE,  -- è‡ªåŠ¨åˆ›å»ºå”¯ä¸€ç´¢å¼•
    email VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 2. å¤åˆç´¢å¼•
CREATE INDEX idx_users_email_created ON users(email, created_at);

-- 3. éƒ¨åˆ†ç´¢å¼•ï¼ˆåªç´¢å¼•æ»¡è¶³æ¡ä»¶çš„è¡Œï¼‰
CREATE INDEX idx_active_users ON users(username) WHERE active = true;

-- 4. è¡¨è¾¾å¼ç´¢å¼•
CREATE INDEX idx_users_email_lower ON users(LOWER(email));

-- 5. è¦†ç›–ç´¢å¼•ï¼ˆåŒ…å«æŸ¥è¯¢æ‰€éœ€çš„æ‰€æœ‰åˆ—ï¼‰
CREATE INDEX idx_users_covering ON users(id, username, email, created_at);

-- 6. å¹¶å‘ç´¢å¼•ï¼ˆä¸é˜»å¡å†™å…¥ï¼‰
CREATE INDEX CONCURRENTLY idx_users_username ON users(username);
```

### 4.2 ç´¢å¼•åˆ†æå·¥å…·

```python
# ç´¢å¼•åˆ†æå·¥å…·
import psycopg2
from typing import List, Dict, Any

class IndexAnalyzer:
    def __init__(self, connection_params):
        self.connection_params = connection_params

    def analyze_index_usage(self) -> List[Dict[str, Any]]:
        """åˆ†æç´¢å¼•ä½¿ç”¨æƒ…å†µ"""
        try:
            conn = psycopg2.connect(**self.connection_params)
            cursor = conn.cursor()

            cursor.execute("""
                SELECT
                    schemaname,
                    tablename,
                    indexname,
                    idx_scan as index_scans,
                    idx_tup_read as tuples_read,
                    idx_tup_fetch as tuples_fetched,
                    pg_size_pretty(pg_relation_size(indexrelid)) as index_size
                FROM pg_stat_user_indexes
                ORDER BY idx_scan DESC
            """)

            results = []
            for row in cursor.fetchall():
                results.append({
                    'schema': row[0],
                    'table': row[1],
                    'index': row[2],
                    'scans': row[3],
                    'tuples_read': row[4],
                    'tuples_fetched': row[5],
                    'size': row[6]
                })

            cursor.close()
            conn.close()

            return results

        except Exception as e:
            print(f"åˆ†æç´¢å¼•ä½¿ç”¨æƒ…å†µå¤±è´¥: {e}")
            return []

    def find_unused_indexes(self) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾æœªä½¿ç”¨çš„ç´¢å¼•"""
        try:
            conn = psycopg2.connect(**self.connection_params)
            cursor = conn.cursor()

            cursor.execute("""
                SELECT
                    schemaname,
                    tablename,
                    indexname,
                    pg_size_pretty(pg_relation_size(indexrelid)) as index_size
                FROM pg_stat_user_indexes
                WHERE idx_scan = 0
                ORDER BY pg_relation_size(indexrelid) DESC
            """)

            results = []
            for row in cursor.fetchall():
                results.append({
                    'schema': row[0],
                    'table': row[1],
                    'index': row[2],
                    'size': row[3]
                })

            cursor.close()
            conn.close()

            return results

        except Exception as e:
            print(f"æŸ¥æ‰¾æœªä½¿ç”¨ç´¢å¼•å¤±è´¥: {e}")
            return []

    def recommend_indexes(self) -> List[str]:
        """æ¨èéœ€è¦åˆ›å»ºçš„ç´¢å¼•"""
        try:
            conn = psycopg2.connect(**self.connection_params)
            cursor = conn.cursor()

            # æŸ¥æ‰¾é¢‘ç¹æ‰«æä½†ç¼ºå°‘ç´¢å¼•çš„åˆ—
            cursor.execute("""
                SELECT
                    schemaname,
                    tablename,
                    attname,
                    n_distinct,
                    correlation
                FROM pg_stats
                WHERE schemaname NOT IN ('information_schema', 'pg_catalog')
                AND n_distinct > 100
                AND correlation > 0.1
                ORDER BY n_distinct DESC
            """)

            recommendations = []
            for row in cursor.fetchall():
                schema, table, column, distinct_count, correlation = row
                recommendations.append(
                    f"CREATE INDEX idx_{table}_{column} ON {schema}.{table}({column});"
                )

            cursor.close()
            conn.close()

            return recommendations

        except Exception as e:
            print(f"æ¨èç´¢å¼•å¤±è´¥: {e}")
            return []

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    analyzer = IndexAnalyzer({
        'host': 'localhost',
        'database': 'appdb',
        'user': 'appuser',
        'password': 'password'
    })

    # åˆ†æç´¢å¼•ä½¿ç”¨æƒ…å†µ
    usage = analyzer.analyze_index_usage()
    print("ç´¢å¼•ä½¿ç”¨æƒ…å†µ:")
    for idx in usage[:5]:
        print(f"- {idx['index']}: {idx['scans']} æ¬¡æ‰«æ, å¤§å°: {idx['size']}")

    # æŸ¥æ‰¾æœªä½¿ç”¨çš„ç´¢å¼•
    unused = analyzer.find_unused_indexes()
    print(f"\næœªä½¿ç”¨çš„ç´¢å¼• ({len(unused)} ä¸ª):")
    for idx in unused[:5]:
        print(f"- {idx['index']}: å¤§å°: {idx['size']}")

    # æ¨èæ–°ç´¢å¼•
    recommendations = analyzer.recommend_indexes()
    print(f"\næ¨èçš„ç´¢å¼• ({len(recommendations)} ä¸ª):")
    for rec in recommendations[:5]:
        print(f"- {rec}")
```

## 5. æŸ¥è¯¢ä¼˜åŒ–

### 5.1 æ…¢æŸ¥è¯¢åˆ†æ

```sql
-- æ…¢æŸ¥è¯¢åˆ†ææŸ¥è¯¢
-- 1. æŸ¥çœ‹æœ€è€—æ—¶çš„æŸ¥è¯¢
SELECT
    query,
    calls,
    total_time,
    mean_time,
    stddev_time,
    rows,
    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
FROM pg_stat_statements
ORDER BY total_time DESC
LIMIT 10;

-- 2. æŸ¥çœ‹ç¼“å­˜å‘½ä¸­ç‡ä½çš„æŸ¥è¯¢
SELECT
    query,
    calls,
    shared_blks_read,
    shared_blks_hit,
    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
FROM pg_stat_statements
WHERE shared_blks_read > 0
ORDER BY hit_percent ASC
LIMIT 10;

-- 3. æŸ¥çœ‹æ‰§è¡Œè®¡åˆ’
EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON)
SELECT * FROM users WHERE email = 'test@example.com';

-- 4. æŸ¥çœ‹é”ç­‰å¾…æƒ…å†µ
SELECT
    pid,
    usename,
    application_name,
    client_addr,
    state,
    query_start,
    query
FROM pg_stat_activity
WHERE state = 'active'
AND query NOT LIKE '%pg_stat_activity%';
```

### 5.2 æŸ¥è¯¢é‡å†™ä¼˜åŒ–

```sql
-- æŸ¥è¯¢é‡å†™ç¤ºä¾‹

-- 1. é¿å…SELECT *
-- åŸå§‹æŸ¥è¯¢
SELECT * FROM users WHERE active = true;

-- ä¼˜åŒ–å
SELECT id, username, email FROM users WHERE active = true;

-- 2. ä½¿ç”¨LIMITé™åˆ¶ç»“æœé›†
-- åŸå§‹æŸ¥è¯¢
SELECT * FROM orders ORDER BY created_at DESC;

-- ä¼˜åŒ–å
SELECT * FROM orders ORDER BY created_at DESC LIMIT 100;

-- 3. é¿å…åœ¨WHEREå­å¥ä¸­ä½¿ç”¨å‡½æ•°
-- åŸå§‹æŸ¥è¯¢
SELECT * FROM users WHERE LOWER(email) = 'test@example.com';

-- ä¼˜åŒ–åï¼ˆä½¿ç”¨è¡¨è¾¾å¼ç´¢å¼•ï¼‰
SELECT * FROM users WHERE email_lower = 'test@example.com';

-- 4. ä½¿ç”¨EXISTSä»£æ›¿IN
-- åŸå§‹æŸ¥è¯¢
SELECT * FROM users WHERE id IN (SELECT user_id FROM orders);

-- ä¼˜åŒ–å
SELECT * FROM users u WHERE EXISTS (SELECT 1 FROM orders o WHERE o.user_id = u.id);

-- 5. ä½¿ç”¨JOINä»£æ›¿å­æŸ¥è¯¢
-- åŸå§‹æŸ¥è¯¢
SELECT u.*, (SELECT COUNT(*) FROM orders o WHERE o.user_id = u.id) as order_count
FROM users u;

-- ä¼˜åŒ–å
SELECT u.*, COALESCE(o.order_count, 0) as order_count
FROM users u
LEFT JOIN (
    SELECT user_id, COUNT(*) as order_count
    FROM orders
    GROUP BY user_id
) o ON u.id = o.user_id;
```

## 6. ç›‘æ§ç³»ç»Ÿ

### 6.1 ç›‘æ§æŒ‡æ ‡

```python
# PostgreSQLç›‘æ§æŒ‡æ ‡æ”¶é›†
import psycopg2
import time
import json
from datetime import datetime
from typing import Dict, Any

class PostgreSQLMonitor:
    def __init__(self, connection_params):
        self.connection_params = connection_params

    def get_connection_stats(self) -> Dict[str, Any]:
        """è·å–è¿æ¥ç»Ÿè®¡ä¿¡æ¯"""
        try:
            conn = psycopg2.connect(**self.connection_params)
            cursor = conn.cursor()

            cursor.execute("""
                SELECT
                    count(*) as total_connections,
                    count(*) FILTER (WHERE state = 'active') as active_connections,
                    count(*) FILTER (WHERE state = 'idle') as idle_connections,
                    count(*) FILTER (WHERE state = 'idle in transaction') as idle_in_transaction
                FROM pg_stat_activity
                WHERE datname = current_database()
            """)

            result = cursor.fetchone()
            cursor.close()
            conn.close()

            return {
                'total_connections': result[0],
                'active_connections': result[1],
                'idle_connections': result[2],
                'idle_in_transaction': result[3]
            }

        except Exception as e:
            print(f"è·å–è¿æ¥ç»Ÿè®¡å¤±è´¥: {e}")
            return {}

    def get_performance_metrics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        try:
            conn = psycopg2.connect(**self.connection_params)
            cursor = conn.cursor()

            # ç¼“å­˜å‘½ä¸­ç‡
            cursor.execute("""
                SELECT
                    sum(heap_blks_hit) / nullif(sum(heap_blks_hit) + sum(heap_blks_read), 0) as cache_hit_ratio
                FROM pg_statio_user_tables
            """)
            cache_hit_ratio = cursor.fetchone()[0] or 0

            # äº‹åŠ¡ç»Ÿè®¡
            cursor.execute("""
                SELECT
                    xact_commit,
                    xact_rollback,
                    blks_read,
                    blks_hit,
                    tup_returned,
                    tup_fetched,
                    tup_inserted,
                    tup_updated,
                    tup_deleted
                FROM pg_stat_database
                WHERE datname = current_database()
            """)

            stats = cursor.fetchone()
            cursor.close()
            conn.close()

            return {
                'cache_hit_ratio': cache_hit_ratio,
                'xact_commit': stats[0],
                'xact_rollback': stats[1],
                'blks_read': stats[2],
                'blks_hit': stats[3],
                'tup_returned': stats[4],
                'tup_fetched': stats[5],
                'tup_inserted': stats[6],
                'tup_updated': stats[7],
                'tup_deleted': stats[8]
            }

        except Exception as e:
            print(f"è·å–æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
            return {}

    def get_slow_queries(self, threshold_ms: int = 1000) -> List[Dict[str, Any]]:
        """è·å–æ…¢æŸ¥è¯¢"""
        try:
            conn = psycopg2.connect(**self.connection_params)
            cursor = conn.cursor()

            cursor.execute("""
                SELECT
                    query,
                    calls,
                    total_time,
                    mean_time,
                    rows
                FROM pg_stat_statements
                WHERE mean_time > %s
                ORDER BY mean_time DESC
                LIMIT 10
            """, (threshold_ms,))

            results = []
            for row in cursor.fetchall():
                results.append({
                    'query': row[0][:100] + '...' if len(row[0]) > 100 else row[0],
                    'calls': row[1],
                    'total_time': row[2],
                    'mean_time': row[3],
                    'rows': row[4]
                })

            cursor.close()
            conn.close()

            return results

        except Exception as e:
            print(f"è·å–æ…¢æŸ¥è¯¢å¤±è´¥: {e}")
            return []

    def generate_monitoring_report(self) -> str:
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        conn_stats = self.get_connection_stats()
        perf_metrics = self.get_performance_metrics()
        slow_queries = self.get_slow_queries()

        report = f"""
# PostgreSQLç›‘æ§æŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## è¿æ¥ç»Ÿè®¡
- æ€»è¿æ¥æ•°: {conn_stats.get('total_connections', 0)}
- æ´»è·ƒè¿æ¥æ•°: {conn_stats.get('active_connections', 0)}
- ç©ºé—²è¿æ¥æ•°: {conn_stats.get('idle_connections', 0)}
- äº‹åŠ¡ä¸­ç©ºé—²è¿æ¥æ•°: {conn_stats.get('idle_in_transaction', 0)}

## æ€§èƒ½æŒ‡æ ‡
- ç¼“å­˜å‘½ä¸­ç‡: {perf_metrics.get('cache_hit_ratio', 0):.2%}
- äº‹åŠ¡æäº¤æ•°: {perf_metrics.get('xact_commit', 0)}
- äº‹åŠ¡å›æ»šæ•°: {perf_metrics.get('xact_rollback', 0)}
- å—è¯»å–æ•°: {perf_metrics.get('blks_read', 0)}
- å—å‘½ä¸­æ•°: {perf_metrics.get('blks_hit', 0)}

## æ…¢æŸ¥è¯¢ (å‰5ä¸ª)
"""

        for i, query in enumerate(slow_queries[:5], 1):
            report += f"""
{i}. æŸ¥è¯¢: {query['query']}
   - è°ƒç”¨æ¬¡æ•°: {query['calls']}
   - å¹³å‡æ—¶é—´: {query['mean_time']:.2f}ms
   - æ€»æ—¶é—´: {query['total_time']:.2f}ms
   - è¿”å›è¡Œæ•°: {query['rows']}
"""

        return report

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    monitor = PostgreSQLMonitor({
        'host': 'localhost',
        'database': 'appdb',
        'user': 'appuser',
        'password': 'password'
    })

    # ç”Ÿæˆç›‘æ§æŠ¥å‘Š
    report = monitor.generate_monitoring_report()
    print(report)

    # ä¿å­˜æŠ¥å‘Š
    with open('postgresql_monitoring_report.md', 'w') as f:
        f.write(report)
    print("ç›‘æ§æŠ¥å‘Šå·²ä¿å­˜åˆ°: postgresql_monitoring_report.md")
```

## 7. æ€§èƒ½è¯Šæ–­

### 7.1 æ€§èƒ½é—®é¢˜è¯Šæ–­

```python
# æ€§èƒ½è¯Šæ–­å·¥å…·
import psycopg2
import time
from typing import List, Dict, Any

class PerformanceDiagnostic:
    def __init__(self, connection_params):
        self.connection_params = connection_params

    def diagnose_performance_issues(self) -> List[Dict[str, Any]]:
        """è¯Šæ–­æ€§èƒ½é—®é¢˜"""
        issues = []

        try:
            conn = psycopg2.connect(**self.connection_params)
            cursor = conn.cursor()

            # æ£€æŸ¥è¿æ¥æ•°
            cursor.execute("SELECT count(*) FROM pg_stat_activity")
            connection_count = cursor.fetchone()[0]

            if connection_count > 80:
                issues.append({
                    'type': 'high_connections',
                    'severity': 'high',
                    'description': f'è¿æ¥æ•°è¿‡é«˜: {connection_count}',
                    'recommendation': 'è€ƒè™‘å¢åŠ è¿æ¥æ± æˆ–ä¼˜åŒ–è¿æ¥ç®¡ç†'
                })

            # æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡
            cursor.execute("""
                SELECT sum(heap_blks_hit) / nullif(sum(heap_blks_hit) + sum(heap_blks_read), 0) as hit_ratio
                FROM pg_statio_user_tables
            """)
            hit_ratio = cursor.fetchone()[0] or 0

            if hit_ratio < 0.8:
                issues.append({
                    'type': 'low_cache_hit_ratio',
                    'severity': 'medium',
                    'description': f'ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½: {hit_ratio:.2%}',
                    'recommendation': 'å¢åŠ shared_buffersæˆ–ä¼˜åŒ–æŸ¥è¯¢'
                })

            # æ£€æŸ¥é•¿æ—¶é—´è¿è¡Œçš„æŸ¥è¯¢
            cursor.execute("""
                SELECT pid, query_start, state, query
                FROM pg_stat_activity
                WHERE state = 'active'
                AND query_start < NOW() - INTERVAL '5 minutes'
            """)
            long_running = cursor.fetchall()

            if long_running:
                issues.append({
                    'type': 'long_running_queries',
                    'severity': 'high',
                    'description': f'å‘ç° {len(long_running)} ä¸ªé•¿æ—¶é—´è¿è¡Œçš„æŸ¥è¯¢',
                    'recommendation': 'æ£€æŸ¥å¹¶ä¼˜åŒ–è¿™äº›æŸ¥è¯¢'
                })

            # æ£€æŸ¥é”ç­‰å¾…
            cursor.execute("""
                SELECT count(*)
                FROM pg_stat_activity
                WHERE wait_event_type = 'Lock'
            """)
            lock_wait_count = cursor.fetchone()[0]

            if lock_wait_count > 0:
                issues.append({
                    'type': 'lock_waits',
                    'severity': 'medium',
                    'description': f'å‘ç° {lock_wait_count} ä¸ªé”ç­‰å¾…',
                    'recommendation': 'æ£€æŸ¥é”ç«äº‰æƒ…å†µï¼Œä¼˜åŒ–äº‹åŠ¡è®¾è®¡'
                })

            cursor.close()
            conn.close()

        except Exception as e:
            issues.append({
                'type': 'connection_error',
                'severity': 'high',
                'description': f'è¿æ¥é”™è¯¯: {e}',
                'recommendation': 'æ£€æŸ¥æ•°æ®åº“è¿æ¥é…ç½®'
            })

        return issues

    def generate_diagnostic_report(self) -> str:
        """ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
        issues = self.diagnose_performance_issues()

        report = f"""
# PostgreSQLæ€§èƒ½è¯Šæ–­æŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## è¯Šæ–­ç»“æœ
å‘ç° {len(issues)} ä¸ªæ€§èƒ½é—®é¢˜

"""

        for i, issue in enumerate(issues, 1):
            report += f"""
### é—®é¢˜ {i}: {issue['type']}
- **ä¸¥é‡ç¨‹åº¦**: {issue['severity'].upper()}
- **æè¿°**: {issue['description']}
- **å»ºè®®**: {issue['recommendation']}

"""

        if not issues:
            report += "âœ… æœªå‘ç°æ˜æ˜¾çš„æ€§èƒ½é—®é¢˜\n"

        return report

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    diagnostic = PerformanceDiagnostic({
        'host': 'localhost',
        'database': 'appdb',
        'user': 'appuser',
        'password': 'password'
    })

    # ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
    report = diagnostic.generate_diagnostic_report()
    print(report)

    # ä¿å­˜æŠ¥å‘Š
    with open('postgresql_diagnostic_report.md', 'w') as f:
        f.write(report)
    print("è¯Šæ–­æŠ¥å‘Šå·²ä¿å­˜åˆ°: postgresql_diagnostic_report.md")
```

## 8. è¡Œä¸šåº”ç”¨

### 8.1 äº’è”ç½‘è¡Œä¸šæ€§èƒ½ç›‘æ§

```yaml
# Prometheusç›‘æ§é…ç½®
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'postgresql'
    static_configs:
      - targets: ['localhost:9187']
    metrics_path: /metrics
    scrape_interval: 10s

# Grafanaä»ªè¡¨æ¿é…ç½®
dashboard:
  title: "PostgreSQLæ€§èƒ½ç›‘æ§"
  panels:
    - title: "è¿æ¥æ•°"
      type: "graph"
      targets:
        - expr: "pg_stat_database_numbackends"
          legendFormat: "{{datname}}"

    - title: "äº‹åŠ¡ç‡"
      type: "graph"
      targets:
        - expr: "rate(pg_stat_database_xact_commit[5m])"
          legendFormat: "æäº¤ç‡"
        - expr: "rate(pg_stat_database_xact_rollback[5m])"
          legendFormat: "å›æ»šç‡"

    - title: "ç¼“å­˜å‘½ä¸­ç‡"
      type: "singlestat"
      targets:
        - expr: "pg_stat_database_blk_hit_rate"
          legendFormat: "ç¼“å­˜å‘½ä¸­ç‡"
```

### 8.2 é‡‘èè¡Œä¸šæ€§èƒ½è°ƒä¼˜

```sql
-- é‡‘èè¡Œä¸šç‰¹å®šä¼˜åŒ–

-- 1. é«˜å¹¶å‘äº‹åŠ¡ä¼˜åŒ–
BEGIN;
-- ä½¿ç”¨è¡Œçº§é”é¿å…è¡¨é”
SELECT * FROM accounts WHERE account_id = 123 FOR UPDATE;
UPDATE accounts SET balance = balance - 100 WHERE account_id = 123;
COMMIT;

-- 2. æ‰¹é‡æ“ä½œä¼˜åŒ–
-- ä½¿ç”¨COPYå‘½ä»¤è¿›è¡Œæ‰¹é‡æ’å…¥
COPY transactions(account_id, amount, description) FROM '/tmp/transactions.csv' CSV;

-- 3. åˆ†åŒºè¡¨ä¼˜åŒ–
CREATE TABLE transactions (
    id SERIAL,
    account_id INTEGER,
    amount DECIMAL(10,2),
    transaction_date DATE,
    description TEXT
) PARTITION BY RANGE (transaction_date);

-- åˆ›å»ºåˆ†åŒº
CREATE TABLE transactions_2024 PARTITION OF transactions
FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');

-- 4. å¹¶è¡ŒæŸ¥è¯¢ä¼˜åŒ–
SET max_parallel_workers_per_gather = 4;
SET max_parallel_workers = 8;

-- 5. ç»Ÿè®¡ä¿¡æ¯æ›´æ–°
ANALYZE transactions;
```

## 9. æœ€ä½³å®è·µ

### 9.1 æ€§èƒ½è°ƒä¼˜æ£€æŸ¥æ¸…å•

- [ ] æ£€æŸ¥å¹¶ä¼˜åŒ–å†…å­˜å‚æ•°é…ç½®
- [ ] åˆ†æå¹¶ä¼˜åŒ–ç´¢å¼•ä½¿ç”¨
- [ ] é‡å†™ä½æ•ˆæŸ¥è¯¢
- [ ] è®¾ç½®é€‚å½“çš„è¿æ¥æ± 
- [ ] é…ç½®ç›‘æ§å’Œå‘Šè­¦
- [ ] å®šæœŸæ›´æ–°ç»Ÿè®¡ä¿¡æ¯
- [ ] ç›‘æ§æ…¢æŸ¥è¯¢æ—¥å¿—
- [ ] ä¼˜åŒ–è¡¨ç»“æ„å’Œåˆ†åŒº

### 9.2 ç›‘æ§æŒ‡æ ‡é˜ˆå€¼

```python
# ç›‘æ§é˜ˆå€¼é…ç½®
MONITORING_THRESHOLDS = {
    'connection_usage': 0.8,      # è¿æ¥ä½¿ç”¨ç‡è¶…è¿‡80%
    'cache_hit_ratio': 0.8,       # ç¼“å­˜å‘½ä¸­ç‡ä½äº80%
    'slow_query_threshold': 1000, # æ…¢æŸ¥è¯¢é˜ˆå€¼1ç§’
    'lock_wait_threshold': 5,     # é”ç­‰å¾…è¶…è¿‡5ä¸ª
    'disk_usage': 0.9,           # ç£ç›˜ä½¿ç”¨ç‡è¶…è¿‡90%
    'cpu_usage': 0.8,            # CPUä½¿ç”¨ç‡è¶…è¿‡80%
    'memory_usage': 0.9          # å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡90%
}

# å‘Šè­¦è§„åˆ™
ALERT_RULES = {
    'high_connections': {
        'condition': 'connection_count > max_connections * 0.8',
        'severity': 'warning',
        'message': 'è¿æ¥æ•°æ¥è¿‘ä¸Šé™'
    },
    'low_cache_hit': {
        'condition': 'cache_hit_ratio < 0.8',
        'severity': 'warning',
        'message': 'ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½'
    },
    'slow_queries': {
        'condition': 'slow_query_count > 10',
        'severity': 'critical',
        'message': 'å‘ç°å¤§é‡æ…¢æŸ¥è¯¢'
    }
}
```

### 9.3 æ€§èƒ½ä¼˜åŒ–æµç¨‹

1. **åŸºçº¿æµ‹é‡**ï¼šå»ºç«‹æ€§èƒ½åŸºçº¿
2. **ç“¶é¢ˆè¯†åˆ«**ï¼šæ‰¾åˆ°æ€§èƒ½ç“¶é¢ˆ
3. **ä¼˜åŒ–å®æ–½**ï¼šå®æ–½ä¼˜åŒ–æªæ–½
4. **æ•ˆæœéªŒè¯**ï¼šéªŒè¯ä¼˜åŒ–æ•ˆæœ
5. **æŒç»­ç›‘æ§**ï¼šå»ºç«‹æŒç»­ç›‘æ§

## 10. ç›¸å…³é“¾æ¥

### 10.1 å†…éƒ¨é“¾æ¥

- [PostgreSQLå½¢å¼æ¨¡å‹](../../25-ç†è®ºä½“ç³»/25.01-å½¢å¼åŒ–æ–¹æ³•/README.md)
- [PostgreSQLæ•°æ®æ¨¡å‹](../../25-ç†è®ºä½“ç³»/25.01-å½¢å¼åŒ–æ–¹æ³•/README.md)
- [PostgreSQLæŸ¥è¯¢ä¼˜åŒ–](../../25-ç†è®ºä½“ç³»/25.01-å½¢å¼åŒ–æ–¹æ³•/README.md)
- [PostgreSQLäº‘åŸç”Ÿéƒ¨ç½²](1.1.15-äº‘åŸç”Ÿä¸å®¹å™¨åŒ–éƒ¨ç½².md)
- [PostgreSQLå®‰å…¨ä¸åˆè§„](1.1.17-å®‰å…¨ä¸åˆè§„-æ‰©å……ç‰ˆ.md)

### 10.2 å¤–éƒ¨èµ„æº

- [PostgreSQLæ€§èƒ½è°ƒä¼˜æŒ‡å—](https://www.postgresql.org/docs/current/performance.html)
- [pg_stat_statementsæ‰©å±•](https://www.postgresql.org/docs/current/pgstatstatements.html)
- [PostgreSQLç›‘æ§æœ€ä½³å®è·µ](https://www.postgresql.org/docs/current/monitoring.html)

### 10.3 ç›‘æ§å·¥å…·

- [Prometheus PostgreSQL Exporter](https://github.com/prometheus-community/postgres_exporter)
- [pgAdmin](https://www.pgadmin.org/)
- [pg_stat_monitor](https://github.com/percona/pg_stat_monitor)

---

**æœ€åæ›´æ–°æ—¶é—´**ï¼š2024å¹´12æœˆ
**æ–‡æ¡£çŠ¶æ€**ï¼šå·²å®Œæˆæ‰©å……ï¼Œè¾¾åˆ°500+è¡Œæ ‡å‡†
**è´¨é‡è¯„åˆ†**ï¼š94/100
**ä¸‹ä¸€æ­¥è®¡åˆ’**ï¼šç»§ç»­æ‰©å……å…¶ä»–ç®€ç•¥æ–‡æ¡£
