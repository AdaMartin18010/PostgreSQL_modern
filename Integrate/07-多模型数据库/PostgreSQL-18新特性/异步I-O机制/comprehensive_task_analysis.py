#!/usr/bin/env python3
"""
å…¨é¢é€’å½’ä»»åŠ¡åˆ†æå·¥å…·
æ£€æŸ¥æ‰€æœ‰æ–‡æ¡£çš„å®Œæ•´æ€§ã€æ ¼å¼ã€å†…å®¹è´¨é‡
"""
import os
import re
from pathlib import Path
from collections import defaultdict

class ComprehensiveTaskAnalyzer:
    def __init__(self, root_dir):
        self.root_dir = Path(root_dir)
        self.issues = defaultdict(list)
        self.stats = {
            'total_docs': 0,
            'docs_with_toc': 0,
            'docs_without_toc': 0,
            'docs_with_nested_toc': 0,
            'docs_with_toc_mismatch': 0,
            'docs_with_placeholder': 0,
            'docs_with_unclosed_code': 0,
            'docs_without_h2': 0,
            'docs_with_broken_links': 0,
            'short_docs': 0,
        }

    def analyze_document(self, file_path):
        """åˆ†æå•ä¸ªæ–‡æ¡£"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.split('\n')
        except Exception as e:
            self.issues['read_errors'].append(f"{file_path}: {e}")
            return

        self.stats['total_docs'] += 1
        rel_path = str(file_path.relative_to(self.root_dir))

        # æ£€æŸ¥æ–‡æ¡£é•¿åº¦
        if len(lines) < 50:
            self.stats['short_docs'] += 1
            self.issues['short_docs'].append(rel_path)

        # æ£€æŸ¥TOC
        toc_pattern = r'^##\s*ğŸ“‘\s*ç›®å½•'
        has_toc = bool(re.search(toc_pattern, content, re.MULTILINE))

        if has_toc:
            self.stats['docs_with_toc'] += 1
            # æ£€æŸ¥åµŒå¥—TOC
            toc_lines = []
            in_toc = False
            for i, line in enumerate(lines):
                if re.match(toc_pattern, line):
                    in_toc = True
                elif in_toc:
                    if line.strip().startswith('-'):
                        toc_lines.append(line)
                    elif line.strip() == '' or line.startswith('#'):
                        if line.startswith('##'):
                            break
                        if line.strip() == '' and toc_lines:
                            continue
                        if not line.strip():
                            continue
                    if line.startswith('---'):
                        break

            # æ£€æŸ¥åµŒå¥—å±‚çº§
            nested = False
            for line in toc_lines:
                if '  -' in line or '    -' in line:
                    nested = True
                    break

            if nested:
                self.stats['docs_with_nested_toc'] += 1
                self.issues['nested_toc'].append(rel_path)

            # æ£€æŸ¥TOCé¡¹æ•°ä¸H3æ ‡é¢˜æ•°åŒ¹é…
            toc_items = len([l for l in toc_lines if l.strip().startswith('-')])
            h3_count = len(re.findall(r'^###\s+', content, re.MULTILINE))

            if toc_items != h3_count:
                self.stats['docs_with_toc_mismatch'] += 1
                self.issues['toc_mismatch'].append(f"{rel_path}: TOCé¡¹æ•°={toc_items}, H3æ•°={h3_count}")
        else:
            self.stats['docs_without_toc'] += 1
            self.issues['no_toc'].append(rel_path)

        # æ£€æŸ¥å ä½ç¬¦å†…å®¹
        if '*æœ¬èŠ‚å†…å®¹å¾…è¡¥å……*' in content or '*å¾…è¡¥å……*' in content or 'TODO' in content.upper():
            self.stats['docs_with_placeholder'] += 1
            self.issues['placeholder'].append(rel_path)

        # æ£€æŸ¥æœªé—­åˆçš„ä»£ç å—
        code_block_count = content.count('```')
        if code_block_count % 2 != 0:
            self.stats['docs_with_unclosed_code'] += 1
            self.issues['unclosed_code'].append(rel_path)

        # æ£€æŸ¥H2æ ‡é¢˜
        h2_pattern = r'^##\s+\d+\.'
        has_h2 = bool(re.search(h2_pattern, content, re.MULTILINE))
        if not has_h2 and self.stats['total_docs'] > 1:  # æ’é™¤README.md
            # æ£€æŸ¥æ˜¯å¦æœ‰ç« èŠ‚æ ‡é¢˜æ ¼å¼
            chapter_pattern = r'^##\s+[^\d]'
            has_chapter = bool(re.search(chapter_pattern, content, re.MULTILINE))
            if not has_chapter:
                self.stats['docs_without_h2'] += 1
                self.issues['no_h2'].append(rel_path)

        # æ£€æŸ¥é“¾æ¥
        link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
        links = re.findall(link_pattern, content)
        for text, url in links:
            if url.startswith('#'):
                # å†…éƒ¨é”šç‚¹é“¾æ¥
                anchor = url[1:].lower().replace(' ', '-')
                if anchor not in content.lower():
                    self.stats['docs_with_broken_links'] += 1
                    self.issues['broken_links'].append(f"{rel_path}: {text} -> {url}")
                    break

    def scan_directory(self, directory):
        """é€’å½’æ‰«æç›®å½•"""
        for root, dirs, files in os.walk(directory):
            # è·³è¿‡éšè—ç›®å½•å’Œç‰¹å®šç›®å½•
            dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']

            for file in files:
                if file.endswith('.md') and file != 'README.md' or file == 'README.md':
                    file_path = Path(root) / file
                    self.analyze_document(file_path)

    def generate_report(self):
        """ç”ŸæˆæŠ¥å‘Š"""
        print("=" * 70)
        print("ğŸ” å…¨é¢ä»»åŠ¡åˆ†ææŠ¥å‘Š")
        print("=" * 70)
        print()
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  æ€»æ–‡æ¡£æ•°: {self.stats['total_docs']}")
        print(f"  æœ‰ç›®å½•çš„æ–‡æ¡£: {self.stats['docs_with_toc']}")
        print(f"  æ— ç›®å½•çš„æ–‡æ¡£: {self.stats['docs_without_toc']}")
        print(f"  åµŒå¥—ç›®å½•çš„æ–‡æ¡£: {self.stats['docs_with_nested_toc']}")
        print(f"  ç›®å½•é¡¹ä¸åŒ¹é…çš„æ–‡æ¡£: {self.stats['docs_with_toc_mismatch']}")
        print(f"  æœ‰å ä½ç¬¦çš„æ–‡æ¡£: {self.stats['docs_with_placeholder']}")
        print(f"  æœªé—­åˆä»£ç å—çš„æ–‡æ¡£: {self.stats['docs_with_unclosed_code']}")
        print(f"  ç¼ºå°‘H2æ ‡é¢˜çš„æ–‡æ¡£: {self.stats['docs_without_h2']}")
        print(f"  æœ‰ broken é“¾æ¥çš„æ–‡æ¡£: {self.stats['docs_with_broken_links']}")
        print(f"  çŸ­æ–‡æ¡£(<50è¡Œ): {self.stats['short_docs']}")
        print()

        total_issues = sum(len(v) for v in self.issues.values())
        print(f"ğŸ“‹ é—®é¢˜ç»Ÿè®¡:")
        print(f"  æ€»é—®é¢˜æ•°: {total_issues}")
        print()

        for issue_type, items in sorted(self.issues.items()):
            if items:
                print(f"âš ï¸  {issue_type} ({len(items)}ä¸ª):")
                for item in items[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                    print(f"     - {item}")
                if len(items) > 10:
                    print(f"     ... è¿˜æœ‰ {len(items) - 10} ä¸ª")
                print()

        print("=" * 70)

if __name__ == '__main__':
    # åˆ†æå¼‚æ­¥I/Oæœºåˆ¶æ–‡æ¡£
    async_io_dir = Path(__file__).parent
    analyzer = ComprehensiveTaskAnalyzer(async_io_dir)
    analyzer.scan_directory(async_io_dir)
    analyzer.generate_report()

    print("\n" + "=" * 70)
    print("ğŸ” æ‰©å±•åˆ†æï¼šæŠ€æœ¯åŸç†ç›®å½•")
    print("=" * 70)

    # åˆ†ææŠ€æœ¯åŸç†ç›®å½•
    tech_dir = async_io_dir.parent.parent / 'æŠ€æœ¯åŸç†'
    if tech_dir.exists():
        tech_analyzer = ComprehensiveTaskAnalyzer(tech_dir.parent)
        tech_analyzer.scan_directory(tech_dir)
        tech_analyzer.generate_report()

    print("\n" + "=" * 70)
    print("ğŸ” æ‰©å±•åˆ†æï¼šå›¾å‘é‡æ··åˆæ£€ç´¢ç›®å½•")
    print("=" * 70)

    # åˆ†æå›¾å‘é‡æ··åˆæ£€ç´¢ç›®å½•
    graph_dir = async_io_dir.parent.parent / 'å›¾å‘é‡æ··åˆæ£€ç´¢'
    if graph_dir.exists():
        graph_analyzer = ComprehensiveTaskAnalyzer(graph_dir.parent)
        graph_analyzer.scan_directory(graph_dir)
        graph_analyzer.generate_report()
