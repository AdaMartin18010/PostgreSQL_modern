# æ—¶åºæ•°æ®æ¨¡å‹

> **åˆ›å»ºæ—¥æœŸ**: 2025å¹´1æœˆ
> **æ¥æº**: æ—¶åºæ•°æ®åº“ç†è®º
> **çŠ¶æ€**: âœ… å·²å®Œæˆ
> **æ–‡æ¡£ç¼–å·**: 06-02

---

## ğŸ“‘ ç›®å½•

- [æ—¶åºæ•°æ®æ¨¡å‹](#æ—¶åºæ•°æ®æ¨¡å‹)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¦‚è¿°](#1-æ¦‚è¿°)
  - [1.1 ç†è®ºåŸºç¡€](#11-ç†è®ºåŸºç¡€)
    - [1.1.1 æ—¶åºæ•°æ®åŸºæœ¬æ¦‚å¿µ](#111-æ—¶åºæ•°æ®åŸºæœ¬æ¦‚å¿µ)
    - [1.1.2 æ—¶åºæ•°æ®æ¨¡å‹ç†è®º](#112-æ—¶åºæ•°æ®æ¨¡å‹ç†è®º)
    - [1.1.3 æ—¶åºæ•°æ®å‹ç¼©ç†è®º](#113-æ—¶åºæ•°æ®å‹ç¼©ç†è®º)
    - [1.1.4 æ—¶åºæ•°æ®æŸ¥è¯¢ç†è®º](#114-æ—¶åºæ•°æ®æŸ¥è¯¢ç†è®º)
    - [1.1.5 æ—¶åºæ•°æ®åˆ†åŒºç†è®º](#115-æ—¶åºæ•°æ®åˆ†åŒºç†è®º)
    - [1.1.6 æ—¶åºæ•°æ®èšåˆç†è®º](#116-æ—¶åºæ•°æ®èšåˆç†è®º)
    - [1.1.7 å¤æ‚åº¦åˆ†æ](#117-å¤æ‚åº¦åˆ†æ)
  - [2. æ—¶åºæ•°æ®ç‰¹å¾](#2-æ—¶åºæ•°æ®ç‰¹å¾)
    - [2.1 æ•°æ®ç‰¹å¾](#21-æ•°æ®ç‰¹å¾)
    - [2.2 æ•°æ®æ¨¡å¼](#22-æ•°æ®æ¨¡å¼)
  - [3. æ•°æ®æ¨¡å‹è®¾è®¡](#3-æ•°æ®æ¨¡å‹è®¾è®¡)
    - [3.1 åŸºç¡€æ¨¡å‹](#31-åŸºç¡€æ¨¡å‹)
    - [3.2 æ ‡ç­¾è®¾è®¡](#32-æ ‡ç­¾è®¾è®¡)
    - [3.3 åˆ†åŒºè®¾è®¡](#33-åˆ†åŒºè®¾è®¡)
  - [4. å‹ç¼©ç­–ç•¥](#4-å‹ç¼©ç­–ç•¥)
    - [4.1 å‹ç¼©ç®—æ³•åŸç†](#41-å‹ç¼©ç®—æ³•åŸç†)
    - [4.2 PostgreSQLå‹ç¼©å®ç°](#42-postgresqlå‹ç¼©å®ç°)
    - [4.3 å‹ç¼©æ•ˆæœè¯„ä¼°](#43-å‹ç¼©æ•ˆæœè¯„ä¼°)
  - [5. æŸ¥è¯¢æ¨¡å¼ä¸ä¼˜åŒ–](#5-æŸ¥è¯¢æ¨¡å¼ä¸ä¼˜åŒ–)
    - [5.1 å¸¸è§æŸ¥è¯¢æ¨¡å¼](#51-å¸¸è§æŸ¥è¯¢æ¨¡å¼)
    - [5.2 è¿ç»­èšåˆ](#52-è¿ç»­èšåˆ)
    - [5.3 æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–](#53-æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–)
  - [6. PostgreSQLå®ç°](#6-postgresqlå®ç°)
    - [6.1 å®Œæ•´æ—¶åºæ•°æ®æ¨¡å‹](#61-å®Œæ•´æ—¶åºæ•°æ®æ¨¡å‹)
    - [6.2 æ•°æ®å†™å…¥å‡½æ•°](#62-æ•°æ®å†™å…¥å‡½æ•°)
  - [7. å®é™…åº”ç”¨åœºæ™¯](#7-å®é™…åº”ç”¨åœºæ™¯)
    - [7.1 IoTä¼ æ„Ÿå™¨ç›‘æ§](#71-iotä¼ æ„Ÿå™¨ç›‘æ§)
    - [7.2 é‡‘èäº¤æ˜“æ•°æ®](#72-é‡‘èäº¤æ˜“æ•°æ®)
    - [7.3 æ—¥å¿—åˆ†æç³»ç»Ÿ](#73-æ—¥å¿—åˆ†æç³»ç»Ÿ)
  - [8. æ€§èƒ½ç›‘æ§ä¸è¯Šæ–­ / Performance Monitoring and Diagnostics](#8-æ€§èƒ½ç›‘æ§ä¸è¯Šæ–­--performance-monitoring-and-diagnostics)
    - [8.1 å†™å…¥æ€§èƒ½ç›‘æ§](#81-å†™å…¥æ€§èƒ½ç›‘æ§)
    - [8.2 æŸ¥è¯¢æ€§èƒ½ç›‘æ§](#82-æŸ¥è¯¢æ€§èƒ½ç›‘æ§)
    - [8.3 å­˜å‚¨ç©ºé—´ç›‘æ§](#83-å­˜å‚¨ç©ºé—´ç›‘æ§)
  - [9. å¸¸è§é—®é¢˜è§£ç­” / FAQ](#9-å¸¸è§é—®é¢˜è§£ç­”--faq)
    - [Q1: æ—¶åºæ•°æ®åº”è¯¥ä¿ç•™å¤šé•¿æ—¶é—´ï¼Ÿ](#q1-æ—¶åºæ•°æ®åº”è¯¥ä¿ç•™å¤šé•¿æ—¶é—´)
    - [Q2: å¦‚ä½•ä¼˜åŒ–æ—¶åºæ•°æ®çš„å†™å…¥æ€§èƒ½ï¼Ÿ](#q2-å¦‚ä½•ä¼˜åŒ–æ—¶åºæ•°æ®çš„å†™å…¥æ€§èƒ½)
    - [Q3: æ—¶åºæ•°æ®æŸ¥è¯¢å¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ](#q3-æ—¶åºæ•°æ®æŸ¥è¯¢å¾ˆæ…¢æ€ä¹ˆåŠ)
    - [Q4: æ—¶åºæ•°æ®å‹ç¼©ç‡å¦‚ä½•æé«˜ï¼Ÿ](#q4-æ—¶åºæ•°æ®å‹ç¼©ç‡å¦‚ä½•æé«˜)
    - [Q5: å¦‚ä½•å¤„ç†æ—¶åºæ•°æ®çš„ç¼ºå¤±å€¼ï¼Ÿ](#q5-å¦‚ä½•å¤„ç†æ—¶åºæ•°æ®çš„ç¼ºå¤±å€¼)
    - [Q6: æ—¶åºæ•°æ®å¦‚ä½•å®ç°é™é‡‡æ ·ï¼Ÿ](#q6-æ—¶åºæ•°æ®å¦‚ä½•å®ç°é™é‡‡æ ·)
  - [10. ç›¸å…³èµ„æº / Related Resources](#10-ç›¸å…³èµ„æº--related-resources)
    - [10.1 æ ¸å¿ƒç›¸å…³æ–‡æ¡£ / Core Related Documents](#101-æ ¸å¿ƒç›¸å…³æ–‡æ¡£--core-related-documents)
    - [10.2 ç†è®ºåŸºç¡€ / Theoretical Foundation](#102-ç†è®ºåŸºç¡€--theoretical-foundation)
    - [10.3 å®è·µæŒ‡å— / Practical Guides](#103-å®è·µæŒ‡å—--practical-guides)
    - [10.4 åº”ç”¨æ¡ˆä¾‹ / Application Cases](#104-åº”ç”¨æ¡ˆä¾‹--application-cases)
    - [10.5 å‚è€ƒèµ„æº / Reference Resources](#105-å‚è€ƒèµ„æº--reference-resources)

---

## 1. æ¦‚è¿°

æ—¶åºæ•°æ®æ¨¡å‹ä¸“é—¨ç”¨äºå¤„ç†æ—¶é—´åºåˆ—æ•°æ®ï¼Œå…·æœ‰é«˜å†™å…¥ååé‡å’Œé«˜æ•ˆå‹ç¼©çš„ç‰¹ç‚¹ã€‚
æ—¶åºæ•°æ®æ˜¯æŒ‰æ—¶é—´é¡ºåºè®°å½•çš„æ•°æ®ç‚¹åºåˆ—ï¼Œå¹¿æ³›åº”ç”¨äºIoTç›‘æ§ã€é‡‘èäº¤æ˜“ã€æ—¥å¿—åˆ†æç­‰åœºæ™¯ã€‚

**æ ¸å¿ƒç‰¹å¾**:

- **æ—¶é—´ç»´åº¦**ï¼šæ•°æ®æŒ‰æ—¶é—´é¡ºåºç»„ç»‡
- **é«˜å†™å…¥ç‡**ï¼šæ”¯æŒé«˜é¢‘æ•°æ®å†™å…¥
- **é«˜æ•ˆå‹ç¼©**ï¼šæ—¶é—´åºåˆ—æ•°æ®å‹ç¼©ç‡é«˜
- **èŒƒå›´æŸ¥è¯¢**ï¼šä¸»è¦æŸ¥è¯¢æ¨¡å¼æ˜¯æ—¶é—´èŒƒå›´æŸ¥è¯¢

---

## 1.1 ç†è®ºåŸºç¡€

### 1.1.1 æ—¶åºæ•°æ®åŸºæœ¬æ¦‚å¿µ

**æ—¶åºæ•°æ®ï¼ˆTime Series Dataï¼‰**æ˜¯æŒ‰æ—¶é—´é¡ºåºè®°å½•çš„æ•°æ®ç‚¹åºåˆ—ï¼š

- **æ—¶é—´æˆ³**: $T = \{t_1, t_2, ..., t_n\}$
- **æ•°å€¼åºåˆ—**: $X = \{x_1, x_2, ..., x_n\}$
- **æ•°æ®ç‚¹**: $(t_i, x_i)$

**æ—¶åºæ•°æ®ç‰¹å¾**:

- **æ—¶é—´æœ‰åº**: æ•°æ®æŒ‰æ—¶é—´é¡ºåºæ’åˆ—
- **é«˜é¢‘å†™å…¥**: æ”¯æŒé«˜é¢‘æ•°æ®å†™å…¥
- **ä¸å¯å˜**: å†å²æ•°æ®ä¸ä¿®æ”¹
- **æ—¶é—´ç›¸å…³æ€§**: ç›¸é‚»æ•°æ®ç‚¹ç›¸å…³

### 1.1.2 æ—¶åºæ•°æ®æ¨¡å‹ç†è®º

**æ•°æ®æ¨¡å‹**:

- **æ—¶é—´æˆ³**: æ¯ä¸ªæ•°æ®ç‚¹éƒ½æœ‰æ—¶é—´æˆ³
- **æŒ‡æ ‡å€¼**: æµ‹é‡çš„æ•°å€¼
- **æ ‡ç­¾é›†**: æ ‡è¯†æ•°æ®æºçš„å…ƒæ•°æ®

**æ•°æ®è¡¨ç¤º**:

- **å•æŒ‡æ ‡**: $(t, v)$ where t is timestamp, v is value
- **å¤šæŒ‡æ ‡**: $(t, \{v_1, v_2, ..., v_n\})$
- **å¸¦æ ‡ç­¾**: $(t, v, \{l_1, l_2, ..., l_m\})$

### 1.1.3 æ—¶åºæ•°æ®å‹ç¼©ç†è®º

**å‹ç¼©åŸç†**:

- **æ—¶é—´å‹ç¼©**: æ—¶é—´æˆ³å·®å€¼å‹ç¼©ï¼ˆDelta Encodingï¼‰
- **å€¼å‹ç¼©**: æ•°å€¼å·®å€¼å‹ç¼©ï¼ˆDelta Encodingï¼‰
- **æ ‡ç­¾å‹ç¼©**: æ ‡ç­¾å»é‡å’Œç¼–ç 

**å‹ç¼©ç®—æ³•**:

- **Delta Encoding**: $\Delta t_i = t_i - t_{i-1}$
- **Gorilla Encoding**: å˜é•¿ç¼–ç 
- **LZ4/ZSTD**: é€šç”¨å‹ç¼©ç®—æ³•

**å‹ç¼©ç‡**:

- **æ—¶é—´å‹ç¼©ç‡**: $R_t = \frac{S_{original}}{S_{compressed}}$
- **å€¼å‹ç¼©ç‡**: $R_v = \frac{S_{original}}{S_{compressed}}$
- **æ€»å‹ç¼©ç‡**: $R_{total} = R_t \times R_v$

### 1.1.4 æ—¶åºæ•°æ®æŸ¥è¯¢ç†è®º

**æŸ¥è¯¢æ¨¡å¼**:

- **èŒƒå›´æŸ¥è¯¢**: $Q = \{x | t_{start} \leq t \leq t_{end}\}$
- **èšåˆæŸ¥è¯¢**: $Q = \text{AGG}(\{x | t_{start} \leq t \leq t_{end}\})$
- **é™é‡‡æ ·æŸ¥è¯¢**: $Q = \text{DOWNSAMPLE}(\{x | t_{start} \leq t \leq t_{end}\}, \Delta t)$

**æŸ¥è¯¢ä¼˜åŒ–**:

- **æ—¶é—´ç´¢å¼•**: ä½¿ç”¨æ—¶é—´ç´¢å¼•åŠ é€ŸèŒƒå›´æŸ¥è¯¢
- **é¢„èšåˆ**: ä½¿ç”¨è¿ç»­èšåˆé¢„è®¡ç®—èšåˆç»“æœ
- **åˆ†åŒºå‰ªæ**: ä½¿ç”¨åˆ†åŒºå‰ªæå‡å°‘æ‰«ææ•°æ®

### 1.1.5 æ—¶åºæ•°æ®åˆ†åŒºç†è®º

**åˆ†åŒºç­–ç•¥**:

- **æ—¶é—´åˆ†åŒº**: æŒ‰æ—¶é—´èŒƒå›´åˆ†åŒº
- **æ ‡ç­¾åˆ†åŒº**: æŒ‰æ ‡ç­¾å€¼åˆ†åŒº
- **æ··åˆåˆ†åŒº**: æ—¶é—´+æ ‡ç­¾åˆ†åŒº

**åˆ†åŒºå¤§å°**:

- **æ—¶é—´åˆ†åŒº**: é€šå¸¸æŒ‰å¤©ã€å‘¨ã€æœˆåˆ†åŒº
- **åˆ†åŒºå¤§å°**: å»ºè®®100MB-1GB per partition
- **åˆ†åŒºæ•°é‡**: å»ºè®®ä¸è¶…è¿‡1000ä¸ªåˆ†åŒº

### 1.1.6 æ—¶åºæ•°æ®èšåˆç†è®º

**è¿ç»­èšåˆï¼ˆContinuous Aggregationï¼‰**:

- **å®šä¹‰**: è‡ªåŠ¨ç»´æŠ¤çš„ç‰©åŒ–è§†å›¾
- **åˆ·æ–°ç­–ç•¥**: å¢é‡åˆ·æ–°
- **èšåˆå‡½æ•°**: SUMã€AVGã€COUNTã€MAXã€MIN

**é™é‡‡æ ·ï¼ˆDownsamplingï¼‰**:

- **å®šä¹‰**: é™ä½æ•°æ®é‡‡æ ·é¢‘ç‡
- **æ–¹æ³•**: æ—¶é—´çª—å£èšåˆ
- **ç›®çš„**: å‡å°‘å­˜å‚¨ç©ºé—´ï¼Œæé«˜æŸ¥è¯¢æ€§èƒ½

### 1.1.7 å¤æ‚åº¦åˆ†æ

**å­˜å‚¨å¤æ‚åº¦**:

- **åŸå§‹æ•°æ®**: $O(N)$ where N is number of data points
- **å‹ç¼©æ•°æ®**: $O(N \times C)$ where C is compression ratio
- **èšåˆæ•°æ®**: $O(N / R)$ where R is aggregation ratio

**æŸ¥è¯¢å¤æ‚åº¦**:

- **èŒƒå›´æŸ¥è¯¢**: $O(\log N)$ with time index
- **èšåˆæŸ¥è¯¢**: $O(N)$ (full scan) or $O(\log N)$ (with aggregation)
- **é™é‡‡æ ·æŸ¥è¯¢**: $O(N / R)$ where R is downsampling ratio

---

## 2. æ—¶åºæ•°æ®ç‰¹å¾

### 2.1 æ•°æ®ç‰¹å¾

**æ—¶åºæ•°æ®çš„å…¸å‹ç‰¹å¾**:

| ç‰¹å¾ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| æ—¶é—´æˆ³ | æ¯ä¸ªæ•°æ®ç‚¹éƒ½æœ‰æ—¶é—´æˆ³ | 2025-01-15 10:30:00 |
| æŒ‡æ ‡å€¼ | æµ‹é‡çš„æ•°å€¼ | æ¸©åº¦25.5Â°C |
| æ ‡ç­¾é›† | æ ‡è¯†æ•°æ®æºçš„å…ƒæ•°æ® | device_id, sensor_type |
| é«˜é¢‘ç‡ | æ•°æ®é‡‡é›†é¢‘ç‡é«˜ | æ¯ç§’ã€æ¯åˆ†é’Ÿ |
| ä¸å¯å˜ | å†å²æ•°æ®ä¸ä¿®æ”¹ | åªè¿½åŠ ï¼Œä¸æ›´æ–° |

### 2.2 æ•°æ®æ¨¡å¼

**å…¸å‹æ—¶åºæ•°æ®æ¨¡å¼**:

```text
æ—¶é—´æˆ³ | è®¾å¤‡ID | ä¼ æ„Ÿå™¨ç±»å‹ | æŒ‡æ ‡å€¼
-------|--------|-----------|--------
10:00:00 | device_001 | temperature | 25.5
10:00:01 | device_001 | temperature | 25.6
10:00:02 | device_001 | temperature | 25.7
10:00:00 | device_002 | humidity | 60.0
10:00:01 | device_002 | humidity | 60.1
```

---

## 3. æ•°æ®æ¨¡å‹è®¾è®¡

### 3.1 åŸºç¡€æ¨¡å‹

**çª„è¡¨æ¨¡å‹ï¼ˆNarrow Tableï¼‰**:

```sql
-- çª„è¡¨ï¼šæ¯ä¸ªæŒ‡æ ‡ä¸€è¡Œ
CREATE TABLE time_series_narrow (
    timestamp TIMESTAMPTZ NOT NULL,
    device_id VARCHAR(50) NOT NULL,
    metric_name VARCHAR(50) NOT NULL,
    metric_value DOUBLE PRECISION NOT NULL,
    PRIMARY KEY (timestamp, device_id, metric_name)
);

-- ç¤ºä¾‹æ•°æ®
INSERT INTO time_series_narrow VALUES
    ('2025-01-15 10:00:00', 'device_001', 'temperature', 25.5),
    ('2025-01-15 10:00:01', 'device_001', 'temperature', 25.6),
    ('2025-01-15 10:00:00', 'device_001', 'humidity', 60.0);
```

**å®½è¡¨æ¨¡å‹ï¼ˆWide Tableï¼‰**:

```sql
-- å®½è¡¨ï¼šæ¯ä¸ªæ—¶é—´ç‚¹ä¸€è¡Œï¼Œå¤šä¸ªæŒ‡æ ‡åˆ—
CREATE TABLE time_series_wide (
    timestamp TIMESTAMPTZ NOT NULL,
    device_id VARCHAR(50) NOT NULL,
    temperature DOUBLE PRECISION,
    humidity DOUBLE PRECISION,
    pressure DOUBLE PRECISION,
    PRIMARY KEY (timestamp, device_id)
);
```

### 3.2 æ ‡ç­¾è®¾è®¡

**æ ‡ç­¾åˆ†ç¦»æ¨¡å‹**:

```sql
-- è®¾å¤‡è¡¨ï¼ˆæ ‡ç­¾ï¼‰
CREATE TABLE devices (
    device_id VARCHAR(50) PRIMARY KEY,
    device_name VARCHAR(200),
    location VARCHAR(200),
    device_type VARCHAR(50),
    manufacturer VARCHAR(100),
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- æ—¶åºæ•°æ®è¡¨ï¼ˆæŒ‡æ ‡å€¼ï¼‰
CREATE TABLE time_series_data (
    timestamp TIMESTAMPTZ NOT NULL,
    device_id VARCHAR(50) NOT NULL REFERENCES devices(device_id),
    metric_name VARCHAR(50) NOT NULL,
    metric_value DOUBLE PRECISION NOT NULL,
    quality_code INT DEFAULT 0, -- æ•°æ®è´¨é‡ç 
    PRIMARY KEY (timestamp, device_id, metric_name)
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_time_series_time ON time_series_data(timestamp DESC);
CREATE INDEX idx_time_series_device ON time_series_data(device_id, timestamp DESC);
```

### 3.3 åˆ†åŒºè®¾è®¡

**æ—¶é—´åˆ†åŒºç­–ç•¥**:

```sql
-- æŒ‰æœˆåˆ†åŒº
CREATE TABLE time_series_data (
    timestamp TIMESTAMPTZ NOT NULL,
    device_id VARCHAR(50) NOT NULL,
    metric_name VARCHAR(50) NOT NULL,
    metric_value DOUBLE PRECISION NOT NULL
) PARTITION BY RANGE (timestamp);

-- åˆ›å»ºåˆ†åŒº
CREATE TABLE time_series_data_2025_01
    PARTITION OF time_series_data
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

CREATE TABLE time_series_data_2025_02
    PARTITION OF time_series_data
    FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');
```

---

## 4. å‹ç¼©ç­–ç•¥

### 4.1 å‹ç¼©ç®—æ³•åŸç†

**æ—¶åºæ•°æ®å‹ç¼©ç‰¹ç‚¹**:

æ—¶åºæ•°æ®å…·æœ‰é«˜åº¦å¯å‹ç¼©æ€§ï¼Œä¸»è¦åŸå› ï¼š

1. **æ—¶é—´æˆ³æœ‰åºæ€§**: æ—¶é—´æˆ³é€šå¸¸æŒ‰é¡ºåºé€’å¢ï¼Œå·®å€¼å‹ç¼©æ•ˆæœå¥½
2. **æ•°å€¼å˜åŒ–ç¼“æ…¢**: ä¼ æ„Ÿå™¨æ•°æ®é€šå¸¸å˜åŒ–å¹…åº¦å°ï¼Œç›¸é‚»å€¼ç›¸ä¼¼åº¦é«˜
3. **æ ‡ç­¾é‡å¤åº¦é«˜**: è®¾å¤‡IDã€æŒ‡æ ‡åç§°ç­‰æ ‡ç­¾æ•°æ®é‡å¤å‡ºç°
4. **ç¨€ç–æ€§**: æŸäº›æ—¶é—´ç‚¹å¯èƒ½æ²¡æœ‰æ•°æ®ï¼Œå­˜åœ¨å¤§é‡ç©ºå€¼

**å‹ç¼©ç®—æ³•ç±»å‹**:

| ç®—æ³•ç±»å‹ | é€‚ç”¨åœºæ™¯ | å‹ç¼©æ¯” | æŸ¥è¯¢æ€§èƒ½ |
|---------|---------|--------|---------|
| Deltaç¼–ç  | æ—¶é—´æˆ³ã€é€’å¢ID | 10:1 - 50:1 | é«˜ |
| æ¸¸ç¨‹ç¼–ç  | é‡å¤å€¼åºåˆ— | 5:1 - 20:1 | ä¸­ |
| å­—å…¸å‹ç¼© | å­—ç¬¦ä¸²æ ‡ç­¾ | 3:1 - 10:1 | é«˜ |
| Gorillaå‹ç¼© | æµ®ç‚¹æ•° | 8:1 - 30:1 | é«˜ |
| LZ4/ZSTD | é€šç”¨å‹ç¼© | 2:1 - 5:1 | ä¸­ |

**TimescaleDBå‹ç¼©åŸç†**:

TimescaleDBä½¿ç”¨åˆ—å¼å‹ç¼©ï¼Œå°†æ•°æ®æŒ‰åˆ—å­˜å‚¨å¹¶å‹ç¼©ï¼š

1. **Segmentby**: æŒ‰æŒ‡å®šåˆ—åˆ†ç»„ï¼Œç›¸åŒåˆ†ç»„çš„è¡Œå­˜å‚¨åœ¨ä¸€èµ·
2. **Orderby**: æŒ‰æŒ‡å®šåˆ—æ’åºï¼Œæé«˜å‹ç¼©ç‡
3. **å‹ç¼©ç®—æ³•**: ä½¿ç”¨PostgreSQLå†…ç½®çš„å‹ç¼©ç®—æ³•ï¼ˆLZ4/ZSTDï¼‰

### 4.2 PostgreSQLå‹ç¼©å®ç°

**åŸºç¡€å‹ç¼©é…ç½®**:

```sql
-- 1. å¯ç”¨å‹ç¼©
ALTER TABLE sensor_data SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'device_id',
    timescaledb.compress_orderby = 'time DESC'
);

-- 2. æ·»åŠ å‹ç¼©ç­–ç•¥ï¼ˆ7å¤©å‰æ•°æ®è‡ªåŠ¨å‹ç¼©ï¼‰
SELECT add_compression_policy(
    'sensor_data',
    INTERVAL '7 days',
    if_not_exists => true
);
```

**é«˜çº§å‹ç¼©é…ç½®**:

```sql
-- 1. å¤šåˆ—segmentbyï¼ˆæé«˜å‹ç¼©ç‡ï¼‰
ALTER TABLE sensor_data SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'device_id, sensor_type',  -- å¤šåˆ—åˆ†ç»„
    timescaledb.compress_orderby = 'time DESC'
);

-- 2. æŸ¥çœ‹å‹ç¼©é…ç½®
SELECT
    hypertable_name,
    compress_segmentby,
    compress_orderby,
    compress_chunk_time_interval
FROM timescaledb_information.hypertables
WHERE hypertable_name = 'sensor_data';
```

**æ‰‹åŠ¨å‹ç¼©æ“ä½œ**:

```sql
-- 1. å‹ç¼©æŒ‡å®šchunk
SELECT compress_chunk('_timescaledb_internal._hyper_1_1_chunk');

-- 2. å‹ç¼©æŒ‡å®šæ—¶é—´èŒƒå›´çš„æ‰€æœ‰chunk
SELECT compress_chunk(chunk)
FROM timescaledb_information.chunks
WHERE hypertable_name = 'sensor_data'
  AND range_start < NOW() - INTERVAL '7 days'
  AND is_compressed = false;

-- 3. æ‰¹é‡å‹ç¼©å‡½æ•°
CREATE OR REPLACE FUNCTION compress_old_chunks(
    p_hypertable_name TEXT,
    p_older_than INTERVAL
)
RETURNS TABLE(chunk_name TEXT, compressed BOOLEAN) AS $$
DECLARE
    v_chunk RECORD;
BEGIN
    FOR v_chunk IN
        SELECT chunk_schema || '.' || chunk_name AS full_chunk_name
        FROM timescaledb_information.chunks
        WHERE hypertable_name = p_hypertable_name
          AND range_end < NOW() - p_older_than
          AND is_compressed = false
    LOOP
        BEGIN
            PERFORM compress_chunk(v_chunk.full_chunk_name);
            chunk_name := v_chunk.full_chunk_name;
            compressed := true;
            RETURN NEXT;
        EXCEPTION WHEN OTHERS THEN
            chunk_name := v_chunk.full_chunk_name;
            compressed := false;
            RETURN NEXT;
        END;
    END LOOP;
END;
$$ LANGUAGE plpgsql;
```

### 4.3 å‹ç¼©æ•ˆæœè¯„ä¼°

**å‹ç¼©ç»Ÿè®¡æŸ¥è¯¢**:

```sql
-- 1. æŸ¥çœ‹å‹ç¼©chunkç»Ÿè®¡
SELECT
    hypertable_name,
    chunk_name,
    range_start,
    range_end,
    pg_size_pretty(before_compression_total_bytes) AS uncompressed_size,
    pg_size_pretty(after_compression_total_bytes) AS compressed_size,
    ROUND(100.0 * (1 - after_compression_total_bytes::NUMERIC / before_compression_total_bytes), 2) AS compression_ratio_pct,
    numrows_pre_compression,
    numrows_post_compression
FROM timescaledb_information.compressed_chunk_stats
WHERE hypertable_name = 'sensor_data'
ORDER BY range_start DESC;

-- 2. å‹ç¼©ç‡æ±‡æ€»ç»Ÿè®¡
SELECT
    hypertable_name,
    COUNT(*) AS compressed_chunks,
    pg_size_pretty(SUM(before_compression_total_bytes)) AS total_uncompressed,
    pg_size_pretty(SUM(after_compression_total_bytes)) AS total_compressed,
    ROUND(100.0 * (1 - SUM(after_compression_total_bytes)::NUMERIC / SUM(before_compression_total_bytes)), 2) AS avg_compression_ratio_pct
FROM timescaledb_information.compressed_chunk_stats
WHERE hypertable_name = 'sensor_data'
GROUP BY hypertable_name;
```

**å‹ç¼©æœ€ä½³å®è·µ**:

1. **segmentbyé€‰æ‹©åŸåˆ™**:
   - é€‰æ‹©æŸ¥è¯¢ä¸­ç»å¸¸ä¸€èµ·è¿‡æ»¤çš„åˆ—
   - é€‰æ‹©åŸºæ•°é€‚ä¸­çš„åˆ—ï¼ˆé¿å…è¿‡é«˜æˆ–è¿‡ä½ï¼‰
   - é€šå¸¸é€‰æ‹©è®¾å¤‡IDã€æŒ‡æ ‡ç±»å‹ç­‰

2. **orderbyé€‰æ‹©åŸåˆ™**:
   - å¿…é¡»åŒ…å«æ—¶é—´åˆ—
   - æŒ‰æ—¶é—´é™åºæ’åˆ—ï¼ˆæœ€æ–°æ•°æ®åœ¨å‰ï¼‰
   - å¯ä»¥æ·»åŠ å…¶ä»–æ’åºåˆ—æé«˜å‹ç¼©ç‡

3. **å‹ç¼©æ—¶æœº**:
   - æ•°æ®å†™å…¥7-30å¤©åå‹ç¼©ï¼ˆå¹³è¡¡æŸ¥è¯¢æ€§èƒ½å’Œå‹ç¼©ç‡ï¼‰
   - é¿å…å‹ç¼©æœ€æ–°æ•°æ®ï¼ˆå½±å“å†™å…¥æ€§èƒ½ï¼‰
   - å®šæœŸç›‘æ§å‹ç¼©ä»»åŠ¡æ‰§è¡Œæƒ…å†µ

4. **å‹ç¼©ç‡ç›®æ ‡**:
   - æ—¶åºæ•°æ®é€šå¸¸å¯è¾¾åˆ°10:1åˆ°50:1çš„å‹ç¼©æ¯”
   - å¦‚æœå‹ç¼©ç‡ä½äº5:1ï¼Œéœ€è¦ä¼˜åŒ–segmentbyå’Œorderbyé…ç½®
   - ç›‘æ§å‹ç¼©åçš„æŸ¥è¯¢æ€§èƒ½ï¼Œç¡®ä¿ä¸ä¼šæ˜¾è‘—ä¸‹é™

---

## 5. æŸ¥è¯¢æ¨¡å¼ä¸ä¼˜åŒ–

### 5.1 å¸¸è§æŸ¥è¯¢æ¨¡å¼

**æ—¶é—´èŒƒå›´æŸ¥è¯¢**:

```sql
-- âœ… ä¼˜åŒ–ï¼šä½¿ç”¨æ—¶é—´èŒƒå›´æŸ¥è¯¢ï¼ˆè‡ªåŠ¨åˆ†åŒºå‰ªæï¼‰
SELECT
    time_bucket('1 hour', timestamp) AS hour,
    device_id,
    AVG(metric_value) AS avg_value,
    MAX(metric_value) AS max_value,
    MIN(metric_value) AS min_value,
    COUNT(*) AS sample_count
FROM time_series_data
WHERE timestamp >= '2025-01-15 00:00:00'
  AND timestamp < '2025-01-16 00:00:00'
  AND device_id = 'device_001'
  AND metric_name = 'temperature'
GROUP BY hour, device_id
ORDER BY hour;

-- âŒ é¿å…ï¼šä½¿ç”¨å‡½æ•°ï¼ˆåˆ†åŒºå‰ªæå¤±æ•ˆï¼‰
SELECT * FROM time_series_data
WHERE DATE_TRUNC('day', timestamp) = CURRENT_DATE;
```

**æœ€æ–°å€¼æŸ¥è¯¢ä¼˜åŒ–**:

```sql
-- æ–¹æ³•1: ä½¿ç”¨DISTINCT ONï¼ˆé€‚åˆå°è§„æ¨¡è®¾å¤‡ï¼‰
SELECT DISTINCT ON (device_id)
    device_id,
    timestamp,
    metric_value
FROM time_series_data
WHERE metric_name = 'temperature'
ORDER BY device_id, timestamp DESC;

-- æ–¹æ³•2: ä½¿ç”¨LATERAL JOINï¼ˆé€‚åˆå¤§è§„æ¨¡è®¾å¤‡ï¼‰
SELECT d.device_id, l.timestamp, l.metric_value
FROM devices d
CROSS JOIN LATERAL (
    SELECT timestamp, metric_value
    FROM time_series_data
    WHERE device_id = d.device_id
      AND metric_name = 'temperature'
    ORDER BY timestamp DESC
    LIMIT 1
) l;

-- æ–¹æ³•3: ä½¿ç”¨çª—å£å‡½æ•°ï¼ˆé€‚åˆéœ€è¦å¤šä¸ªæœ€æ–°å€¼ï¼‰
SELECT device_id, timestamp, metric_value
FROM (
    SELECT
        device_id,
        timestamp,
        metric_value,
        ROW_NUMBER() OVER (PARTITION BY device_id ORDER BY timestamp DESC) AS rn
    FROM time_series_data
    WHERE metric_name = 'temperature'
) t
WHERE rn = 1;
```

**èšåˆæŸ¥è¯¢ä¼˜åŒ–**:

```sql
-- åŸºç¡€èšåˆæŸ¥è¯¢
SELECT
    time_bucket('5 minutes', timestamp) AS bucket,
    device_id,
    COUNT(*) AS data_points,
    AVG(metric_value) AS avg_value,
    STDDEV(metric_value) AS stddev_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY metric_value) AS median_value,
    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY metric_value) AS p95_value
FROM time_series_data
WHERE timestamp > NOW() - INTERVAL '1 hour'
GROUP BY bucket, device_id
ORDER BY bucket DESC;

-- å¤šæŒ‡æ ‡èšåˆæŸ¥è¯¢
SELECT
    time_bucket('1 hour', timestamp) AS bucket,
    device_id,
    COUNT(*) FILTER (WHERE metric_name = 'temperature') AS temp_samples,
    AVG(metric_value) FILTER (WHERE metric_name = 'temperature') AS avg_temp,
    COUNT(*) FILTER (WHERE metric_name = 'humidity') AS humidity_samples,
    AVG(metric_value) FILTER (WHERE metric_name = 'humidity') AS avg_humidity
FROM time_series_data
WHERE timestamp >= NOW() - INTERVAL '24 hours'
GROUP BY bucket, device_id
ORDER BY bucket DESC;
```

**Gap-fillingæŸ¥è¯¢**:

```sql
-- ä½¿ç”¨time_bucket_gapfillå¡«å……ç¼ºå¤±æ—¶é—´ç‚¹
SELECT
    time_bucket_gapfill('1 hour', timestamp,
        start => NOW() - INTERVAL '24 hours',
        finish => NOW()) AS bucket,
    device_id,
    LOCF(AVG(metric_value)) AS avg_value,  -- Last Observation Carried Forward
    INTERPOLATE(AVG(metric_value)) AS interpolated_value  -- çº¿æ€§æ’å€¼
FROM time_series_data
WHERE timestamp >= NOW() - INTERVAL '24 hours'
  AND device_id = 'device_001'
GROUP BY bucket, device_id
ORDER BY bucket;
```

### 5.2 è¿ç»­èšåˆ

**åˆ›å»ºå¤šçº§è¿ç»­èšåˆ**:

```sql
-- 1. åˆ†é’Ÿçº§èšåˆï¼ˆå®æ—¶ç›‘æ§ï¼‰
CREATE MATERIALIZED VIEW time_series_1min
WITH (timescaledb.continuous, timescaledb.materialized_only = false) AS
SELECT
    time_bucket('1 minute', timestamp) AS bucket,
    device_id,
    metric_name,
    AVG(metric_value) AS avg_value,
    MIN(metric_value) AS min_value,
    MAX(metric_value) AS max_value,
    COUNT(*) AS sample_count,
    STDDEV(metric_value) AS stddev_value
FROM time_series_data
GROUP BY bucket, device_id, metric_name;

-- 2. å°æ—¶çº§èšåˆï¼ˆåŸºäºåˆ†é’Ÿçº§ï¼‰
CREATE MATERIALIZED VIEW time_series_1hour
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 hour', bucket) AS bucket,
    device_id,
    metric_name,
    AVG(avg_value) AS avg_value,
    MIN(min_value) AS min_value,
    MAX(max_value) AS max_value,
    SUM(sample_count) AS total_samples
FROM time_series_1min
GROUP BY bucket, device_id, metric_name;

-- 3. å¤©çº§èšåˆï¼ˆåŸºäºå°æ—¶çº§ï¼‰
CREATE MATERIALIZED VIEW time_series_1day
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 day', bucket) AS bucket,
    device_id,
    metric_name,
    AVG(avg_value) AS avg_value,
    MIN(min_value) AS min_value,
    MAX(max_value) AS max_value,
    SUM(total_samples) AS total_samples
FROM time_series_1hour
GROUP BY bucket, device_id, metric_name;

-- 4. é…ç½®åˆ·æ–°ç­–ç•¥
SELECT add_continuous_aggregate_policy(
    'time_series_1min',
    start_offset => INTERVAL '3 hours',
    end_offset => INTERVAL '1 minute',
    schedule_interval => INTERVAL '1 minute'
);

SELECT add_continuous_aggregate_policy(
    'time_series_1hour',
    start_offset => INTERVAL '3 days',
    end_offset => INTERVAL '1 hour',
    schedule_interval => INTERVAL '1 hour'
);

SELECT add_continuous_aggregate_policy(
    'time_series_1day',
    start_offset => INTERVAL '7 days',
    end_offset => INTERVAL '1 day',
    schedule_interval => INTERVAL '1 day'
);
```

**æŸ¥è¯¢è¿ç»­èšåˆ**:

```sql
-- æŸ¥è¯¢æœ€è¿‘24å°æ—¶æ•°æ®ï¼ˆä½¿ç”¨å°æ—¶çº§èšåˆï¼‰
SELECT * FROM time_series_1hour
WHERE bucket >= NOW() - INTERVAL '24 hours'
  AND device_id = 'device_001'
ORDER BY bucket DESC;

-- æŸ¥è¯¢æœ€è¿‘7å¤©æ•°æ®ï¼ˆä½¿ç”¨å¤©çº§èšåˆï¼‰
SELECT * FROM time_series_1day
WHERE bucket >= NOW() - INTERVAL '7 days'
  AND device_id = 'device_001'
ORDER BY bucket DESC;

-- æ··åˆæŸ¥è¯¢ï¼ˆæœ€æ–°æ•°æ®ç”¨åˆ†é’Ÿçº§ï¼Œå†å²æ•°æ®ç”¨å°æ—¶çº§ï¼‰
SELECT * FROM time_series_1min
WHERE bucket >= NOW() - INTERVAL '1 hour'
  AND device_id = 'device_001'
UNION ALL
SELECT * FROM time_series_1hour
WHERE bucket >= NOW() - INTERVAL '24 hours'
  AND bucket < NOW() - INTERVAL '1 hour'
  AND device_id = 'device_001'
ORDER BY bucket DESC;
```

### 5.3 æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–

**ç´¢å¼•ä¼˜åŒ–**:

```sql
-- 1. å¤åˆç´¢å¼•ï¼ˆè®¾å¤‡ID + æ—¶é—´ï¼‰
CREATE INDEX idx_time_series_device_time
ON time_series_data(device_id, timestamp DESC);

-- 2. éƒ¨åˆ†ç´¢å¼•ï¼ˆä»…ç´¢å¼•æ´»è·ƒè®¾å¤‡ï¼‰
CREATE INDEX idx_time_series_active_device
ON time_series_data(device_id, timestamp DESC)
WHERE device_id IN (SELECT device_id FROM active_devices);

-- 3. è¡¨è¾¾å¼ç´¢å¼•ï¼ˆç”¨äºç‰¹å®šæŸ¥è¯¢æ¨¡å¼ï¼‰
CREATE INDEX idx_time_series_date_device
ON time_series_data((timestamp::DATE), device_id);

-- 4. GINç´¢å¼•ï¼ˆç”¨äºJSONBæ ‡ç­¾æŸ¥è¯¢ï¼‰
CREATE INDEX idx_time_series_tags
ON time_series_data USING GIN(tags);
```

**æŸ¥è¯¢ä¼˜åŒ–æŠ€å·§**:

```sql
-- 1. ä½¿ç”¨LIMITé™åˆ¶ç»“æœé›†
SELECT * FROM time_series_data
WHERE device_id = 'device_001'
ORDER BY timestamp DESC
LIMIT 1000;

-- 2. ä½¿ç”¨æ—¶é—´èŒƒå›´é™åˆ¶
SELECT * FROM time_series_data
WHERE device_id = 'device_001'
  AND timestamp >= NOW() - INTERVAL '1 hour'
ORDER BY timestamp DESC;

-- 3. ä½¿ç”¨è¿ç»­èšåˆä»£æ›¿åŸå§‹æ•°æ®æŸ¥è¯¢
-- âŒ æ…¢ï¼šæŸ¥è¯¢åŸå§‹æ•°æ®
SELECT AVG(metric_value)
FROM time_series_data
WHERE timestamp >= NOW() - INTERVAL '7 days'
  AND device_id = 'device_001';

-- âœ… å¿«ï¼šä½¿ç”¨è¿ç»­èšåˆ
SELECT AVG(avg_value)
FROM time_series_1hour
WHERE bucket >= NOW() - INTERVAL '7 days'
  AND device_id = 'device_001';

-- 4. å¹¶è¡ŒæŸ¥è¯¢ï¼ˆPostgreSQL 13+ï¼‰
SET max_parallel_workers_per_gather = 4;
SET parallel_setup_cost = 100;
SET parallel_tuple_cost = 0.01;

EXPLAIN ANALYZE
SELECT AVG(metric_value)
FROM time_series_data
WHERE timestamp >= NOW() - INTERVAL '30 days'
GROUP BY device_id;
```

---

## 6. PostgreSQLå®ç°

### 6.1 å®Œæ•´æ—¶åºæ•°æ®æ¨¡å‹

```sql
-- è®¾å¤‡è¡¨
CREATE TABLE devices (
    device_id VARCHAR(50) PRIMARY KEY,
    device_name VARCHAR(200) NOT NULL,
    device_type VARCHAR(50),
    location JSONB,
    metadata JSONB,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- æ—¶åºæ•°æ®è¡¨ï¼ˆä½¿ç”¨TimescaleDBï¼‰
CREATE TABLE time_series_metrics (
    time TIMESTAMPTZ NOT NULL,
    device_id VARCHAR(50) NOT NULL REFERENCES devices(device_id),
    metric_name VARCHAR(50) NOT NULL,
    metric_value DOUBLE PRECISION NOT NULL,
    quality_code INT DEFAULT 0,
    tags JSONB
);

-- è½¬æ¢ä¸ºHypertable
SELECT create_hypertable('time_series_metrics', 'time',
    chunk_time_interval => INTERVAL '1 day');

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_metrics_device_time ON time_series_metrics(device_id, time DESC);
CREATE INDEX idx_metrics_metric ON time_series_metrics(metric_name, time DESC);
CREATE INDEX idx_metrics_tags ON time_series_metrics USING GIN(tags);

-- å¯ç”¨å‹ç¼©
ALTER TABLE time_series_metrics SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'device_id, metric_name',
    timescaledb.compress_orderby = 'time DESC'
);

-- æ·»åŠ å‹ç¼©ç­–ç•¥
SELECT add_compression_policy('time_series_metrics', INTERVAL '7 days');

-- æ•°æ®ä¿ç•™ç­–ç•¥
SELECT add_retention_policy('time_series_metrics', INTERVAL '90 days');
```

### 6.2 æ•°æ®å†™å…¥å‡½æ•°

```sql
-- æ‰¹é‡å†™å…¥å‡½æ•°
CREATE OR REPLACE FUNCTION insert_time_series_batch(
    p_data JSONB
)
RETURNS INT AS $$
DECLARE
    v_count INT := 0;
    v_item JSONB;
BEGIN
    FOR v_item IN SELECT * FROM jsonb_array_elements(p_data)
    LOOP
        INSERT INTO time_series_metrics (
            time, device_id, metric_name, metric_value, tags
        ) VALUES (
            (v_item->>'time')::TIMESTAMPTZ,
            v_item->>'device_id',
            v_item->>'metric_name',
            (v_item->>'metric_value')::DOUBLE PRECISION,
            v_item->'tags'
        );
        v_count := v_count + 1;
    END LOOP;

    RETURN v_count;
END;
$$ LANGUAGE plpgsql;

-- ä½¿ç”¨ç¤ºä¾‹
SELECT insert_time_series_batch('[
    {"time": "2025-01-15T10:00:00Z", "device_id": "device_001",
     "metric_name": "temperature", "metric_value": 25.5, "tags": {}},
    {"time": "2025-01-15T10:00:01Z", "device_id": "device_001",
     "metric_name": "temperature", "metric_value": 25.6, "tags": {}}
]'::JSONB);
```

---

## 7. å®é™…åº”ç”¨åœºæ™¯

### 7.1 IoTä¼ æ„Ÿå™¨ç›‘æ§

**åœºæ™¯æè¿°**: 1000ä¸ªä¼ æ„Ÿå™¨ï¼Œæ¯ç§’é‡‡é›†ä¸€æ¬¡æ•°æ®ï¼Œéœ€è¦å®æ—¶ç›‘æ§å’Œå†å²åˆ†æã€‚

**æ•°æ®æ¨¡å‹**:

```sql
-- ä¼ æ„Ÿå™¨è®¾å¤‡è¡¨
CREATE TABLE sensors (
    sensor_id VARCHAR(50) PRIMARY KEY,
    sensor_name VARCHAR(200) NOT NULL,
    sensor_type VARCHAR(50) NOT NULL,
    location JSONB,
    metadata JSONB,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ä¼ æ„Ÿå™¨æ•°æ®è¡¨
CREATE TABLE sensor_readings (
    time TIMESTAMPTZ NOT NULL,
    sensor_id VARCHAR(50) NOT NULL REFERENCES sensors(sensor_id),
    metric_name VARCHAR(50) NOT NULL,
    metric_value DOUBLE PRECISION NOT NULL,
    quality_code INT DEFAULT 100,
    tags JSONB DEFAULT '{}'
);

-- åˆ›å»ºHypertable
SELECT create_hypertable(
    'sensor_readings',
    'time',
    chunk_time_interval => INTERVAL '1 day'
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_sensor_readings_sensor_time
ON sensor_readings(sensor_id, time DESC);
CREATE INDEX idx_sensor_readings_metric_time
ON sensor_readings(metric_name, time DESC);

-- åˆ›å»ºè¿ç»­èšåˆ
CREATE MATERIALIZED VIEW sensor_readings_1min
WITH (timescaledb.continuous, timescaledb.materialized_only = false) AS
SELECT
    time_bucket('1 minute', time) AS bucket,
    sensor_id,
    metric_name,
    AVG(metric_value) AS avg_value,
    MIN(metric_value) AS min_value,
    MAX(metric_value) AS max_value,
    COUNT(*) AS sample_count
FROM sensor_readings
GROUP BY bucket, sensor_id, metric_name;
```

**æ€§èƒ½æŒ‡æ ‡**:

- **å†™å…¥æ€§èƒ½**: 100,000è¡Œ/ç§’
- **æŸ¥è¯¢æ€§èƒ½**:
  - å®æ—¶æŸ¥è¯¢ï¼ˆæœ€è¿‘1å°æ—¶ï¼‰: < 100ms
  - å†å²æŸ¥è¯¢ï¼ˆ90å¤©ï¼‰: < 1sï¼ˆä½¿ç”¨è¿ç»­èšåˆï¼‰
- **å‹ç¼©ç‡**: 15:1

---

### 7.2 é‡‘èäº¤æ˜“æ•°æ®

**åœºæ™¯æè¿°**: é«˜é¢‘äº¤æ˜“ç³»ç»Ÿï¼Œæ¯ç§’ç™¾ä¸‡çº§äº¤æ˜“è®°å½•ï¼Œéœ€è¦å¿«é€ŸæŸ¥è¯¢å’Œé•¿æœŸå­˜å‚¨ã€‚

**æ•°æ®æ¨¡å‹**:

```sql
-- äº¤æ˜“è¡¨
CREATE TABLE trades (
    time TIMESTAMPTZ NOT NULL,
    symbol VARCHAR(20) NOT NULL,
    exchange VARCHAR(10) NOT NULL,
    price DECIMAL(20, 8) NOT NULL,
    volume DECIMAL(20, 8) NOT NULL,
    trade_type CHAR(1) NOT NULL,
    trade_id BIGINT NOT NULL
);

-- åˆ›å»ºHypertableï¼ˆæŒ‰å°æ—¶åˆ†åŒºï¼‰
SELECT create_hypertable(
    'trades',
    'time',
    chunk_time_interval => INTERVAL '1 hour'
);

-- åˆ›å»ºç§’çº§èšåˆï¼ˆå®æ—¶ç›‘æ§ï¼‰
CREATE MATERIALIZED VIEW trades_1sec
WITH (timescaledb.continuous, timescaledb.materialized_only = false) AS
SELECT
    time_bucket('1 second', time) AS bucket,
    symbol,
    exchange,
    COUNT(*) AS trade_count,
    SUM(volume) AS total_volume,
    AVG(price) AS avg_price,
    MIN(price) AS min_price,
    MAX(price) AS max_price
FROM trades
GROUP BY bucket, symbol, exchange;
```

---

### 7.3 æ—¥å¿—åˆ†æç³»ç»Ÿ

**åœºæ™¯æè¿°**: åº”ç”¨æ—¥å¿—åˆ†æï¼Œæ¯å¤©TBçº§æ—¥å¿—æ•°æ®ï¼Œéœ€è¦å¿«é€Ÿæ£€ç´¢å’Œåˆ†æã€‚

**æ•°æ®æ¨¡å‹**:

```sql
-- æ—¥å¿—è¡¨
CREATE TABLE application_logs (
    time TIMESTAMPTZ NOT NULL,
    service_name VARCHAR(100) NOT NULL,
    log_level VARCHAR(20) NOT NULL,
    message TEXT NOT NULL,
    context JSONB DEFAULT '{}',
    user_id VARCHAR(50),
    request_id VARCHAR(100)
);

-- åˆ›å»ºHypertable
SELECT create_hypertable(
    'application_logs',
    'time',
    chunk_time_interval => INTERVAL '1 day'
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_logs_service_time
ON application_logs(service_name, time DESC);
CREATE INDEX idx_logs_level_time
ON application_logs(log_level, time DESC);
CREATE INDEX idx_logs_context
ON application_logs USING GIN(context);

-- åˆ›å»ºé”™è¯¯æ—¥å¿—èšåˆ
CREATE MATERIALIZED VIEW error_logs_hourly
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 hour', time) AS bucket,
    service_name,
    log_level,
    COUNT(*) AS error_count,
    COUNT(DISTINCT user_id) AS affected_users
FROM application_logs
WHERE log_level IN ('ERROR', 'FATAL')
GROUP BY bucket, service_name, log_level;
```

---

## 8. æ€§èƒ½ç›‘æ§ä¸è¯Šæ–­ / Performance Monitoring and Diagnostics

### 8.1 å†™å…¥æ€§èƒ½ç›‘æ§

**ç›‘æ§å†™å…¥æ€§èƒ½**:

```sql
-- ç›‘æ§å†™å…¥é€Ÿç‡
SELECT
    schemaname,
    tablename,
    n_tup_ins AS inserts,
    n_tup_upd AS updates,
    n_tup_del AS deletes,
    n_live_tup AS live_tuples,
    n_dead_tup AS dead_tuples,
    last_vacuum,
    last_autovacuum,
    last_analyze,
    last_autoanalyze
FROM pg_stat_user_tables
WHERE schemaname = 'public'
  AND tablename LIKE '%telemetry%'
ORDER BY n_tup_ins DESC;

-- è®¡ç®—å†™å…¥é€Ÿç‡ï¼ˆæ¯ç§’ï¼‰
SELECT
    tablename,
    n_tup_ins / NULLIF(EXTRACT(EPOCH FROM (NOW() - last_analyze)), 0) AS inserts_per_second
FROM pg_stat_user_tables
WHERE schemaname = 'public'
  AND tablename LIKE '%telemetry%'
  AND last_analyze IS NOT NULL;
```

**æ‰¹é‡å†™å…¥æ€§èƒ½æµ‹è¯•**:

```sql
-- æµ‹è¯•æ‰¹é‡å†™å…¥æ€§èƒ½
\timing on

-- å•æ¡æ’å…¥
INSERT INTO sensor_readings (device_id, timestamp, value) VALUES (1, NOW(), 25.5);
-- Time: 0.5ms

-- æ‰¹é‡æ’å…¥ï¼ˆ1000æ¡ï¼‰
INSERT INTO sensor_readings (device_id, timestamp, value)
SELECT i, NOW() + (i || ' seconds')::INTERVAL, random() * 100
FROM generate_series(1, 1000) i;
-- Time: 50msï¼ˆå¿«20å€ï¼‰

-- COPYæ‰¹é‡å¯¼å…¥ï¼ˆæœ€å¿«ï¼‰
COPY sensor_readings (device_id, timestamp, value)
FROM '/path/to/data.csv' WITH CSV HEADER;
-- Time: 10msï¼ˆå¿«50å€ï¼‰
```

### 8.2 æŸ¥è¯¢æ€§èƒ½ç›‘æ§

**ç›‘æ§æŸ¥è¯¢æ€§èƒ½**:

```sql
-- ä½¿ç”¨pg_stat_statementsç›‘æ§æŸ¥è¯¢
SELECT
    LEFT(query, 100) AS query_preview,
    calls,
    mean_exec_time,
    max_exec_time,
    total_exec_time,
    rows
FROM pg_stat_statements
WHERE query LIKE '%sensor_readings%'
ORDER BY mean_exec_time DESC
LIMIT 10;

-- ç›‘æ§æ…¢æŸ¥è¯¢ï¼ˆTimescaleDBï¼‰
SELECT
    hypertable_name,
    COUNT(*) AS chunk_count,
    pg_size_pretty(SUM(chunk_size)) AS total_size
FROM timescaledb_information.chunks
GROUP BY hypertable_name
ORDER BY SUM(chunk_size) DESC;
```

**æŸ¥è¯¢æ€§èƒ½åˆ†æ**:

```sql
-- åˆ†ææŸ¥è¯¢è®¡åˆ’
EXPLAIN (ANALYZE, BUFFERS, VERBOSE)
SELECT
    time_bucket('1 hour', timestamp) AS hour,
    device_id,
    AVG(value) AS avg_value,
    MAX(value) AS max_value,
    MIN(value) AS min_value
FROM sensor_readings
WHERE timestamp >= NOW() - INTERVAL '24 hours'
GROUP BY hour, device_id
ORDER BY hour DESC;

-- æ£€æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…å†µ
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan AS index_scans,
    idx_tup_read AS tuples_read,
    idx_tup_fetch AS tuples_fetched
FROM pg_stat_user_indexes
WHERE schemaname = 'public'
  AND tablename LIKE '%readings%'
ORDER BY idx_scan DESC;
```

### 8.3 å­˜å‚¨ç©ºé—´ç›‘æ§

**ç›‘æ§å­˜å‚¨ç©ºé—´**:

```sql
-- ç›‘æ§è¡¨å¤§å°
SELECT
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS total_size,
    pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) AS table_size,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) -
                   pg_relation_size(schemaname||'.'||tablename)) AS indexes_size
FROM pg_tables
WHERE schemaname = 'public'
  AND tablename LIKE '%readings%'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- ç›‘æ§å‹ç¼©æ•ˆæœï¼ˆTimescaleDBï¼‰
SELECT
    hypertable_name,
    COUNT(*) AS total_chunks,
    COUNT(*) FILTER (WHERE is_compressed = true) AS compressed_chunks,
    pg_size_pretty(SUM(chunk_size)) AS total_size,
    pg_size_pretty(SUM(chunk_size) FILTER (WHERE is_compressed = true)) AS compressed_size
FROM timescaledb_information.chunks
GROUP BY hypertable_name;

-- è®¡ç®—å‹ç¼©ç‡
SELECT
    hypertable_name,
    SUM(chunk_size) FILTER (WHERE is_compressed = false) AS uncompressed_size,
    SUM(chunk_size) FILTER (WHERE is_compressed = true) AS compressed_size,
    ROUND(100.0 * (1 - SUM(chunk_size) FILTER (WHERE is_compressed = true)::NUMERIC /
                   NULLIF(SUM(chunk_size) FILTER (WHERE is_compressed = false), 0)), 2) AS compression_ratio_percent
FROM timescaledb_information.chunks
GROUP BY hypertable_name;
```

---

## 9. å¸¸è§é—®é¢˜è§£ç­” / FAQ

### Q1: æ—¶åºæ•°æ®åº”è¯¥ä¿ç•™å¤šé•¿æ—¶é—´ï¼Ÿ

**A**: æ•°æ®ä¿ç•™ç­–ç•¥ï¼š

- **å®æ—¶æ•°æ®**: ä¿ç•™æœ€è¿‘24-48å°æ—¶ï¼ˆç”¨äºå®æ—¶ç›‘æ§ï¼‰
- **å†å²æ•°æ®**: ä¿ç•™1-3ä¸ªæœˆï¼ˆç”¨äºåˆ†æå’ŒæŠ¥å‘Šï¼‰
- **èšåˆæ•°æ®**: æ°¸ä¹…ä¿ç•™ï¼ˆç”¨äºè¶‹åŠ¿åˆ†æï¼‰

**å®ç°**:

```sql
-- TimescaleDBæ•°æ®ä¿ç•™ç­–ç•¥
SELECT add_retention_policy('sensor_readings', INTERVAL '90 days');

-- æ‰‹åŠ¨åˆ é™¤æ—§æ•°æ®
DELETE FROM sensor_readings
WHERE timestamp < NOW() - INTERVAL '90 days';
```

### Q2: å¦‚ä½•ä¼˜åŒ–æ—¶åºæ•°æ®çš„å†™å…¥æ€§èƒ½ï¼Ÿ

**A**: å†™å…¥ä¼˜åŒ–ç­–ç•¥ï¼š

1. **æ‰¹é‡å†™å…¥**: ä½¿ç”¨æ‰¹é‡INSERTæˆ–COPY
2. **å¼‚æ­¥å†™å…¥**: ä½¿ç”¨æ¶ˆæ¯é˜Ÿåˆ—ç¼“å†²å†™å…¥
3. **åˆ†åŒºä¼˜åŒ–**: åˆç†è®¾ç½®åˆ†åŒºå¤§å°
4. **ç´¢å¼•ä¼˜åŒ–**: å‡å°‘ä¸å¿…è¦çš„ç´¢å¼•

```sql
-- âœ… ä¼˜åŒ–ï¼šæ‰¹é‡å†™å…¥
INSERT INTO sensor_readings (device_id, timestamp, value)
SELECT device_id, timestamp, value FROM unnest($1::INT[], $2::TIMESTAMPTZ[], $3::DOUBLE PRECISION[]);

-- âœ… ä¼˜åŒ–ï¼šä½¿ç”¨COPY
COPY sensor_readings (device_id, timestamp, value) FROM STDIN WITH CSV;
```

### Q3: æ—¶åºæ•°æ®æŸ¥è¯¢å¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ

**A**: æŸ¥è¯¢ä¼˜åŒ–ç­–ç•¥ï¼š

1. **ä½¿ç”¨è¿ç»­èšåˆ**: é¢„è®¡ç®—èšåˆç»“æœ
2. **åˆç†ä½¿ç”¨ç´¢å¼•**: ä¸ºæ—¶é—´èŒƒå›´æŸ¥è¯¢åˆ›å»ºç´¢å¼•
3. **åˆ†åŒºå‰ªæ**: ç¡®ä¿æŸ¥è¯¢æ¡ä»¶åŒ…å«æ—¶é—´èŒƒå›´
4. **é™åˆ¶æŸ¥è¯¢èŒƒå›´**: é¿å…æŸ¥è¯¢è¿‡é•¿æ—¶é—´èŒƒå›´

```sql
-- âœ… ä¼˜åŒ–ï¼šä½¿ç”¨è¿ç»­èšåˆ
CREATE MATERIALIZED VIEW sensor_readings_hourly
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 hour', timestamp) AS bucket,
    device_id,
    AVG(value) AS avg_value
FROM sensor_readings
GROUP BY bucket, device_id;

-- âœ… ä¼˜åŒ–ï¼šæŸ¥è¯¢æ—¶ä½¿ç”¨è¿ç»­èšåˆ
SELECT * FROM sensor_readings_hourly
WHERE bucket >= NOW() - INTERVAL '7 days';
```

### Q4: æ—¶åºæ•°æ®å‹ç¼©ç‡å¦‚ä½•æé«˜ï¼Ÿ

**A**: å‹ç¼©ä¼˜åŒ–ç­–ç•¥ï¼š

1. **è°ƒæ•´å‹ç¼©å‚æ•°**: ä½¿ç”¨åˆé€‚çš„å‹ç¼©ç®—æ³•
2. **å»¶è¿Ÿå‹ç¼©**: ç­‰å¾…æ•°æ®ç¨³å®šåå†å‹ç¼©
3. **é€‰æ‹©å‹ç¼©åˆ—**: åªå‹ç¼©é€‚åˆå‹ç¼©çš„åˆ—

```sql
-- TimescaleDBå‹ç¼©ä¼˜åŒ–
ALTER TABLE sensor_readings SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'device_id',
    timescaledb.compress_orderby = 'timestamp DESC'
);

-- è®¾ç½®å‹ç¼©ç­–ç•¥ï¼ˆå»¶è¿Ÿå‹ç¼©ï¼‰
SELECT add_compression_policy('sensor_readings', INTERVAL '7 days');
```

### Q5: å¦‚ä½•å¤„ç†æ—¶åºæ•°æ®çš„ç¼ºå¤±å€¼ï¼Ÿ

**A**: ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥ï¼š

1. **å‰å‘å¡«å……**: ä½¿ç”¨å‰ä¸€ä¸ªå€¼å¡«å……
2. **æ’å€¼**: ä½¿ç”¨çº¿æ€§æ’å€¼æˆ–æ ·æ¡æ’å€¼
3. **æ ‡è®°ç¼ºå¤±**: ä½¿ç”¨NULLæ ‡è®°ç¼ºå¤±å€¼

```sql
-- å‰å‘å¡«å……ç¼ºå¤±å€¼
SELECT
    timestamp,
    device_id,
    COALESCE(value, LAG(value) OVER (PARTITION BY device_id ORDER BY timestamp)) AS value_filled
FROM sensor_readings
WHERE timestamp >= NOW() - INTERVAL '1 day'
ORDER BY device_id, timestamp;

-- çº¿æ€§æ’å€¼
WITH interpolated AS (
    SELECT
        timestamp,
        device_id,
        value,
        LAG(value) OVER (PARTITION BY device_id ORDER BY timestamp) AS prev_value,
        LEAD(value) OVER (PARTITION BY device_id ORDER BY timestamp) AS next_value,
        LAG(timestamp) OVER (PARTITION BY device_id ORDER BY timestamp) AS prev_time,
        LEAD(timestamp) OVER (PARTITION BY device_id ORDER BY timestamp) AS next_time
    FROM sensor_readings
)
SELECT
    timestamp,
    device_id,
    COALESCE(
        value,
        prev_value + (next_value - prev_value) *
        EXTRACT(EPOCH FROM (timestamp - prev_time)) /
        NULLIF(EXTRACT(EPOCH FROM (next_time - prev_time)), 0)
    ) AS interpolated_value
FROM interpolated;
```

### Q6: æ—¶åºæ•°æ®å¦‚ä½•å®ç°é™é‡‡æ ·ï¼Ÿ

**A**: é™é‡‡æ ·å®ç°ï¼š

```sql
-- ä½¿ç”¨è¿ç»­èšåˆå®ç°é™é‡‡æ ·
CREATE MATERIALIZED VIEW sensor_readings_daily
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 day', timestamp) AS bucket,
    device_id,
    AVG(value) AS avg_value,
    MAX(value) AS max_value,
    MIN(value) AS min_value,
    COUNT(*) AS data_points
FROM sensor_readings
GROUP BY bucket, device_id;

-- è®¾ç½®åˆ·æ–°ç­–ç•¥
SELECT add_continuous_aggregate_policy('sensor_readings_daily',
    start_offset => INTERVAL '3 days',
    end_offset => INTERVAL '1 day',
    schedule_interval => INTERVAL '1 hour');
```

---

## 10. ç›¸å…³èµ„æº / Related Resources

### 10.1 æ ¸å¿ƒç›¸å…³æ–‡æ¡£ / Core Related Documents

- [TimescaleDBå®è·µ](./TimescaleDBå®è·µ.md) - TimescaleDBè¯¦ç»†æŒ‡å—
- [è®¾å¤‡å­ªç”Ÿæ¨¡å‹](./è®¾å¤‡å­ªç”Ÿæ¨¡å‹.md) - è®¾å¤‡å­ªç”Ÿå»ºæ¨¡
- [åˆ†åŒºç­–ç•¥](../08-PostgreSQLå»ºæ¨¡å®è·µ/åˆ†åŒºç­–ç•¥.md) - æ—¶åºæ•°æ®åˆ†åŒºç­–ç•¥
- [ç´¢å¼•ç­–ç•¥](../08-PostgreSQLå»ºæ¨¡å®è·µ/ç´¢å¼•ç­–ç•¥.md) - æ—¶åºæ•°æ®ç´¢å¼•è®¾è®¡
- [æ€§èƒ½ä¼˜åŒ–](../08-PostgreSQLå»ºæ¨¡å®è·µ/æ€§èƒ½ä¼˜åŒ–.md) - æ—¶åºæŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–

### 10.2 ç†è®ºåŸºç¡€ / Theoretical Foundation

- [èŒƒå¼ç†è®º](../01-æ•°æ®å»ºæ¨¡ç†è®ºåŸºç¡€/èŒƒå¼ç†è®º.md) - æ—¶åºæ•°æ®èŒƒå¼è®¾è®¡

### 10.3 å®è·µæŒ‡å— / Practical Guides

- [æ€§èƒ½ç›‘æ§ä¸è¯Šæ–­](#9-æ€§èƒ½ç›‘æ§ä¸è¯Šæ–­--performance-monitoring-and-diagnostics) - æœ¬æ–‡æ¡£çš„æ€§èƒ½ç›‘æ§ç« èŠ‚
- [IoTç›‘æ§ç³»ç»Ÿæ¡ˆä¾‹](../10-ç»¼åˆåº”ç”¨æ¡ˆä¾‹/IoTç›‘æ§ç³»ç»Ÿæ¡ˆä¾‹.md) - IoTæ—¶åºæ•°æ®åº”ç”¨æ¡ˆä¾‹

### 10.4 åº”ç”¨æ¡ˆä¾‹ / Application Cases

- [IoTç›‘æ§ç³»ç»Ÿæ¡ˆä¾‹](../10-ç»¼åˆåº”ç”¨æ¡ˆä¾‹/IoTç›‘æ§ç³»ç»Ÿæ¡ˆä¾‹.md) - IoTæ—¶åºæ•°æ®å®Œæ•´æ¡ˆä¾‹

### 10.5 å‚è€ƒèµ„æº / Reference Resources

- [æƒå¨èµ„æºç´¢å¼•](../00-å¯¼èˆªä¸ç´¢å¼•/æƒå¨èµ„æºç´¢å¼•.md) - æƒå¨èµ„æºåˆ—è¡¨
- [æœ¯è¯­å¯¹ç…§è¡¨](../00-å¯¼èˆªä¸ç´¢å¼•/æœ¯è¯­å¯¹ç…§è¡¨.md) - æœ¯è¯­å¯¹ç…§
- [å¿«é€ŸæŸ¥æ‰¾æŒ‡å—](../00-å¯¼èˆªä¸ç´¢å¼•/å¿«é€ŸæŸ¥æ‰¾æŒ‡å—.md) - å¿«é€ŸæŸ¥æ‰¾å·¥å…·
- TimescaleDBå®˜æ–¹æ–‡æ¡£: [TimescaleDB Documentation](https://docs.timescale.com/)
- TimescaleDBåšå®¢: [TimescaleDB Blog](https://www.timescale.com/blog/)

---

**æœ€åæ›´æ–°**: 2025å¹´1æœˆ
**ç»´æŠ¤è€…**: PostgreSQL Modern Team
