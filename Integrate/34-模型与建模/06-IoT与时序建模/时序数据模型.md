# æ—¶åºæ•°æ®æ¨¡å‹

> **åˆ›å»ºæ—¥æœŸ**: 2025å¹´1æœˆ
> **æ¥æº**: æ—¶åºæ•°æ®åº“ç†è®º
> **çŠ¶æ€**: å¾…å®Œå–„
> **æ–‡æ¡£ç¼–å·**: 06-02

---

## ğŸ“‘ ç›®å½•

- [æ—¶åºæ•°æ®æ¨¡å‹](#æ—¶åºæ•°æ®æ¨¡å‹)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¦‚è¿°](#1-æ¦‚è¿°)
  - [2. æ—¶åºæ•°æ®ç‰¹å¾](#2-æ—¶åºæ•°æ®ç‰¹å¾)
    - [2.1 æ•°æ®ç‰¹å¾](#21-æ•°æ®ç‰¹å¾)
    - [2.2 æ•°æ®æ¨¡å¼](#22-æ•°æ®æ¨¡å¼)
  - [3. æ•°æ®æ¨¡å‹è®¾è®¡](#3-æ•°æ®æ¨¡å‹è®¾è®¡)
    - [3.1 åŸºç¡€æ¨¡å‹](#31-åŸºç¡€æ¨¡å‹)
    - [3.2 æ ‡ç­¾è®¾è®¡](#32-æ ‡ç­¾è®¾è®¡)
    - [3.3 åˆ†åŒºè®¾è®¡](#33-åˆ†åŒºè®¾è®¡)
  - [4. å‹ç¼©ç­–ç•¥](#4-å‹ç¼©ç­–ç•¥)
    - [4.1 å‹ç¼©ç®—æ³•åŸç†](#41-å‹ç¼©ç®—æ³•åŸç†)
    - [4.2 PostgreSQLå‹ç¼©å®ç°](#42-postgresqlå‹ç¼©å®ç°)
    - [4.3 å‹ç¼©æ•ˆæœè¯„ä¼°](#43-å‹ç¼©æ•ˆæœè¯„ä¼°)
  - [5. æŸ¥è¯¢æ¨¡å¼ä¸ä¼˜åŒ–](#5-æŸ¥è¯¢æ¨¡å¼ä¸ä¼˜åŒ–)
    - [5.1 å¸¸è§æŸ¥è¯¢æ¨¡å¼](#51-å¸¸è§æŸ¥è¯¢æ¨¡å¼)
    - [5.2 è¿ç»­èšåˆ](#52-è¿ç»­èšåˆ)
    - [5.3 æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–](#53-æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–)
  - [6. PostgreSQLå®ç°](#6-postgresqlå®ç°)
    - [6.1 å®Œæ•´æ—¶åºæ•°æ®æ¨¡å‹](#61-å®Œæ•´æ—¶åºæ•°æ®æ¨¡å‹)
    - [6.2 æ•°æ®å†™å…¥å‡½æ•°](#62-æ•°æ®å†™å…¥å‡½æ•°)
  - [7. å®é™…åº”ç”¨åœºæ™¯](#7-å®é™…åº”ç”¨åœºæ™¯)
    - [7.1 IoTä¼ æ„Ÿå™¨ç›‘æ§](#71-iotä¼ æ„Ÿå™¨ç›‘æ§)
    - [7.2 é‡‘èäº¤æ˜“æ•°æ®](#72-é‡‘èäº¤æ˜“æ•°æ®)
    - [7.3 æ—¥å¿—åˆ†æç³»ç»Ÿ](#73-æ—¥å¿—åˆ†æç³»ç»Ÿ)
  - [8. ç›¸å…³èµ„æº](#8-ç›¸å…³èµ„æº)

---

## 1. æ¦‚è¿°

æ—¶åºæ•°æ®æ¨¡å‹ä¸“é—¨ç”¨äºå¤„ç†æ—¶é—´åºåˆ—æ•°æ®ï¼Œå…·æœ‰é«˜å†™å…¥ååé‡å’Œé«˜æ•ˆå‹ç¼©çš„ç‰¹ç‚¹ã€‚
æ—¶åºæ•°æ®æ˜¯æŒ‰æ—¶é—´é¡ºåºè®°å½•çš„æ•°æ®ç‚¹åºåˆ—ï¼Œå¹¿æ³›åº”ç”¨äºIoTç›‘æ§ã€é‡‘èäº¤æ˜“ã€æ—¥å¿—åˆ†æç­‰åœºæ™¯ã€‚

**æ ¸å¿ƒç‰¹å¾**:

- **æ—¶é—´ç»´åº¦**ï¼šæ•°æ®æŒ‰æ—¶é—´é¡ºåºç»„ç»‡
- **é«˜å†™å…¥ç‡**ï¼šæ”¯æŒé«˜é¢‘æ•°æ®å†™å…¥
- **é«˜æ•ˆå‹ç¼©**ï¼šæ—¶é—´åºåˆ—æ•°æ®å‹ç¼©ç‡é«˜
- **èŒƒå›´æŸ¥è¯¢**ï¼šä¸»è¦æŸ¥è¯¢æ¨¡å¼æ˜¯æ—¶é—´èŒƒå›´æŸ¥è¯¢

---

## 2. æ—¶åºæ•°æ®ç‰¹å¾

### 2.1 æ•°æ®ç‰¹å¾

**æ—¶åºæ•°æ®çš„å…¸å‹ç‰¹å¾**:

| ç‰¹å¾ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| æ—¶é—´æˆ³ | æ¯ä¸ªæ•°æ®ç‚¹éƒ½æœ‰æ—¶é—´æˆ³ | 2025-01-15 10:30:00 |
| æŒ‡æ ‡å€¼ | æµ‹é‡çš„æ•°å€¼ | æ¸©åº¦25.5Â°C |
| æ ‡ç­¾é›† | æ ‡è¯†æ•°æ®æºçš„å…ƒæ•°æ® | device_id, sensor_type |
| é«˜é¢‘ç‡ | æ•°æ®é‡‡é›†é¢‘ç‡é«˜ | æ¯ç§’ã€æ¯åˆ†é’Ÿ |
| ä¸å¯å˜ | å†å²æ•°æ®ä¸ä¿®æ”¹ | åªè¿½åŠ ï¼Œä¸æ›´æ–° |

### 2.2 æ•°æ®æ¨¡å¼

**å…¸å‹æ—¶åºæ•°æ®æ¨¡å¼**:

```text
æ—¶é—´æˆ³ | è®¾å¤‡ID | ä¼ æ„Ÿå™¨ç±»å‹ | æŒ‡æ ‡å€¼
-------|--------|-----------|--------
10:00:00 | device_001 | temperature | 25.5
10:00:01 | device_001 | temperature | 25.6
10:00:02 | device_001 | temperature | 25.7
10:00:00 | device_002 | humidity | 60.0
10:00:01 | device_002 | humidity | 60.1
```

---

## 3. æ•°æ®æ¨¡å‹è®¾è®¡

### 3.1 åŸºç¡€æ¨¡å‹

**çª„è¡¨æ¨¡å‹ï¼ˆNarrow Tableï¼‰**:

```sql
-- çª„è¡¨ï¼šæ¯ä¸ªæŒ‡æ ‡ä¸€è¡Œ
CREATE TABLE time_series_narrow (
    timestamp TIMESTAMPTZ NOT NULL,
    device_id VARCHAR(50) NOT NULL,
    metric_name VARCHAR(50) NOT NULL,
    metric_value DOUBLE PRECISION NOT NULL,
    PRIMARY KEY (timestamp, device_id, metric_name)
);

-- ç¤ºä¾‹æ•°æ®
INSERT INTO time_series_narrow VALUES
    ('2025-01-15 10:00:00', 'device_001', 'temperature', 25.5),
    ('2025-01-15 10:00:01', 'device_001', 'temperature', 25.6),
    ('2025-01-15 10:00:00', 'device_001', 'humidity', 60.0);
```

**å®½è¡¨æ¨¡å‹ï¼ˆWide Tableï¼‰**:

```sql
-- å®½è¡¨ï¼šæ¯ä¸ªæ—¶é—´ç‚¹ä¸€è¡Œï¼Œå¤šä¸ªæŒ‡æ ‡åˆ—
CREATE TABLE time_series_wide (
    timestamp TIMESTAMPTZ NOT NULL,
    device_id VARCHAR(50) NOT NULL,
    temperature DOUBLE PRECISION,
    humidity DOUBLE PRECISION,
    pressure DOUBLE PRECISION,
    PRIMARY KEY (timestamp, device_id)
);
```

### 3.2 æ ‡ç­¾è®¾è®¡

**æ ‡ç­¾åˆ†ç¦»æ¨¡å‹**:

```sql
-- è®¾å¤‡è¡¨ï¼ˆæ ‡ç­¾ï¼‰
CREATE TABLE devices (
    device_id VARCHAR(50) PRIMARY KEY,
    device_name VARCHAR(200),
    location VARCHAR(200),
    device_type VARCHAR(50),
    manufacturer VARCHAR(100),
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- æ—¶åºæ•°æ®è¡¨ï¼ˆæŒ‡æ ‡å€¼ï¼‰
CREATE TABLE time_series_data (
    timestamp TIMESTAMPTZ NOT NULL,
    device_id VARCHAR(50) NOT NULL REFERENCES devices(device_id),
    metric_name VARCHAR(50) NOT NULL,
    metric_value DOUBLE PRECISION NOT NULL,
    quality_code INT DEFAULT 0, -- æ•°æ®è´¨é‡ç 
    PRIMARY KEY (timestamp, device_id, metric_name)
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_time_series_time ON time_series_data(timestamp DESC);
CREATE INDEX idx_time_series_device ON time_series_data(device_id, timestamp DESC);
```

### 3.3 åˆ†åŒºè®¾è®¡

**æ—¶é—´åˆ†åŒºç­–ç•¥**:

```sql
-- æŒ‰æœˆåˆ†åŒº
CREATE TABLE time_series_data (
    timestamp TIMESTAMPTZ NOT NULL,
    device_id VARCHAR(50) NOT NULL,
    metric_name VARCHAR(50) NOT NULL,
    metric_value DOUBLE PRECISION NOT NULL
) PARTITION BY RANGE (timestamp);

-- åˆ›å»ºåˆ†åŒº
CREATE TABLE time_series_data_2025_01
    PARTITION OF time_series_data
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

CREATE TABLE time_series_data_2025_02
    PARTITION OF time_series_data
    FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');
```

---

## 4. å‹ç¼©ç­–ç•¥

### 4.1 å‹ç¼©ç®—æ³•åŸç†

**æ—¶åºæ•°æ®å‹ç¼©ç‰¹ç‚¹**:

æ—¶åºæ•°æ®å…·æœ‰é«˜åº¦å¯å‹ç¼©æ€§ï¼Œä¸»è¦åŸå› ï¼š

1. **æ—¶é—´æˆ³æœ‰åºæ€§**: æ—¶é—´æˆ³é€šå¸¸æŒ‰é¡ºåºé€’å¢ï¼Œå·®å€¼å‹ç¼©æ•ˆæœå¥½
2. **æ•°å€¼å˜åŒ–ç¼“æ…¢**: ä¼ æ„Ÿå™¨æ•°æ®é€šå¸¸å˜åŒ–å¹…åº¦å°ï¼Œç›¸é‚»å€¼ç›¸ä¼¼åº¦é«˜
3. **æ ‡ç­¾é‡å¤åº¦é«˜**: è®¾å¤‡IDã€æŒ‡æ ‡åç§°ç­‰æ ‡ç­¾æ•°æ®é‡å¤å‡ºç°
4. **ç¨€ç–æ€§**: æŸäº›æ—¶é—´ç‚¹å¯èƒ½æ²¡æœ‰æ•°æ®ï¼Œå­˜åœ¨å¤§é‡ç©ºå€¼

**å‹ç¼©ç®—æ³•ç±»å‹**:

| ç®—æ³•ç±»å‹ | é€‚ç”¨åœºæ™¯ | å‹ç¼©æ¯” | æŸ¥è¯¢æ€§èƒ½ |
|---------|---------|--------|---------|
| Deltaç¼–ç  | æ—¶é—´æˆ³ã€é€’å¢ID | 10:1 - 50:1 | é«˜ |
| æ¸¸ç¨‹ç¼–ç  | é‡å¤å€¼åºåˆ— | 5:1 - 20:1 | ä¸­ |
| å­—å…¸å‹ç¼© | å­—ç¬¦ä¸²æ ‡ç­¾ | 3:1 - 10:1 | é«˜ |
| Gorillaå‹ç¼© | æµ®ç‚¹æ•° | 8:1 - 30:1 | é«˜ |
| LZ4/ZSTD | é€šç”¨å‹ç¼© | 2:1 - 5:1 | ä¸­ |

**TimescaleDBå‹ç¼©åŸç†**:

TimescaleDBä½¿ç”¨åˆ—å¼å‹ç¼©ï¼Œå°†æ•°æ®æŒ‰åˆ—å­˜å‚¨å¹¶å‹ç¼©ï¼š

1. **Segmentby**: æŒ‰æŒ‡å®šåˆ—åˆ†ç»„ï¼Œç›¸åŒåˆ†ç»„çš„è¡Œå­˜å‚¨åœ¨ä¸€èµ·
2. **Orderby**: æŒ‰æŒ‡å®šåˆ—æ’åºï¼Œæé«˜å‹ç¼©ç‡
3. **å‹ç¼©ç®—æ³•**: ä½¿ç”¨PostgreSQLå†…ç½®çš„å‹ç¼©ç®—æ³•ï¼ˆLZ4/ZSTDï¼‰

### 4.2 PostgreSQLå‹ç¼©å®ç°

**åŸºç¡€å‹ç¼©é…ç½®**:

```sql
-- 1. å¯ç”¨å‹ç¼©
ALTER TABLE sensor_data SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'device_id',
    timescaledb.compress_orderby = 'time DESC'
);

-- 2. æ·»åŠ å‹ç¼©ç­–ç•¥ï¼ˆ7å¤©å‰æ•°æ®è‡ªåŠ¨å‹ç¼©ï¼‰
SELECT add_compression_policy(
    'sensor_data',
    INTERVAL '7 days',
    if_not_exists => true
);
```

**é«˜çº§å‹ç¼©é…ç½®**:

```sql
-- 1. å¤šåˆ—segmentbyï¼ˆæé«˜å‹ç¼©ç‡ï¼‰
ALTER TABLE sensor_data SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'device_id, sensor_type',  -- å¤šåˆ—åˆ†ç»„
    timescaledb.compress_orderby = 'time DESC'
);

-- 2. æŸ¥çœ‹å‹ç¼©é…ç½®
SELECT
    hypertable_name,
    compress_segmentby,
    compress_orderby,
    compress_chunk_time_interval
FROM timescaledb_information.hypertables
WHERE hypertable_name = 'sensor_data';
```

**æ‰‹åŠ¨å‹ç¼©æ“ä½œ**:

```sql
-- 1. å‹ç¼©æŒ‡å®šchunk
SELECT compress_chunk('_timescaledb_internal._hyper_1_1_chunk');

-- 2. å‹ç¼©æŒ‡å®šæ—¶é—´èŒƒå›´çš„æ‰€æœ‰chunk
SELECT compress_chunk(chunk)
FROM timescaledb_information.chunks
WHERE hypertable_name = 'sensor_data'
  AND range_start < NOW() - INTERVAL '7 days'
  AND is_compressed = false;

-- 3. æ‰¹é‡å‹ç¼©å‡½æ•°
CREATE OR REPLACE FUNCTION compress_old_chunks(
    p_hypertable_name TEXT,
    p_older_than INTERVAL
)
RETURNS TABLE(chunk_name TEXT, compressed BOOLEAN) AS $$
DECLARE
    v_chunk RECORD;
BEGIN
    FOR v_chunk IN
        SELECT chunk_schema || '.' || chunk_name AS full_chunk_name
        FROM timescaledb_information.chunks
        WHERE hypertable_name = p_hypertable_name
          AND range_end < NOW() - p_older_than
          AND is_compressed = false
    LOOP
        BEGIN
            PERFORM compress_chunk(v_chunk.full_chunk_name);
            chunk_name := v_chunk.full_chunk_name;
            compressed := true;
            RETURN NEXT;
        EXCEPTION WHEN OTHERS THEN
            chunk_name := v_chunk.full_chunk_name;
            compressed := false;
            RETURN NEXT;
        END;
    END LOOP;
END;
$$ LANGUAGE plpgsql;
```

### 4.3 å‹ç¼©æ•ˆæœè¯„ä¼°

**å‹ç¼©ç»Ÿè®¡æŸ¥è¯¢**:

```sql
-- 1. æŸ¥çœ‹å‹ç¼©chunkç»Ÿè®¡
SELECT
    hypertable_name,
    chunk_name,
    range_start,
    range_end,
    pg_size_pretty(before_compression_total_bytes) AS uncompressed_size,
    pg_size_pretty(after_compression_total_bytes) AS compressed_size,
    ROUND(100.0 * (1 - after_compression_total_bytes::NUMERIC / before_compression_total_bytes), 2) AS compression_ratio_pct,
    numrows_pre_compression,
    numrows_post_compression
FROM timescaledb_information.compressed_chunk_stats
WHERE hypertable_name = 'sensor_data'
ORDER BY range_start DESC;

-- 2. å‹ç¼©ç‡æ±‡æ€»ç»Ÿè®¡
SELECT
    hypertable_name,
    COUNT(*) AS compressed_chunks,
    pg_size_pretty(SUM(before_compression_total_bytes)) AS total_uncompressed,
    pg_size_pretty(SUM(after_compression_total_bytes)) AS total_compressed,
    ROUND(100.0 * (1 - SUM(after_compression_total_bytes)::NUMERIC / SUM(before_compression_total_bytes)), 2) AS avg_compression_ratio_pct
FROM timescaledb_information.compressed_chunk_stats
WHERE hypertable_name = 'sensor_data'
GROUP BY hypertable_name;
```

**å‹ç¼©æœ€ä½³å®è·µ**:

1. **segmentbyé€‰æ‹©åŸåˆ™**:
   - é€‰æ‹©æŸ¥è¯¢ä¸­ç»å¸¸ä¸€èµ·è¿‡æ»¤çš„åˆ—
   - é€‰æ‹©åŸºæ•°é€‚ä¸­çš„åˆ—ï¼ˆé¿å…è¿‡é«˜æˆ–è¿‡ä½ï¼‰
   - é€šå¸¸é€‰æ‹©è®¾å¤‡IDã€æŒ‡æ ‡ç±»å‹ç­‰

2. **orderbyé€‰æ‹©åŸåˆ™**:
   - å¿…é¡»åŒ…å«æ—¶é—´åˆ—
   - æŒ‰æ—¶é—´é™åºæ’åˆ—ï¼ˆæœ€æ–°æ•°æ®åœ¨å‰ï¼‰
   - å¯ä»¥æ·»åŠ å…¶ä»–æ’åºåˆ—æé«˜å‹ç¼©ç‡

3. **å‹ç¼©æ—¶æœº**:
   - æ•°æ®å†™å…¥7-30å¤©åå‹ç¼©ï¼ˆå¹³è¡¡æŸ¥è¯¢æ€§èƒ½å’Œå‹ç¼©ç‡ï¼‰
   - é¿å…å‹ç¼©æœ€æ–°æ•°æ®ï¼ˆå½±å“å†™å…¥æ€§èƒ½ï¼‰
   - å®šæœŸç›‘æ§å‹ç¼©ä»»åŠ¡æ‰§è¡Œæƒ…å†µ

4. **å‹ç¼©ç‡ç›®æ ‡**:
   - æ—¶åºæ•°æ®é€šå¸¸å¯è¾¾åˆ°10:1åˆ°50:1çš„å‹ç¼©æ¯”
   - å¦‚æœå‹ç¼©ç‡ä½äº5:1ï¼Œéœ€è¦ä¼˜åŒ–segmentbyå’Œorderbyé…ç½®
   - ç›‘æ§å‹ç¼©åçš„æŸ¥è¯¢æ€§èƒ½ï¼Œç¡®ä¿ä¸ä¼šæ˜¾è‘—ä¸‹é™

---

## 5. æŸ¥è¯¢æ¨¡å¼ä¸ä¼˜åŒ–

### 5.1 å¸¸è§æŸ¥è¯¢æ¨¡å¼

**æ—¶é—´èŒƒå›´æŸ¥è¯¢**:

```sql
-- âœ… ä¼˜åŒ–ï¼šä½¿ç”¨æ—¶é—´èŒƒå›´æŸ¥è¯¢ï¼ˆè‡ªåŠ¨åˆ†åŒºå‰ªæï¼‰
SELECT
    time_bucket('1 hour', timestamp) AS hour,
    device_id,
    AVG(metric_value) AS avg_value,
    MAX(metric_value) AS max_value,
    MIN(metric_value) AS min_value,
    COUNT(*) AS sample_count
FROM time_series_data
WHERE timestamp >= '2025-01-15 00:00:00'
  AND timestamp < '2025-01-16 00:00:00'
  AND device_id = 'device_001'
  AND metric_name = 'temperature'
GROUP BY hour, device_id
ORDER BY hour;

-- âŒ é¿å…ï¼šä½¿ç”¨å‡½æ•°ï¼ˆåˆ†åŒºå‰ªæå¤±æ•ˆï¼‰
SELECT * FROM time_series_data
WHERE DATE_TRUNC('day', timestamp) = CURRENT_DATE;
```

**æœ€æ–°å€¼æŸ¥è¯¢ä¼˜åŒ–**:

```sql
-- æ–¹æ³•1: ä½¿ç”¨DISTINCT ONï¼ˆé€‚åˆå°è§„æ¨¡è®¾å¤‡ï¼‰
SELECT DISTINCT ON (device_id)
    device_id,
    timestamp,
    metric_value
FROM time_series_data
WHERE metric_name = 'temperature'
ORDER BY device_id, timestamp DESC;

-- æ–¹æ³•2: ä½¿ç”¨LATERAL JOINï¼ˆé€‚åˆå¤§è§„æ¨¡è®¾å¤‡ï¼‰
SELECT d.device_id, l.timestamp, l.metric_value
FROM devices d
CROSS JOIN LATERAL (
    SELECT timestamp, metric_value
    FROM time_series_data
    WHERE device_id = d.device_id
      AND metric_name = 'temperature'
    ORDER BY timestamp DESC
    LIMIT 1
) l;

-- æ–¹æ³•3: ä½¿ç”¨çª—å£å‡½æ•°ï¼ˆé€‚åˆéœ€è¦å¤šä¸ªæœ€æ–°å€¼ï¼‰
SELECT device_id, timestamp, metric_value
FROM (
    SELECT
        device_id,
        timestamp,
        metric_value,
        ROW_NUMBER() OVER (PARTITION BY device_id ORDER BY timestamp DESC) AS rn
    FROM time_series_data
    WHERE metric_name = 'temperature'
) t
WHERE rn = 1;
```

**èšåˆæŸ¥è¯¢ä¼˜åŒ–**:

```sql
-- åŸºç¡€èšåˆæŸ¥è¯¢
SELECT
    time_bucket('5 minutes', timestamp) AS bucket,
    device_id,
    COUNT(*) AS data_points,
    AVG(metric_value) AS avg_value,
    STDDEV(metric_value) AS stddev_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY metric_value) AS median_value,
    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY metric_value) AS p95_value
FROM time_series_data
WHERE timestamp > NOW() - INTERVAL '1 hour'
GROUP BY bucket, device_id
ORDER BY bucket DESC;

-- å¤šæŒ‡æ ‡èšåˆæŸ¥è¯¢
SELECT
    time_bucket('1 hour', timestamp) AS bucket,
    device_id,
    COUNT(*) FILTER (WHERE metric_name = 'temperature') AS temp_samples,
    AVG(metric_value) FILTER (WHERE metric_name = 'temperature') AS avg_temp,
    COUNT(*) FILTER (WHERE metric_name = 'humidity') AS humidity_samples,
    AVG(metric_value) FILTER (WHERE metric_name = 'humidity') AS avg_humidity
FROM time_series_data
WHERE timestamp >= NOW() - INTERVAL '24 hours'
GROUP BY bucket, device_id
ORDER BY bucket DESC;
```

**Gap-fillingæŸ¥è¯¢**:

```sql
-- ä½¿ç”¨time_bucket_gapfillå¡«å……ç¼ºå¤±æ—¶é—´ç‚¹
SELECT
    time_bucket_gapfill('1 hour', timestamp,
        start => NOW() - INTERVAL '24 hours',
        finish => NOW()) AS bucket,
    device_id,
    LOCF(AVG(metric_value)) AS avg_value,  -- Last Observation Carried Forward
    INTERPOLATE(AVG(metric_value)) AS interpolated_value  -- çº¿æ€§æ’å€¼
FROM time_series_data
WHERE timestamp >= NOW() - INTERVAL '24 hours'
  AND device_id = 'device_001'
GROUP BY bucket, device_id
ORDER BY bucket;
```

### 5.2 è¿ç»­èšåˆ

**åˆ›å»ºå¤šçº§è¿ç»­èšåˆ**:

```sql
-- 1. åˆ†é’Ÿçº§èšåˆï¼ˆå®æ—¶ç›‘æ§ï¼‰
CREATE MATERIALIZED VIEW time_series_1min
WITH (timescaledb.continuous, timescaledb.materialized_only = false) AS
SELECT
    time_bucket('1 minute', timestamp) AS bucket,
    device_id,
    metric_name,
    AVG(metric_value) AS avg_value,
    MIN(metric_value) AS min_value,
    MAX(metric_value) AS max_value,
    COUNT(*) AS sample_count,
    STDDEV(metric_value) AS stddev_value
FROM time_series_data
GROUP BY bucket, device_id, metric_name;

-- 2. å°æ—¶çº§èšåˆï¼ˆåŸºäºåˆ†é’Ÿçº§ï¼‰
CREATE MATERIALIZED VIEW time_series_1hour
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 hour', bucket) AS bucket,
    device_id,
    metric_name,
    AVG(avg_value) AS avg_value,
    MIN(min_value) AS min_value,
    MAX(max_value) AS max_value,
    SUM(sample_count) AS total_samples
FROM time_series_1min
GROUP BY bucket, device_id, metric_name;

-- 3. å¤©çº§èšåˆï¼ˆåŸºäºå°æ—¶çº§ï¼‰
CREATE MATERIALIZED VIEW time_series_1day
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 day', bucket) AS bucket,
    device_id,
    metric_name,
    AVG(avg_value) AS avg_value,
    MIN(min_value) AS min_value,
    MAX(max_value) AS max_value,
    SUM(total_samples) AS total_samples
FROM time_series_1hour
GROUP BY bucket, device_id, metric_name;

-- 4. é…ç½®åˆ·æ–°ç­–ç•¥
SELECT add_continuous_aggregate_policy(
    'time_series_1min',
    start_offset => INTERVAL '3 hours',
    end_offset => INTERVAL '1 minute',
    schedule_interval => INTERVAL '1 minute'
);

SELECT add_continuous_aggregate_policy(
    'time_series_1hour',
    start_offset => INTERVAL '3 days',
    end_offset => INTERVAL '1 hour',
    schedule_interval => INTERVAL '1 hour'
);

SELECT add_continuous_aggregate_policy(
    'time_series_1day',
    start_offset => INTERVAL '7 days',
    end_offset => INTERVAL '1 day',
    schedule_interval => INTERVAL '1 day'
);
```

**æŸ¥è¯¢è¿ç»­èšåˆ**:

```sql
-- æŸ¥è¯¢æœ€è¿‘24å°æ—¶æ•°æ®ï¼ˆä½¿ç”¨å°æ—¶çº§èšåˆï¼‰
SELECT * FROM time_series_1hour
WHERE bucket >= NOW() - INTERVAL '24 hours'
  AND device_id = 'device_001'
ORDER BY bucket DESC;

-- æŸ¥è¯¢æœ€è¿‘7å¤©æ•°æ®ï¼ˆä½¿ç”¨å¤©çº§èšåˆï¼‰
SELECT * FROM time_series_1day
WHERE bucket >= NOW() - INTERVAL '7 days'
  AND device_id = 'device_001'
ORDER BY bucket DESC;

-- æ··åˆæŸ¥è¯¢ï¼ˆæœ€æ–°æ•°æ®ç”¨åˆ†é’Ÿçº§ï¼Œå†å²æ•°æ®ç”¨å°æ—¶çº§ï¼‰
SELECT * FROM time_series_1min
WHERE bucket >= NOW() - INTERVAL '1 hour'
  AND device_id = 'device_001'
UNION ALL
SELECT * FROM time_series_1hour
WHERE bucket >= NOW() - INTERVAL '24 hours'
  AND bucket < NOW() - INTERVAL '1 hour'
  AND device_id = 'device_001'
ORDER BY bucket DESC;
```

### 5.3 æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–

**ç´¢å¼•ä¼˜åŒ–**:

```sql
-- 1. å¤åˆç´¢å¼•ï¼ˆè®¾å¤‡ID + æ—¶é—´ï¼‰
CREATE INDEX idx_time_series_device_time
ON time_series_data(device_id, timestamp DESC);

-- 2. éƒ¨åˆ†ç´¢å¼•ï¼ˆä»…ç´¢å¼•æ´»è·ƒè®¾å¤‡ï¼‰
CREATE INDEX idx_time_series_active_device
ON time_series_data(device_id, timestamp DESC)
WHERE device_id IN (SELECT device_id FROM active_devices);

-- 3. è¡¨è¾¾å¼ç´¢å¼•ï¼ˆç”¨äºç‰¹å®šæŸ¥è¯¢æ¨¡å¼ï¼‰
CREATE INDEX idx_time_series_date_device
ON time_series_data((timestamp::DATE), device_id);

-- 4. GINç´¢å¼•ï¼ˆç”¨äºJSONBæ ‡ç­¾æŸ¥è¯¢ï¼‰
CREATE INDEX idx_time_series_tags
ON time_series_data USING GIN(tags);
```

**æŸ¥è¯¢ä¼˜åŒ–æŠ€å·§**:

```sql
-- 1. ä½¿ç”¨LIMITé™åˆ¶ç»“æœé›†
SELECT * FROM time_series_data
WHERE device_id = 'device_001'
ORDER BY timestamp DESC
LIMIT 1000;

-- 2. ä½¿ç”¨æ—¶é—´èŒƒå›´é™åˆ¶
SELECT * FROM time_series_data
WHERE device_id = 'device_001'
  AND timestamp >= NOW() - INTERVAL '1 hour'
ORDER BY timestamp DESC;

-- 3. ä½¿ç”¨è¿ç»­èšåˆä»£æ›¿åŸå§‹æ•°æ®æŸ¥è¯¢
-- âŒ æ…¢ï¼šæŸ¥è¯¢åŸå§‹æ•°æ®
SELECT AVG(metric_value)
FROM time_series_data
WHERE timestamp >= NOW() - INTERVAL '7 days'
  AND device_id = 'device_001';

-- âœ… å¿«ï¼šä½¿ç”¨è¿ç»­èšåˆ
SELECT AVG(avg_value)
FROM time_series_1hour
WHERE bucket >= NOW() - INTERVAL '7 days'
  AND device_id = 'device_001';

-- 4. å¹¶è¡ŒæŸ¥è¯¢ï¼ˆPostgreSQL 13+ï¼‰
SET max_parallel_workers_per_gather = 4;
SET parallel_setup_cost = 100;
SET parallel_tuple_cost = 0.01;

EXPLAIN ANALYZE
SELECT AVG(metric_value)
FROM time_series_data
WHERE timestamp >= NOW() - INTERVAL '30 days'
GROUP BY device_id;
```

---

## 6. PostgreSQLå®ç°

### 6.1 å®Œæ•´æ—¶åºæ•°æ®æ¨¡å‹

```sql
-- è®¾å¤‡è¡¨
CREATE TABLE devices (
    device_id VARCHAR(50) PRIMARY KEY,
    device_name VARCHAR(200) NOT NULL,
    device_type VARCHAR(50),
    location JSONB,
    metadata JSONB,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- æ—¶åºæ•°æ®è¡¨ï¼ˆä½¿ç”¨TimescaleDBï¼‰
CREATE TABLE time_series_metrics (
    time TIMESTAMPTZ NOT NULL,
    device_id VARCHAR(50) NOT NULL REFERENCES devices(device_id),
    metric_name VARCHAR(50) NOT NULL,
    metric_value DOUBLE PRECISION NOT NULL,
    quality_code INT DEFAULT 0,
    tags JSONB
);

-- è½¬æ¢ä¸ºHypertable
SELECT create_hypertable('time_series_metrics', 'time',
    chunk_time_interval => INTERVAL '1 day');

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_metrics_device_time ON time_series_metrics(device_id, time DESC);
CREATE INDEX idx_metrics_metric ON time_series_metrics(metric_name, time DESC);
CREATE INDEX idx_metrics_tags ON time_series_metrics USING GIN(tags);

-- å¯ç”¨å‹ç¼©
ALTER TABLE time_series_metrics SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'device_id, metric_name',
    timescaledb.compress_orderby = 'time DESC'
);

-- æ·»åŠ å‹ç¼©ç­–ç•¥
SELECT add_compression_policy('time_series_metrics', INTERVAL '7 days');

-- æ•°æ®ä¿ç•™ç­–ç•¥
SELECT add_retention_policy('time_series_metrics', INTERVAL '90 days');
```

### 6.2 æ•°æ®å†™å…¥å‡½æ•°

```sql
-- æ‰¹é‡å†™å…¥å‡½æ•°
CREATE OR REPLACE FUNCTION insert_time_series_batch(
    p_data JSONB
)
RETURNS INT AS $$
DECLARE
    v_count INT := 0;
    v_item JSONB;
BEGIN
    FOR v_item IN SELECT * FROM jsonb_array_elements(p_data)
    LOOP
        INSERT INTO time_series_metrics (
            time, device_id, metric_name, metric_value, tags
        ) VALUES (
            (v_item->>'time')::TIMESTAMPTZ,
            v_item->>'device_id',
            v_item->>'metric_name',
            (v_item->>'metric_value')::DOUBLE PRECISION,
            v_item->'tags'
        );
        v_count := v_count + 1;
    END LOOP;

    RETURN v_count;
END;
$$ LANGUAGE plpgsql;

-- ä½¿ç”¨ç¤ºä¾‹
SELECT insert_time_series_batch('[
    {"time": "2025-01-15T10:00:00Z", "device_id": "device_001",
     "metric_name": "temperature", "metric_value": 25.5, "tags": {}},
    {"time": "2025-01-15T10:00:01Z", "device_id": "device_001",
     "metric_name": "temperature", "metric_value": 25.6, "tags": {}}
]'::JSONB);
```

---

## 7. å®é™…åº”ç”¨åœºæ™¯

### 7.1 IoTä¼ æ„Ÿå™¨ç›‘æ§

**åœºæ™¯æè¿°**: 1000ä¸ªä¼ æ„Ÿå™¨ï¼Œæ¯ç§’é‡‡é›†ä¸€æ¬¡æ•°æ®ï¼Œéœ€è¦å®æ—¶ç›‘æ§å’Œå†å²åˆ†æã€‚

**æ•°æ®æ¨¡å‹**:

```sql
-- ä¼ æ„Ÿå™¨è®¾å¤‡è¡¨
CREATE TABLE sensors (
    sensor_id VARCHAR(50) PRIMARY KEY,
    sensor_name VARCHAR(200) NOT NULL,
    sensor_type VARCHAR(50) NOT NULL,
    location JSONB,
    metadata JSONB,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ä¼ æ„Ÿå™¨æ•°æ®è¡¨
CREATE TABLE sensor_readings (
    time TIMESTAMPTZ NOT NULL,
    sensor_id VARCHAR(50) NOT NULL REFERENCES sensors(sensor_id),
    metric_name VARCHAR(50) NOT NULL,
    metric_value DOUBLE PRECISION NOT NULL,
    quality_code INT DEFAULT 100,
    tags JSONB DEFAULT '{}'
);

-- åˆ›å»ºHypertable
SELECT create_hypertable(
    'sensor_readings',
    'time',
    chunk_time_interval => INTERVAL '1 day'
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_sensor_readings_sensor_time
ON sensor_readings(sensor_id, time DESC);
CREATE INDEX idx_sensor_readings_metric_time
ON sensor_readings(metric_name, time DESC);

-- åˆ›å»ºè¿ç»­èšåˆ
CREATE MATERIALIZED VIEW sensor_readings_1min
WITH (timescaledb.continuous, timescaledb.materialized_only = false) AS
SELECT
    time_bucket('1 minute', time) AS bucket,
    sensor_id,
    metric_name,
    AVG(metric_value) AS avg_value,
    MIN(metric_value) AS min_value,
    MAX(metric_value) AS max_value,
    COUNT(*) AS sample_count
FROM sensor_readings
GROUP BY bucket, sensor_id, metric_name;
```

**æ€§èƒ½æŒ‡æ ‡**:

- **å†™å…¥æ€§èƒ½**: 100,000è¡Œ/ç§’
- **æŸ¥è¯¢æ€§èƒ½**:
  - å®æ—¶æŸ¥è¯¢ï¼ˆæœ€è¿‘1å°æ—¶ï¼‰: < 100ms
  - å†å²æŸ¥è¯¢ï¼ˆ90å¤©ï¼‰: < 1sï¼ˆä½¿ç”¨è¿ç»­èšåˆï¼‰
- **å‹ç¼©ç‡**: 15:1

---

### 7.2 é‡‘èäº¤æ˜“æ•°æ®

**åœºæ™¯æè¿°**: é«˜é¢‘äº¤æ˜“ç³»ç»Ÿï¼Œæ¯ç§’ç™¾ä¸‡çº§äº¤æ˜“è®°å½•ï¼Œéœ€è¦å¿«é€ŸæŸ¥è¯¢å’Œé•¿æœŸå­˜å‚¨ã€‚

**æ•°æ®æ¨¡å‹**:

```sql
-- äº¤æ˜“è¡¨
CREATE TABLE trades (
    time TIMESTAMPTZ NOT NULL,
    symbol VARCHAR(20) NOT NULL,
    exchange VARCHAR(10) NOT NULL,
    price DECIMAL(20, 8) NOT NULL,
    volume DECIMAL(20, 8) NOT NULL,
    trade_type CHAR(1) NOT NULL,
    trade_id BIGINT NOT NULL
);

-- åˆ›å»ºHypertableï¼ˆæŒ‰å°æ—¶åˆ†åŒºï¼‰
SELECT create_hypertable(
    'trades',
    'time',
    chunk_time_interval => INTERVAL '1 hour'
);

-- åˆ›å»ºç§’çº§èšåˆï¼ˆå®æ—¶ç›‘æ§ï¼‰
CREATE MATERIALIZED VIEW trades_1sec
WITH (timescaledb.continuous, timescaledb.materialized_only = false) AS
SELECT
    time_bucket('1 second', time) AS bucket,
    symbol,
    exchange,
    COUNT(*) AS trade_count,
    SUM(volume) AS total_volume,
    AVG(price) AS avg_price,
    MIN(price) AS min_price,
    MAX(price) AS max_price
FROM trades
GROUP BY bucket, symbol, exchange;
```

---

### 7.3 æ—¥å¿—åˆ†æç³»ç»Ÿ

**åœºæ™¯æè¿°**: åº”ç”¨æ—¥å¿—åˆ†æï¼Œæ¯å¤©TBçº§æ—¥å¿—æ•°æ®ï¼Œéœ€è¦å¿«é€Ÿæ£€ç´¢å’Œåˆ†æã€‚

**æ•°æ®æ¨¡å‹**:

```sql
-- æ—¥å¿—è¡¨
CREATE TABLE application_logs (
    time TIMESTAMPTZ NOT NULL,
    service_name VARCHAR(100) NOT NULL,
    log_level VARCHAR(20) NOT NULL,
    message TEXT NOT NULL,
    context JSONB DEFAULT '{}',
    user_id VARCHAR(50),
    request_id VARCHAR(100)
);

-- åˆ›å»ºHypertable
SELECT create_hypertable(
    'application_logs',
    'time',
    chunk_time_interval => INTERVAL '1 day'
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_logs_service_time
ON application_logs(service_name, time DESC);
CREATE INDEX idx_logs_level_time
ON application_logs(log_level, time DESC);
CREATE INDEX idx_logs_context
ON application_logs USING GIN(context);

-- åˆ›å»ºé”™è¯¯æ—¥å¿—èšåˆ
CREATE MATERIALIZED VIEW error_logs_hourly
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 hour', time) AS bucket,
    service_name,
    log_level,
    COUNT(*) AS error_count,
    COUNT(DISTINCT user_id) AS affected_users
FROM application_logs
WHERE log_level IN ('ERROR', 'FATAL')
GROUP BY bucket, service_name, log_level;
```

---

## 8. ç›¸å…³èµ„æº

- [TimescaleDBå®è·µ](./TimescaleDBå®è·µ.md) - TimescaleDBè¯¦ç»†æŒ‡å—
- [è®¾å¤‡å­ªç”Ÿæ¨¡å‹](./è®¾å¤‡å­ªç”Ÿæ¨¡å‹.md) - è®¾å¤‡å­ªç”Ÿå»ºæ¨¡
- [TimescaleDBå®˜æ–¹æ–‡æ¡£](https://docs.timescale.com/) - TimescaleDBå®˜æ–¹æ–‡æ¡£
- [æ—¶åºæ•°æ®åº“è®¾è®¡æœ€ä½³å®è·µ](https://www.timescale.com/blog/) - TimescaleDBåšå®¢

---

**æœ€åæ›´æ–°**: 2025å¹´1æœˆ
**ç»´æŠ¤è€…**: PostgreSQL Modern Team
