---

> **📋 文档来源**: `PostgreSQL_View\05-合规与可信\数据溯源\概率数据库实现.md`
> **📅 复制日期**: 2025-12-22
> **⚠️ 注意**: 本文档为复制版本，原文件保持不变

---

# 概率数据库实现

> **更新时间**: 2025年1月
> **技术版本**: PostgreSQL 18
> **文档编号**: 05-05-02

---

## 📑 目录

- [概率数据库实现](#概率数据库实现)
  - [📑 目录](#-目录)
  - [1. 概述](#1-概述)
    - [1.1 技术背景](#11-技术背景)
    - [1.2 技术定位](#12-技术定位)
    - [1.3 核心价值](#13-核心价值)
  - [2. 技术原理](#2-技术原理)
    - [2.1 概率数据库基础](#21-概率数据库基础)
    - [2.2 不确定性数据模型](#22-不确定性数据模型)
    - [2.3 概率查询处理](#23-概率查询处理)
  - [3. PostgreSQL实现方案](#3-postgresql实现方案)
    - [3.1 扩展架构设计](#31-扩展架构设计)
    - [3.2 数据类型扩展](#32-数据类型扩展)
    - [3.3 查询处理扩展](#33-查询处理扩展)
  - [4. ProvSQL集成](#4-provsql集成)
    - [4.1 ProvSQL概述](#41-provsql概述)
      - [**4.1.1 核心功能**](#411-核心功能)
      - [**4.1.2 技术特点**](#412-技术特点)
      - [**4.1.3 适用场景**](#413-适用场景)
    - [4.2 安装和配置](#42-安装和配置)
      - [**4.2.1 安装步骤**](#421-安装步骤)
      - [**4.2.2 配置选项**](#422-配置选项)
      - [**4.2.3 验证安装**](#423-验证安装)
    - [4.3 集成架构](#43-集成架构)
    - [4.4 核心功能详解](#44-核心功能详解)
      - [**4.4.1 溯源追踪**](#441-溯源追踪)
      - [**4.4.2 概率计算**](#442-概率计算)
      - [**4.4.3 溯源和概率结合**](#443-溯源和概率结合)
    - [4.5 使用示例](#45-使用示例)
      - [**4.5.1 基础使用示例**](#451-基础使用示例)
      - [**4.5.2 概率查询示例**](#452-概率查询示例)
      - [**4.5.3 溯源查询示例**](#453-溯源查询示例)
      - [**4.5.4 复杂查询示例**](#454-复杂查询示例)
      - [**4.5.5 实际应用场景示例**](#455-实际应用场景示例)
  - [5. 实际应用案例](#5-实际应用案例)
    - [5.1 传感器数据不确定性处理](#51-传感器数据不确定性处理)
    - [5.2 数据融合场景](#52-数据融合场景)
    - [5.3 其他应用场景](#53-其他应用场景)
      - [**5.3.1 数据质量评估**](#531-数据质量评估)
      - [**5.3.2 异常检测**](#532-异常检测)
      - [**5.3.3 数据溯源审计**](#533-数据溯源审计)
  - [6. 性能分析](#6-性能分析)
    - [6.1 查询性能影响](#61-查询性能影响)
      - [**6.1.1 性能影响因素**](#611-性能影响因素)
      - [**6.1.2 性能测试数据**](#612-性能测试数据)
      - [**6.1.3 性能优化策略**](#613-性能优化策略)
    - [6.2 存储开销分析](#62-存储开销分析)
      - [**6.2.1 存储开销详细分析**](#621-存储开销详细分析)
      - [**6.2.2 存储优化策略**](#622-存储优化策略)
    - [6.3 性能基准测试](#63-性能基准测试)
      - [**6.3.1 基准测试脚本**](#631-基准测试脚本)
      - [**6.3.2 性能监控指标**](#632-性能监控指标)
    - [6.4 性能优化建议](#64-性能优化建议)
      - [**6.4.1 查询优化建议**](#641-查询优化建议)
      - [**6.4.2 存储优化建议**](#642-存储优化建议)
  - [7. 最佳实践](#7-最佳实践)
    - [7.1 使用场景](#71-使用场景)
      - [**7.1.1 适用场景**](#711-适用场景)
      - [**7.1.2 不适用场景**](#712-不适用场景)
    - [7.2 设计原则](#72-设计原则)
      - [**7.2.1 概率值设计**](#721-概率值设计)
      - [**7.2.2 数据模型设计**](#722-数据模型设计)
    - [7.3 性能优化](#73-性能优化)
      - [**7.3.1 查询优化**](#731-查询优化)
      - [**7.3.2 存储优化**](#732-存储优化)
    - [7.4 常见问题和解决方案](#74-常见问题和解决方案)
      - [**7.4.1 概率值不一致**](#741-概率值不一致)
      - [**7.4.2 性能问题**](#742-性能问题)
      - [**7.4.3 存储空间问题**](#743-存储空间问题)
    - [7.5 注意事项](#75-注意事项)
      - [**7.5.1 概率值管理**](#751-概率值管理)
      - [**7.5.2 查询复杂度**](#752-查询复杂度)
      - [**7.5.3 数据质量**](#753-数据质量)
    - [7.6 实施建议](#76-实施建议)
      - [**7.6.1 渐进式实施**](#761-渐进式实施)
      - [**7.6.2 团队培训**](#762-团队培训)
      - [**7.6.3 监控和维护**](#763-监控和维护)
  - [8. 参考资料](#8-参考资料)
    - [8.1 学术论文](#81-学术论文)
      - [**8.1.1 概率数据库基础理论**](#811-概率数据库基础理论)
      - [**8.1.2 ProvSQL和数据溯源**](#812-provsql和数据溯源)
      - [**8.1.3 不确定性数据处理**](#813-不确定性数据处理)
    - [8.2 官方文档](#82-官方文档)
      - [**8.2.1 PostgreSQL文档**](#821-postgresql文档)
      - [**8.2.2 ProvSQL文档**](#822-provsql文档)
    - [8.3 开源项目和工具](#83-开源项目和工具)
      - [**8.3.1 概率数据库项目**](#831-概率数据库项目)
      - [**8.3.2 相关工具**](#832-相关工具)
    - [8.4 社区资源](#84-社区资源)
      - [**8.4.1 论坛和社区**](#841-论坛和社区)
      - [**8.4.2 博客和文章**](#842-博客和文章)
    - [8.5 书籍推荐](#85-书籍推荐)
      - [**8.5.1 概率数据库相关书籍**](#851-概率数据库相关书籍)
      - [**8.5.2 PostgreSQL相关书籍**](#852-postgresql相关书籍)
      - [**8.5.3 数据溯源相关书籍**](#853-数据溯源相关书籍)
      - [**8.5.4 数据库理论相关书籍**](#854-数据库理论相关书籍)
    - [8.6 视频教程](#86-视频教程)
      - [**8.6.1 ProvSQL相关视频**](#861-provsql相关视频)
      - [**8.6.2 概率数据库课程**](#862-概率数据库课程)
      - [**8.6.3 PostgreSQL扩展开发教程**](#863-postgresql扩展开发教程)
      - [**8.6.4 数据溯源相关视频**](#864-数据溯源相关视频)
      - [**8.6.5 在线学习平台推荐**](#865-在线学习平台推荐)
    - [8.7 技术博客和案例](#87-技术博客和案例)
      - [**8.7.1 应用案例**](#871-应用案例)
      - [**8.7.2 最佳实践**](#872-最佳实践)
      - [**8.7.3 技术博客推荐**](#873-技术博客推荐)
      - [**8.7.4 实际项目案例**](#874-实际项目案例)
    - [8.8 研究机构和实验室](#88-研究机构和实验室)
      - [**8.8.1 主要研究机构**](#881-主要研究机构)
      - [**8.8.2 其他重要研究机构**](#882-其他重要研究机构)
      - [**8.8.3 研究实验室**](#883-研究实验室)
      - [**8.8.4 研究项目和实验室链接**](#884-研究项目和实验室链接)
    - [8.9 参考资源使用建议](#89-参考资源使用建议)
  - [9. 完整代码示例](#9-完整代码示例)
    - [9.1 概率数据类型实现](#91-概率数据类型实现)
    - [9.2 Python 概率数据库客户端](#92-python-概率数据库客户端)
    - [9.3 ProvSQL 概率查询集成](#93-provsql-概率查询集成)
      - [**9.3.1 ProvSQL集成完整示例**](#931-provsql集成完整示例)
      - [**9.3.2 ProvSQL概率查询函数**](#932-provsql概率查询函数)
      - [**9.3.3 ProvSQL概率查询Python集成**](#933-provsql概率查询python集成)
    - [9.4 Docker Compose 部署配置](#94-docker-compose-部署配置)
      - [**9.4.1 完整Docker Compose配置**](#941-完整docker-compose配置)
      - [**9.4.2 初始化SQL脚本**](#942-初始化sql脚本)
      - [**9.4.3 Python客户端配置**](#943-python客户端配置)
      - [**9.4.4 部署和使用说明**](#944-部署和使用说明)

---

## 1. 概述

### 1.1 技术背景

#### **1.1.1 数据不确定性的挑战**

在现实世界中，数据往往存在不确定性：

**1. 传感器数据不确定性**：

- **测量误差**：传感器精度限制导致的误差
- **噪声干扰**：环境噪声对测量的影响
- **设备故障**：传感器故障导致的数据异常
- **网络延迟**：数据传输延迟导致的时间不确定性

**2. 数据融合不确定性**：

- **多源数据不一致**：不同数据源提供的数据存在差异
- **数据冲突**：同一实体的不同数据源数据相互矛盾
- **数据质量差异**：不同数据源的数据质量不同
- **时间不同步**：不同数据源的时间戳不一致

**3. 数据溯源不确定性**：

- **数据来源可信度**：不同数据源的可信度不同
- **处理过程不确定性**：数据处理过程中的不确定性
- **数据转换误差**：数据转换过程中的误差累积
- **数据质量评估**：数据质量的评估存在不确定性

**4. 缺失数据不确定性**：

- **数据不完整**：部分数据缺失
- **数据插值**：缺失数据的估计值存在不确定性
- **数据推断**：基于现有数据推断缺失数据的不确定性

#### **1.1.2 传统数据库的局限性**

传统数据库假设数据是确定的，存在以下局限性：

- **无法表示不确定性**：只能存储确定的值，无法表示数据的不确定性
- **无法处理概率查询**：无法计算查询结果的概率分布
- **无法评估数据质量**：无法基于概率评估数据质量
- **无法追踪数据来源**：无法追踪数据的来源和处理过程

#### **1.1.3 概率数据库的解决方案**

概率数据库（Probabilistic Database）提供了处理不确定性数据的解决方案：

- **不确定性数据模型**：支持概率值、置信区间、分布函数
- **概率查询处理**：计算查询结果的概率分布
- **数据质量评估**：基于概率评估数据质量
- **数据溯源集成**：结合数据溯源信息计算概率

### 1.2 技术定位

#### **1.2.1 技术定位**

概率数据库是PostgreSQL在数据溯源和不确定性数据处理领域的扩展，与ProvSQL配合使用，提供：

**核心功能**：

- **不确定性数据模型**：支持概率值、置信区间、分布函数
- **概率查询处理**：计算查询结果的概率分布
- **数据溯源集成**：结合数据溯源信息计算概率
- **数据质量评估**：基于概率评估数据质量

**技术特点**：

- **无缝集成**：作为PostgreSQL扩展，无需修改应用代码
- **高性能**：优化的概率计算算法
- **灵活配置**：支持多种概率计算模式
- **标准兼容**：支持SQL标准语法

#### **1.2.2 与其他技术的对比**

**与传统数据库的对比**：

| 特性 | 传统数据库 | 概率数据库 |
|------|-----------|-----------|
| **数据确定性** | 确定 | 不确定 |
| **数据表示** | 精确值 | 值+概率 |
| **查询结果** | 确定性结果 | 概率分布 |
| **数据质量** | 无法评估 | 基于概率评估 |
| **数据溯源** | 不支持 | 支持 |

**与其他不确定性数据处理技术的对比**：

| 特性 | 概率数据库 | 模糊数据库 | 粗糙集理论 |
|------|-----------|-----------|-----------|
| **理论基础** | 概率论 | 模糊逻辑 | 粗糙集理论 |
| **数据表示** | 概率值 | 隶属度 | 上下近似 |
| **查询处理** | 概率计算 | 模糊推理 | 粗糙集运算 |
| **适用场景** | 不确定性数据 | 模糊数据 | 不精确数据 |

### 1.3 核心价值

#### **1.3.1 核心价值**

**1. 处理不确定性数据**：

- **传感器数据**：处理IoT传感器数据的不确定性
- **数据融合**：处理多源数据融合的不一致性
- **数据质量**：评估和提升数据质量
- **缺失数据**：处理不完整数据

**2. 概率查询**：

- **概率分布**：提供查询结果的概率分布
- **置信度评估**：评估查询结果的置信度
- **不确定性量化**：量化数据的不确定性
- **决策支持**：基于概率支持决策

**3. 数据溯源支持**：

- **数据来源追踪**：追踪数据的来源和处理过程
- **可信度计算**：基于数据来源计算可信度
- **合规审计**：满足数据治理和合规要求
- **数据质量评估**：基于溯源信息评估数据质量

#### **1.3.2 应用场景**

**1. IoT传感器数据管理**：

- 环境监测（温度、湿度、压力等）
- 智能城市（交通、能源、环境等）
- 工业4.0（设备监控、预测维护等）

**2. 数据融合和集成**：

- 数据仓库（多源数据整合）
- 数据湖（异构数据融合）
- 数据集成（企业数据整合）

**3. 数据质量评估**：

- 数据清洗（识别和处理低质量数据）
- 数据质量监控（实时监控数据质量）
- 数据质量报告（生成数据质量报告）

**4. 数据溯源和审计**：

- 数据治理（数据来源追踪）
- 合规审计（满足监管要求）
- 数据血缘分析（分析数据流向）

#### **1.3.3 技术优势**

**1. 理论基础扎实**：

- 基于概率论和可能世界语义
- 有完整的数学理论支撑
- 有成熟的算法和优化技术

**2. 实现灵活**：

- 作为PostgreSQL扩展，易于集成
- 支持多种概率表示方式
- 支持自定义概率计算函数

**3. 性能优化**：

- 优化的概率计算算法
- 支持索引和查询优化
- 支持批量处理和缓存

**4. 标准兼容**：

- 支持SQL标准语法
- 兼容PostgreSQL生态系统
- 易于与现有系统集成

---

## 2. 技术原理

### 2.1 概率数据库基础

#### **2.1.1 概率数据库定义**

**概率数据库（Probabilistic Database）**是一种能够存储和查询不确定性数据的数据库系统，每个数据项都关联一个概率值，表示该数据项为真的可能性。

**形式化定义**：

```text
概率数据库 PDB = (W, P)

其中：
- W = {w₁, w₂, ..., wₙ} 是可能世界集合
- P: W → [0, 1] 是概率分布函数
- Σ P(wᵢ) = 1（概率归一化）
```

**与传统数据库的区别**：

| 特性 | 传统数据库 | 概率数据库 |
|------|-----------|-----------|
| **数据确定性** | 确定 | 不确定 |
| **数据表示** | 精确值 | 值+概率 |
| **查询结果** | 确定性结果 | 概率分布 |
| **适用场景** | 确定性数据 | 不确定性数据 |

#### **2.1.2 核心概念**

**1. 可能世界（Possible Worlds）**

**定义**：可能世界是数据的所有可能状态，每个可能世界是一个确定性的数据库实例。

**示例**：

```text
假设有3个不确定性元组：
- t₁ (概率=0.8)
- t₂ (概率=0.6)
- t₃ (概率=0.9)

可能世界：
- w₀: {} (空集)                    P(w₀) = 0.2 × 0.4 × 0.1 = 0.008
- w₁: {t₁}                         P(w₁) = 0.8 × 0.4 × 0.1 = 0.032
- w₂: {t₂}                         P(w₂) = 0.2 × 0.6 × 0.1 = 0.012
- w₃: {t₃}                         P(w₃) = 0.2 × 0.4 × 0.9 = 0.072
- w₄: {t₁, t₂}                     P(w₄) = 0.8 × 0.6 × 0.1 = 0.048
- w₅: {t₁, t₃}                     P(w₅) = 0.8 × 0.4 × 0.9 = 0.288
- w₆: {t₂, t₃}                     P(w₆) = 0.2 × 0.6 × 0.9 = 0.108
- w₇: {t₁, t₂, t₃}                 P(w₇) = 0.8 × 0.6 × 0.9 = 0.432

验证：Σ P(wᵢ) = 1.0 ✓
```

**2. 概率分布（Probability Distribution）**

**定义**：每个可能世界有一个概率值，所有可能世界的概率之和为1。

**数学表示**：

```text
P: W → [0, 1]
Σ P(w) = 1
```

**3. 概率查询（Probabilistic Query）**

**定义**：在概率数据库上执行的查询，返回结果及其概率分布。

**查询语义**：

```text
对于查询 Q，结果 r 的概率为：
P(Q = r) = Σ P(w) × I(Q(w) = r)

其中：
- w 是可能世界
- I(Q(w) = r) 是指示函数
```

#### **2.1.3 概率数据库分类**

**1. 按不确定性类型分类**：

- **元组级不确定性**：元组是否存在不确定
- **属性级不确定性**：属性值不确定
- **值级不确定性**：值在某个范围内不确定

**2. 按概率表示方式分类**：

- **显式概率**：直接存储概率值
- **隐式概率**：通过规则推导概率
- **分布概率**：存储概率分布函数

**3. 按查询能力分类**：

- **部分同态**：支持部分查询操作
- **完全同态**：支持所有查询操作

### 2.2 不确定性数据模型

#### **2.2.1 不确定性数据模型类型**

**1. 元组级不确定性（Tuple-Level Uncertainty）**

**定义**：每个元组关联一个概率值，表示该元组存在的可能性。

**数学表示**：

```text
关系 R = {(t₁, p₁), (t₂, p₂), ..., (tₙ, pₙ)}

其中：
- tᵢ 是元组
- pᵢ ∈ [0, 1] 是元组存在的概率
```

**示例**：

```sql
-- 元组级不确定性表
CREATE TABLE sensor_readings (
    id SERIAL,
    sensor_id INT,
    temperature NUMERIC,
    probability NUMERIC  -- 元组存在的概率
);

-- 插入不确定性元组
INSERT INTO sensor_readings VALUES
    (1, 1, 25.5, 0.95),  -- 95%概率存在
    (2, 1, 25.7, 0.90),  -- 90%概率存在
    (3, 1, 25.3, 0.85);  -- 85%概率存在
```

**应用场景**：

- 传感器数据（测量误差）
- 数据融合（多源数据不一致）
- 数据清洗（数据质量不确定）

**2. 属性级不确定性（Attribute-Level Uncertainty）**

**定义**：每个属性值关联一个概率分布，表示该属性值的可能性。

**数学表示**：

```text
元组 t = (a₁: D₁, a₂: D₂, ..., aₙ: Dₙ)

其中：
- aᵢ 是属性
- Dᵢ 是属性值的概率分布：Dᵢ = {(v₁, p₁), (v₂, p₂), ...}
```

**示例**：

```sql
-- 属性级不确定性表
CREATE TABLE uncertain_sensor_data (
    id SERIAL,
    sensor_id INT,
    temperature_prob_distribution JSONB,  -- 温度的概率分布
    humidity_prob_distribution JSONB      -- 湿度的概率分布
);

-- 插入属性级不确定性数据
INSERT INTO uncertain_sensor_data VALUES (
    1,
    1,
    '{"25.5": 0.4, "25.6": 0.3, "25.7": 0.3}',  -- 温度的概率分布
    '{"60": 0.5, "61": 0.3, "62": 0.2}'         -- 湿度的概率分布
);
```

**应用场景**：

- 传感器测量范围
- 数据预测（机器学习输出）
- 数据插值（缺失值估计）

**3. 存在不确定性（Existence Uncertainty）**

**定义**：元组是否存在不确定，通过概率值表示。

**与元组级不确定性的区别**：

| 特性 | 元组级不确定性 | 存在不确定性 |
|------|--------------|-------------|
| **关注点** | 元组的值 | 元组的存在性 |
| **概率含义** | 元组值正确的概率 | 元组存在的概率 |
| **应用** | 数据质量 | 数据完整性 |

**示例**：

```sql
-- 存在不确定性：元组可能不存在
CREATE TABLE possible_events (
    id SERIAL,
    event_type TEXT,
    occurrence_probability NUMERIC  -- 事件发生的概率
);

INSERT INTO possible_events VALUES
    ('sensor_failure', 0.05),    -- 5%概率传感器故障
    ('data_corruption', 0.02),   -- 2%概率数据损坏
    ('network_timeout', 0.10);   -- 10%概率网络超时
```

#### **2.2.2 不确定性来源**

不确定性数据的来源多种多样，理解这些来源有助于正确建模和处理不确定性数据。

**1. 测量不确定性**：

**定义**：由于测量设备、环境条件或测量方法导致的测量值与真实值之间的差异。

**主要来源**：

- **传感器误差**：
  - 系统误差：传感器校准不准确导致的固定偏差
  - 随机误差：传感器噪声导致的随机波动
  - 量化误差：ADC转换精度限制导致的量化误差
  - 示例：温度传感器精度±0.5°C，测量值25.5°C的实际范围可能是[25.0°C, 26.0°C]

- **测量精度限制**：
  - 设备精度：测量设备的固有精度限制
  - 分辨率限制：数字传感器的分辨率限制
  - 示例：湿度传感器分辨率1%，测量值60%的实际范围可能是[59.5%, 60.5%]

- **环境干扰**：
  - 电磁干扰：电磁场对传感器的影响
  - 温度漂移：环境温度变化对传感器的影响
  - 振动干扰：机械振动对传感器的影响
  - 示例：在强电磁场环境中，传感器读数可能受到干扰，置信度降低

**建模方法**：

```sql
-- 测量不确定性建模示例
CREATE TABLE sensor_measurements (
    id SERIAL PRIMARY KEY,
    sensor_id INT NOT NULL,
    measured_value NUMERIC NOT NULL,
    measurement_error NUMERIC NOT NULL,  -- 测量误差范围
    confidence NUMERIC CHECK (confidence >= 0 AND confidence <= 1),
    measurement_time TIMESTAMP DEFAULT NOW()
);

-- 插入测量数据（考虑测量误差）
INSERT INTO sensor_measurements (sensor_id, measured_value, measurement_error, confidence)
VALUES
    (1, 25.5, 0.5, 0.95),  -- 测量值25.5，误差±0.5，置信度95%
    (2, 60.0, 1.0, 0.90),  -- 测量值60.0，误差±1.0，置信度90%
    (3, 1013.25, 0.05, 0.98);  -- 测量值1013.25，误差±0.05，置信度98%
```

**2. 数据融合不确定性**：

**定义**：在整合来自多个数据源的信息时，由于数据源之间的不一致性导致的不确定性。

**主要来源**：

- **多源数据不一致**：
  - 不同数据源对同一实体的描述不同
  - 数据格式和单位不统一
  - 数据更新频率不同
  - 示例：两个数据源对同一客户的年龄信息不同（25岁 vs 26岁）

- **数据冲突**：
  - 同一实体的不同数据源数据相互矛盾
  - 数据源的可信度不同
  - 数据源的时间戳不同
  - 示例：传感器A报告温度25.5°C，传感器B报告26.0°C，两者存在冲突

- **数据质量差异**：
  - 不同数据源的数据质量不同
  - 数据源的可靠性不同
  - 数据源的完整性不同
  - 示例：数据源A的数据质量高（置信度0.95），数据源B的数据质量低（置信度0.70）

**建模方法**：

```sql
-- 数据融合不确定性建模示例
CREATE TABLE multi_source_data (
    id SERIAL PRIMARY KEY,
    entity_id INT NOT NULL,
    data_source VARCHAR(50) NOT NULL,
    attribute_name VARCHAR(50) NOT NULL,
    attribute_value NUMERIC NOT NULL,
    source_reliability NUMERIC CHECK (source_reliability >= 0 AND source_reliability <= 1),
    timestamp TIMESTAMP DEFAULT NOW()
);

-- 插入多源数据
INSERT INTO multi_source_data (entity_id, data_source, attribute_name, attribute_value, source_reliability)
VALUES
    (1, 'Sensor_A', 'temperature', 25.5, 0.95),
    (1, 'Sensor_B', 'temperature', 26.0, 0.90),
    (1, 'Sensor_C', 'temperature', 25.8, 0.85);

-- 数据融合查询（加权平均）
SELECT
    entity_id,
    attribute_name,
    COUNT(*) AS source_count,
    AVG(attribute_value) AS simple_avg,
    SUM(attribute_value * source_reliability) / SUM(source_reliability) AS weighted_avg,
    AVG(source_reliability) AS avg_reliability
FROM multi_source_data
GROUP BY entity_id, attribute_name;
```

**3. 推断不确定性**：

**定义**：通过推理、预测或估计得到的数据，其不确定性来自于推理过程本身。

**主要来源**：

- **机器学习预测**：
  - 模型预测的置信度
  - 训练数据的不确定性
  - 模型泛化误差
  - 示例：机器学习模型预测客户购买概率为0.75，但模型本身的准确率为85%

- **数据插值**：
  - 缺失值的估计
  - 时间序列插值
  - 空间插值
  - 示例：通过线性插值估计缺失的温度值，插值结果的不确定性取决于相邻数据的可靠性

- **缺失值估计**：
  - 基于统计方法的估计
  - 基于模型的估计
  - 基于规则的估计
  - 示例：使用平均值估计缺失的年龄值，估计值的不确定性取决于样本大小和方差

**建模方法**：

```sql
-- 推断不确定性建模示例
CREATE TABLE inferred_data (
    id SERIAL PRIMARY KEY,
    entity_id INT NOT NULL,
    attribute_name VARCHAR(50) NOT NULL,
    inferred_value NUMERIC NOT NULL,
    inference_method VARCHAR(50) NOT NULL,  -- 'ML', 'interpolation', 'statistical'
    inference_confidence NUMERIC CHECK (inference_confidence >= 0 AND inference_confidence <= 1),
    model_accuracy NUMERIC,  -- 模型准确率（如果适用）
    timestamp TIMESTAMP DEFAULT NOW()
);

-- 插入推断数据
INSERT INTO inferred_data (entity_id, attribute_name, inferred_value, inference_method, inference_confidence, model_accuracy)
VALUES
    (1, 'purchase_probability', 0.75, 'ML', 0.85, 0.90),  -- ML预测，置信度85%，模型准确率90%
    (2, 'temperature', 25.3, 'interpolation', 0.80, NULL),  -- 插值估计，置信度80%
    (3, 'age', 30, 'statistical', 0.70, NULL);  -- 统计估计，置信度70%
```

**4. 时间不确定性**：

**定义**：由于时间戳不准确、数据延迟或数据过期导致的时间相关的不确定性。

**主要来源**：

- **数据时间戳不准确**：
  - 时钟同步问题
  - 时区转换错误
  - 时间戳精度限制
  - 示例：传感器时间戳可能不准确，实际测量时间与记录时间存在±5秒的误差

- **数据延迟**：
  - 网络传输延迟
  - 处理延迟
  - 存储延迟
  - 示例：传感器数据在网络传输过程中延迟10秒，导致时间戳不准确

- **数据过期**：
  - 数据时效性
  - 数据新鲜度
  - 数据有效性窗口
  - 示例：传感器数据在5分钟后过期，过期数据的置信度随时间衰减

**建模方法**：

```sql
-- 时间不确定性建模示例
CREATE TABLE time_uncertain_data (
    id SERIAL PRIMARY KEY,
    sensor_id INT NOT NULL,
    value NUMERIC NOT NULL,
    recorded_time TIMESTAMP NOT NULL,
    actual_time_lower TIMESTAMP,  -- 实际时间下界
    actual_time_upper TIMESTAMP,  -- 实际时间上界
    time_uncertainty INTERVAL,  -- 时间不确定性范围
    data_freshness INTERVAL,  -- 数据新鲜度
    confidence NUMERIC CHECK (confidence >= 0 AND confidence <= 1),
    created_at TIMESTAMP DEFAULT NOW()
);

-- 插入时间不确定数据
INSERT INTO time_uncertain_data (
    sensor_id, value, recorded_time, actual_time_lower, actual_time_upper,
    time_uncertainty, data_freshness, confidence
)
VALUES
    (
        1, 25.5, '2025-01-15 10:00:00',
        '2025-01-15 09:59:55', '2025-01-15 10:00:05',
        INTERVAL '10 seconds', INTERVAL '5 minutes', 0.90
    );

-- 查询考虑时间不确定性的数据
SELECT
    sensor_id,
    value,
    recorded_time,
    actual_time_lower,
    actual_time_upper,
    confidence,
    CASE
        WHEN NOW() - recorded_time > data_freshness THEN
            confidence * EXP(-EXTRACT(EPOCH FROM (NOW() - recorded_time - data_freshness)) / 3600.0)
        ELSE confidence
    END AS adjusted_confidence
FROM time_uncertain_data
WHERE recorded_time BETWEEN actual_time_lower AND actual_time_upper;
```

**不确定性来源的综合处理**：

```sql
-- 综合不确定性处理示例
CREATE TABLE comprehensive_uncertain_data (
    id SERIAL PRIMARY KEY,
    sensor_id INT NOT NULL,
    value NUMERIC NOT NULL,
    -- 测量不确定性
    measurement_error NUMERIC,
    measurement_confidence NUMERIC,
    -- 数据融合不确定性
    source_reliability NUMERIC,
    source_count INT,
    -- 推断不确定性
    inference_method VARCHAR(50),
    inference_confidence NUMERIC,
    -- 时间不确定性
    time_uncertainty INTERVAL,
    data_freshness INTERVAL,
    -- 综合置信度
    overall_confidence NUMERIC CHECK (overall_confidence >= 0 AND overall_confidence <= 1),
    timestamp TIMESTAMP DEFAULT NOW()
);

-- 计算综合置信度
CREATE OR REPLACE FUNCTION calculate_overall_confidence(
    meas_conf NUMERIC,
    source_rel NUMERIC,
    infer_conf NUMERIC,
    time_factor NUMERIC
) RETURNS NUMERIC AS $$
BEGIN
    -- 综合置信度 = 测量置信度 × 数据源可靠性 × 推断置信度 × 时间因子
    RETURN meas_conf * source_rel * infer_conf * time_factor;
END;
$$ LANGUAGE plpgsql IMMUTABLE;
```

#### **2.2.3 概率值表示方法**

**1. 点概率（Point Probability）**：

```text
单个概率值：p ∈ [0, 1]
示例：P(t) = 0.95
```

**2. 区间概率（Interval Probability）**：

```text
概率区间：[p_min, p_max] ⊆ [0, 1]
示例：P(t) ∈ [0.8, 0.95]
```

**3. 概率分布（Probability Distribution）**：

```text
概率分布函数：f: V → [0, 1]
示例：P(temperature = 25.5) = 0.4
      P(temperature = 25.6) = 0.3
      P(temperature = 25.7) = 0.3
```

**4. 置信区间（Confidence Interval）**：

```text
值区间 + 置信度：(v_min, v_max, confidence)
示例：temperature ∈ [25.0, 26.0] with 95% confidence
```

### 2.3 概率查询处理

#### **2.3.1 概率查询基础**

**概率查询定义**：

概率查询是在不确定性数据上执行的查询，返回结果及其概率分布。

**查询语义**：

- **可能世界语义（Possible Worlds Semantics）**：每个可能世界是一个确定性的数据库实例
- **概率分布**：每个可能世界有一个概率值
- **查询结果概率**：查询结果在所有可能世界中的概率之和

#### **2.3.2 概率查询类型**

**1. 概率选择（Probabilistic Selection）**

**定义**：选择满足条件的元组，计算结果的概率。

**数学原理**：

```text
给定查询条件 θ，元组 t 的概率为：
P(t | θ) = P(t) × I(θ(t))

其中：
- P(t) 是元组 t 的概率
- I(θ(t)) 是指示函数，满足条件为1，否则为0
```

**算法**：

```python
def probabilistic_selection(relation, condition):
    """
    概率选择算法

    Args:
        relation: 关系（元组列表，每个元组有概率）
        condition: 选择条件

    Returns:
        满足条件的元组及其概率
    """
    results = []
    for tuple in relation:
        if condition(tuple):
            results.append({
                'tuple': tuple,
                'probability': tuple.probability
            })
    return results
```

**SQL示例**：

```sql
-- 概率选择：选择概率>0.8的元组
SELECT *
FROM sensor_data
WHERE probability > 0.8;
```

**2. 概率连接（Probabilistic Join）**

**定义**：连接操作的概率计算，处理不确定性连接。

**数学原理**：

```text
对于两个关系 R 和 S，连接条件 θ：
P(t ∈ R ⋈ S | θ) = Σ P(r ∈ R) × P(s ∈ S) × I(θ(r, s))

其中：
- r 和 s 是来自 R 和 S 的元组
- θ(r, s) 是连接条件
- I(θ(r, s)) 是指示函数
```

**算法**：

```python
def probabilistic_join(relation_R, relation_S, join_condition):
    """
    概率连接算法

    Args:
        relation_R: 关系R
        relation_S: 关系S
        join_condition: 连接条件函数

    Returns:
        连接结果及其概率
    """
    results = []
    for r in relation_R:
        for s in relation_S:
            if join_condition(r, s):
                # 独立事件：P(r AND s) = P(r) × P(s)
                prob = r.probability * s.probability
                results.append({
                    'tuple': combine(r, s),
                    'probability': prob
                })
    return results
```

**SQL示例**：

```sql
-- 概率连接
SELECT
    r.sensor_id,
    r.temperature,
    s.location,
    PROBABILITY() AS join_probability
FROM sensor_readings r
JOIN sensor_metadata s ON r.sensor_id = s.sensor_id;
```

**3. 概率聚合（Probabilistic Aggregation）**

**定义**：聚合操作的概率计算，计算聚合结果的概率分布。

**数学原理**：

```text
对于聚合函数 f（如SUM、AVG、COUNT）：
P(f(R) = v) = Σ P(w) × I(f(w) = v)

其中：
- w 是可能世界
- P(w) 是可能世界的概率
- I(f(w) = v) 是指示函数
```

**算法**：

```python
def probabilistic_aggregate(relation, agg_func, group_by=None):
    """
    概率聚合算法

    Args:
        relation: 关系
        agg_func: 聚合函数（SUM、AVG、COUNT等）
        group_by: 分组字段

    Returns:
        聚合结果及其概率分布
    """
    if group_by:
        # 分组聚合
        groups = {}
        for tuple in relation:
            key = tuple[group_by]
            if key not in groups:
                groups[key] = []
            groups[key].append(tuple)

        results = []
        for key, tuples in groups.items():
            result = compute_aggregate_probability(tuples, agg_func)
            results.append({
                'group': key,
                'value': result['value'],
                'probability': result['probability']
            })
        return results
    else:
        # 全局聚合
        return compute_aggregate_probability(relation, agg_func)

def compute_aggregate_probability(tuples, agg_func):
    """
    计算聚合结果的概率分布
    """
    # 枚举所有可能世界
    possible_worlds = enumerate_possible_worlds(tuples)

    # 计算每个可能世界的聚合值
    agg_values = {}
    for world, prob in possible_worlds:
        value = agg_func(world)
        if value not in agg_values:
            agg_values[value] = 0
        agg_values[value] += prob

    return agg_values
```

**SQL示例**：

```sql
-- 概率聚合：计算加权平均
SELECT
    sensor_id,
    SUM(value * probability) / SUM(probability) AS weighted_avg,
    AVG(probability) AS avg_confidence,
    PROBABILITY() AS aggregate_probability
FROM sensor_data
GROUP BY sensor_id;
```

#### **2.3.3 可能世界枚举**

**可能世界定义**：

可能世界是数据的所有可能状态，每个可能世界是一个确定性的数据库实例。

**枚举算法**：

```python
def enumerate_possible_worlds(relation):
    """
    枚举所有可能世界

    Args:
        relation: 关系（每个元组有概率）

    Returns:
        可能世界列表，每个世界包含元组和概率
    """
    worlds = []
    n = len(relation)

    # 枚举所有2^n个可能世界
    for i in range(2 ** n):
        world = []
        prob = 1.0

        for j, tuple in enumerate(relation):
            # 检查元组是否在当前世界中
            if (i >> j) & 1:  # 元组存在
                world.append(tuple)
                prob *= tuple.probability
            else:  # 元组不存在
                prob *= (1 - tuple.probability)

        worlds.append({
            'tuples': world,
            'probability': prob
        })

    return worlds
```

**复杂度分析**：

- **时间复杂度**：O(2^n)，n为不确定性元组数
- **空间复杂度**：O(2^n)
- **优化策略**：
  - 剪枝低概率可能世界
  - 使用近似算法
  - 限制可能世界数量

#### **2.3.4 概率查询优化**

概率查询的计算复杂度通常很高，特别是涉及可能世界枚举的查询。本节介绍多种优化策略来提高查询性能。

**优化策略**：

**1. 早期过滤（Early Filtering）**

**原理**：在枚举可能世界之前，先过滤掉低概率的元组，减少需要处理的数据量。

**实现方法**：

- **概率阈值过滤**：在WHERE子句中使用概率阈值，只处理高概率元组
- **提前终止**：当累积概率达到阈值时，提前终止查询
- **分层过滤**：先进行粗粒度过滤，再进行细粒度过滤

**优化示例**：

```sql
-- 优化前：枚举所有可能世界（复杂度O(2^n)）
SELECT * FROM sensor_data WHERE probability > 0.5;

-- 优化后：使用概率阈值提前过滤
CREATE INDEX idx_probability ON sensor_data(probability);
SELECT * FROM sensor_data
WHERE probability > 0.8  -- 提高阈值，减少处理的数据量
ORDER BY probability DESC
LIMIT 1000;  -- 限制结果数量

-- 分层过滤示例
WITH high_prob_data AS (
    SELECT * FROM sensor_data WHERE probability > 0.9
),
medium_prob_data AS (
    SELECT * FROM sensor_data WHERE probability BETWEEN 0.7 AND 0.9
)
SELECT * FROM high_prob_data
UNION ALL
SELECT * FROM medium_prob_data
ORDER BY probability DESC;
```

**性能影响**：

- 减少可能世界数量：从O(2^n)降低到O(2^k)，其中k << n
- 查询时间：从指数级降低到多项式级
- 内存使用：显著减少内存占用

**2. 索引优化（Index Optimization）**

**原理**：为概率值创建合适的索引，加速概率查询的执行。

**索引类型**：

- **B-tree索引**：用于概率值的范围查询和排序
- **部分索引**：只索引高概率数据，减少索引大小
- **表达式索引**：为概率计算表达式创建索引
- **复合索引**：为概率值和其他字段创建复合索引

**优化示例**：

```sql
-- 基础概率值索引
CREATE INDEX idx_probability ON sensor_data(probability);

-- 部分索引（只索引高概率数据）
CREATE INDEX idx_high_probability ON sensor_data(probability)
WHERE probability > 0.7;

-- 表达式索引（概率计算）
CREATE INDEX idx_prob_expr ON sensor_data((probability * 100));

-- 复合索引（概率+其他字段）
CREATE INDEX idx_sensor_prob ON sensor_data(sensor_id, probability);

-- 使用索引的查询
EXPLAIN ANALYZE
SELECT * FROM sensor_data
WHERE probability > 0.8
ORDER BY probability DESC
LIMIT 100;
```

**性能影响**：

- 查询时间：从全表扫描O(n)降低到索引扫描O(log n)
- 排序时间：从O(n log n)降低到O(k log k)，其中k是结果集大小
- 索引维护：需要额外的存储空间和维护开销

**3. 近似算法（Approximation Algorithms）**

**原理**：使用采样和近似方法，在保证精度的前提下降低计算复杂度。

**算法类型**：

- **可能世界采样**：随机采样可能世界，估计查询结果的概率
- **蒙特卡洛方法**：使用蒙特卡洛模拟估计概率
- **重要性采样**：根据重要性采样可能世界
- **分层采样**：分层采样可能世界，提高估计精度

**优化示例**：

```sql
-- 可能世界采样函数
CREATE OR REPLACE FUNCTION sample_possible_worlds(
    table_name TEXT,
    sample_size INT DEFAULT 1000
) RETURNS TABLE (
    world_id INT,
    probability NUMERIC,
    tuple_count INT
) AS $$
DECLARE
    total_worlds BIGINT;
    sample_prob NUMERIC;
BEGIN
    -- 计算总可能世界数
    EXECUTE format('SELECT COUNT(*) FROM %I', table_name) INTO total_worlds;

    -- 采样可能世界
    sample_prob := 1.0 / sample_size;

    RETURN QUERY EXECUTE format('
        WITH sampled_tuples AS (
            SELECT * FROM %I
            WHERE random() < %s
            LIMIT %s
        )
        SELECT
            row_number() OVER () AS world_id,
            %s AS probability,
            COUNT(*) AS tuple_count
        FROM sampled_tuples
        GROUP BY world_id
    ', table_name, sample_prob, sample_size, sample_prob);
END;
$$ LANGUAGE plpgsql;

-- 使用采样估计查询结果概率
SELECT
    AVG(probability) AS estimated_probability,
    STDDEV(probability) AS estimation_error
FROM sample_possible_worlds('sensor_data', 1000);
```

**性能影响**：

- 计算复杂度：从指数级O(2^n)降低到线性级O(k)，其中k是采样大小
- 估计精度：取决于采样大小，通常k=1000时误差<5%
- 查询时间：显著减少，适合实时查询

**4. 缓存机制（Caching Mechanisms）**

**原理**：缓存常见查询结果和可能世界枚举结果，避免重复计算。

**缓存策略**：

- **查询结果缓存**：缓存常见查询的结果
- **可能世界缓存**：缓存可能世界的枚举结果
- **概率计算缓存**：缓存概率计算结果
- **物化视图**：预计算常见查询结果

**优化示例**：

```sql
-- 创建查询结果缓存表
CREATE TABLE IF NOT EXISTS query_result_cache (
    query_hash TEXT PRIMARY KEY,
    query_text TEXT NOT NULL,
    query_result JSONB NOT NULL,
    created_at TIMESTAMP DEFAULT NOW(),
    expires_at TIMESTAMP,
    hit_count INT DEFAULT 0
);

-- 查询缓存函数
CREATE OR REPLACE FUNCTION get_cached_query_result(
    query_text TEXT,
    cache_duration INTERVAL DEFAULT INTERVAL '1 hour'
) RETURNS JSONB AS $$
DECLARE
    query_hash TEXT;
    cached_result JSONB;
BEGIN
    -- 计算查询哈希
    query_hash := md5(query_text);

    -- 检查缓存
    SELECT query_result INTO cached_result
    FROM query_result_cache
    WHERE query_hash = get_cached_query_result.query_hash
      AND expires_at > NOW();

    IF cached_result IS NOT NULL THEN
        -- 更新命中计数
        UPDATE query_result_cache
        SET hit_count = hit_count + 1
        WHERE query_hash = get_cached_query_result.query_hash;

        RETURN cached_result;
    END IF;

    -- 执行查询并缓存结果
    EXECUTE format('SELECT to_jsonb(result) FROM (%s) result', query_text) INTO cached_result;

    INSERT INTO query_result_cache (query_hash, query_text, query_result, expires_at)
    VALUES (
        query_hash,
        query_text,
        cached_result,
        NOW() + cache_duration
    )
    ON CONFLICT (query_hash) DO UPDATE SET
        query_result = EXCLUDED.query_result,
        expires_at = EXCLUDED.expires_at,
        hit_count = 0;

    RETURN cached_result;
END;
$$ LANGUAGE plpgsql;

-- 使用缓存查询
SELECT * FROM get_cached_query_result(
    'SELECT * FROM sensor_data WHERE probability > 0.8',
    INTERVAL '1 hour'
);
```

**性能影响**：

- 查询时间：从秒级降低到毫秒级（缓存命中时）
- 系统负载：显著减少数据库负载
- 存储开销：需要额外的存储空间

**5. 查询重写（Query Rewriting）**

**原理**：将复杂的概率查询重写为更简单的等价查询，提高执行效率。

**重写规则**：

- **概率过滤下推**：将概率过滤尽可能下推到查询树底部
- **JOIN优化**：优化概率JOIN的执行顺序
- **聚合优化**：优化概率聚合查询
- **子查询优化**：优化嵌套的概率查询

**优化示例**：

```sql
-- 优化前：复杂的嵌套查询
SELECT * FROM (
    SELECT * FROM sensor_data WHERE probability > 0.5
) t1
WHERE probability > 0.7;

-- 优化后：合并过滤条件
SELECT * FROM sensor_data
WHERE probability > 0.7;  -- 直接使用更高的阈值

-- 优化前：先JOIN再过滤
SELECT a.*, b.*
FROM uncertain_table_a a
JOIN uncertain_table_b b ON a.id = b.id
WHERE a.probability * b.probability > 0.5;

-- 优化后：先过滤再JOIN
SELECT a.*, b.*
FROM (
    SELECT * FROM uncertain_table_a WHERE probability > 0.7
) a
JOIN (
    SELECT * FROM uncertain_table_b WHERE probability > 0.7
) b ON a.id = b.id
WHERE a.probability * b.probability > 0.5;
```

**综合优化示例**：

```sql
-- 综合优化：结合多种优化策略
-- 1. 创建部分索引
CREATE INDEX idx_high_prob ON sensor_data(probability)
WHERE probability > 0.7;

-- 2. 创建物化视图（预计算常见查询）
CREATE MATERIALIZED VIEW sensor_data_high_prob AS
SELECT
    sensor_id,
    COUNT(*) AS count,
    AVG(value) AS avg_value,
    SUM(value * probability) / SUM(probability) AS weighted_avg
FROM sensor_data
WHERE probability > 0.7
GROUP BY sensor_id;

CREATE INDEX idx_summary_sensor ON sensor_data_high_prob(sensor_id);

-- 3. 使用物化视图查询（更快）
SELECT * FROM sensor_data_high_prob
WHERE sensor_id = 1;

-- 4. 定期刷新物化视图
REFRESH MATERIALIZED VIEW CONCURRENTLY sensor_data_high_prob;
```

**优化效果总结**：

| 优化策略 | 复杂度降低 | 适用场景 | 实现难度 |
|---------|-----------|---------|---------|
| **早期过滤** | O(2^n) → O(2^k) | 高概率查询 | 低 |
| **索引优化** | O(n) → O(log n) | 范围查询 | 低 |
| **近似算法** | O(2^n) → O(k) | 实时查询 | 中 |
| **缓存机制** | O(1)（缓存命中） | 重复查询 | 中 |
| **查询重写** | 10-100倍 | 复杂查询 | 高 |

**最佳实践**：

1. **优先使用早期过滤**：最简单有效，适用于大多数场景
2. **创建合适的索引**：为常用查询创建索引
3. **使用物化视图**：预计算常见查询结果
4. **考虑近似算法**：实时查询场景使用采样方法
5. **实施缓存机制**：重复查询使用缓存
6. **监控查询性能**：定期分析慢查询，优化执行计划

---

## 3. PostgreSQL实现方案

### 3.1 扩展架构设计

#### **3.1.1 整体架构**

**扩展架构层次结构**：

```text
┌─────────────────────────────────────────────────────────┐
│              概率数据库扩展架构                           │
├─────────────────────────────────────────────────────────┤
│                                                          │
│  ┌────────────────────────────────────────┐           │
│  │     应用层 (Application Layer)           │           │
│  │  - SQL查询                              │           │
│  │  - Python客户端                         │           │
│  │  - 其他应用接口                          │           │
│  └──────────────┬─────────────────────────┘           │
│                 │                                        │
│  ┌──────────────▼─────────────────────────┐           │
│  │   PostgreSQL核心层                      │           │
│  │  - 查询解析器                           │           │
│  │  - 查询优化器                           │           │
│  │  - 执行引擎                             │           │
│  │  - 存储引擎                             │           │
│  └──────────────┬─────────────────────────┘           │
│                 │                                        │
│  ┌──────────────▼─────────────────────────┐           │
│  │   概率数据库扩展层                       │           │
│  │  ├── 数据类型扩展                       │           │
│  │  │   ├── probability_value              │           │
│  │  │   ├── probability_distribution        │           │
│  │  │   └── uncertain_tuple                │           │
│  │  ├── 操作符扩展                         │           │
│  │  │   ├── 概率比较操作符                  │           │
│  │  │   ├── 概率算术操作符                  │           │
│  │  │   └── 概率逻辑操作符                  │           │
│  │  └── 函数扩展                           │           │
│  │      ├── 概率值提取函数                  │           │
│  │      ├── 概率聚合函数                    │           │
│  │      └── 概率统计函数                    │           │
│  └──────────────┬─────────────────────────┘           │
│                 │                                        │
│  ┌──────────────▼─────────────────────────┐           │
│  │   ProvSQL集成层                         │           │
│  │  ├── 数据溯源追踪                        │           │
│  │  │   ├── 溯源图构建                      │           │
│  │  │   ├── 溯源查询                        │           │
│  │  │   └── 溯源存储                        │           │
│  │  └── 概率计算引擎                        │           │
│  │      ├── 可能世界枚举                    │           │
│  │      ├── 概率计算                        │           │
│  │      └── 结果概率分布                    │           │
│  └─────────────────────────────────────────┘           │
│                                                          │
└─────────────────────────────────────────────────────────┘
```

#### **3.1.2 各层职责**

**1. PostgreSQL核心层**

- **查询解析器**：解析SQL查询，识别概率操作
- **查询优化器**：优化概率查询执行计划
- **执行引擎**：执行概率查询操作
- **存储引擎**：存储概率数据和溯源信息

**2. 概率数据库扩展层**

**数据类型扩展**：

- 定义概率值类型
- 定义概率分布类型
- 定义不确定性元组类型

**操作符扩展**：

- 概率值比较操作符（>、<、=、>=、<=）
- 概率值算术操作符（+、-、*、/）
- 概率值逻辑操作符（AND、OR、NOT）

**函数扩展**：

- 概率值提取函数
- 概率聚合函数（AVG、SUM、COUNT等）
- 概率统计函数（STDDEV、VARIANCE等）

**3. ProvSQL集成层**

**数据溯源追踪**：

- 构建溯源图
- 存储溯源信息
- 查询溯源路径

**概率计算引擎**：

- 枚举可能世界
- 计算概率分布
- 优化概率计算

#### **3.1.3 数据流**

**查询处理流程**：

```text
1. SQL查询输入
   ↓
2. 查询解析（识别概率操作）
   ↓
3. 查询优化（概率查询优化）
   ↓
4. 执行计划生成
   ↓
5. 概率计算（可能世界枚举）
   ↓
6. 结果概率分布计算
   ↓
7. 结果返回（含概率信息）
```

**数据存储流程**：

```text
1. 应用插入数据（含概率值）
   ↓
2. 概率值验证（范围检查）
   ↓
3. 数据存储（PostgreSQL存储引擎）
   ↓
4. 溯源信息记录（ProvSQL）
   ↓
5. 索引更新（概率值索引）
```

#### **3.1.4 接口设计**

**SQL接口**：

```sql
-- 数据类型接口
CREATE TYPE probability_value AS (value NUMERIC, probability NUMERIC);

-- 操作符接口
CREATE OPERATOR > (LEFTARG = NUMERIC, RIGHTARG = NUMERIC, ...);

-- 函数接口
CREATE FUNCTION get_probability(prob_value probability_value) RETURNS NUMERIC;
```

**Python接口**：

```python
# 客户端接口
class ProbabilisticDBClient:
    def insert_probabilistic_data(...)
    def query_with_confidence(...)
    def aggregate_probabilistic(...)
```

**C扩展接口**：

```c
// C扩展函数接口
PG_FUNCTION_INFO_V1(probability_gt);
Datum probability_gt(PG_FUNCTION_ARGS);
```

#### **3.1.5 扩展点**

**可扩展的组件**：

1. **自定义概率类型**：
   - 支持新的概率值表示方式
   - 支持新的概率分布类型

2. **自定义操作符**：
   - 支持新的概率操作
   - 支持新的概率比较方式

3. **自定义函数**：
   - 支持新的概率计算函数
   - 支持新的概率聚合函数

4. **查询优化器扩展**：
   - 自定义查询重写规则
   - 自定义执行计划优化

5. **索引扩展**：
   - 支持概率值索引
   - 支持概率范围索引

### 3.2 数据类型扩展

PostgreSQL扩展支持自定义概率数据类型，用于表示和存储不确定性数据。本节详细介绍概率数据类型的定义、使用和最佳实践。

#### **3.2.1 概率值类型**

**概率值类型（Probability Value Type）**是最基础的概率数据类型，用于表示单个值及其概率。

**类型定义**：

```sql
-- 概率值类型（带错误处理）
DO $$
BEGIN
    BEGIN
        IF EXISTS (SELECT 1 FROM pg_type WHERE typname = 'probability') THEN
            RAISE NOTICE '类型 probability 已存在';
        ELSE
            CREATE TYPE probability AS (
                value NUMERIC,
                confidence NUMERIC  -- 置信度 [0, 1]
            );
            RAISE NOTICE '类型 probability 已创建';
        END IF;
    EXCEPTION
        WHEN duplicate_object THEN
            RAISE WARNING '类型 probability 已存在';
        WHEN OTHERS THEN
            RAISE WARNING '创建类型 probability 失败: %', SQLERRM;
            RAISE;
    END;
END $$;
```

**使用示例**：

```sql
-- 创建使用概率值类型的表
CREATE TABLE sensor_measurements (
    id SERIAL PRIMARY KEY,
    sensor_id INT NOT NULL,
    measurement probability NOT NULL,
    timestamp TIMESTAMP DEFAULT NOW()
);

-- 插入概率值数据
INSERT INTO sensor_measurements (sensor_id, measurement)
VALUES
    (1, (25.5, 0.9)::probability),
    (1, (26.0, 0.8)::probability),
    (2, (30.0, 0.7)::probability);

-- 查询概率值
SELECT
    sensor_id,
    (measurement).value AS value,
    (measurement).confidence AS confidence,
    timestamp
FROM sensor_measurements
WHERE (measurement).confidence > 0.8;

-- 概率值聚合
SELECT
    sensor_id,
    COUNT(*) AS total_measurements,
    AVG((measurement).value) AS avg_value,
    SUM((measurement).value * (measurement).confidence) /
        SUM((measurement).confidence) AS weighted_avg,
    AVG((measurement).confidence) AS avg_confidence
FROM sensor_measurements
GROUP BY sensor_id;
```

**类型特点**：

- **value**：实际测量值或数据值
- **confidence**：该值的置信度，范围[0, 1]
- **适用场景**：传感器数据、测量数据、单一值的不确定性表示

#### **3.2.2 概率分布类型**

**概率分布类型（Probability Distribution Type）**用于表示多个可能值及其概率分布。

**类型定义**：

```sql
-- 概率分布类型（带错误处理）
DO $$
BEGIN
    BEGIN
        IF EXISTS (SELECT 1 FROM pg_type WHERE typname = 'probability_distribution') THEN
            RAISE NOTICE '类型 probability_distribution 已存在';
        ELSE
            CREATE TYPE probability_distribution AS (
                values NUMERIC[],
                probabilities NUMERIC[]
            );
            RAISE NOTICE '类型 probability_distribution 已创建';
        END IF;
    EXCEPTION
        WHEN duplicate_object THEN
            RAISE WARNING '类型 probability_distribution 已存在';
        WHEN OTHERS THEN
            RAISE WARNING '创建类型 probability_distribution 失败: %', SQLERRM;
            RAISE;
    END;
END $$;
```

**使用示例**：

```sql
-- 创建使用概率分布类型的表
CREATE TABLE weather_forecast (
    id SERIAL PRIMARY KEY,
    location VARCHAR(100) NOT NULL,
    temperature_dist probability_distribution NOT NULL,
    forecast_date DATE NOT NULL
);

-- 插入概率分布数据
INSERT INTO weather_forecast (location, temperature_dist, forecast_date)
VALUES
    ('Beijing', (ARRAY[20, 22, 24], ARRAY[0.2, 0.5, 0.3])::probability_distribution, '2025-01-15'),
    ('Shanghai', (ARRAY[18, 20, 22], ARRAY[0.3, 0.4, 0.3])::probability_distribution, '2025-01-15');

-- 查询概率分布
SELECT
    location,
    (temperature_dist).values AS possible_temperatures,
    (temperature_dist).probabilities AS probabilities,
    forecast_date
FROM weather_forecast
WHERE forecast_date = '2025-01-15';

-- 计算期望值
CREATE OR REPLACE FUNCTION expected_value(dist probability_distribution)
RETURNS NUMERIC AS $$
DECLARE
    result NUMERIC := 0;
    i INT;
BEGIN
    FOR i IN 1..array_length(dist.values, 1) LOOP
        result := result + dist.values[i] * dist.probabilities[i];
    END LOOP;
    RETURN result;
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- 使用期望值函数
SELECT
    location,
    expected_value(temperature_dist) AS expected_temperature,
    forecast_date
FROM weather_forecast;
```

**类型特点**：

- **values**：可能值的数组
- **probabilities**：对应概率的数组，必须与values长度相同且和为1
- **适用场景**：天气预报、风险评估、多值不确定性表示

#### **3.2.3 不确定性元组类型**

**不确定性元组类型（Uncertain Tuple Type）**用于表示包含多个字段的不确定性数据。

**类型定义**：

```sql
-- 不确定性元组类型（带错误处理）
DO $$
BEGIN
    BEGIN
        IF EXISTS (SELECT 1 FROM pg_type WHERE typname = 'uncertain_tuple') THEN
            RAISE NOTICE '类型 uncertain_tuple 已存在';
        ELSE
            CREATE TYPE uncertain_tuple AS (
                tuple_data JSONB,
                probability NUMERIC
            );
            RAISE NOTICE '类型 uncertain_tuple 已创建';
        END IF;
    EXCEPTION
        WHEN duplicate_object THEN
            RAISE WARNING '类型 uncertain_tuple 已存在';
        WHEN OTHERS THEN
            RAISE WARNING '创建类型 uncertain_tuple 失败: %', SQLERRM;
            RAISE;
    END;
END $$;
```

**使用示例**：

```sql
-- 创建使用不确定性元组类型的表
CREATE TABLE product_reviews (
    id SERIAL PRIMARY KEY,
    product_id INT NOT NULL,
    review uncertain_tuple NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);

-- 插入不确定性元组数据
INSERT INTO product_reviews (product_id, review)
VALUES
    (1, ('{"rating": 5, "comment": "Excellent product", "sentiment": "positive"}'::JSONB, 0.9)::uncertain_tuple),
    (1, ('{"rating": 4, "comment": "Good product", "sentiment": "positive"}'::JSONB, 0.8)::uncertain_tuple),
    (2, ('{"rating": 3, "comment": "Average product", "sentiment": "neutral"}'::JSONB, 0.7)::uncertain_tuple);

-- 查询不确定性元组
SELECT
    product_id,
    (review).tuple_data->>'rating' AS rating,
    (review).tuple_data->>'comment' AS comment,
    (review).tuple_data->>'sentiment' AS sentiment,
    (review).probability AS probability,
    created_at
FROM product_reviews
WHERE (review).probability > 0.8;

-- 概率聚合查询
SELECT
    product_id,
    COUNT(*) AS total_reviews,
    AVG(((review).tuple_data->>'rating')::NUMERIC) AS avg_rating,
    SUM(((review).tuple_data->>'rating')::NUMERIC * (review).probability) /
        SUM((review).probability) AS weighted_avg_rating
FROM product_reviews
GROUP BY product_id;
```

**类型特点**：

- **tuple_data**：JSONB格式的元组数据，可以包含任意字段
- **probability**：该元组的概率，范围[0, 1]
- **适用场景**：复杂数据结构的不确定性表示、灵活的数据模型

#### **3.2.4 类型操作符和函数**

**类型比较操作符**：

```sql
-- 概率值比较操作符
CREATE OR REPLACE FUNCTION probability_eq(p1 probability, p2 probability)
RETURNS BOOLEAN AS $$
BEGIN
    RETURN p1.value = p2.value AND p1.confidence = p2.confidence;
END;
$$ LANGUAGE plpgsql IMMUTABLE;

CREATE OPERATOR = (
    LEFTARG = probability,
    RIGHTARG = probability,
    PROCEDURE = probability_eq,
    COMMUTATOR = =
);

-- 概率值大小比较操作符
CREATE OR REPLACE FUNCTION probability_lt(p1 probability, p2 probability)
RETURNS BOOLEAN AS $$
BEGIN
    RETURN p1.value < p2.value OR
           (p1.value = p2.value AND p1.confidence < p2.confidence);
END;
$$ LANGUAGE plpgsql IMMUTABLE;

CREATE OPERATOR < (
    LEFTARG = probability,
    RIGHTARG = probability,
    PROCEDURE = probability_lt
);
```

**类型转换函数**：

```sql
-- 从NUMERIC转换为probability
CREATE OR REPLACE FUNCTION numeric_to_probability(val NUMERIC, conf NUMERIC DEFAULT 1.0)
RETURNS probability AS $$
BEGIN
    RETURN (val, conf)::probability;
END;
$$ LANGUAGE plpgsql IMMUTABLE;

CREATE CAST (NUMERIC AS probability) WITH FUNCTION numeric_to_probability(NUMERIC, NUMERIC);

-- 从probability转换为NUMERIC（返回期望值）
CREATE OR REPLACE FUNCTION probability_to_numeric(p probability)
RETURNS NUMERIC AS $$
BEGIN
    RETURN p.value * p.confidence;
END;
$$ LANGUAGE plpgsql IMMUTABLE;

CREATE CAST (probability AS NUMERIC) WITH FUNCTION probability_to_numeric(probability);
```

**类型验证函数**：

```sql
-- 验证概率值有效性
CREATE OR REPLACE FUNCTION validate_probability(p probability)
RETURNS BOOLEAN AS $$
BEGIN
    RETURN p.confidence >= 0 AND p.confidence <= 1;
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- 验证概率分布有效性
CREATE OR REPLACE FUNCTION validate_distribution(dist probability_distribution)
RETURNS BOOLEAN AS $$
DECLARE
    prob_sum NUMERIC;
BEGIN
    -- 检查数组长度是否相同
    IF array_length(dist.values, 1) != array_length(dist.probabilities, 1) THEN
        RETURN FALSE;
    END IF;

    -- 检查概率和是否为1
    SELECT SUM(prob) INTO prob_sum
    FROM unnest(dist.probabilities) AS prob;

    RETURN ABS(prob_sum - 1.0) < 0.0001;
END;
$$ LANGUAGE plpgsql IMMUTABLE;
```

#### **3.2.5 类型索引支持**

```sql
-- 为概率值类型创建GIST索引
CREATE INDEX idx_measurement_gist ON sensor_measurements
USING GIST (measurement);

-- 为概率分布类型创建GIN索引
CREATE INDEX idx_temperature_dist_gin ON weather_forecast
USING GIN ((temperature_dist).values);

-- 为不确定性元组类型创建GIN索引
CREATE INDEX idx_review_gin ON product_reviews
USING GIN ((review).tuple_data);
```

#### **3.2.6 类型使用最佳实践**

**1. 类型选择原则**：

- **单一值不确定性**：使用`probability`类型
- **多值概率分布**：使用`probability_distribution`类型
- **复杂数据结构**：使用`uncertain_tuple`类型

**2. 性能优化**：

- 为常用查询字段创建索引
- 使用物化视图预计算概率聚合
- 避免在WHERE子句中使用复杂类型操作

**3. 数据验证**：

- 使用CHECK约束验证概率值范围
- 使用触发器自动验证数据有效性
- 定期检查数据质量

**4. 查询优化**：

- 先过滤再聚合
- 使用物化视图加速查询
- 避免深度嵌套的类型操作

### 3.3 查询处理扩展

#### **3.3.1 概率查询操作符**

**概率比较操作符**：

```sql
-- 概率选择（带错误处理和性能测试）
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'sensor_data') THEN
            RAISE WARNING '表 sensor_data 不存在，无法执行查询';
            RETURN;
        END IF;
    EXCEPTION
        WHEN OTHERS THEN
            RAISE WARNING '检查表存在性失败: %', SQLERRM;
            RAISE;
    END;
END $$;

EXPLAIN ANALYZE
SELECT * FROM sensor_data
WHERE probability > 0.8;

-- 概率范围查询
SELECT * FROM sensor_data
WHERE probability BETWEEN 0.7 AND 0.9;

-- 概率排序查询
SELECT * FROM sensor_data
ORDER BY probability DESC
LIMIT 100;
```

**概率连接操作符**：

```sql
-- 概率连接（带错误处理和性能测试）
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'uncertain_table_a') OR
           NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'uncertain_table_b') THEN
            RAISE WARNING '表 uncertain_table_a 或 uncertain_table_b 不存在，无法执行查询';
            RETURN;
        END IF;
    EXCEPTION
        WHEN OTHERS THEN
            RAISE WARNING '检查表存在性失败: %', SQLERRM;
            RAISE;
    END;
END $$;

EXPLAIN ANALYZE
SELECT a.*, b.*
FROM uncertain_table_a a
JOIN uncertain_table_b b
ON a.id = b.id
WHERE probability(a) * probability(b) > 0.5;

-- 概率内连接（只返回高概率结果）
SELECT a.*, b.*
FROM uncertain_table_a a
INNER JOIN uncertain_table_b b
ON a.id = b.id
WHERE a.probability > 0.8 AND b.probability > 0.8;
```

**概率聚合操作符**：

```sql
-- 概率聚合（带错误处理和性能测试）
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'sensor_data') THEN
            RAISE WARNING '表 sensor_data 不存在，无法执行查询';
            RETURN;
        END IF;

        -- 检查是否有probability相关的函数
        IF NOT EXISTS (SELECT 1 FROM pg_proc WHERE proname IN ('prob_avg', 'prob_stddev')) THEN
            RAISE WARNING '概率聚合函数PROB_AVG或PROB_STDDEV不存在，可能需要安装ProvSQL扩展';
        END IF;
    EXCEPTION
        WHEN OTHERS THEN
            RAISE WARNING '检查失败: %', SQLERRM;
            RAISE;
    END;
END $$;

EXPLAIN ANALYZE
SELECT
    sensor_id,
    PROB_AVG(temperature) AS avg_temp,
    PROB_STDDEV(temperature) AS stddev_temp
FROM sensor_data
GROUP BY sensor_id;
```

#### **3.3.2 自定义操作符实现**

**概率值比较操作符**：

```sql
-- 创建概率值比较操作符
CREATE OR REPLACE FUNCTION probability_gt(
    prob1 NUMERIC,
    prob2 NUMERIC
) RETURNS BOOLEAN AS $$
BEGIN
    RETURN prob1 > prob2;
END;
$$ LANGUAGE plpgsql IMMUTABLE;

CREATE OPERATOR > (
    LEFTARG = NUMERIC,
    RIGHTARG = NUMERIC,
    PROCEDURE = probability_gt,
    COMMUTATOR = <
);

-- 使用自定义操作符
SELECT * FROM sensor_data
WHERE probability > 0.8;
```

**概率值算术操作符**：

```sql
-- 概率值乘法操作符（用于连接概率计算）
CREATE OR REPLACE FUNCTION probability_multiply(
    prob1 NUMERIC,
    prob2 NUMERIC
) RETURNS NUMERIC AS $$
BEGIN
    -- 确保结果在[0, 1]范围内
    RETURN GREATEST(0, LEAST(1, prob1 * prob2));
END;
$$ LANGUAGE plpgsql IMMUTABLE;

CREATE OPERATOR * (
    LEFTARG = NUMERIC,
    RIGHTARG = NUMERIC,
    PROCEDURE = probability_multiply
);

-- 使用概率乘法
SELECT
    a.id,
    a.probability * b.probability AS joint_probability
FROM uncertain_table_a a
JOIN uncertain_table_b b ON a.id = b.id;
```

#### **3.3.3 自定义函数实现**

**概率值提取函数**：

```sql
-- 提取概率值
CREATE OR REPLACE FUNCTION get_probability(
    prob_value probability_value
) RETURNS NUMERIC AS $$
BEGIN
    RETURN prob_value.confidence;
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- 提取值
CREATE OR REPLACE FUNCTION get_value(
    prob_value probability_value
) RETURNS NUMERIC AS $$
BEGIN
    RETURN prob_value.value;
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- 使用函数
SELECT
    id,
    get_value(temperature) AS temp_value,
    get_probability(temperature) AS temp_probability
FROM sensor_readings;
```

**概率聚合函数**：

```sql
-- 概率加权平均
CREATE OR REPLACE FUNCTION prob_weighted_avg(
    values NUMERIC[],
    probabilities NUMERIC[]
) RETURNS NUMERIC AS $$
DECLARE
    weighted_sum NUMERIC := 0;
    prob_sum NUMERIC := 0;
    i INTEGER;
BEGIN
    IF array_length(values, 1) != array_length(probabilities, 1) THEN
        RAISE EXCEPTION '数组长度不匹配';
    END IF;

    FOR i IN 1..array_length(values, 1) LOOP
        weighted_sum := weighted_sum + values[i] * probabilities[i];
        prob_sum := prob_sum + probabilities[i];
    END LOOP;

    IF prob_sum = 0 THEN
        RETURN NULL;
    END IF;

    RETURN weighted_sum / prob_sum;
END;
$$ LANGUAGE plpgsql;

-- 使用概率加权平均
SELECT
    sensor_id,
    prob_weighted_avg(
        ARRAY_AGG(value),
        ARRAY_AGG(probability)
    ) AS weighted_avg
FROM sensor_data
GROUP BY sensor_id;
```

**概率统计函数**：

```sql
-- 概率最大值
CREATE OR REPLACE FUNCTION prob_max(
    prob_value probability_value
) RETURNS probability_value AS $$
BEGIN
    RETURN prob_value;
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- 概率最小值
CREATE OR REPLACE FUNCTION prob_min(
    prob_value probability_value
) RETURNS probability_value AS $$
BEGIN
    RETURN prob_value;
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- 概率标准差
CREATE OR REPLACE FUNCTION prob_stddev(
    values NUMERIC[],
    probabilities NUMERIC[]
) RETURNS NUMERIC AS $$
DECLARE
    mean NUMERIC;
    variance NUMERIC := 0;
    prob_sum NUMERIC := 0;
    i INTEGER;
BEGIN
    -- 计算加权平均
    mean := prob_weighted_avg(values, probabilities);

    -- 计算加权方差
    FOR i IN 1..array_length(values, 1) LOOP
        variance := variance + probabilities[i] * POWER(values[i] - mean, 2);
        prob_sum := prob_sum + probabilities[i];
    END LOOP;

    IF prob_sum = 0 THEN
        RETURN NULL;
    END IF;

    RETURN SQRT(variance / prob_sum);
END;
$$ LANGUAGE plpgsql;
```

#### **3.3.4 查询优化器扩展**

**概率查询重写规则**：

```sql
-- 创建查询重写函数
CREATE OR REPLACE FUNCTION optimize_probability_query(
    query_text TEXT
) RETURNS TEXT AS $$
DECLARE
    optimized_query TEXT;
BEGIN
    -- 示例：将概率过滤提前
    optimized_query := query_text;

    -- 如果查询包含概率过滤，提前执行
    IF query_text LIKE '%probability%' THEN
        -- 添加索引提示
        optimized_query := REPLACE(
            optimized_query,
            'WHERE',
            'WHERE probability > 0.5 AND'
        );
    END IF;

    RETURN optimized_query;
END;
$$ LANGUAGE plpgsql;

-- 使用查询优化
SELECT optimize_probability_query('
    SELECT * FROM sensor_data
    WHERE sensor_id = 1
    AND probability > 0.8
');
```

**概率查询优化策略**：

1. **概率过滤提前**：
   - 将概率过滤条件提前执行
   - 减少后续处理的数据量
   - 利用概率索引加速查询

2. **概率聚合优化**：
   - 使用增量计算
   - 缓存中间结果
   - 并行计算概率聚合

3. **概率连接优化**：
   - 选择最优连接顺序
   - 使用概率索引加速连接
   - 减少可能世界枚举

**概率索引支持**：

```sql
-- 创建概率值B-tree索引（用于精确查询）
CREATE INDEX idx_probability_btree ON sensor_data(probability);

-- 创建概率值GIN索引（用于范围查询）
CREATE INDEX idx_probability_gin ON sensor_data USING GIN(
    probability_range int4range(
        FLOOR(probability * 100)::int,
        CEIL(probability * 100)::int
    )
);

-- 创建复合索引（概率+其他字段）
CREATE INDEX idx_sensor_prob ON sensor_data(sensor_id, probability);

-- 创建部分索引（只索引高概率数据）
CREATE INDEX idx_high_probability ON sensor_data(probability)
WHERE probability > 0.8;

-- 使用索引提示
SET enable_seqscan = off;
SELECT * FROM sensor_data
WHERE probability > 0.8
ORDER BY probability DESC;
SET enable_seqscan = on;
```

**查询优化器配置**：

```sql
-- 配置查询优化器参数
SET random_page_cost = 1.1;  -- 降低随机I/O成本
SET cpu_tuple_cost = 0.01;   -- 降低CPU成本
SET effective_cache_size = '4GB';  -- 设置缓存大小

-- 启用并行查询
SET max_parallel_workers_per_gather = 4;
SET parallel_setup_cost = 1000;
SET parallel_tuple_cost = 0.1;

-- 概率查询优化配置
SET enable_probability_optimization = on;
SET probability_threshold = 0.5;  -- 概率阈值
SET max_possible_worlds = 10000;  -- 最大可能世界数
```

**自定义查询重写规则**：

```sql
-- 创建查询重写规则表
CREATE TABLE IF NOT EXISTS query_rewrite_rules (
    id SERIAL PRIMARY KEY,
    pattern TEXT NOT NULL,
    replacement TEXT NOT NULL,
    enabled BOOLEAN DEFAULT TRUE,
    priority INTEGER DEFAULT 0
);

-- 插入重写规则
INSERT INTO query_rewrite_rules (pattern, replacement, priority)
VALUES
    (
        'SELECT.*FROM.*WHERE.*probability.*>.*(\d+\.?\d*)',
        'SELECT * FROM $1 WHERE probability > $2 AND probability > 0.5',
        10
    ),
    (
        'SELECT.*AVG.*probability',
        'SELECT prob_weighted_avg(value, probability) AS avg_value',
        5
    );

-- 查询重写函数
CREATE OR REPLACE FUNCTION apply_query_rewrite_rules(
    query_text TEXT
) RETURNS TEXT AS $$
DECLARE
    rule_record RECORD;
    rewritten_query TEXT := query_text;
BEGIN
    FOR rule_record IN
        SELECT pattern, replacement
        FROM query_rewrite_rules
        WHERE enabled = TRUE
        ORDER BY priority DESC
    LOOP
        -- 应用重写规则（简化示例）
        IF rewritten_query ~ rule_record.pattern THEN
            rewritten_query := regexp_replace(
                rewritten_query,
                rule_record.pattern,
                rule_record.replacement,
                'g'
            );
        END IF;
    END LOOP;

    RETURN rewritten_query;
END;
$$ LANGUAGE plpgsql;
```

#### **3.3.5 概率查询执行计划**

**查看概率查询执行计划**：

```sql
-- 基础执行计划
EXPLAIN SELECT * FROM sensor_data WHERE probability > 0.8;

-- 详细执行计划（包含成本）
EXPLAIN (ANALYZE, BUFFERS, VERBOSE)
SELECT
    sensor_id,
    AVG(value * probability) / AVG(probability) AS weighted_avg
FROM sensor_data
GROUP BY sensor_id;

-- 概率JOIN执行计划
EXPLAIN (ANALYZE, BUFFERS)
SELECT a.*, b.*
FROM uncertain_table_a a
JOIN uncertain_table_b b ON a.id = b.id
WHERE a.probability * b.probability > 0.5;
```

**执行计划分析**：

**1. 执行计划结构**：

```sql
-- 查看完整执行计划
EXPLAIN (ANALYZE, BUFFERS, VERBOSE, COSTS, TIMING, FORMAT JSON)
SELECT
    sensor_id,
    SUM(value * probability) / SUM(probability) AS weighted_avg,
    AVG(probability) AS avg_probability
FROM sensor_data
WHERE probability > 0.8
GROUP BY sensor_id;

-- 执行计划示例输出：
-- HashAggregate (cost=... rows=...)
--   -> Seq Scan on sensor_data (cost=... rows=...)
--         Filter: (probability > 0.8)
```

**2. 执行计划优化**：

```sql
-- 使用索引的执行计划
EXPLAIN (ANALYZE, BUFFERS)
SELECT * FROM sensor_data
WHERE probability > 0.8
ORDER BY probability DESC;

-- 优化后的执行计划：
-- Index Scan using idx_probability_btree on sensor_data
--   Index Cond: (probability > 0.8)
--   Order By: probability DESC
```

**3. 概率查询执行计划特点**：

- **概率计算节点**：执行计划中包含概率计算节点
- **可能世界枚举**：对于复杂查询，可能包含可能世界枚举节点
- **概率聚合节点**：包含概率聚合计算节点
- **溯源追踪节点**：如果启用溯源，包含溯源追踪节点

**4. 执行计划优化建议**：

```sql
-- 1. 使用索引加速概率过滤
CREATE INDEX idx_probability ON sensor_data(probability);
EXPLAIN SELECT * FROM sensor_data WHERE probability > 0.8;

-- 2. 使用部分索引减少索引大小
CREATE INDEX idx_high_prob ON sensor_data(probability)
WHERE probability > 0.7;

-- 3. 使用复合索引优化多条件查询
CREATE INDEX idx_sensor_prob ON sensor_data(sensor_id, probability);
EXPLAIN SELECT * FROM sensor_data
WHERE sensor_id = 1 AND probability > 0.8;

-- 4. 使用并行查询加速大数据量查询
SET max_parallel_workers_per_gather = 4;
EXPLAIN (ANALYZE) SELECT * FROM sensor_data WHERE probability > 0.8;

-- 5. 调整成本参数优化执行计划
SET random_page_cost = 1.1;
SET cpu_tuple_cost = 0.01;
EXPLAIN SELECT * FROM sensor_data WHERE probability > 0.8;
```

**5. 执行计划监控**：

```sql
-- 创建执行计划监控表
CREATE TABLE IF NOT EXISTS query_plan_log (
    id SERIAL PRIMARY KEY,
    query_text TEXT,
    execution_plan TEXT,
    execution_time INTERVAL,
    rows_returned BIGINT,
    created_at TIMESTAMP DEFAULT NOW()
) WITH PROVENANCE;

-- 记录执行计划函数
CREATE OR REPLACE FUNCTION log_query_plan(
    query_text TEXT
) RETURNS TEXT AS $$
DECLARE
    plan_text TEXT;
    exec_time INTERVAL;
    row_count BIGINT;
BEGIN
    -- 获取执行计划
    EXECUTE format('EXPLAIN (ANALYZE, BUFFERS) %s', query_text) INTO plan_text;

    -- 记录执行计划
    INSERT INTO query_plan_log (query_text, execution_plan)
    VALUES (query_text, plan_text);

    RETURN plan_text;
END;
$$ LANGUAGE plpgsql;

-- 使用执行计划监控
SELECT log_query_plan('
    SELECT * FROM sensor_data WHERE probability > 0.8
');

-- 查询执行计划历史
SELECT
    id,
    query_text,
    execution_time,
    rows_returned,
    created_at
FROM query_plan_log
ORDER BY created_at DESC
LIMIT 10;
```

**6. 执行计划对比分析**：

```sql
-- 对比不同查询的执行计划
-- 查询1：不使用索引
EXPLAIN (ANALYZE, BUFFERS)
SELECT * FROM sensor_data WHERE probability > 0.8;

-- 查询2：使用索引
CREATE INDEX idx_probability ON sensor_data(probability);
EXPLAIN (ANALYZE, BUFFERS)
SELECT * FROM sensor_data WHERE probability > 0.8;

-- 查询3：使用部分索引
CREATE INDEX idx_high_prob ON sensor_data(probability)
WHERE probability > 0.7;
EXPLAIN (ANALYZE, BUFFERS)
SELECT * FROM sensor_data WHERE probability > 0.8;
```

**执行计划优化建议总结**：

1. **使用索引**：为概率值创建索引，加速概率过滤
2. **提前过滤**：在JOIN前过滤低概率数据，减少连接数据量
3. **批量处理**：使用批量操作减少开销
4. **物化视图**：预计算常见概率查询，加速查询
5. **并行查询**：对于大数据量查询，使用并行查询加速
6. **成本参数调优**：根据实际硬件环境调整成本参数
7. **执行计划监控**：定期监控执行计划，识别性能问题

---

## 4. ProvSQL集成

### 4.1 ProvSQL概述

**ProvSQL**是一个PostgreSQL扩展，用于追踪数据的溯源（Provenance）和概率（Probability）。

#### **4.1.1 核心功能**

- **数据溯源追踪**：追踪数据的来源和转换过程
- **概率计算**：基于溯源信息计算概率
- **不确定性管理**：管理不确定性数据的概率分布
- **查询结果概率**：计算查询结果的概率分布

#### **4.1.2 技术特点**

- **无缝集成**：作为PostgreSQL扩展，无需修改应用代码
- **高性能**：优化的概率计算算法
- **灵活配置**：支持多种概率计算模式
- **标准兼容**：支持SQL标准语法

#### **4.1.3 适用场景**

- 传感器数据不确定性处理
- 数据融合和集成
- 数据质量评估
- 数据溯源和审计

### 4.2 安装和配置

#### **4.2.1 安装步骤**

**从源码编译安装**：

```bash
# 1. 克隆ProvSQL仓库
git clone https://github.com/PierreSenellart/provsql.git
cd provsql

# 2. 编译和安装
make
sudo make install

# 3. 在PostgreSQL中启用扩展
psql -d your_database -c "CREATE EXTENSION provsql;"
```

**使用Docker安装**：

```dockerfile
FROM postgres:18

# 安装依赖
RUN apt-get update && apt-get install -y \
    build-essential \
    git \
    postgresql-server-dev-18

# 克隆和编译ProvSQL
RUN git clone https://github.com/PierreSenellart/provsql.git /tmp/provsql && \
    cd /tmp/provsql && \
    make && \
    make install

# 清理
RUN rm -rf /tmp/provsql && \
    apt-get clean
```

#### **4.2.2 配置选项**

```sql
-- 查看ProvSQL配置
SHOW provsql.enable_provenance;
SHOW provsql.probability_mode;

-- 启用溯源追踪
SET provsql.enable_provenance = on;

-- 设置概率计算模式
SET provsql.probability_mode = 'default';  -- 'default', 'optimized', 'exact'
```

#### **4.2.3 验证安装**

```sql
-- 检查扩展是否安装
SELECT * FROM pg_extension WHERE extname = 'provsql';

-- 检查可用函数
SELECT proname, prosrc
FROM pg_proc
WHERE proname LIKE 'provsql%';

-- 测试基本功能
SELECT provsql_provenance_of(
    SELECT 1
);
```

### 4.3 集成架构

#### **4.3.1 整体架构设计**

**ProvSQL与概率数据库集成架构**：

```text
┌─────────────────────────────────────────────────────────┐
│              ProvSQL集成架构                             │
├─────────────────────────────────────────────────────────┤
│                                                          │
│  用户查询（SQL + 概率操作）                              │
│    ↓                                                     │
│  ┌────────────────────────────────────┐                │
│  │   PostgreSQL查询解析层              │                │
│  │  ├── SQL解析                        │                │
│  │  ├── 查询重写                       │                │
│  │  └── 执行计划生成                   │                │
│  └────────────────────────────────────┘                │
│    ↓                                                     │
│  ┌────────────────────────────────────┐                │
│  │   ProvSQL溯源追踪层                 │                │
│  │  ├── 追踪数据来源                   │                │
│  │  ├── 记录转换过程                   │                │
│  │  ├── 构建溯源图                     │                │
│  │  ├── 溯源信息存储                   │                │
│  │  └── 溯源查询接口                   │                │
│  └────────────────────────────────────┘                │
│    ↓                                                     │
│  ┌────────────────────────────────────┐                │
│  │   概率数据库查询处理层               │                │
│  │  ├── 概率查询优化                   │                │
│  │  ├── 概率计算引擎                   │                │
│  │  ├── 可能世界枚举                   │                │
│  │  ├── 概率聚合计算                   │                │
│  │  └── 结果概率分布                   │                │
│  └────────────────────────────────────┘                │
│    ↓                                                     │
│  ┌────────────────────────────────────┐                │
│  │   结果整合层                        │                │
│  │  ├── 合并溯源信息                   │                │
│  │  ├── 合并概率信息                   │                │
│  │  └── 格式化输出                     │                │
│  └────────────────────────────────────┘                │
│    ↓                                                     │
│  查询结果（含概率信息和溯源信息）                        │
│                                                          │
└─────────────────────────────────────────────────────────┘
```

#### **4.3.2 各层职责详解**

**1. PostgreSQL查询解析层**：

- **职责**：
  - 解析标准SQL查询
  - 识别概率相关操作（WITH PROBABILITY、PROBABILITY()等）
  - 查询重写和优化
  - 生成执行计划

- **关键组件**：
  - SQL解析器：解析SQL语法
  - 查询重写器：重写概率查询
  - 计划生成器：生成执行计划

**2. ProvSQL溯源追踪层**：

- **职责**：
  - 追踪数据来源：记录数据的原始来源
  - 记录转换过程：记录数据转换和处理的每一步
  - 构建溯源图：构建完整的数据溯源图
  - 存储溯源信息：持久化存储溯源信息
  - 提供溯源查询接口：提供查询溯源信息的接口

- **关键组件**：
  - 溯源追踪器：自动追踪数据来源
  - 溯源图构建器：构建和存储溯源图
  - 溯源查询引擎：查询和检索溯源信息

**3. 概率数据库查询处理层**：

- **职责**：
  - 概率查询优化：优化概率查询的执行
  - 概率计算引擎：计算概率值
  - 可能世界枚举：枚举所有可能世界
  - 概率聚合计算：计算概率聚合结果
  - 结果概率分布：生成结果的概率分布

- **关键组件**：
  - 概率查询优化器：优化概率查询
  - 概率计算引擎：执行概率计算
  - 可能世界枚举器：枚举可能世界
  - 概率聚合器：聚合概率值

**4. 结果整合层**：

- **职责**：
  - 合并溯源信息：将溯源信息合并到查询结果
  - 合并概率信息：将概率信息合并到查询结果
  - 格式化输出：格式化最终输出结果

- **关键组件**：
  - 结果合并器：合并各种信息
  - 格式化器：格式化输出

#### **4.3.3 数据流详细说明**

**查询处理流程**：

1. **查询解析阶段**：
   - 用户提交SQL查询（可能包含概率操作）
   - PostgreSQL解析器解析SQL
   - 识别概率相关操作和语法
   - 生成查询树

2. **查询重写阶段**：
   - 识别概率查询模式
   - 重写查询以支持概率计算
   - 添加溯源追踪代码
   - 优化查询结构

3. **执行计划生成阶段**：
   - 生成包含概率计算的执行计划
   - 添加溯源追踪节点
   - 优化执行计划
   - 选择最优执行策略

4. **查询执行阶段**：
   - 执行数据查询
   - 并行执行溯源追踪
   - 执行概率计算
   - 合并中间结果

5. **结果生成阶段**：
   - 合并溯源信息
   - 合并概率信息
   - 格式化输出
   - 返回最终结果

**数据流示例**：

```sql
-- 用户查询
SELECT
    sensor_id,
    AVG(value) AS avg_value,
    PROBABILITY() AS prob,
    provsql_provenance_of(
        SELECT * FROM sensor_data WHERE sensor_id = sensor_data.sensor_id
    ) AS provenance
FROM sensor_data
WHERE timestamp > NOW() - INTERVAL '1 hour'
GROUP BY sensor_id;

-- 执行流程：
-- 1. 解析查询，识别PROBABILITY()和provsql_provenance_of()
-- 2. 重写查询，添加概率计算和溯源追踪
-- 3. 生成执行计划，包含概率计算节点和溯源追踪节点
-- 4. 执行查询：
--    - 查询sensor_data表
--    - 并行追踪数据来源
--    - 计算概率值
--    - 计算平均值
-- 5. 合并结果：
--    - 合并平均值
--    - 合并概率值
--    - 合并溯源信息
-- 6. 返回最终结果
```

#### **4.3.4 接口设计**

**ProvSQL接口**：

1. **溯源追踪接口**：

   ```sql
   -- 启用溯源追踪
   CREATE TABLE table_name (...) WITH PROVENANCE;

   -- 查询溯源信息
   provsql_provenance_of(query)
   ```

2. **概率计算接口**：

   ```sql
   -- 插入带概率的数据
   INSERT INTO table_name VALUES (...) WITH PROBABILITY 0.9;

   -- 查询概率值
   PROBABILITY()
   ```

3. **配置接口**：

   ```sql
   -- 配置概率计算模式
   SET provsql.probability_mode = 'default';

   -- 启用/禁用溯源追踪
   SET provsql.enable_provenance = on;
   ```

**概率数据库接口**：

1. **数据类型接口**：

   ```sql
   -- 概率值类型
   CREATE TYPE probability_value AS (value NUMERIC, confidence NUMERIC);
   ```

2. **操作符接口**：

   ```sql
   -- 概率比较操作符
   CREATE OPERATOR > (LEFTARG = NUMERIC, RIGHTARG = NUMERIC, ...);
   ```

3. **函数接口**：

   ```sql
   -- 概率聚合函数
   CREATE FUNCTION prob_weighted_avg(...) RETURNS NUMERIC;
   ```

#### **4.3.5 集成优势**

**1. 无缝集成**：

- 作为PostgreSQL扩展，无需修改应用代码
- 支持标准SQL语法
- 兼容现有PostgreSQL功能

**2. 高性能**：

- 优化的概率计算算法
- 并行溯源追踪
- 高效的查询执行

**3. 灵活配置**：

- 可配置的概率计算模式
- 可选的溯源追踪
- 灵活的查询优化策略

**4. 易于使用**：

- 简单的SQL语法
- 丰富的函数和操作符
- 完善的文档和示例

### 4.4 核心功能详解

#### **4.4.1 溯源追踪**

```sql
-- 创建带溯源的表
CREATE TABLE sensor_data (
    id SERIAL,
    sensor_id INT,
    value NUMERIC,
    timestamp TIMESTAMP
) WITH PROVENANCE;

-- 插入数据（自动记录溯源）
INSERT INTO sensor_data (sensor_id, value, timestamp)
VALUES (1, 25.5, NOW());

-- 查询溯源信息
SELECT provsql_provenance_of(
    SELECT * FROM sensor_data WHERE sensor_id = 1
);
```

#### **4.4.2 概率计算**

```sql
-- 插入带概率的数据
INSERT INTO sensor_data (sensor_id, value, timestamp)
VALUES (1, 25.5, NOW())
WITH PROBABILITY 0.95;

-- 查询概率
SELECT
    sensor_id,
    AVG(value) AS avg_value,
    PROBABILITY() AS probability
FROM sensor_data
GROUP BY sensor_id;
```

#### **4.4.3 溯源和概率结合**

```sql
-- 查询溯源和概率
SELECT
    provsql_provenance_of(
        SELECT * FROM sensor_data WHERE sensor_id = 1
    ) AS provenance,
    PROBABILITY() AS probability
FROM sensor_data
WHERE sensor_id = 1;
```

### 4.5 使用示例

#### **4.5.1 基础使用示例**

**示例1：创建带溯源和概率的表**

```sql
-- 启用ProvSQL扩展（带错误处理）
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM pg_extension WHERE extname = 'provsql') THEN
            CREATE EXTENSION provsql;
            RAISE NOTICE 'ProvSQL扩展已创建';
        ELSE
            RAISE NOTICE 'ProvSQL扩展已存在';
        END IF;
    EXCEPTION
        WHEN undefined_file THEN
            RAISE WARNING 'ProvSQL扩展文件不存在，请确保已正确安装';
        WHEN insufficient_privilege THEN
            RAISE WARNING '权限不足，无法创建扩展';
        WHEN OTHERS THEN
            RAISE WARNING '创建ProvSQL扩展失败: %', SQLERRM;
            RAISE;
    END;
END $$;

-- 创建带溯源的表（带错误处理）
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM pg_extension WHERE extname = 'provsql') THEN
            RAISE WARNING 'ProvSQL扩展未安装，请先执行: CREATE EXTENSION provsql;';
            RETURN;
        END IF;

        IF EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'sensor_readings') THEN
            RAISE NOTICE '表 sensor_readings 已存在';
        ELSE
            CREATE TABLE sensor_readings (
                id SERIAL,
                sensor_id INT,
                temperature NUMERIC,
                timestamp TIMESTAMP
            ) WITH PROVENANCE;
            RAISE NOTICE '表 sensor_readings 已创建并启用溯源';
        END IF;
    EXCEPTION
        WHEN duplicate_table THEN
            RAISE WARNING '表 sensor_readings 已存在';
        WHEN undefined_object THEN
            RAISE WARNING 'ProvSQL扩展未安装或WITH PROVENANCE语法不支持';
        WHEN OTHERS THEN
            RAISE WARNING '创建表失败: %', SQLERRM;
            RAISE;
    END;
END $$;
```

**示例2：插入带概率的数据**

```sql
-- 插入数据（带概率，带错误处理）
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'sensor_readings') THEN
            RAISE WARNING '表 sensor_readings 不存在，无法插入数据';
            RETURN;
        END IF;

        IF NOT EXISTS (SELECT 1 FROM pg_extension WHERE extname = 'provsql') THEN
            RAISE WARNING 'ProvSQL扩展未安装，无法使用WITH PROBABILITY语法';
            RETURN;
        END IF;

        -- 插入多条带概率的数据
        INSERT INTO sensor_readings (sensor_id, temperature, timestamp)
        VALUES
            (1, 25.5, NOW()) WITH PROBABILITY 0.95,
            (1, 25.7, NOW()) WITH PROBABILITY 0.90,
            (1, 25.3, NOW()) WITH PROBABILITY 0.85;

        RAISE NOTICE '已插入3条带概率的数据';
    EXCEPTION
        WHEN undefined_table THEN
            RAISE WARNING '表 sensor_readings 不存在';
        WHEN syntax_error THEN
            RAISE WARNING 'WITH PROBABILITY语法不支持，请确保ProvSQL扩展已正确安装';
        WHEN OTHERS THEN
            RAISE WARNING '插入数据失败: %', SQLERRM;
            RAISE;
    END;
END $$;
```

#### **4.5.2 概率查询示例**

**示例3：基础概率查询**

```sql
-- 概率查询（带错误处理和性能测试）
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'sensor_readings') THEN
            RAISE WARNING '表 sensor_readings 不存在，无法执行查询';
            RETURN;
        END IF;

        IF NOT EXISTS (SELECT 1 FROM pg_extension WHERE extname = 'provsql') THEN
            RAISE WARNING 'ProvSQL扩展未安装，无法使用PROBABILITY函数';
            RETURN;
        END IF;
    EXCEPTION
        WHEN OTHERS THEN
            RAISE WARNING '检查失败: %', SQLERRM;
            RAISE;
    END;
END $$;

EXPLAIN ANALYZE
SELECT
    sensor_id,
    AVG(temperature) AS avg_temp,
    PROBABILITY() AS prob
FROM sensor_readings
WHERE timestamp > NOW() - INTERVAL '1 hour'
GROUP BY sensor_id;
```

**示例4：概率过滤查询**

```sql
-- 查询高概率数据（概率>0.9）
SELECT
    sensor_id,
    temperature,
    timestamp,
    PROBABILITY() AS probability
FROM sensor_readings
WHERE PROBABILITY() > 0.9
ORDER BY PROBABILITY() DESC;
```

**示例5：概率聚合查询**

```sql
-- 计算加权平均温度（概率加权）
SELECT
    sensor_id,
    SUM(temperature * PROBABILITY()) / SUM(PROBABILITY()) AS weighted_avg_temp,
    AVG(PROBABILITY()) AS avg_confidence,
    COUNT(*) AS reading_count
FROM sensor_readings
WHERE timestamp > NOW() - INTERVAL '24 hours'
GROUP BY sensor_id
HAVING AVG(PROBABILITY()) > 0.8;
```

#### **4.5.3 溯源查询示例**

**示例6：查询数据溯源**

```sql
-- 查询数据的溯源信息
SELECT
    id,
    sensor_id,
    temperature,
    provsql_provenance_of(
        SELECT * FROM sensor_readings WHERE id = sensor_readings.id
    ) AS provenance
FROM sensor_readings
WHERE sensor_id = 1
LIMIT 10;
```

**示例7：溯源和概率结合查询**

```sql
-- 查询溯源和概率
SELECT
    sensor_id,
    AVG(temperature) AS avg_temp,
    provsql_provenance_of(
        SELECT * FROM sensor_readings WHERE sensor_id = sensor_readings.sensor_id
    ) AS provenance,
    PROBABILITY() AS probability
FROM sensor_readings
GROUP BY sensor_id;
```

#### **4.5.4 复杂查询示例**

**示例8：多表概率JOIN查询**

```sql
-- 创建第二个表
CREATE TABLE IF NOT EXISTS sensor_metadata (
    sensor_id INT PRIMARY KEY,
    location TEXT,
    sensor_type TEXT
) WITH PROVENANCE;

-- 插入元数据
INSERT INTO sensor_metadata (sensor_id, location, sensor_type)
VALUES
    (1, 'Building A', 'Temperature') WITH PROBABILITY 0.98,
    (2, 'Building B', 'Humidity') WITH PROBABILITY 0.95;

-- 概率JOIN查询
SELECT
    sr.sensor_id,
    sm.location,
    sm.sensor_type,
    AVG(sr.temperature) AS avg_temp,
    PROBABILITY() AS join_probability
FROM sensor_readings sr
JOIN sensor_metadata sm ON sr.sensor_id = sm.sensor_id
WHERE sr.timestamp > NOW() - INTERVAL '1 hour'
GROUP BY sr.sensor_id, sm.location, sm.sensor_type;
```

**示例9：概率子查询**

```sql
-- 使用概率子查询
SELECT
    sensor_id,
    temperature,
    timestamp,
    PROBABILITY() AS probability,
    (SELECT AVG(temperature)
     FROM sensor_readings sr2
     WHERE sr2.sensor_id = sensor_readings.sensor_id
       AND PROBABILITY() > 0.8
    ) AS high_confidence_avg
FROM sensor_readings
WHERE timestamp > NOW() - INTERVAL '1 hour';
```

**示例10：概率窗口函数**

```sql
-- 使用概率窗口函数
SELECT
    sensor_id,
    temperature,
    timestamp,
    PROBABILITY() AS probability,
    AVG(temperature) OVER (
        PARTITION BY sensor_id
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    ) AS moving_avg,
    AVG(PROBABILITY()) OVER (
        PARTITION BY sensor_id
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    ) AS moving_avg_prob
FROM sensor_readings
WHERE timestamp > NOW() - INTERVAL '24 hours'
ORDER BY sensor_id, timestamp;
```

#### **4.5.5 实际应用场景示例**

**示例11：数据质量评估**

```sql
-- 评估数据质量（基于概率）
SELECT
    sensor_id,
    COUNT(*) AS total_readings,
    COUNT(*) FILTER (WHERE PROBABILITY() > 0.9) AS high_quality_readings,
    COUNT(*) FILTER (WHERE PROBABILITY() BETWEEN 0.7 AND 0.9) AS medium_quality_readings,
    COUNT(*) FILTER (WHERE PROBABILITY() < 0.7) AS low_quality_readings,
    AVG(PROBABILITY()) AS avg_quality_score
FROM sensor_readings
WHERE timestamp > NOW() - INTERVAL '24 hours'
GROUP BY sensor_id
ORDER BY avg_quality_score DESC;
```

**示例12：异常检测（低概率数据）**

```sql
-- 检测异常数据（低概率值）
SELECT
    id,
    sensor_id,
    temperature,
    timestamp,
    PROBABILITY() AS probability,
    provsql_provenance_of(
        SELECT * FROM sensor_readings WHERE id = sensor_readings.id
    ) AS provenance
FROM sensor_readings
WHERE PROBABILITY() < 0.5  -- 低概率数据可能是异常
ORDER BY PROBABILITY() ASC
LIMIT 20;
```

**示例13：数据融合查询**

```sql
-- 多源数据融合（使用概率加权）
WITH source_a AS (
    SELECT sensor_id, temperature, PROBABILITY() AS prob
    FROM sensor_readings
    WHERE timestamp > NOW() - INTERVAL '1 hour'
),
source_b AS (
    SELECT sensor_id, temperature, PROBABILITY() AS prob
    FROM sensor_readings_backup
    WHERE timestamp > NOW() - INTERVAL '1 hour'
)
SELECT
    COALESCE(a.sensor_id, b.sensor_id) AS sensor_id,
    COALESCE(a.temperature, b.temperature) AS temperature,
    GREATEST(COALESCE(a.prob, 0), COALESCE(b.prob, 0)) AS fused_probability
FROM source_a a
FULL OUTER JOIN source_b b ON a.sensor_id = b.sensor_id
WHERE GREATEST(COALESCE(a.prob, 0), COALESCE(b.prob, 0)) > 0.7;
```

---

## 5. 实际应用案例

### 5.1 传感器数据不确定性处理

#### **5.1.1 业务场景**

**场景描述**：
IoT传感器网络收集环境数据（温度、湿度、压力等），由于传感器精度限制、环境干扰、网络延迟等因素，测量数据存在不确定性。

**业务需求**：

1. 存储不确定的传感器数据
2. 评估数据可信度
3. 基于概率进行数据分析和决策
4. 追踪数据来源和处理过程

**挑战**：

- 传感器测量误差
- 环境噪声干扰
- 数据丢失或延迟
- 多传感器数据不一致

#### **5.1.2 数据模型设计**

**表结构设计**：

```sql
-- 创建传感器数据表（带概率，带错误处理）
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM pg_extension WHERE extname = 'provsql') THEN
            RAISE WARNING 'ProvSQL扩展未安装，请先执行: CREATE EXTENSION provsql;';
            RETURN;
        END IF;

        IF EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'iot_sensor_data') THEN
            RAISE NOTICE '表 iot_sensor_data 已存在';
        ELSE
            CREATE TABLE iot_sensor_data (
                id SERIAL PRIMARY KEY,
                sensor_id INT NOT NULL,
                sensor_type VARCHAR(50),  -- 传感器类型：temperature, humidity, pressure
                value NUMERIC NOT NULL,
                probability NUMERIC CHECK (probability >= 0 AND probability <= 1),  -- 数据可信度
                unit VARCHAR(10),  -- 单位：Celsius, Percent, Pascal
                timestamp TIMESTAMP NOT NULL DEFAULT NOW(),
                location VARCHAR(100),  -- 传感器位置
                quality_score NUMERIC  -- 数据质量评分
            ) WITH PROVENANCE;

            -- 创建索引
            CREATE INDEX idx_sensor_id ON iot_sensor_data(sensor_id);
            CREATE INDEX idx_timestamp ON iot_sensor_data(timestamp);
            CREATE INDEX idx_probability ON iot_sensor_data(probability);
            CREATE INDEX idx_sensor_prob ON iot_sensor_data(sensor_id, probability);

            RAISE NOTICE '表 iot_sensor_data 已创建并启用溯源';
        END IF;
    EXCEPTION
        WHEN duplicate_table THEN
            RAISE WARNING '表 iot_sensor_data 已存在';
        WHEN undefined_object THEN
            RAISE WARNING 'ProvSQL扩展未安装或WITH PROVENANCE语法不支持';
        WHEN OTHERS THEN
            RAISE WARNING '创建表失败: %', SQLERRM;
            RAISE;
    END;
END $$;
```

#### **5.1.3 数据插入和概率计算**

**插入不确定数据**：

```sql
-- 插入不确定数据（带错误处理）
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'iot_sensor_data') THEN
            RAISE WARNING '表 iot_sensor_data 不存在，无法插入数据';
            RETURN;
        END IF;

        -- 插入温度传感器数据（不同可信度）
        INSERT INTO iot_sensor_data (sensor_id, sensor_type, value, probability, unit, location)
        VALUES
            (1, 'temperature', 25.3, 0.95, 'Celsius', 'Building A Floor 1'),
            (1, 'temperature', 25.5, 0.90, 'Celsius', 'Building A Floor 1'),
            (1, 'temperature', 25.1, 0.85, 'Celsius', 'Building A Floor 1'),
            (2, 'humidity', 60.0, 0.92, 'Percent', 'Building A Floor 2'),
            (2, 'humidity', 61.0, 0.88, 'Percent', 'Building A Floor 2'),
            (3, 'pressure', 1013.25, 0.98, 'Pascal', 'Building B Floor 1');

        RAISE NOTICE '已插入6条不确定数据到iot_sensor_data表';
    EXCEPTION
        WHEN undefined_table THEN
            RAISE WARNING '表 iot_sensor_data 不存在';
        WHEN check_violation THEN
            RAISE WARNING '概率值超出范围[0, 1]';
        WHEN OTHERS THEN
            RAISE WARNING '插入数据失败: %', SQLERRM;
            RAISE;
    END;
END $$;
```

**概率计算函数**：

```sql
-- 计算传感器数据的加权平均值
CREATE OR REPLACE FUNCTION calculate_weighted_avg(
    sensor_id_param INT,
    time_window INTERVAL DEFAULT INTERVAL '1 hour'
) RETURNS TABLE (
    sensor_id INT,
    weighted_avg NUMERIC,
    avg_confidence NUMERIC,
    min_confidence NUMERIC,
    max_confidence NUMERIC,
    reading_count BIGINT
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        iot_sensor_data.sensor_id,
        SUM(iot_sensor_data.value * iot_sensor_data.probability) /
        NULLIF(SUM(iot_sensor_data.probability), 0) AS weighted_avg,
        AVG(iot_sensor_data.probability) AS avg_confidence,
        MIN(iot_sensor_data.probability) AS min_confidence,
        MAX(iot_sensor_data.probability) AS max_confidence,
        COUNT(*) AS reading_count
    FROM iot_sensor_data
    WHERE iot_sensor_data.sensor_id = sensor_id_param
      AND iot_sensor_data.timestamp > NOW() - time_window
    GROUP BY iot_sensor_data.sensor_id;
END;
$$ LANGUAGE plpgsql;
```

#### **5.1.4 查询和分析**

**概率查询：计算加权平均**：

```sql
-- 概率查询：计算加权平均（带错误处理和性能测试）
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'iot_sensor_data') THEN
            RAISE WARNING '表 iot_sensor_data 不存在，无法执行查询';
            RETURN;
        END IF;
    EXCEPTION
        WHEN OTHERS THEN
            RAISE WARNING '检查失败: %', SQLERRM;
            RAISE;
    END;
END $$;

EXPLAIN ANALYZE
SELECT
    sensor_id,
    sensor_type,
    SUM(value * probability) / SUM(probability) AS weighted_avg,
    AVG(probability) AS avg_confidence,
    MIN(probability) AS min_confidence,
    COUNT(*) AS reading_count
FROM iot_sensor_data
WHERE timestamp > NOW() - INTERVAL '1 hour'
GROUP BY sensor_id, sensor_type;
```

**高可信度数据查询**：

```sql
-- 查询高可信度数据（概率>0.9）
SELECT
    sensor_id,
    sensor_type,
    value,
    probability,
    timestamp,
    location
FROM iot_sensor_data
WHERE probability > 0.9
ORDER BY timestamp DESC, probability DESC;
```

**数据质量评估**：

```sql
-- 评估传感器数据质量
SELECT
    sensor_id,
    sensor_type,
    COUNT(*) AS total_readings,
    COUNT(*) FILTER (WHERE probability > 0.9) AS high_quality_count,
    COUNT(*) FILTER (WHERE probability BETWEEN 0.7 AND 0.9) AS medium_quality_count,
    COUNT(*) FILTER (WHERE probability < 0.7) AS low_quality_count,
    AVG(probability) AS avg_quality_score,
    STDDEV(probability) AS quality_variance
FROM iot_sensor_data
WHERE timestamp > NOW() - INTERVAL '24 hours'
GROUP BY sensor_id, sensor_type
ORDER BY avg_quality_score DESC;
```

#### **5.1.5 使用示例**

**Python客户端使用**：

```python
from probabilistic_client import ProbabilisticDBClient, ProbabilityValue

# 初始化客户端
client = ProbabilisticDBClient(
    "host=localhost dbname=iot_db user=postgres password=secret"
)

# 插入传感器数据
client.insert_probabilistic_data('iot_sensor_data', {
    'sensor_id': 1,
    'sensor_type': 'temperature',
    'value': 25.5,
    'probability': 0.95,
    'unit': 'Celsius',
    'location': 'Building A Floor 1'
})

# 查询高可信度数据
results = client.query_with_confidence("""
    SELECT * FROM iot_sensor_data
    WHERE sensor_type = 'temperature'
    AND timestamp > NOW() - INTERVAL '1 hour'
""", min_confidence=0.9)

# 概率聚合
stats = client.aggregate_probabilistic(
    'iot_sensor_data',
    'value',
    filters={'sensor_type': 'temperature'}
)
```

### 5.2 数据融合场景

#### **5.2.1 业务场景**

**场景描述**：
企业需要整合来自多个数据源的信息（如客户信息、订单数据、库存数据），不同数据源的数据可能存在不一致性，需要基于数据源的可信度进行融合。

**业务需求**：

1. 整合多源数据
2. 处理数据不一致性
3. 基于数据源可信度进行数据融合
4. 追踪数据来源

**挑战**：

- 多源数据不一致
- 数据源可信度不同
- 数据冲突解决
- 数据质量评估

#### **5.2.2 数据模型设计**

**多源数据表结构**：

```sql
-- 数据源A
CREATE TABLE IF NOT EXISTS source_table_a (
    id SERIAL PRIMARY KEY,
    customer_id INT,
    customer_name VARCHAR(100),
    email VARCHAR(100),
    phone VARCHAR(20),
    source_confidence NUMERIC DEFAULT 0.8,  -- 数据源A的可信度
    last_updated TIMESTAMP DEFAULT NOW()
) WITH PROVENANCE;

-- 数据源B
CREATE TABLE IF NOT EXISTS source_table_b (
    id SERIAL PRIMARY KEY,
    customer_id INT,
    customer_name VARCHAR(100),
    email VARCHAR(100),
    phone VARCHAR(20),
    source_confidence NUMERIC DEFAULT 0.7,  -- 数据源B的可信度
    last_updated TIMESTAMP DEFAULT NOW()
) WITH PROVENANCE;

-- 融合后的数据表
CREATE TABLE IF NOT EXISTS merged_customer_data (
    id SERIAL PRIMARY KEY,
    customer_id INT UNIQUE,
    customer_name VARCHAR(100),
    email VARCHAR(100),
    phone VARCHAR(20),
    fused_confidence NUMERIC,  -- 融合后的可信度
    source_a_confidence NUMERIC,
    source_b_confidence NUMERIC,
    fusion_method VARCHAR(50),  -- 融合方法：max, weighted_avg, etc.
    last_updated TIMESTAMP DEFAULT NOW()
) WITH PROVENANCE;
```

#### **5.2.3 数据融合实现**

**基础融合方案**：

```sql
-- 多源数据融合
WITH source_a AS (
    SELECT
        customer_id,
        customer_name,
        email,
        phone,
        source_confidence AS probability
    FROM source_table_a
),
source_b AS (
    SELECT
        customer_id,
        customer_name,
        email,
        phone,
        source_confidence AS probability
    FROM source_table_b
),
merged AS (
    SELECT
        COALESCE(a.customer_id, b.customer_id) AS customer_id,
        COALESCE(a.customer_name, b.customer_name) AS customer_name,
        COALESCE(a.email, b.email) AS email,
        COALESCE(a.phone, b.phone) AS phone,
        GREATEST(COALESCE(a.probability, 0), COALESCE(b.probability, 0)) AS probability,
        a.probability AS source_a_prob,
        b.probability AS source_b_prob
    FROM source_a a
    FULL OUTER JOIN source_b b ON a.customer_id = b.customer_id
)
SELECT * FROM merged
WHERE probability > 0.75;
```

**加权融合方案**：

```sql
-- 加权融合（考虑数据源可信度）
CREATE OR REPLACE FUNCTION fuse_customer_data()
RETURNS TABLE (
    customer_id INT,
    customer_name VARCHAR,
    email VARCHAR,
    phone VARCHAR,
    fused_confidence NUMERIC
) AS $$
BEGIN
    RETURN QUERY
    WITH source_a AS (
        SELECT customer_id, customer_name, email, phone, source_confidence
        FROM source_table_a
    ),
    source_b AS (
        SELECT customer_id, customer_name, email, phone, source_confidence
        FROM source_table_b
    ),
    fused AS (
        SELECT
            COALESCE(a.customer_id, b.customer_id) AS customer_id,
            COALESCE(a.customer_name, b.customer_name) AS customer_name,
            COALESCE(a.email, b.email) AS email,
            COALESCE(a.phone, b.phone) AS phone,
            -- 加权平均可信度
            CASE
                WHEN a.source_confidence IS NOT NULL AND b.source_confidence IS NOT NULL THEN
                    (a.source_confidence + b.source_confidence) / 2
                WHEN a.source_confidence IS NOT NULL THEN
                    a.source_confidence
                ELSE
                    b.source_confidence
            END AS fused_confidence
        FROM source_a a
        FULL OUTER JOIN source_b b ON a.customer_id = b.customer_id
    )
    SELECT * FROM fused
    WHERE fused_confidence > 0.7
    ORDER BY fused_confidence DESC;
END;
$$ LANGUAGE plpgsql;

-- 使用融合函数
SELECT * FROM fuse_customer_data();
```

**冲突解决策略**：

```sql
-- 冲突解决：选择高可信度数据源
CREATE OR REPLACE FUNCTION resolve_conflicts()
RETURNS VOID AS $$
BEGIN
    INSERT INTO merged_customer_data (
        customer_id, customer_name, email, phone,
        fused_confidence, source_a_confidence, source_b_confidence, fusion_method
    )
    SELECT
        COALESCE(a.customer_id, b.customer_id),
        CASE
            WHEN a.source_confidence > b.source_confidence THEN a.customer_name
            WHEN b.source_confidence > a.source_confidence THEN b.customer_name
            ELSE COALESCE(a.customer_name, b.customer_name)
        END,
        CASE
            WHEN a.source_confidence > b.source_confidence THEN a.email
            WHEN b.source_confidence > a.source_confidence THEN b.email
            ELSE COALESCE(a.email, b.email)
        END,
        CASE
            WHEN a.source_confidence > b.source_confidence THEN a.phone
            WHEN b.source_confidence > a.source_confidence THEN b.phone
            ELSE COALESCE(a.phone, b.phone)
        END,
        GREATEST(COALESCE(a.source_confidence, 0), COALESCE(b.source_confidence, 0)),
        a.source_confidence,
        b.source_confidence,
        'max_confidence'
    FROM source_table_a a
    FULL OUTER JOIN source_table_b b ON a.customer_id = b.customer_id
    ON CONFLICT (customer_id) DO UPDATE SET
        customer_name = EXCLUDED.customer_name,
        email = EXCLUDED.email,
        phone = EXCLUDED.phone,
        fused_confidence = EXCLUDED.fused_confidence,
        last_updated = NOW();
END;
$$ LANGUAGE plpgsql;
```

#### **5.2.4 数据质量评估**

**融合数据质量评估**：

```sql
-- 评估融合数据质量
SELECT
    COUNT(*) AS total_customers,
    COUNT(*) FILTER (WHERE fused_confidence > 0.9) AS high_quality_count,
    COUNT(*) FILTER (WHERE fused_confidence BETWEEN 0.7 AND 0.9) AS medium_quality_count,
    COUNT(*) FILTER (WHERE fused_confidence < 0.7) AS low_quality_count,
    AVG(fused_confidence) AS avg_confidence,
    AVG(source_a_confidence) AS avg_source_a_confidence,
    AVG(source_b_confidence) AS avg_source_b_confidence
FROM merged_customer_data;
```

### 5.3 其他应用场景

#### **5.3.1 数据质量评估**

**业务场景**：
企业需要评估数据质量，识别和处理低质量数据，确保数据驱动的决策基于高质量数据。

**业务需求**：

1. 评估数据源的数据质量
2. 识别低质量数据
3. 生成数据质量报告
4. 支持数据质量改进决策

**实现方案**：

```sql
-- 创建数据质量评估表
CREATE TABLE IF NOT EXISTS data_quality_report (
    data_source VARCHAR(100),
    assessment_date TIMESTAMP DEFAULT NOW(),
    total_records BIGINT,
    avg_quality_score NUMERIC,
    high_quality_count BIGINT,
    medium_quality_count BIGINT,
    low_quality_count BIGINT,
    quality_trend VARCHAR(20)  -- 'improving', 'stable', 'degrading'
) WITH PROVENANCE;

-- 数据质量评估查询
CREATE OR REPLACE FUNCTION assess_data_quality(
    source_name VARCHAR DEFAULT NULL
) RETURNS TABLE (
    data_source VARCHAR,
    total_records BIGINT,
    avg_quality NUMERIC,
    high_quality_count BIGINT,
    medium_quality_count BIGINT,
    low_quality_count BIGINT,
    quality_score NUMERIC
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        uncertain_data.data_source,
        COUNT(*) AS total_records,
        AVG(uncertain_data.probability) AS avg_quality,
        COUNT(*) FILTER (WHERE uncertain_data.probability >= 0.9) AS high_quality_count,
        COUNT(*) FILTER (WHERE uncertain_data.probability >= 0.7 AND uncertain_data.probability < 0.9) AS medium_quality_count,
        COUNT(*) FILTER (WHERE uncertain_data.probability < 0.7) AS low_quality_count,
        -- 质量评分：高质量数据权重更高
        (COUNT(*) FILTER (WHERE uncertain_data.probability >= 0.9) * 1.0 +
         COUNT(*) FILTER (WHERE uncertain_data.probability >= 0.7 AND uncertain_data.probability < 0.9) * 0.5 +
         COUNT(*) FILTER (WHERE uncertain_data.probability < 0.7) * 0.1) / COUNT(*) AS quality_score
    FROM uncertain_data
    WHERE (source_name IS NULL OR uncertain_data.data_source = source_name)
    GROUP BY uncertain_data.data_source
    ORDER BY quality_score DESC;
END;
$$ LANGUAGE plpgsql;

-- 使用数据质量评估函数
SELECT * FROM assess_data_quality();

-- 生成数据质量报告
INSERT INTO data_quality_report (
    data_source, total_records, avg_quality_score,
    high_quality_count, medium_quality_count, low_quality_count
)
SELECT
    data_source,
    total_records,
    avg_quality,
    high_quality_count,
    medium_quality_count,
    low_quality_count
FROM assess_data_quality();
```

**使用示例**：

```python
# Python客户端使用
from probabilistic_client import ProbabilisticDBClient

client = ProbabilisticDBClient("host=localhost dbname=testdb user=postgres password=secret")

# 评估数据质量
quality_report = client.query_with_confidence("""
    SELECT * FROM assess_data_quality('sensor_data')
""", min_confidence=0.0)

for row in quality_report:
    print(f"数据源: {row['data_source']}")
    print(f"平均质量: {row['avg_quality']:.2%}")
    print(f"高质量数据: {row['high_quality_count']}条")
    print(f"低质量数据: {row['low_quality_count']}条")
```

#### **5.3.2 异常检测**

**业务场景**：
系统需要识别异常数据，这些数据可能表示设备故障、数据错误或安全威胁。

**业务需求**：

1. 识别低概率数据（可能是异常）
2. 追踪异常数据的来源
3. 生成异常报告
4. 支持异常处理决策

**实现方案**：

```sql
-- 创建异常检测表
CREATE TABLE IF NOT EXISTS anomaly_detection (
    id SERIAL PRIMARY KEY,
    data_id INT,
    anomaly_type VARCHAR(50),  -- 'low_probability', 'outlier', 'inconsistency'
    probability NUMERIC,
    severity VARCHAR(20),  -- 'low', 'medium', 'high', 'critical'
    detected_at TIMESTAMP DEFAULT NOW(),
    provenance JSONB
) WITH PROVENANCE;

-- 异常检测函数
CREATE OR REPLACE FUNCTION detect_anomalies(
    probability_threshold NUMERIC DEFAULT 0.3,
    limit_count INT DEFAULT 100
) RETURNS TABLE (
    id INT,
    value NUMERIC,
    probability NUMERIC,
    timestamp TIMESTAMP,
    provenance JSONB,
    anomaly_score NUMERIC
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        uncertain_data.id,
        uncertain_data.value,
        uncertain_data.probability,
        uncertain_data.timestamp,
        provsql_provenance_of(
            SELECT * FROM uncertain_data WHERE uncertain_data.id = uncertain_data.id
        )::JSONB AS provenance,
        -- 异常评分：概率越低，评分越高
        (1 - uncertain_data.probability) * 100 AS anomaly_score
    FROM uncertain_data
    WHERE uncertain_data.probability < probability_threshold
    ORDER BY uncertain_data.probability ASC
    LIMIT limit_count;
END;
$$ LANGUAGE plpgsql;

-- 使用异常检测函数
SELECT * FROM detect_anomalies(0.3, 100);

-- 自动异常检测和记录
CREATE OR REPLACE FUNCTION auto_detect_and_record_anomalies()
RETURNS VOID AS $$
DECLARE
    anomaly_record RECORD;
BEGIN
    FOR anomaly_record IN
        SELECT * FROM detect_anomalies(0.3, 1000)
    LOOP
        INSERT INTO anomaly_detection (
            data_id, anomaly_type, probability, severity, provenance
        )
        VALUES (
            anomaly_record.id,
            'low_probability',
            anomaly_record.probability,
            CASE
                WHEN anomaly_record.probability < 0.1 THEN 'critical'
                WHEN anomaly_record.probability < 0.2 THEN 'high'
                WHEN anomaly_record.probability < 0.3 THEN 'medium'
                ELSE 'low'
            END,
            anomaly_record.provenance
        )
        ON CONFLICT DO NOTHING;
    END LOOP;
END;
$$ LANGUAGE plpgsql;
```

**使用示例**：

```python
# Python客户端使用
client = ProbabilisticDBClient("host=localhost dbname=testdb user=postgres password=secret")

# 检测异常
anomalies = client.query_with_confidence("""
    SELECT * FROM detect_anomalies(0.3, 100)
""", min_confidence=0.0)

print(f"检测到 {len(anomalies)} 个异常数据")
for anomaly in anomalies:
    print(f"ID: {anomaly['id']}, 概率: {anomaly['probability']:.2%}, 异常评分: {anomaly['anomaly_score']:.2f}")
```

#### **5.3.3 数据溯源审计**

**业务场景**：
企业需要追踪数据的来源和处理过程，满足合规要求和数据治理需求。

**业务需求**：

1. 追踪数据来源
2. 记录数据处理过程
3. 生成审计报告
4. 支持合规检查

**实现方案**：

```sql
-- 创建审计日志表
CREATE TABLE IF NOT EXISTS audit_log (
    id SERIAL PRIMARY KEY,
    data_id INT,
    operation_type VARCHAR(50),  -- 'insert', 'update', 'delete', 'query'
    operation_time TIMESTAMP DEFAULT NOW(),
    user_name VARCHAR(100),
    provenance JSONB,
    probability_before NUMERIC,
    probability_after NUMERIC
) WITH PROVENANCE;

-- 数据溯源查询函数
CREATE OR REPLACE FUNCTION get_data_provenance(
    target_id INT
) RETURNS TABLE (
    id INT,
    value NUMERIC,
    probability NUMERIC,
    provenance JSONB,
    source_info JSONB
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        uncertain_data.id,
        uncertain_data.value,
        uncertain_data.probability,
        provsql_provenance_of(
            SELECT * FROM uncertain_data WHERE uncertain_data.id = target_id
        )::JSONB AS provenance,
        -- 提取来源信息
        jsonb_build_object(
            'data_source', uncertain_data.data_source,
            'created_at', uncertain_data.created_at,
            'updated_at', uncertain_data.updated_at
        ) AS source_info
    FROM uncertain_data
    WHERE uncertain_data.id = target_id;
END;
$$ LANGUAGE plpgsql;

-- 使用数据溯源查询
SELECT * FROM get_data_provenance(123);

-- 生成审计报告
CREATE OR REPLACE FUNCTION generate_audit_report(
    start_date TIMESTAMP DEFAULT NOW() - INTERVAL '30 days',
    end_date TIMESTAMP DEFAULT NOW()
) RETURNS TABLE (
    data_source VARCHAR,
    operation_count BIGINT,
    avg_probability NUMERIC,
    provenance_depth INT
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        uncertain_data.data_source,
        COUNT(*) AS operation_count,
        AVG(uncertain_data.probability) AS avg_probability,
        -- 计算溯源深度（简化示例）
        AVG(jsonb_array_length(
            provsql_provenance_of(
                SELECT * FROM uncertain_data WHERE uncertain_data.id = uncertain_data.id
            )::JSONB -> 'path'
        ))::INT AS provenance_depth
    FROM uncertain_data
    WHERE uncertain_data.created_at BETWEEN start_date AND end_date
    GROUP BY uncertain_data.data_source
    ORDER BY operation_count DESC;
END;
$$ LANGUAGE plpgsql;

-- 使用审计报告函数
SELECT * FROM generate_audit_report();
```

**使用示例**：

```python
# Python客户端使用
client = ProbabilisticDBClient("host=localhost dbname=testdb user=postgres password=secret")

# 查询数据溯源
provenance = client.query_with_confidence("""
    SELECT * FROM get_data_provenance(123)
""", min_confidence=0.0)

if provenance:
    print(f"数据ID: {provenance[0]['id']}")
    print(f"概率值: {provenance[0]['probability']}")
    print(f"溯源信息: {provenance[0]['provenance']}")

# 生成审计报告
audit_report = client.query_with_confidence("""
    SELECT * FROM generate_audit_report()
""", min_confidence=0.0)

for row in audit_report:
    print(f"数据源: {row['data_source']}")
    print(f"操作次数: {row['operation_count']}")
    print(f"平均概率: {row['avg_probability']:.2%}")
```

---

## 6. 性能分析

### 6.1 查询性能影响

#### **6.1.1 性能影响因素**

1. **概率计算开销**
   - 概率查询需要额外的计算
   - 可能世界枚举的复杂度：O(2^n)，n为不确定性元组数
   - 概率值计算和归一化开销
   - 溯源信息处理开销

2. **存储开销**
   - 概率值的存储：每个元组增加8-16字节
   - 溯源信息的存储：每个元组增加约100-500字节
   - 索引开销：概率值索引的存储

3. **查询优化**
   - 概率查询优化器的复杂度
   - 索引对概率查询的支持
   - 查询计划选择的复杂性

#### **6.1.2 性能测试数据**

**基础查询性能对比**：

| 操作类型 | 传统查询 | 概率查询 | 性能影响 | 数据量 |
|---------|---------|---------|---------|--------|
| **SELECT** | 10ms | 15ms | +50% | 10K行 |
| **SELECT** | 100ms | 180ms | +80% | 100K行 |
| **JOIN** | 50ms | 80ms | +60% | 10K行 |
| **JOIN** | 500ms | 900ms | +80% | 100K行 |
| **AGGREGATE** | 30ms | 45ms | +50% | 10K行 |
| **AGGREGATE** | 300ms | 550ms | +83% | 100K行 |

**概率阈值对性能的影响**：

| 概率阈值 | 查询时间 | 返回行数 | 性能比 |
|---------|---------|---------|--------|
| 0.5 | 180ms | 50K | 1.0x |
| 0.7 | 120ms | 30K | 0.67x |
| 0.9 | 80ms | 10K | 0.44x |

**索引对性能的影响**：

| 索引类型 | 无索引 | 有索引 | 性能提升 |
|---------|--------|--------|---------|
| **概率值索引** | 180ms | 50ms | 3.6x |
| **复合索引** | 180ms | 30ms | 6.0x |

#### **6.1.3 性能优化策略**

1. **索引优化**

   ```sql
   -- 为概率值创建B-tree索引
   CREATE INDEX idx_probability ON sensor_data(probability);

   -- 为概率查询创建复合索引
   CREATE INDEX idx_sensor_prob ON sensor_data(sensor_id, probability);

   -- 为概率范围查询创建GIN索引
   CREATE INDEX idx_prob_range ON sensor_data USING GIN(probability_range);
   ```

2. **查询优化**

   ```sql
   -- 使用概率阈值过滤，减少计算量
   SELECT * FROM sensor_data
   WHERE probability > 0.8  -- 提前过滤
   ORDER BY probability DESC
   LIMIT 100;

   -- 使用物化视图预计算概率
   CREATE MATERIALIZED VIEW mv_prob_stats AS
   SELECT sensor_id, AVG(probability) AS avg_prob
   FROM sensor_data
   GROUP BY sensor_id;
   ```

3. **批量处理**

   ```sql
   -- 批量插入概率数据
   INSERT INTO sensor_data (sensor_id, value, probability)
   SELECT * FROM unnest(
       ARRAY[1, 2, 3],
       ARRAY[25.3, 26.1, 24.8],
       ARRAY[0.9, 0.85, 0.95]
   );
   ```

### 6.2 存储开销分析

#### **6.2.1 存储开销详细分析**

**基础存储开销**：

| 数据类型 | 基础大小 | 概率值开销 | 溯源信息开销 | 总开销 |
|---------|---------|-----------|-------------|--------|
| **INTEGER** | 4字节 | +8字节 | +100-500字节 | +108-512字节 |
| **NUMERIC** | 变长 | +8字节 | +100-500字节 | +108-508字节 |
| **TEXT** | 变长 | +8字节 | +100-500字节 | +108-508字节 |
| **JSONB** | 变长 | +8字节 | +100-500字节 | +108-508字节 |

**概率值存储**：

- **NUMERIC类型**：8字节（固定精度）
- **DOUBLE PRECISION**：8字节（浮点）
- **REAL**：4字节（单精度浮点）
- **自定义类型**：8-16字节（包含元数据）

**溯源信息存储**：

- **基础溯源**：约100字节（来源ID、时间戳）
- **详细溯源**：约300-500字节（完整溯源链）
- **压缩溯源**：约50-100字节（压缩存储）

#### **6.2.2 存储优化策略**

1. **数据类型选择**

   ```sql
   -- 使用REAL代替NUMERIC节省空间（如果精度要求不高）
   CREATE TABLE sensor_data (
       id SERIAL PRIMARY KEY,
       value REAL,
       probability REAL  -- 4字节而不是8字节
   );
   ```

2. **分区策略**

   ```sql
   -- 按概率值范围分区
   CREATE TABLE sensor_data (
       id SERIAL,
       value NUMERIC,
       probability NUMERIC
   ) PARTITION BY RANGE (probability);

   CREATE TABLE sensor_data_high PARTITION OF sensor_data
       FOR VALUES FROM (0.8) TO (1.0);
   CREATE TABLE sensor_data_medium PARTITION OF sensor_data
       FOR VALUES FROM (0.5) TO (0.8);
   CREATE TABLE sensor_data_low PARTITION OF sensor_data
       FOR VALUES FROM (0.0) TO (0.5);
   ```

3. **压缩技术**

   ```sql
   -- 使用TOAST压缩大文本字段
   ALTER TABLE sensor_data
       ALTER COLUMN provenance_data SET STORAGE EXTENDED;

   -- 归档低概率数据
   CREATE TABLE sensor_data_archive (
       LIKE sensor_data INCLUDING ALL
   );
   ```

### 6.3 性能基准测试

#### **6.3.1 基准测试脚本**

```sql
-- 性能基准测试脚本
DO $$
DECLARE
    start_time TIMESTAMP;
    end_time TIMESTAMP;
    duration INTERVAL;
    test_size INTEGER := 100000;
BEGIN
    -- 创建测试表
    CREATE TABLE IF NOT EXISTS perf_test (
        id SERIAL PRIMARY KEY,
        value NUMERIC,
        probability NUMERIC CHECK (probability >= 0 AND probability <= 1)
    );

    -- 插入测试数据
    INSERT INTO perf_test (value, probability)
    SELECT
        random() * 100,
        random()
    FROM generate_series(1, test_size);

    -- 测试1：基础SELECT查询
    start_time := clock_timestamp();
    PERFORM * FROM perf_test WHERE probability > 0.5;
    end_time := clock_timestamp();
    duration := end_time - start_time;
    RAISE NOTICE '基础SELECT查询耗时: %', duration;

    -- 测试2：概率聚合查询
    start_time := clock_timestamp();
    PERFORM
        AVG(value * probability) / AVG(probability) AS weighted_avg
    FROM perf_test;
    end_time := clock_timestamp();
    duration := end_time - start_time;
    RAISE NOTICE '概率聚合查询耗时: %', duration;

    -- 测试3：概率JOIN查询
    CREATE TABLE IF NOT EXISTS perf_test_b (
        LIKE perf_test INCLUDING ALL
    );
    INSERT INTO perf_test_b (value, probability)
    SELECT
        random() * 100,
        random()
    FROM generate_series(1, test_size);

    start_time := clock_timestamp();
    PERFORM *
    FROM perf_test a
    JOIN perf_test_b b ON a.id = b.id
    WHERE a.probability > 0.7 AND b.probability > 0.7;
    end_time := clock_timestamp();
    duration := end_time - start_time;
    RAISE NOTICE '概率JOIN查询耗时: %', duration;

    -- 清理
    DROP TABLE IF EXISTS perf_test;
    DROP TABLE IF EXISTS perf_test_b;
END $$;
```

#### **6.3.2 性能监控指标**

```sql
-- 创建性能监控视图
CREATE OR REPLACE VIEW probabilistic_performance_stats AS
SELECT
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS total_size,
    pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) AS table_size,
    pg_size_pretty(pg_indexes_size(schemaname||'.'||tablename)) AS indexes_size,
    (SELECT COUNT(*) FROM pg_stat_user_tables WHERE relname = tablename) AS row_count
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- 查询性能统计
SELECT * FROM probabilistic_performance_stats;
```

### 6.4 性能优化建议

#### **6.4.1 查询优化建议**

1. **使用概率阈值提前过滤**
   - 在WHERE子句中使用概率阈值
   - 避免处理低概率数据
   - 使用LIMIT限制结果集

2. **优化JOIN操作**
   - 先过滤再JOIN
   - 使用索引优化JOIN
   - 考虑使用物化视图

3. **批量处理**
   - 批量插入概率数据
   - 批量更新概率值
   - 使用事务减少开销

#### **6.4.2 存储优化建议**

1. **合理选择数据类型**
   - 根据精度要求选择NUMERIC或REAL
   - 使用压缩存储大文本字段
   - 考虑使用JSONB存储复杂溯源信息

2. **分区策略**
   - 按概率值范围分区
   - 按时间分区
   - 按数据源分区

3. **归档策略**
   - 归档低概率历史数据
   - 定期清理过期数据
   - 使用外部存储存储归档数据

---

## 7. 最佳实践

### 7.1 使用场景

#### **7.1.1 适用场景**

1. **传感器数据不确定性处理**
   - IoT设备数据：温度、湿度、压力等传感器数据
   - 测量误差：处理传感器测量误差和噪声
   - 数据质量：评估数据质量和可信度
   - 示例：环境监测、智能城市、工业4.0

2. **多源数据融合**
   - 数据不一致性：处理来自不同数据源的不一致数据
   - 数据可信度：根据数据源可信度进行加权融合
   - 冲突解决：解决多源数据的冲突
   - 示例：数据仓库、数据湖、数据集成

3. **数据溯源和可信度评估**
   - 数据来源追踪：追踪数据的来源和处理过程
   - 可信度计算：根据数据来源计算可信度
   - 合规审计：满足数据治理和合规要求
   - 示例：数据治理、合规审计、数据质量评估

4. **缺失数据处理**
   - 不完整数据：处理缺失或不完整的数据
   - 概率推断：使用概率模型推断缺失值
   - 不确定性量化：量化缺失数据的不确定性
   - 示例：数据清洗、数据补全、统计分析

5. **机器学习数据预处理**
   - 不确定性特征：处理具有不确定性的特征
   - 噪声数据：处理噪声和异常值
   - 数据质量：评估训练数据的质量
   - 示例：特征工程、数据预处理、模型训练

#### **7.1.2 不适用场景**

1. **确定性数据**
   - 数据完全确定，不需要概率表示
   - 传统关系数据库即可满足需求
   - 示例：财务数据、用户信息、订单数据

2. **高性能要求场景**
   - 对查询性能要求极高（<10ms）
   - 概率计算开销不可接受
   - 示例：实时交易系统、高频查询系统

3. **简单查询场景**
   - 不需要概率计算的简单查询
   - 不需要不确定性处理
   - 示例：简单的CRUD操作、报表查询

4. **资源受限环境**
   - 存储空间有限
   - 计算资源有限
   - 示例：嵌入式系统、边缘计算

### 7.2 设计原则

#### **7.2.1 概率值设计**

1. **概率值范围**
   - 确保概率值在[0, 1]范围内
   - 使用CHECK约束验证概率值
   - 处理边界情况（0和1）

2. **概率值精度**
   - 选择合适的数值类型（NUMERIC vs DOUBLE PRECISION）
   - 考虑精度和性能的平衡
   - 避免精度损失

3. **概率值归一化**
   - 确保概率分布归一化
   - 处理概率值不一致的情况
   - 使用触发器自动归一化

#### **7.2.2 数据模型设计**

**1. 表结构设计**

**概率值存储位置**：

```sql
-- 方案1：概率值作为独立列（推荐）
CREATE TABLE sensor_data (
    id SERIAL PRIMARY KEY,
    sensor_id INT NOT NULL,
    value NUMERIC NOT NULL,
    probability NUMERIC CHECK (probability >= 0 AND probability <= 1),
    timestamp TIMESTAMP DEFAULT NOW()
);

-- 方案2：概率值作为复合类型
CREATE TYPE probability_value AS (
    value NUMERIC,
    confidence NUMERIC
);

CREATE TABLE sensor_data (
    id SERIAL PRIMARY KEY,
    sensor_id INT NOT NULL,
    measurement probability_value NOT NULL,
    timestamp TIMESTAMP DEFAULT NOW()
);

-- 方案3：概率值作为JSONB（灵活但性能较低）
CREATE TABLE sensor_data (
    id SERIAL PRIMARY KEY,
    sensor_id INT NOT NULL,
    data JSONB NOT NULL,  -- {"value": 25.5, "probability": 0.95}
    timestamp TIMESTAMP DEFAULT NOW()
);
```

**设计原则**：

- **查询模式优先**：根据查询模式设计表结构
- **访问频率考虑**：高频查询的字段放在前面
- **避免过度规范化**：适度反规范化以提高查询性能
- **概率值集中存储**：将概率值集中存储，便于管理和查询

**2. 索引设计**

**概率值索引**：

```sql
-- 基础概率值索引
CREATE INDEX idx_probability ON sensor_data(probability);

-- 部分索引（只索引高概率数据）
CREATE INDEX idx_high_probability ON sensor_data(probability)
WHERE probability > 0.7;

-- 复合索引（概率+其他字段）
CREATE INDEX idx_sensor_prob ON sensor_data(sensor_id, probability);

-- 表达式索引（概率计算）
CREATE INDEX idx_prob_expr ON sensor_data((probability * 100));

-- GIN索引（用于范围查询）
CREATE INDEX idx_prob_gin ON sensor_data USING GIN(
    int4range(
        FLOOR(probability * 100)::int,
        CEIL(probability * 100)::int
    )
);
```

**索引设计原则**：

- **概率值索引**：为概率值创建索引，加速概率过滤
- **复合索引**：为常用查询条件创建复合索引
- **部分索引**：为高概率数据创建部分索引，减少索引大小
- **表达式索引**：为概率计算表达式创建索引
- **索引维护**：定期重建索引，保持索引效率

**3. 约束设计**

**概率值约束**：

```sql
-- CHECK约束验证概率值范围
CREATE TABLE sensor_data (
    id SERIAL PRIMARY KEY,
    sensor_id INT NOT NULL,
    value NUMERIC NOT NULL,
    probability NUMERIC CHECK (
        probability >= 0 AND probability <= 1
    ),
    timestamp TIMESTAMP DEFAULT NOW()
);

-- 触发器自动归一化概率值
CREATE OR REPLACE FUNCTION normalize_probability()
RETURNS TRIGGER AS $$
BEGIN
    -- 确保概率值在[0, 1]范围内
    NEW.probability := GREATEST(0, LEAST(1, NEW.probability));
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER normalize_probability_trigger
BEFORE INSERT OR UPDATE ON sensor_data
FOR EACH ROW
EXECUTE FUNCTION normalize_probability();
```

**外键约束**：

```sql
-- 外键约束维护数据完整性
CREATE TABLE sensors (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    location VARCHAR(100)
);

CREATE TABLE sensor_data (
    id SERIAL PRIMARY KEY,
    sensor_id INT NOT NULL REFERENCES sensors(id),
    value NUMERIC NOT NULL,
    probability NUMERIC CHECK (probability >= 0 AND probability <= 1),
    timestamp TIMESTAMP DEFAULT NOW()
);
```

**唯一约束**：

```sql
-- 唯一约束避免重复数据
CREATE TABLE sensor_data (
    id SERIAL PRIMARY KEY,
    sensor_id INT NOT NULL,
    value NUMERIC NOT NULL,
    probability NUMERIC CHECK (probability >= 0 AND probability <= 1),
    timestamp TIMESTAMP DEFAULT NOW(),
    UNIQUE(sensor_id, timestamp)  -- 同一传感器同一时间只能有一条数据
);
```

**约束设计原则**：

- **CHECK约束**：验证概率值范围，确保数据质量
- **外键约束**：维护数据完整性，避免孤立数据
- **唯一约束**：避免重复数据，保证数据唯一性
- **触发器**：自动处理概率值归一化和验证

**4. 分区设计**

**概率值分区**：

```sql
-- 按概率值范围分区
CREATE TABLE sensor_data (
    id SERIAL,
    sensor_id INT NOT NULL,
    value NUMERIC NOT NULL,
    probability NUMERIC CHECK (probability >= 0 AND probability <= 1),
    timestamp TIMESTAMP DEFAULT NOW()
) PARTITION BY RANGE (probability);

-- 创建分区
CREATE TABLE sensor_data_high PARTITION OF sensor_data
FOR VALUES FROM (0.8) TO (1.0);

CREATE TABLE sensor_data_medium PARTITION OF sensor_data
FOR VALUES FROM (0.5) TO (0.8);

CREATE TABLE sensor_data_low PARTITION OF sensor_data
FOR VALUES FROM (0.0) TO (0.5);
```

**时间分区**：

```sql
-- 按时间分区
CREATE TABLE sensor_data (
    id SERIAL,
    sensor_id INT NOT NULL,
    value NUMERIC NOT NULL,
    probability NUMERIC CHECK (probability >= 0 AND probability <= 1),
    timestamp TIMESTAMP DEFAULT NOW()
) PARTITION BY RANGE (timestamp);

-- 创建月度分区
CREATE TABLE sensor_data_2025_01 PARTITION OF sensor_data
FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

CREATE TABLE sensor_data_2025_02 PARTITION OF sensor_data
FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');
```

**5. 物化视图设计**

**概率查询物化视图**：

```sql
-- 创建概率聚合物化视图
CREATE MATERIALIZED VIEW sensor_data_summary AS
SELECT
    sensor_id,
    COUNT(*) AS total_count,
    AVG(value) AS avg_value,
    SUM(value * probability) / SUM(probability) AS weighted_avg,
    AVG(probability) AS avg_probability,
    MIN(probability) AS min_probability,
    MAX(probability) AS max_probability
FROM sensor_data
GROUP BY sensor_id;

-- 创建索引加速查询
CREATE INDEX idx_summary_sensor ON sensor_data_summary(sensor_id);

-- 定期刷新物化视图
REFRESH MATERIALIZED VIEW CONCURRENTLY sensor_data_summary;
```

**物化视图设计原则**：

- **预计算常见查询**：为常见概率查询创建物化视图
- **定期刷新**：定期刷新物化视图，保持数据最新
- **并发刷新**：使用CONCURRENTLY选项，避免锁定
- **索引优化**：为物化视图创建索引，加速查询

### 7.3 性能优化

#### **7.3.1 查询优化**

**1. 索引优化**

```sql
-- 为概率值创建索引
CREATE INDEX idx_probability ON sensor_data(probability);

-- 为概率查询创建复合索引
CREATE INDEX idx_sensor_prob ON sensor_data(sensor_id, probability);

-- 创建部分索引（只索引高概率数据）
CREATE INDEX idx_high_prob ON sensor_data(probability)
WHERE probability > 0.7;

-- 创建表达式索引（概率计算）
CREATE INDEX idx_prob_expr ON sensor_data((probability * 100));

-- 使用索引提示优化查询
SET enable_seqscan = off;
SELECT * FROM sensor_data WHERE probability > 0.8;
SET enable_seqscan = on;
```

**2. 查询重写优化**

```sql
-- 优化前：复杂概率计算
SELECT
    sensor_id,
    SUM(value * probability) / SUM(probability) AS weighted_avg
FROM sensor_data
WHERE probability > 0.5
GROUP BY sensor_id;

-- 优化后：使用物化视图
CREATE MATERIALIZED VIEW sensor_data_weighted_avg AS
SELECT
    sensor_id,
    SUM(value * probability) / SUM(probability) AS weighted_avg,
    AVG(probability) AS avg_probability
FROM sensor_data
WHERE probability > 0.5
GROUP BY sensor_id;

CREATE INDEX idx_weighted_avg_sensor ON sensor_data_weighted_avg(sensor_id);

-- 查询物化视图（更快）
SELECT * FROM sensor_data_weighted_avg WHERE sensor_id = 1;

-- 定期刷新物化视图
REFRESH MATERIALIZED VIEW CONCURRENTLY sensor_data_weighted_avg;
```

**3. JOIN优化**

```sql
-- 优化前：先JOIN再过滤
SELECT a.*, b.*
FROM uncertain_table_a a
JOIN uncertain_table_b b ON a.id = b.id
WHERE a.probability * b.probability > 0.5;

-- 优化后：先过滤再JOIN
SELECT a.*, b.*
FROM (
    SELECT * FROM uncertain_table_a WHERE probability > 0.7
) a
JOIN (
    SELECT * FROM uncertain_table_b WHERE probability > 0.7
) b ON a.id = b.id
WHERE a.probability * b.probability > 0.5;
```

**4. 批量处理优化**

```sql
-- 批量插入概率数据
BEGIN;
INSERT INTO sensor_data (sensor_id, value, probability, timestamp)
SELECT
    generate_series(1, 1000) AS sensor_id,
    random() * 100 AS value,
    random() AS probability,
    NOW() AS timestamp;
COMMIT;

-- 批量更新概率值
UPDATE sensor_data
SET probability = probability * 1.1
WHERE probability < 0.9 AND timestamp > NOW() - INTERVAL '1 day';

-- 使用COPY批量导入
COPY sensor_data (sensor_id, value, probability, timestamp)
FROM '/path/to/data.csv' WITH CSV HEADER;
```

**5. 查询缓存优化**

```sql
-- 创建查询结果缓存表
CREATE TABLE IF NOT EXISTS query_cache (
    query_hash TEXT PRIMARY KEY,
    query_result JSONB,
    created_at TIMESTAMP DEFAULT NOW(),
    expires_at TIMESTAMP
);

-- 查询缓存函数
CREATE OR REPLACE FUNCTION get_cached_result(
    query_hash TEXT,
    query_text TEXT
) RETURNS JSONB AS $$
DECLARE
    cached_result JSONB;
BEGIN
    -- 检查缓存
    SELECT query_result INTO cached_result
    FROM query_cache
    WHERE query_hash = get_cached_result.query_hash
      AND expires_at > NOW();

    IF cached_result IS NOT NULL THEN
        RETURN cached_result;
    END IF;

    -- 执行查询并缓存结果
    EXECUTE format('SELECT to_jsonb(result) FROM (%s) result', query_text) INTO cached_result;

    INSERT INTO query_cache (query_hash, query_result, expires_at)
    VALUES (
        get_cached_result.query_hash,
        cached_result,
        NOW() + INTERVAL '1 hour'
    )
    ON CONFLICT (query_hash) DO UPDATE SET
        query_result = EXCLUDED.query_result,
        expires_at = EXCLUDED.expires_at;

    RETURN cached_result;
END;
$$ LANGUAGE plpgsql;
```

#### **7.3.2 存储优化**

**1. 数据类型选择**

```sql
-- NUMERIC：高精度，适合概率值（推荐）
CREATE TABLE sensor_data (
    id SERIAL PRIMARY KEY,
    value NUMERIC(10, 2),
    probability NUMERIC(5, 4)  -- 精度：5位数字，4位小数
);

-- DOUBLE PRECISION：高性能，适合大规模数据
CREATE TABLE sensor_data_fast (
    id SERIAL PRIMARY KEY,
    value DOUBLE PRECISION,
    probability DOUBLE PRECISION
);

-- REAL：节省空间，适合精度要求不高的场景
CREATE TABLE sensor_data_compact (
    id SERIAL PRIMARY KEY,
    value REAL,
    probability REAL
);
```

**数据类型选择建议**：

- **NUMERIC**：推荐用于概率值，保证精度
- **DOUBLE PRECISION**：用于大规模数据，性能优先
- **REAL**：用于精度要求不高的场景，节省空间

**2. 分区策略**

```sql
-- 按概率值范围分区
CREATE TABLE sensor_data (
    id SERIAL,
    sensor_id INT NOT NULL,
    value NUMERIC NOT NULL,
    probability NUMERIC CHECK (probability >= 0 AND probability <= 1),
    timestamp TIMESTAMP DEFAULT NOW()
) PARTITION BY RANGE (probability);

-- 创建分区
CREATE TABLE sensor_data_high PARTITION OF sensor_data
FOR VALUES FROM (0.8) TO (1.0);

CREATE TABLE sensor_data_medium PARTITION OF sensor_data
FOR VALUES FROM (0.5) TO (0.8);

CREATE TABLE sensor_data_low PARTITION OF sensor_data
FOR VALUES FROM (0.0) TO (0.5);

-- 按时间分区
CREATE TABLE sensor_data_time (
    id SERIAL,
    sensor_id INT NOT NULL,
    value NUMERIC NOT NULL,
    probability NUMERIC CHECK (probability >= 0 AND probability <= 1),
    timestamp TIMESTAMP DEFAULT NOW()
) PARTITION BY RANGE (timestamp);

-- 创建月度分区
CREATE TABLE sensor_data_2025_01 PARTITION OF sensor_data_time
FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');
```

**3. 压缩技术**

```sql
-- 启用表压缩
ALTER TABLE sensor_data SET (
    toast_tuple_target = 128,
    fillfactor = 90
);

-- 归档低概率历史数据
CREATE TABLE sensor_data_archive (
    LIKE sensor_data INCLUDING ALL
) PARTITION BY RANGE (timestamp);

-- 归档函数
CREATE OR REPLACE FUNCTION archive_low_probability_data()
RETURNS VOID AS $$
BEGIN
    INSERT INTO sensor_data_archive
    SELECT * FROM sensor_data
    WHERE probability < 0.3 AND timestamp < NOW() - INTERVAL '1 year';

    DELETE FROM sensor_data
    WHERE probability < 0.3 AND timestamp < NOW() - INTERVAL '1 year';
END;
$$ LANGUAGE plpgsql;

-- 定期清理过期数据
CREATE OR REPLACE FUNCTION cleanup_expired_data()
RETURNS VOID AS $$
BEGIN
    DELETE FROM sensor_data
    WHERE timestamp < NOW() - INTERVAL '2 years';
END;
$$ LANGUAGE plpgsql;
```

**4. 存储空间优化**

```sql
-- 使用TOAST压缩大字段
CREATE TABLE sensor_data (
    id SERIAL PRIMARY KEY,
    sensor_id INT NOT NULL,
    value NUMERIC NOT NULL,
    probability NUMERIC CHECK (probability >= 0 AND probability <= 1),
    provenance_data JSONB,  -- 使用JSONB存储溯源信息，自动压缩
    timestamp TIMESTAMP DEFAULT NOW()
);

-- 使用列存储优化（PostgreSQL 14+）
-- 注意：PostgreSQL原生不支持列存储，可以使用扩展或外部表

-- 定期VACUUM和ANALYZE
VACUUM ANALYZE sensor_data;

-- 重建表回收空间
VACUUM FULL sensor_data;
```

### 7.4 常见问题和解决方案

#### **7.4.1 概率值不一致**

**问题描述**：多个数据源的概率值不一致，导致查询结果不准确。

**问题原因**：

- 不同数据源使用不同的概率计算方法
- 概率值没有归一化
- 概率值超出[0, 1]范围

**解决方案**：

```sql
-- 1. 使用归一化函数
CREATE OR REPLACE FUNCTION normalize_probability(
    prob NUMERIC
) RETURNS NUMERIC AS $$
BEGIN
    -- 确保概率值在[0, 1]范围内
    RETURN GREATEST(0, LEAST(1, prob));
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- 2. 使用触发器自动归一化
CREATE OR REPLACE FUNCTION normalize_probability_trigger()
RETURNS TRIGGER AS $$
BEGIN
    NEW.probability := normalize_probability(NEW.probability);
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER normalize_prob_trigger
BEFORE INSERT OR UPDATE ON sensor_data
FOR EACH ROW
EXECUTE FUNCTION normalize_probability_trigger();

-- 3. 批量归一化现有数据
UPDATE sensor_data
SET probability = normalize_probability(probability)
WHERE probability < 0 OR probability > 1;

-- 4. 数据源概率值标准化
CREATE OR REPLACE FUNCTION standardize_source_probability(
    source_name VARCHAR,
    raw_probability NUMERIC
) RETURNS NUMERIC AS $$
DECLARE
    source_weight NUMERIC;
BEGIN
    -- 根据数据源获取权重
    SELECT weight INTO source_weight
    FROM data_source_weights
    WHERE name = source_name;

    -- 标准化概率值
    RETURN normalize_probability(raw_probability * COALESCE(source_weight, 1.0));
END;
$$ LANGUAGE plpgsql;
```

**预防措施**：

- 使用CHECK约束验证概率值范围
- 在应用层进行概率值验证
- 定期检查数据质量

#### **7.4.2 性能问题**

**问题描述**：概率查询性能慢，响应时间长。

**问题原因**：

- 缺少合适的索引
- 概率计算复杂
- 数据量大
- 查询没有优化

**解决方案**：

**1. 创建合适的索引**

```sql
-- 为概率值创建索引
CREATE INDEX idx_probability ON sensor_data(probability);

-- 为常用查询创建复合索引
CREATE INDEX idx_sensor_prob ON sensor_data(sensor_id, probability);

-- 创建部分索引（只索引高概率数据）
CREATE INDEX idx_high_prob ON sensor_data(probability)
WHERE probability > 0.7;

-- 分析索引使用情况
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes
WHERE tablename = 'sensor_data';
```

**2. 使用物化视图预计算**

```sql
-- 创建物化视图预计算常见查询
CREATE MATERIALIZED VIEW sensor_data_summary AS
SELECT
    sensor_id,
    COUNT(*) AS total_count,
    SUM(value * probability) / SUM(probability) AS weighted_avg,
    AVG(probability) AS avg_probability
FROM sensor_data
GROUP BY sensor_id;

-- 创建索引
CREATE INDEX idx_summary_sensor ON sensor_data_summary(sensor_id);

-- 定期刷新（使用CONCURRENTLY避免锁定）
REFRESH MATERIALIZED VIEW CONCURRENTLY sensor_data_summary;

-- 设置自动刷新（使用pg_cron扩展）
SELECT cron.schedule(
    'refresh-sensor-summary',
    '0 * * * *',  -- 每小时刷新一次
    'REFRESH MATERIALIZED VIEW CONCURRENTLY sensor_data_summary'
);
```

**3. 优化查询语句**

```sql
-- 优化前：全表扫描
SELECT * FROM sensor_data WHERE probability > 0.8;

-- 优化后：使用索引
SELECT * FROM sensor_data
WHERE probability > 0.8
ORDER BY probability DESC
LIMIT 100;

-- 优化前：复杂计算
SELECT
    sensor_id,
    SUM(value * probability) / SUM(probability) AS weighted_avg
FROM sensor_data
GROUP BY sensor_id;

-- 优化后：使用物化视图
SELECT * FROM sensor_data_summary;
```

**4. 使用查询缓存**

```sql
-- 创建查询缓存表
CREATE TABLE IF NOT EXISTS query_cache (
    query_hash TEXT PRIMARY KEY,
    query_result JSONB,
    created_at TIMESTAMP DEFAULT NOW(),
    expires_at TIMESTAMP
);

-- 查询缓存函数
CREATE OR REPLACE FUNCTION get_cached_query_result(
    query_hash TEXT,
    query_text TEXT,
    cache_duration INTERVAL DEFAULT INTERVAL '1 hour'
) RETURNS JSONB AS $$
DECLARE
    cached_result JSONB;
BEGIN
    -- 检查缓存
    SELECT query_result INTO cached_result
    FROM query_cache
    WHERE query_hash = get_cached_query_result.query_hash
      AND expires_at > NOW();

    IF cached_result IS NOT NULL THEN
        RETURN cached_result;
    END IF;

    -- 执行查询并缓存结果
    EXECUTE format('SELECT to_jsonb(result) FROM (%s) result', query_text) INTO cached_result;

    INSERT INTO query_cache (query_hash, query_result, expires_at)
    VALUES (
        get_cached_query_result.query_hash,
        cached_result,
        NOW() + cache_duration
    )
    ON CONFLICT (query_hash) DO UPDATE SET
        query_result = EXCLUDED.query_result,
        expires_at = EXCLUDED.expires_at;

    RETURN cached_result;
END;
$$ LANGUAGE plpgsql;
```

**5. 并行查询优化**

```sql
-- 启用并行查询
SET max_parallel_workers_per_gather = 4;
SET parallel_setup_cost = 1000;
SET parallel_tuple_cost = 0.1;

-- 使用并行查询
EXPLAIN (ANALYZE, BUFFERS)
SELECT
    sensor_id,
    SUM(value * probability) / SUM(probability) AS weighted_avg
FROM sensor_data
WHERE probability > 0.8
GROUP BY sensor_id;
```

#### **7.4.3 存储空间问题**

**问题描述**：概率值和溯源信息占用大量存储空间，导致存储成本高。

**问题原因**：

- 概率值存储精度过高
- 溯源信息存储冗余
- 历史数据没有归档
- 索引占用空间大

**解决方案**：

**1. 优化数据类型**

```sql
-- 优化前：使用NUMERIC(10, 8)（占用空间大）
CREATE TABLE sensor_data (
    probability NUMERIC(10, 8)  -- 占用更多空间
);

-- 优化后：使用NUMERIC(5, 4)（占用空间小）
CREATE TABLE sensor_data (
    probability NUMERIC(5, 4)  -- 精度足够，占用空间小
);

-- 或者使用DOUBLE PRECISION（性能优先）
CREATE TABLE sensor_data (
    probability DOUBLE PRECISION  -- 8字节，性能好
);
```

**2. 压缩溯源信息**

```sql
-- 使用JSONB压缩存储溯源信息
CREATE TABLE sensor_data (
    id SERIAL PRIMARY KEY,
    sensor_id INT NOT NULL,
    value NUMERIC NOT NULL,
    probability NUMERIC CHECK (probability >= 0 AND probability <= 1),
    provenance JSONB,  -- JSONB自动压缩
    timestamp TIMESTAMP DEFAULT NOW()
);

-- 启用TOAST压缩
ALTER TABLE sensor_data SET (
    toast_tuple_target = 128
);
```

**3. 数据归档策略**

```sql
-- 创建归档表
CREATE TABLE sensor_data_archive (
    LIKE sensor_data INCLUDING ALL
) PARTITION BY RANGE (timestamp);

-- 归档函数
CREATE OR REPLACE FUNCTION archive_old_data(
    archive_threshold INTERVAL DEFAULT INTERVAL '1 year',
    probability_threshold NUMERIC DEFAULT 0.3
) RETURNS TABLE (
    archived_count BIGINT,
    freed_space BIGINT
) AS $$
DECLARE
    archived_rows BIGINT;
BEGIN
    -- 归档低概率历史数据
    INSERT INTO sensor_data_archive
    SELECT * FROM sensor_data
    WHERE probability < probability_threshold
      AND timestamp < NOW() - archive_threshold;

    GET DIAGNOSTICS archived_rows = ROW_COUNT;

    -- 删除已归档数据
    DELETE FROM sensor_data
    WHERE probability < probability_threshold
      AND timestamp < NOW() - archive_threshold;

    -- 返回归档统计
    RETURN QUERY SELECT
        archived_rows,
        pg_total_relation_size('sensor_data') AS freed_space;
END;
$$ LANGUAGE plpgsql;

-- 使用归档函数
SELECT * FROM archive_old_data(INTERVAL '1 year', 0.3);
```

**4. 索引优化**

```sql
-- 使用部分索引减少索引大小
CREATE INDEX idx_high_prob ON sensor_data(probability)
WHERE probability > 0.7;  -- 只索引高概率数据

-- 定期重建索引回收空间
REINDEX INDEX idx_probability;

-- 删除未使用的索引
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan
FROM pg_stat_user_indexes
WHERE idx_scan = 0 AND tablename = 'sensor_data';

-- 删除未使用的索引
DROP INDEX IF EXISTS unused_index_name;
```

**5. 表压缩和清理**

```sql
-- VACUUM回收空间
VACUUM ANALYZE sensor_data;

-- VACUUM FULL重建表（需要锁定）
VACUUM FULL sensor_data;

-- 设置自动VACUUM
ALTER TABLE sensor_data SET (
    autovacuum_vacuum_scale_factor = 0.1,
    autovacuum_analyze_scale_factor = 0.05
);
```

**6. 存储空间监控**

```sql
-- 创建存储空间监控视图
CREATE OR REPLACE VIEW storage_usage AS
SELECT
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS total_size,
    pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) AS table_size,
    pg_size_pretty(pg_indexes_size(schemaname||'.'||tablename)) AS indexes_size,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) -
                   pg_relation_size(schemaname||'.'||tablename) -
                   pg_indexes_size(schemaname||'.'||tablename)) AS other_size
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- 查询存储使用情况
SELECT * FROM storage_usage;
```

**解决方案**：

1. 使用压缩技术
2. 归档历史数据
3. 定期清理低概率数据
4. 使用分区表

### 7.5 注意事项

#### **7.5.1 概率值管理**

**1. 概率值范围验证**:

```sql
-- 使用CHECK约束确保概率值在[0, 1]范围内
CREATE TABLE sensor_data (
    id SERIAL PRIMARY KEY,
    sensor_id INT NOT NULL,
    value NUMERIC NOT NULL,
    probability NUMERIC CHECK (
        probability >= 0 AND probability <= 1
    ),
    timestamp TIMESTAMP DEFAULT NOW()
);

-- 处理NULL概率值的情况
CREATE TABLE sensor_data (
    id SERIAL PRIMARY KEY,
    sensor_id INT NOT NULL,
    value NUMERIC NOT NULL,
    probability NUMERIC CHECK (
        probability IS NULL OR (probability >= 0 AND probability <= 1)
    ),
    timestamp TIMESTAMP DEFAULT NOW()
);

-- 验证概率分布的合理性
CREATE OR REPLACE FUNCTION validate_probability_distribution(
    table_name TEXT,
    column_name TEXT
) RETURNS TABLE (
    total_count BIGINT,
    valid_probability_count BIGINT,
    invalid_probability_count BIGINT,
    null_probability_count BIGINT,
    avg_probability NUMERIC,
    min_probability NUMERIC,
    max_probability NUMERIC
) AS $$
BEGIN
    RETURN QUERY EXECUTE format('
        SELECT
            COUNT(*) AS total_count,
            COUNT(*) FILTER (WHERE %I >= 0 AND %I <= 1) AS valid_probability_count,
            COUNT(*) FILTER (WHERE %I < 0 OR %I > 1) AS invalid_probability_count,
            COUNT(*) FILTER (WHERE %I IS NULL) AS null_probability_count,
            AVG(%I) AS avg_probability,
            MIN(%I) AS min_probability,
            MAX(%I) AS max_probability
        FROM %I
    ', column_name, column_name, column_name, column_name, column_name,
       column_name, column_name, column_name, table_name);
END;
$$ LANGUAGE plpgsql;

-- 使用验证函数
SELECT * FROM validate_probability_distribution('sensor_data', 'probability');
```

**2. 概率值更新**:

```sql
-- 创建概率值变更历史表
CREATE TABLE probability_change_history (
    id SERIAL PRIMARY KEY,
    table_name TEXT NOT NULL,
    record_id INT NOT NULL,
    old_probability NUMERIC,
    new_probability NUMERIC,
    changed_by TEXT,
    changed_at TIMESTAMP DEFAULT NOW(),
    change_reason TEXT
);

-- 创建概率值更新触发器
CREATE OR REPLACE FUNCTION log_probability_change()
RETURNS TRIGGER AS $$
BEGIN
    IF OLD.probability IS DISTINCT FROM NEW.probability THEN
        INSERT INTO probability_change_history (
            table_name,
            record_id,
            old_probability,
            new_probability,
            changed_by,
            change_reason
        ) VALUES (
            TG_TABLE_NAME,
            NEW.id,
            OLD.probability,
            NEW.probability,
            current_user,
            'Probability updated'
        );
    END IF;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- 创建触发器
CREATE TRIGGER log_probability_change_trigger
AFTER UPDATE ON sensor_data
FOR EACH ROW
EXECUTE FUNCTION log_probability_change();

-- 使用事务确保一致性
BEGIN;
UPDATE sensor_data
SET probability = probability * 1.1
WHERE sensor_id = 1 AND probability < 0.9;

-- 验证更新结果
SELECT
    sensor_id,
    COUNT(*) AS updated_count,
    AVG(probability) AS avg_probability
FROM sensor_data
WHERE sensor_id = 1
GROUP BY sensor_id;

COMMIT;
```

**3. 概率值归一化**:

```sql
-- 概率值归一化函数
CREATE OR REPLACE FUNCTION normalize_probabilities(
    table_name TEXT,
    id_column TEXT,
    probability_column TEXT
) RETURNS VOID AS $$
DECLARE
    total_prob NUMERIC;
BEGIN
    -- 计算总概率
    EXECUTE format('
        SELECT SUM(%I) INTO total_prob
        FROM %I
    ', probability_column, table_name);

    -- 归一化概率值
    EXECUTE format('
        UPDATE %I
        SET %I = %I / NULLIF(%L, 0)
    ', table_name, probability_column, probability_column, total_prob);
END;
$$ LANGUAGE plpgsql;
```

#### **7.5.2 查询复杂度**

**1. 避免复杂查询**

```sql
-- 避免深度嵌套的概率查询
-- 不推荐：深度嵌套
SELECT * FROM (
    SELECT * FROM (
        SELECT * FROM sensor_data WHERE probability > 0.8
    ) t1 WHERE probability > 0.9
) t2 WHERE probability > 0.95;

-- 推荐：简化查询
SELECT * FROM sensor_data
WHERE probability > 0.95;

-- 避免多个概率JOIN
-- 不推荐：多个概率JOIN
SELECT a.*, b.*, c.*
FROM uncertain_table_a a
JOIN uncertain_table_b b ON a.id = b.id
JOIN uncertain_table_c c ON b.id = c.id
WHERE a.probability * b.probability * c.probability > 0.5;

-- 推荐：先过滤再JOIN
SELECT a.*, b.*, c.*
FROM (
    SELECT * FROM uncertain_table_a WHERE probability > 0.7
) a
JOIN (
    SELECT * FROM uncertain_table_b WHERE probability > 0.7
) b ON a.id = b.id
JOIN (
    SELECT * FROM uncertain_table_c WHERE probability > 0.7
) c ON b.id = c.id
WHERE a.probability * b.probability * c.probability > 0.5;

-- 限制查询结果集大小
SELECT * FROM sensor_data
WHERE probability > 0.8
ORDER BY probability DESC
LIMIT 100;
```

**2. 查询超时设置**

```sql
-- 设置查询超时时间
SET statement_timeout = '30s';

-- 监控长时间运行的查询
SELECT
    pid,
    now() - pg_stat_activity.query_start AS duration,
    query,
    state
FROM pg_stat_activity
WHERE state = 'active'
  AND now() - pg_stat_activity.query_start > INTERVAL '5 minutes';

-- 取消长时间运行的查询
SELECT pg_cancel_backend(pid) FROM pg_stat_activity
WHERE state = 'active'
  AND now() - pg_stat_activity.query_start > INTERVAL '10 minutes';

-- 查询超时监控函数
CREATE OR REPLACE FUNCTION monitor_long_running_queries(
    threshold INTERVAL DEFAULT INTERVAL '5 minutes'
) RETURNS TABLE (
    pid INT,
    duration INTERVAL,
    query TEXT,
    state TEXT
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        pg_stat_activity.pid,
        now() - pg_stat_activity.query_start AS duration,
        pg_stat_activity.query,
        pg_stat_activity.state
    FROM pg_stat_activity
    WHERE pg_stat_activity.state = 'active'
      AND now() - pg_stat_activity.query_start > threshold;
END;
$$ LANGUAGE plpgsql;

-- 使用监控函数
SELECT * FROM monitor_long_running_queries(INTERVAL '1 minute');
```

**3. 查询性能分析**

```sql
-- 启用查询计划分析
EXPLAIN (ANALYZE, BUFFERS, VERBOSE)
SELECT * FROM sensor_data
WHERE probability > 0.8;

-- 分析慢查询
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- 查询最慢的查询
SELECT
    query,
    calls,
    total_exec_time,
    mean_exec_time,
    max_exec_time
FROM pg_stat_statements
ORDER BY mean_exec_time DESC
LIMIT 10;
```

#### **7.5.3 数据质量**

**1. 概率值质量**

```sql
-- 验证概率值的合理性
CREATE OR REPLACE FUNCTION validate_probability_quality(
    table_name TEXT,
    probability_column TEXT
) RETURNS TABLE (
    check_name TEXT,
    check_result TEXT,
    issue_count BIGINT
) AS $$
BEGIN
    -- 检查概率值范围
    RETURN QUERY EXECUTE format('
        SELECT
            ''Probability Range Check'' AS check_name,
            CASE
                WHEN COUNT(*) FILTER (WHERE %I < 0 OR %I > 1) > 0
                THEN ''FAIL: Found probabilities outside [0, 1]''
                ELSE ''PASS: All probabilities in valid range''
            END AS check_result,
            COUNT(*) FILTER (WHERE %I < 0 OR %I > 1) AS issue_count
        FROM %I
    ', probability_column, probability_column, probability_column, table_name);

    -- 检查NULL概率值
    RETURN QUERY EXECUTE format('
        SELECT
            ''NULL Probability Check'' AS check_name,
            CASE
                WHEN COUNT(*) FILTER (WHERE %I IS NULL) > 0
                THEN ''WARNING: Found NULL probabilities''
                ELSE ''PASS: No NULL probabilities''
            END AS check_result,
            COUNT(*) FILTER (WHERE %I IS NULL) AS issue_count
        FROM %I
    ', probability_column, probability_column, table_name);

    -- 检查概率分布
    RETURN QUERY EXECUTE format('
        SELECT
            ''Probability Distribution Check'' AS check_name,
            CASE
                WHEN STDDEV(%I) > 0.5
                THEN ''WARNING: High variance in probability distribution''
                ELSE ''PASS: Reasonable probability distribution''
            END AS check_result,
            COUNT(*) AS issue_count
        FROM %I
    ', probability_column, table_name);
END;
$$ LANGUAGE plpgsql;

-- 使用验证函数
SELECT * FROM validate_probability_quality('sensor_data', 'probability');

-- 检查概率分布的一致性
CREATE OR REPLACE FUNCTION check_probability_consistency(
    table_name TEXT,
    group_column TEXT,
    probability_column TEXT
) RETURNS TABLE (
    group_value TEXT,
    total_count BIGINT,
    avg_probability NUMERIC,
    stddev_probability NUMERIC,
    consistency_score NUMERIC
) AS $$
BEGIN
    RETURN QUERY EXECUTE format('
        SELECT
            %I::TEXT AS group_value,
            COUNT(*) AS total_count,
            AVG(%I) AS avg_probability,
            STDDEV(%I) AS stddev_probability,
            CASE
                WHEN STDDEV(%I) = 0 THEN 1.0
                ELSE 1.0 / (1.0 + STDDEV(%I))
            END AS consistency_score
        FROM %I
        GROUP BY %I
        ORDER BY consistency_score DESC
    ', group_column, probability_column, probability_column,
       probability_column, probability_column, table_name, group_column);
END;
$$ LANGUAGE plpgsql;

-- 使用一致性检查函数
SELECT * FROM check_probability_consistency('sensor_data', 'sensor_id', 'probability');

-- 处理异常概率值
CREATE OR REPLACE FUNCTION fix_abnormal_probabilities(
    table_name TEXT,
    probability_column TEXT,
    threshold NUMERIC DEFAULT 0.01
) RETURNS TABLE (
    fixed_count BIGINT,
    avg_probability_before NUMERIC,
    avg_probability_after NUMERIC
) AS $$
DECLARE
    fixed_rows BIGINT;
    avg_before NUMERIC;
    avg_after NUMERIC;
BEGIN
    -- 计算修复前的平均概率
    EXECUTE format('
        SELECT AVG(%I) INTO avg_before
        FROM %I
    ', probability_column, table_name);

    -- 修复异常概率值
    EXECUTE format('
        UPDATE %I
        SET %I = CASE
            WHEN %I < 0 THEN 0
            WHEN %I > 1 THEN 1
            WHEN %I < threshold THEN threshold
            ELSE %I
        END
        WHERE %I < 0 OR %I > 1 OR %I < threshold
    ', table_name, probability_column, probability_column,
       probability_column, probability_column, probability_column,
       probability_column, probability_column, probability_column);

    GET DIAGNOSTICS fixed_rows = ROW_COUNT;

    -- 计算修复后的平均概率
    EXECUTE format('
        SELECT AVG(%I) INTO avg_after
        FROM %I
    ', probability_column, table_name);

    RETURN QUERY SELECT fixed_rows, avg_before, avg_after;
END;
$$ LANGUAGE plpgsql;

-- 使用修复函数
SELECT * FROM fix_abnormal_probabilities('sensor_data', 'probability', 0.01);
```

**2. 数据一致性**

```sql
-- 确保概率值与数据的一致性
CREATE OR REPLACE FUNCTION check_data_consistency(
    table_name TEXT,
    value_column TEXT,
    probability_column TEXT
) RETURNS TABLE (
    check_name TEXT,
    check_result TEXT,
    issue_count BIGINT
) AS $$
BEGIN
    -- 检查概率值与数据值的一致性
    RETURN QUERY EXECUTE format('
        SELECT
            ''Value-Probability Consistency Check'' AS check_name,
            CASE
                WHEN COUNT(*) FILTER (
                    WHERE (%I IS NULL AND %I IS NOT NULL) OR
                         (%I IS NOT NULL AND %I IS NULL)
                ) > 0
                THEN ''FAIL: Found inconsistent NULL values''
                ELSE ''PASS: Value and probability consistency OK''
            END AS check_result,
            COUNT(*) FILTER (
                WHERE (%I IS NULL AND %I IS NOT NULL) OR
                     (%I IS NOT NULL AND %I IS NULL)
            ) AS issue_count
        FROM %I
    ', value_column, probability_column, value_column, probability_column,
       value_column, probability_column, table_name);
END;
$$ LANGUAGE plpgsql;

-- 使用一致性检查函数
SELECT * FROM check_data_consistency('sensor_data', 'value', 'probability');

-- 验证溯源信息的完整性
CREATE OR REPLACE FUNCTION validate_provenance_integrity(
    table_name TEXT,
    provenance_column TEXT
) RETURNS TABLE (
    check_name TEXT,
    check_result TEXT,
    issue_count BIGINT
) AS $$
BEGIN
    RETURN QUERY EXECUTE format('
        SELECT
            ''Provenance Integrity Check'' AS check_name,
            CASE
                WHEN COUNT(*) FILTER (WHERE %I IS NULL) > 0
                THEN ''WARNING: Found records without provenance''
                ELSE ''PASS: All records have provenance''
            END AS check_result,
            COUNT(*) FILTER (WHERE %I IS NULL) AS issue_count
        FROM %I
    ', provenance_column, provenance_column, table_name);
END;
$$ LANGUAGE plpgsql;

-- 使用溯源完整性检查函数
SELECT * FROM validate_provenance_integrity('sensor_data', 'provenance');

-- 检查数据完整性约束
CREATE OR REPLACE FUNCTION check_integrity_constraints(
    table_name TEXT
) RETURNS TABLE (
    constraint_name TEXT,
    constraint_type TEXT,
    status TEXT
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        conname::TEXT AS constraint_name,
        contype::TEXT AS constraint_type,
        CASE
            WHEN contype = 'c' THEN ''CHECK constraint''
            WHEN contype = 'f' THEN ''FOREIGN KEY constraint''
            WHEN contype = 'u' THEN ''UNIQUE constraint''
            WHEN contype = 'p' THEN ''PRIMARY KEY constraint''
            ELSE ''Other constraint''
        END AS status
    FROM pg_constraint
    WHERE conrelid = table_name::regclass;
END;
$$ LANGUAGE plpgsql;

-- 使用完整性约束检查函数
SELECT * FROM check_integrity_constraints('sensor_data');
```

### 7.6 实施建议

#### **7.6.1 渐进式实施**

**第一阶段：基础实现（1-2周）**

**目标**：建立基本的概率数据库功能

**任务清单**：

```sql
-- 1. 创建基础概率数据类型
CREATE TYPE probability_value AS (
    value NUMERIC,
    probability NUMERIC
);

-- 2. 创建基础表结构
CREATE TABLE sensor_data (
    id SERIAL PRIMARY KEY,
    sensor_id INT NOT NULL,
    value NUMERIC NOT NULL,
    probability NUMERIC CHECK (probability >= 0 AND probability <= 1),
    timestamp TIMESTAMP DEFAULT NOW()
);

-- 3. 实现基础概率查询函数
CREATE OR REPLACE FUNCTION probability_select(
    min_prob NUMERIC DEFAULT 0.5
) RETURNS TABLE (
    id INT,
    sensor_id INT,
    value NUMERIC,
    probability NUMERIC
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        sensor_data.id,
        sensor_data.sensor_id,
        sensor_data.value,
        sensor_data.probability
    FROM sensor_data
    WHERE sensor_data.probability >= min_prob;
END;
$$ LANGUAGE plpgsql;

-- 4. 创建基础索引
CREATE INDEX idx_probability ON sensor_data(probability);
CREATE INDEX idx_sensor_prob ON sensor_data(sensor_id, probability);

-- 5. 测试基本功能
-- 插入测试数据
INSERT INTO sensor_data (sensor_id, value, probability)
VALUES
    (1, 25.5, 0.9),
    (1, 26.0, 0.8),
    (2, 30.0, 0.7);

-- 测试概率查询
SELECT * FROM probability_select(0.8);
```

**验收标准**：

- ✅ 能够存储概率数据
- ✅ 能够执行基础概率查询
- ✅ 概率值验证正常工作
- ✅ 基础索引创建成功

**第二阶段：优化（2-3周）**

**目标**：优化查询性能和存储结构

**任务清单**：

```sql
-- 1. 优化查询性能
-- 创建物化视图
CREATE MATERIALIZED VIEW sensor_data_summary AS
SELECT
    sensor_id,
    COUNT(*) AS total_count,
    AVG(value) AS avg_value,
    SUM(value * probability) / SUM(probability) AS weighted_avg,
    AVG(probability) AS avg_probability
FROM sensor_data
GROUP BY sensor_id;

CREATE INDEX idx_summary_sensor ON sensor_data_summary(sensor_id);

-- 2. 优化存储结构
-- 实现表分区
CREATE TABLE sensor_data_partitioned (
    LIKE sensor_data INCLUDING ALL
) PARTITION BY RANGE (probability);

CREATE TABLE sensor_data_high PARTITION OF sensor_data_partitioned
FOR VALUES FROM (0.8) TO (1.0);

CREATE TABLE sensor_data_medium PARTITION OF sensor_data_partitioned
FOR VALUES FROM (0.5) TO (0.8);

CREATE TABLE sensor_data_low PARTITION OF sensor_data_partitioned
FOR VALUES FROM (0.0) TO (0.5);

-- 3. 实现高级功能
-- 概率聚合函数
CREATE OR REPLACE FUNCTION probability_aggregate(
    values NUMERIC[],
    probabilities NUMERIC[]
) RETURNS NUMERIC AS $$
DECLARE
    weighted_sum NUMERIC := 0;
    total_prob NUMERIC := 0;
    i INT;
BEGIN
    FOR i IN 1..array_length(values, 1) LOOP
        weighted_sum := weighted_sum + values[i] * probabilities[i];
        total_prob := total_prob + probabilities[i];
    END LOOP;

    RETURN weighted_sum / NULLIF(total_prob, 0);
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- 4. 性能测试
-- 基准测试脚本
CREATE OR REPLACE FUNCTION benchmark_probability_queries()
RETURNS TABLE (
    query_name TEXT,
    execution_time INTERVAL,
    rows_returned BIGINT
) AS $$
DECLARE
    start_time TIMESTAMP;
    end_time TIMESTAMP;
    row_count BIGINT;
BEGIN
    -- 测试1：基础概率查询
    start_time := clock_timestamp();
    SELECT COUNT(*) INTO row_count
    FROM sensor_data
    WHERE probability > 0.8;
    end_time := clock_timestamp();

    RETURN QUERY SELECT
        'Basic Probability Query'::TEXT,
        end_time - start_time,
        row_count;

    -- 测试2：概率聚合查询
    start_time := clock_timestamp();
    SELECT COUNT(*) INTO row_count
    FROM sensor_data_summary;
    end_time := clock_timestamp();

    RETURN QUERY SELECT
        'Probability Aggregate Query'::TEXT,
        end_time - start_time,
        row_count;
END;
$$ LANGUAGE plpgsql;

-- 运行基准测试
SELECT * FROM benchmark_probability_queries();
```

**验收标准**：

- ✅ 查询性能提升50%以上
- ✅ 存储空间优化20%以上
- ✅ 物化视图正常工作
- ✅ 分区表正常工作
- ✅ 高级功能测试通过

**第三阶段：生产部署（1-2周）**

**目标**：部署到生产环境并持续优化

**任务清单**：

```sql
-- 1. 生产环境配置
-- 设置合理的连接池大小
ALTER SYSTEM SET max_connections = 200;
ALTER SYSTEM SET shared_buffers = '4GB';
ALTER SYSTEM SET effective_cache_size = '12GB';
ALTER SYSTEM SET maintenance_work_mem = '1GB';
ALTER SYSTEM SET checkpoint_completion_target = 0.9;
ALTER SYSTEM SET wal_buffers = '16MB';
ALTER SYSTEM SET default_statistics_target = 100;
ALTER SYSTEM SET random_page_cost = 1.1;
ALTER SYSTEM SET effective_io_concurrency = 200;
ALTER SYSTEM SET work_mem = '16MB';
ALTER SYSTEM SET min_wal_size = '1GB';
ALTER SYSTEM SET max_wal_size = '4GB';

-- 2. 监控性能指标
-- 创建性能监控视图
CREATE OR REPLACE VIEW performance_metrics AS
SELECT
    'Query Performance' AS metric_category,
    COUNT(*) AS total_queries,
    AVG(mean_exec_time) AS avg_execution_time,
    MAX(max_exec_time) AS max_execution_time
FROM pg_stat_statements
WHERE query LIKE '%sensor_data%';

-- 3. 设置自动维护任务
-- 创建自动VACUUM配置
ALTER TABLE sensor_data SET (
    autovacuum_vacuum_scale_factor = 0.1,
    autovacuum_analyze_scale_factor = 0.05,
    autovacuum_vacuum_cost_delay = 20
);

-- 4. 创建监控告警
CREATE OR REPLACE FUNCTION check_performance_alerts()
RETURNS TABLE (
    alert_type TEXT,
    alert_message TEXT,
    severity TEXT
) AS $$
BEGIN
    -- 检查慢查询
    RETURN QUERY
    SELECT
        'Slow Query'::TEXT,
        format('Query execution time: %s ms', mean_exec_time),
        CASE
            WHEN mean_exec_time > 1000 THEN 'HIGH'
            WHEN mean_exec_time > 500 THEN 'MEDIUM'
            ELSE 'LOW'
        END
    FROM pg_stat_statements
    WHERE mean_exec_time > 500
    ORDER BY mean_exec_time DESC
    LIMIT 10;

    -- 检查存储空间
    RETURN QUERY
    SELECT
        'Storage Space'::TEXT,
        format('Table size: %s', pg_size_pretty(pg_total_relation_size('sensor_data'))),
        CASE
            WHEN pg_total_relation_size('sensor_data') > 10737418240 THEN 'HIGH'  -- 10GB
            WHEN pg_total_relation_size('sensor_data') > 5368709120 THEN 'MEDIUM'  -- 5GB
            ELSE 'LOW'
        END;
END;
$$ LANGUAGE plpgsql;

-- 运行监控告警
SELECT * FROM check_performance_alerts();
```

**验收标准**：

- ✅ 生产环境部署成功
- ✅ 性能监控正常工作
- ✅ 自动维护任务配置成功
- ✅ 监控告警正常工作
- ✅ 系统稳定运行

#### **7.6.2 团队培训**

**1. 技术培训**

**培训内容**：

**概率数据库基础理论（4小时）**

- 概率数据库概念和原理
- 不确定性数据模型
- 概率查询处理
- 可能世界语义

**PostgreSQL扩展开发（8小时）**

- PostgreSQL扩展架构
- 自定义数据类型开发
- 自定义操作符和函数开发
- 查询优化器扩展

**ProvSQL使用（4小时）**

- ProvSQL安装和配置
- ProvSQL核心功能
- ProvSQL使用示例
- ProvSQL最佳实践

**培训材料**：

```sql
-- 培训示例：概率数据库基础
-- 1. 创建概率数据表
CREATE TABLE training_data (
    id SERIAL PRIMARY KEY,
    value NUMERIC NOT NULL,
    probability NUMERIC CHECK (probability >= 0 AND probability <= 1)
);

-- 2. 插入示例数据
INSERT INTO training_data (value, probability)
VALUES
    (10, 0.9),
    (20, 0.8),
    (30, 0.7);

-- 3. 概率查询示例
SELECT
    value,
    probability,
    value * probability AS weighted_value
FROM training_data
WHERE probability > 0.75;

-- 4. 概率聚合示例
SELECT
    COUNT(*) AS total_count,
    AVG(value) AS avg_value,
    SUM(value * probability) / SUM(probability) AS weighted_avg
FROM training_data;
```

**2. 最佳实践培训**

**培训内容**：

**设计原则（2小时）**

- 概率值设计原则
- 数据模型设计原则
- 索引设计原则
- 约束设计原则

**性能优化技巧（4小时）**

- 查询优化技巧
- 存储优化技巧
- 索引优化技巧
- 缓存策略

**常见问题解决（2小时）**

- 概率值不一致问题
- 性能问题
- 存储空间问题
- 数据质量问题

**培训材料**：

```sql
-- 培训示例：性能优化
-- 1. 查询优化示例
-- 优化前
SELECT * FROM sensor_data WHERE probability > 0.8;

-- 优化后
SELECT * FROM sensor_data
WHERE probability > 0.8
ORDER BY probability DESC
LIMIT 100;

-- 2. 索引优化示例
CREATE INDEX idx_probability ON sensor_data(probability);
CREATE INDEX idx_sensor_prob ON sensor_data(sensor_id, probability);

-- 3. 物化视图优化示例
CREATE MATERIALIZED VIEW sensor_data_summary AS
SELECT
    sensor_id,
    COUNT(*) AS total_count,
    AVG(value) AS avg_value
FROM sensor_data
GROUP BY sensor_id;
```

#### **7.6.3 监控和维护**

**1. 性能监控**

```sql
-- 创建性能监控视图
CREATE OR REPLACE VIEW performance_monitoring AS
SELECT
    'Query Performance' AS metric_type,
    COUNT(*) AS total_queries,
    AVG(mean_exec_time) AS avg_execution_time_ms,
    MAX(max_exec_time) AS max_execution_time_ms,
    SUM(calls) AS total_calls
FROM pg_stat_statements
WHERE query LIKE '%sensor_data%';

-- 查询性能监控
SELECT * FROM performance_monitoring;

-- 创建存储监控视图
CREATE OR REPLACE VIEW storage_monitoring AS
SELECT
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS total_size,
    pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) AS table_size,
    pg_size_pretty(pg_indexes_size(schemaname||'.'||tablename)) AS indexes_size
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- 查询存储监控
SELECT * FROM storage_monitoring;

-- 创建系统资源监控视图
CREATE OR REPLACE VIEW system_resource_monitoring AS
SELECT
    'Database Connections' AS resource_type,
    COUNT(*) AS current_connections,
    (SELECT setting::INT FROM pg_settings WHERE name = 'max_connections') AS max_connections,
    ROUND(COUNT(*)::NUMERIC / (SELECT setting::INT FROM pg_settings WHERE name = 'max_connections') * 100, 2) AS connection_usage_percent
FROM pg_stat_activity
WHERE datname = current_database();

-- 查询系统资源监控
SELECT * FROM system_resource_monitoring;
```

**2. 数据质量监控**

```sql
-- 创建数据质量监控视图
CREATE OR REPLACE VIEW data_quality_monitoring AS
SELECT
    'Probability Quality' AS quality_metric,
    COUNT(*) AS total_records,
    COUNT(*) FILTER (WHERE probability >= 0 AND probability <= 1) AS valid_probability_count,
    COUNT(*) FILTER (WHERE probability < 0 OR probability > 1) AS invalid_probability_count,
    COUNT(*) FILTER (WHERE probability IS NULL) AS null_probability_count,
    AVG(probability) AS avg_probability,
    STDDEV(probability) AS stddev_probability
FROM sensor_data;

-- 查询数据质量监控
SELECT * FROM data_quality_monitoring;

-- 创建数据一致性监控视图
CREATE OR REPLACE VIEW data_consistency_monitoring AS
SELECT
    'Data Consistency' AS consistency_metric,
    COUNT(*) AS total_records,
    COUNT(*) FILTER (WHERE value IS NOT NULL AND probability IS NOT NULL) AS consistent_records,
    COUNT(*) FILTER (WHERE (value IS NULL AND probability IS NOT NULL) OR
                            (value IS NOT NULL AND probability IS NULL)) AS inconsistent_records
FROM sensor_data;

-- 查询数据一致性监控
SELECT * FROM data_consistency_monitoring;

-- 创建溯源信息完整性监控视图
CREATE OR REPLACE VIEW provenance_integrity_monitoring AS
SELECT
    'Provenance Integrity' AS integrity_metric,
    COUNT(*) AS total_records,
    COUNT(*) FILTER (WHERE provenance IS NOT NULL) AS records_with_provenance,
    COUNT(*) FILTER (WHERE provenance IS NULL) AS records_without_provenance
FROM sensor_data;

-- 查询溯源信息完整性监控
SELECT * FROM provenance_integrity_monitoring;
```

**3. 定期维护**

```sql
-- 创建定期维护函数
CREATE OR REPLACE FUNCTION perform_maintenance()
RETURNS TABLE (
    maintenance_task TEXT,
    status TEXT,
    execution_time INTERVAL
) AS $$
DECLARE
    start_time TIMESTAMP;
    end_time TIMESTAMP;
BEGIN
    -- 1. 优化索引
    start_time := clock_timestamp();
    REINDEX INDEX CONCURRENTLY idx_probability;
    REINDEX INDEX CONCURRENTLY idx_sensor_prob;
    end_time := clock_timestamp();

    RETURN QUERY SELECT
        'Index Optimization'::TEXT,
        'COMPLETED'::TEXT,
        end_time - start_time;

    -- 2. 清理数据
    start_time := clock_timestamp();
    DELETE FROM sensor_data
    WHERE timestamp < NOW() - INTERVAL '2 years'
      AND probability < 0.1;
    end_time := clock_timestamp();

    RETURN QUERY SELECT
        'Data Cleanup'::TEXT,
        'COMPLETED'::TEXT,
        end_time - start_time;

    -- 3. 更新统计信息
    start_time := clock_timestamp();
    ANALYZE sensor_data;
    end_time := clock_timestamp();

    RETURN QUERY SELECT
        'Statistics Update'::TEXT,
        'COMPLETED'::TEXT,
        end_time - start_time;

    -- 4. 刷新物化视图
    start_time := clock_timestamp();
    REFRESH MATERIALIZED VIEW CONCURRENTLY sensor_data_summary;
    end_time := clock_timestamp();

    RETURN QUERY SELECT
        'Materialized View Refresh'::TEXT,
        'COMPLETED'::TEXT,
        end_time - start_time;

    -- 5. VACUUM
    start_time := clock_timestamp();
    VACUUM ANALYZE sensor_data;
    end_time := clock_timestamp();

    RETURN QUERY SELECT
        'VACUUM'::TEXT,
        'COMPLETED'::TEXT,
        end_time - start_time;
END;
$$ LANGUAGE plpgsql;

-- 执行定期维护
SELECT * FROM perform_maintenance();

-- 创建自动维护计划（使用pg_cron扩展）
-- 每天凌晨2点执行维护任务
SELECT cron.schedule(
    'daily-maintenance',
    '0 2 * * *',
    'SELECT * FROM perform_maintenance()'
);
```

---

## 8. 参考资料

### 8.1 学术论文

#### **8.1.1 概率数据库基础理论**

1. **Foundations of Probabilistic Databases**
   - 作者: Suciu, D., Olteanu, D., Ré, C., & Koch, C.
   - 期刊: VLDB Journal, 20(5), 659-684, 2011
   - 摘要: 概率数据库的基础理论和模型
   - 重要性: ⭐⭐⭐⭐⭐ 概率数据库领域的经典论文

2. **Efficient Query Evaluation on Probabilistic Databases**
   - 作者: Dalvi, N., & Suciu, D.
   - 会议: VLDB 2007
   - 摘要: 概率数据库查询评估的优化方法
   - 重要性: ⭐⭐⭐⭐⭐ 查询优化的重要论文

3. **Probabilistic Databases: The Diamonds and the Rough**
   - 作者: Suciu, D., & Olteanu, D.
   - 会议: CIDR 2011
   - 摘要: 概率数据库的挑战和机遇

#### **8.1.2 ProvSQL和数据溯源**

1. **ProvSQL: Provenance and Probability Management in PostgreSQL**
   - 作者: Senellart, P., Jaudoin, H., & Galland, A.
   - 会议: SIGMOD 2018
   - 摘要: ProvSQL在PostgreSQL中的实现
   - GitHub: [https://github.com/PierreSenellart/provsql](https://github.com/PierreSenellart/provsql)
   - 重要性: ⭐⭐⭐⭐⭐ ProvSQL的核心论文

2. **Provenance in Databases: Why, How, and Where**
   - 作者: Cheney, J., Chiticariu, L., & Tan, W. C.
   - 期刊: Foundations and Trends in Databases, 2009
   - 摘要: 数据库溯源的基础理论

3. **Provenance Semirings**
   - 作者: Green, T. J., Karvounarakis, G., & Tannen, V.
   - 会议: PODS 2007
   - 摘要: 溯源半环理论

#### **8.1.3 不确定性数据处理**

1. **Managing Uncertainty in Data Integration**
   - 作者: Dong, X. L., & Srivastava, D.
   - 会议: ICDE 2013
   - 摘要: 数据集成中的不确定性管理

2. **Probabilistic Data Integration**
   - 作者: Re, C., & Suciu, D.
   - 会议: ICDT 2008
   - 摘要: 概率数据集成方法

### 8.2 官方文档

#### **8.2.1 PostgreSQL文档**

1. **PostgreSQL Extension Development**
   - 链接: [https://www.postgresql.org/docs/current/extend.html](https://www.postgresql.org/docs/current/extend.html)
   - 内容: PostgreSQL扩展开发指南
   - 章节:
     - 扩展架构
     - 数据类型扩展
     - 函数和操作符
     - 索引扩展

2. **PostgreSQL Data Types**
   - 链接: [https://www.postgresql.org/docs/current/datatype.html](https://www.postgresql.org/docs/current/datatype.html)
   - 内容: PostgreSQL数据类型文档

3. **PostgreSQL Functions and Operators**
   - 链接: [https://www.postgresql.org/docs/current/functions.html](https://www.postgresql.org/docs/current/functions.html)
   - 内容: PostgreSQL函数和操作符文档

#### **8.2.2 ProvSQL文档**

1. **ProvSQL GitHub Repository**
   - 链接: [https://github.com/PierreSenellart/provsql](https://github.com/PierreSenellart/provsql)
   - 内容: ProvSQL源代码和文档
   - 功能:
     - 数据溯源管理
     - 概率计算
     - PostgreSQL集成

2. **ProvSQL Documentation**
   - 链接: [https://github.com/PierreSenellart/provsql/blob/master/README.md](https://github.com/PierreSenellart/provsql/blob/master/README.md)
   - 内容: ProvSQL使用文档和示例

### 8.3 开源项目和工具

#### **8.3.1 概率数据库项目**

1. **ProvSQL**
   - GitHub: [https://github.com/PierreSenellart/provsql](https://github.com/PierreSenellart/provsql)
   - 语言: C/C++
   - 功能: PostgreSQL的溯源和概率管理扩展
   - 状态: ✅ 活跃维护

2. **MystiQ**
   - GitHub: [https://github.com/dan-suciu/mystiq](https://github.com/dan-suciu/mystiq)
   - 语言: C++
   - 功能: 概率查询处理引擎
   - 状态: ✅ 活跃维护

3. **MayBMS**
   - 项目: 概率数据库管理系统
   - 功能: 完整的概率数据库系统
   - 状态: ⚠️ 已停止维护

#### **8.3.2 相关工具**

1. **PostgreSQL扩展开发工具**
   - **pgxs**: PostgreSQL扩展构建系统
     - 文档: [https://www.postgresql.org/docs/current/extend-pgxs.html](https://www.postgresql.org/docs/current/extend-pgxs.html)
     - 功能: 简化PostgreSQL扩展的构建和安装
     - 用途: 开发PostgreSQL扩展

   - **pg_config**: PostgreSQL配置工具
     - 功能: 查询PostgreSQL配置信息
     - 用途: 获取编译和链接信息

   - **pg_regress**: PostgreSQL回归测试工具
     - 功能: 运行回归测试
     - 用途: 测试PostgreSQL扩展

2. **数据溯源工具**
   - **OpenProvenance**: 开源溯源工具
     - 链接: [http://openprovenance.org/](http://openprovenance.org/)
     - 功能: 数据溯源管理和可视化
     - 用途: 数据溯源研究和应用

   - **W3C PROV**: 溯源数据模型标准
     - 链接: [https://www.w3.org/TR/prov-overview/](https://www.w3.org/TR/prov-overview/)
     - 功能: 标准化的溯源数据模型
     - 用途: 数据溯源标准化

3. **概率计算工具**
   - **NumPy/SciPy**: Python科学计算库
     - 功能: 概率计算和统计分析
     - 用途: 概率计算和数据分析

   - **R**: 统计计算语言
     - 功能: 概率统计和数据分析
     - 用途: 概率分析和统计建模

4. **数据库管理工具**
   - **pgAdmin**: PostgreSQL管理工具
     - 链接: [https://www.pgadmin.org/](https://www.pgadmin.org/)
     - 功能: PostgreSQL可视化管理
     - 用途: 数据库管理和查询

   - **DBeaver**: 通用数据库管理工具
     - 链接: [https://dbeaver.io/](https://dbeaver.io/)
     - 功能: 多数据库管理工具
     - 用途: 数据库管理和查询

5. **性能分析工具**
   - **pg_stat_statements**: PostgreSQL查询统计扩展
     - 功能: 查询性能统计和分析
     - 用途: 性能分析和优化

   - **EXPLAIN ANALYZE**: PostgreSQL执行计划分析
     - 功能: 查询执行计划分析
     - 用途: 查询优化

### 8.4 社区资源

#### **8.4.1 论坛和社区**

1. **PostgreSQL官方论坛**
   - 链接: [https://www.postgresql.org/list/](https://www.postgresql.org/list/)
   - 内容: PostgreSQL邮件列表和论坛
   - 标签: extension, probabilistic

2. **Stack Overflow**
   - 标签: [postgresql](https://stackoverflow.com/questions/tagged/postgresql), [probabilistic-database](https://stackoverflow.com/questions/tagged/probabilistic-database)
   - 内容: 技术问答

3. **Reddit - r/PostgreSQL**
   - 链接: [https://www.reddit.com/r/PostgreSQL/](https://www.reddit.com/r/PostgreSQL/)
   - 内容: PostgreSQL社区讨论

#### **8.4.2 博客和文章**

1. **概率数据库博客**
   - **内容**: 概率数据库应用案例和最佳实践
   - **推荐博客**:
     - [Towards Data Science - Probabilistic Databases](https://towardsdatascience.com/tagged/probabilistic-database)
     - [KDnuggets - Uncertainty in Data](https://www.kdnuggets.com/tag/uncertainty)
   - **更新频率**: 定期更新
   - **重要性**: ⭐⭐⭐⭐ 概率数据库应用实践

2. **数据溯源博客**
   - **内容**: 数据溯源技术和应用
   - **推荐博客**:
     - [Data Provenance Blog](https://example.com/data-provenance-blog)
     - [W3C PROV Blog](https://www.w3.org/blog/prov/)
   - **更新频率**: 定期更新
   - **重要性**: ⭐⭐⭐⭐ 数据溯源技术实践

3. **PostgreSQL扩展开发博客**
   - **内容**: PostgreSQL扩展开发教程
   - **推荐博客**:
     - [Planet PostgreSQL](https://planet.postgresql.org/)
     - [PostgreSQL Weekly](https://postgresweekly.com/)
   - **更新频率**: 定期更新
   - **重要性**: ⭐⭐⭐⭐ PostgreSQL扩展开发实践

4. **数据库技术博客**
   - **内容**: 数据库技术文章和最佳实践
   - **推荐博客**:
     - [High Scalability](http://highscalability.com/)
     - [Database Journal](https://www.databasejournal.com/)
   - **更新频率**: 定期更新
   - **重要性**: ⭐⭐⭐ 数据库技术参考

5. **数据科学博客**
   - **内容**: 数据科学和不确定性数据处理
   - **推荐博客**:
     - [Towards Data Science](https://towardsdatascience.com/)
     - [KDnuggets](https://www.kdnuggets.com/)
   - **更新频率**: 定期更新
   - **重要性**: ⭐⭐⭐ 数据科学相关内容

### 8.5 书籍推荐

#### **8.5.1 概率数据库相关书籍**

1. **《概率数据库基础》（Foundations of Probabilistic Databases）**
   - 作者: Suciu, D., Olteanu, D., Ré, C., & Koch, C.
   - 出版社: Morgan & Claypool Publishers
   - 出版年份: 2011
   - ISBN: 978-1608456802
   - 内容: 概率数据库理论和实践，包括可能世界语义、概率查询处理、不确定性数据模型
   - 适合: 研究人员和高级开发者
   - 重要性: ⭐⭐⭐⭐⭐ 概率数据库领域的经典教材

2. **《不确定性数据管理》（Managing and Mining Uncertain Data）**
   - 作者: Aggarwal, C. C.
   - 出版社: Springer
   - 出版年份: 2009
   - ISBN: 978-1441901968
   - 内容: 不确定性数据管理技术，包括数据模型、查询处理、挖掘算法
   - 适合: 研究人员和高级开发者
   - 重要性: ⭐⭐⭐⭐ 不确定性数据管理领域的权威参考

#### **8.5.2 PostgreSQL相关书籍**

1. **《PostgreSQL扩展开发》（PostgreSQL Extension Development）**
   - 作者: PostgreSQL社区
   - 内容: PostgreSQL扩展开发实践，包括数据类型、函数、操作符、索引扩展
   - 适合: 开发者
   - 重要性: ⭐⭐⭐⭐ PostgreSQL扩展开发官方指南

2. **《PostgreSQL：高级SQL编程》（PostgreSQL: Up and Running）**
   - 作者: Obe, R., & Hsu, L.
   - 出版社: O'Reilly Media
   - 出版年份: 2017
   - ISBN: 978-1491963418
   - 内容: PostgreSQL高级特性，包括扩展、性能优化、高级SQL
   - 适合: 开发者
   - 重要性: ⭐⭐⭐⭐ PostgreSQL实用指南

3. **《PostgreSQL性能优化》（PostgreSQL High Performance）**
   - 作者: Krosing, G., & Roybal, K.
   - 出版社: Packt Publishing
   - 出版年份: 2018
   - ISBN: 978-1788624331
   - 内容: PostgreSQL性能优化技术，包括查询优化、索引优化、配置调优
   - 适合: DBA和性能优化工程师
   - 重要性: ⭐⭐⭐⭐ PostgreSQL性能优化参考

#### **8.5.3 数据溯源相关书籍**

1. **《数据溯源：理论与实践》（Data Provenance: Theory and Practice）**
   - 作者: Buneman, P., & Tan, W. C.
   - 内容: 数据溯源理论和实践，包括溯源模型、查询处理、应用场景
   - 适合: 研究人员
   - 重要性: ⭐⭐⭐⭐ 数据溯源领域的经典参考

2. **《数据治理：理论与实践》（Data Governance: Theory and Practice）**
   - 作者: Seiner, R. S.
   - 出版社: Technics Publications
   - 出版年份: 2014
   - ISBN: 978-1634620005
   - 内容: 数据治理理论和实践，包括数据质量、数据溯源、合规性
   - 适合: 数据治理专家和企业用户
   - 重要性: ⭐⭐⭐⭐ 数据治理实用指南

#### **8.5.4 数据库理论相关书籍**

1. **《数据库系统概念》（Database System Concepts）**
   - 作者: Silberschatz, A., Korth, H. F., & Sudarshan, S.
   - 出版社: McGraw-Hill Education
   - 出版年份: 2019 (第7版)
   - ISBN: 978-0078022159
   - 内容: 数据库系统基础理论，包括数据模型、查询处理、事务管理
   - 适合: 学生和研究人员
   - 重要性: ⭐⭐⭐⭐⭐ 数据库系统经典教材

2. **《数据库系统实现》（Database System Implementation）**
   - 作者: Garcia-Molina, H., Ullman, J. D., & Widom, J.
   - 出版社: Prentice Hall
   - 出版年份: 2008
   - ISBN: 978-0130402648
   - 内容: 数据库系统实现技术，包括存储管理、查询处理、事务处理
   - 适合: 高级开发者和研究人员
   - 重要性: ⭐⭐⭐⭐⭐ 数据库系统实现权威参考

### 8.6 视频教程

#### **8.6.1 ProvSQL相关视频**

1. **ProvSQL演示视频**
   - 平台: YouTube / 会议视频
   - 内容: ProvSQL使用演示和教程
   - 链接: [ProvSQL GitHub](https://github.com/PierreSenellart/provsql) (查看README中的演示链接)
   - 时长: 30-60分钟
   - 适合: 初学者和开发者

2. **SIGMOD 2018 - ProvSQL演示**
   - 平台: ACM SIGMOD会议视频
   - 内容: ProvSQL在SIGMOD 2018的演示和论文介绍
   - 适合: 研究人员和高级开发者
   - 重要性: ⭐⭐⭐⭐⭐ ProvSQL官方演示

#### **8.6.2 概率数据库课程**

1. **不确定性数据处理课程**
   - 平台: Coursera / edX
   - 课程名称: "Uncertainty in Data Management"
   - 内容: 不确定性数据处理课程，包括概率数据库、不确定性数据模型、概率查询处理
   - 时长: 6-8周
   - 适合: 学生和研究人员
   - 重要性: ⭐⭐⭐⭐ 系统性学习不确定性数据处理

2. **概率数据库理论课程**
   - 平台: 大学在线课程
   - 内容: 概率数据库理论基础，包括可能世界语义、概率查询处理、算法实现
   - 适合: 研究人员
   - 重要性: ⭐⭐⭐⭐ 概率数据库理论深入学习

#### **8.6.3 PostgreSQL扩展开发教程**

1. **PostgreSQL扩展开发实践**
   - 平台: YouTube / 在线课程
   - 内容: PostgreSQL扩展开发实践，包括数据类型扩展、函数扩展、操作符扩展
   - 时长: 2-4小时
   - 适合: 开发者
   - 重要性: ⭐⭐⭐⭐ PostgreSQL扩展开发实用教程

2. **PostgreSQL高级特性教程**
   - 平台: PostgreSQL官方视频 / 社区教程
   - 内容: PostgreSQL高级特性，包括扩展系统、自定义类型、查询优化
   - 适合: 高级开发者
   - 重要性: ⭐⭐⭐⭐ PostgreSQL高级特性学习

#### **8.6.4 数据溯源相关视频**

1. **数据溯源技术介绍**
   - 平台: YouTube / 技术会议
   - 内容: 数据溯源技术介绍，包括溯源模型、查询处理、应用场景
   - 适合: 初学者和开发者
   - 重要性: ⭐⭐⭐ 数据溯源入门

2. **数据治理和合规视频**
   - 平台: 企业培训平台 / 在线课程
   - 内容: 数据治理和合规实践，包括数据溯源、数据质量、合规要求
   - 适合: 企业用户和数据治理专家
   - 重要性: ⭐⭐⭐⭐ 数据治理实践指南

#### **8.6.5 在线学习平台推荐**

1. **Coursera**
   - 链接: [https://www.coursera.org](https://www.coursera.org)
   - 内容: 数据库系统、数据管理相关课程
   - 特点: 系统性学习，有证书

2. **edX**
   - 链接: [https://www.edx.org](https://www.edx.org)
   - 内容: 数据库系统、数据科学相关课程
   - 特点: 大学课程，高质量内容

3. **YouTube**
   - 内容: 技术教程、会议视频、演示视频
   - 特点: 免费，内容丰富

4. **PostgreSQL官方视频**
   - 链接: [PostgreSQL YouTube频道](https://www.youtube.com/user/PostgreSQLChannel)
   - 内容: PostgreSQL官方教程、会议视频
   - 特点: 官方内容，权威性强

### 8.7 技术博客和案例

#### **8.7.1 应用案例**

1. **传感器数据不确定性处理**
   - **场景**: IoT传感器数据的不确定性管理
   - **技术**: 概率数据库、数据溯源
   - **案例**: 环境监测、智能城市
   - **详细描述**:
     - 环境监测系统使用概率数据库处理传感器数据的不确定性
     - 智能城市项目使用概率查询优化决策支持系统
     - 工业4.0场景中使用概率数据库进行设备状态预测
   - **参考链接**:
     - [IoT数据不确定性处理案例研究](https://example.com/iot-probabilistic)
     - [智能城市数据管理最佳实践](https://example.com/smart-city)

2. **数据融合场景**
   - **场景**: 多源数据融合的不一致性处理
   - **技术**: 概率查询、数据溯源
   - **案例**: 数据仓库、数据湖
   - **详细描述**:
     - 企业数据仓库使用概率数据库融合多个数据源
     - 数据湖项目使用概率查询处理异构数据
     - 数据集成平台使用概率计算评估数据质量
   - **参考链接**:
     - [数据融合最佳实践](https://example.com/data-fusion)
     - [数据仓库概率查询应用](https://example.com/data-warehouse)

3. **数据溯源应用**
   - **场景**: 数据来源追踪和可信度评估
   - **技术**: ProvSQL、概率计算
   - **案例**: 数据治理、合规审计
   - **详细描述**:
     - 金融机构使用数据溯源满足监管要求
     - 医疗系统使用概率数据库追踪数据来源
     - 企业数据治理平台使用ProvSQL进行数据审计
   - **参考链接**:
     - [数据治理实践案例](https://example.com/data-governance)
     - [合规审计数据溯源应用](https://example.com/compliance-audit)

4. **数据质量评估**
   - **场景**: 数据质量评估和监控
   - **技术**: 概率数据库、数据质量指标
   - **案例**: 数据质量平台、数据清洗系统
   - **详细描述**:
     - 数据质量平台使用概率值评估数据可信度
     - 数据清洗系统使用概率查询识别低质量数据
     - 数据监控系统使用概率计算实时评估数据质量
   - **参考链接**:
     - [数据质量评估最佳实践](https://example.com/data-quality)
     - [数据清洗概率方法](https://example.com/data-cleaning)

#### **8.7.2 最佳实践**

1. **概率数据库设计最佳实践**
   - **内容**: 概率值存储、查询优化、性能调优
   - **详细建议**:
     - 使用NUMERIC类型存储概率值，确保精度
     - 为概率值创建索引，加速查询
     - 使用概率阈值提前过滤，减少计算量
     - 合理设计概率值范围，避免过度存储
   - **参考链接**:
     - [概率数据库设计指南](https://example.com/probabilistic-design)
     - [概率查询优化技巧](https://example.com/query-optimization)

2. **ProvSQL集成最佳实践**
   - **内容**: ProvSQL安装、配置、使用技巧
   - **详细建议**:
     - 从源码编译安装，确保版本兼容性
     - 合理配置概率计算模式，平衡性能和精度
     - 使用WITH PROVENANCE语法启用溯源追踪
     - 定期检查ProvSQL扩展状态，确保正常运行
   - **参考链接**:
     - [ProvSQL集成指南](https://example.com/provsql-integration)
     - [ProvSQL配置最佳实践](https://example.com/provsql-config)

3. **不确定性数据处理最佳实践**
   - **内容**: 不确定性数据建模、查询处理、结果解释
   - **详细建议**:
     - 根据业务需求选择合适的概率表示方式
     - 使用概率阈值过滤低概率数据
     - 提供概率值的解释和说明
     - 定期评估和更新概率值
   - **参考链接**:
     - [不确定性数据建模指南](https://example.com/uncertainty-modeling)
     - [概率查询结果解释](https://example.com/probability-interpretation)

#### **8.7.3 技术博客推荐**

1. **PostgreSQL官方博客**
   - 链接: [https://www.postgresql.org/about/newsarchive/](https://www.postgresql.org/about/newsarchive/)
   - 内容: PostgreSQL最新动态、技术文章、最佳实践
   - 更新频率: 定期更新
   - 重要性: ⭐⭐⭐⭐⭐ PostgreSQL官方内容

2. **数据库技术博客**
   - 内容: 数据库技术文章、性能优化、最佳实践
   - 推荐博客:
     - [Planet PostgreSQL](https://planet.postgresql.org/) - PostgreSQL社区博客聚合
     - [PostgreSQL Weekly](https://postgresweekly.com/) - PostgreSQL周报
   - 重要性: ⭐⭐⭐⭐ 社区技术内容

3. **数据科学博客**
   - 内容: 数据科学、不确定性数据处理、概率方法
   - 推荐博客:
     - [Towards Data Science](https://towardsdatascience.com/) - 数据科学文章
     - [KDnuggets](https://www.kdnuggets.com/) - 数据挖掘和机器学习
   - 重要性: ⭐⭐⭐⭐ 数据科学相关内容

#### **8.7.4 实际项目案例**

1. **开源项目案例**
   - **MystiQ项目**
     - 描述: 概率查询处理引擎
     - 链接: [https://github.com/dan-suciu/mystiq](https://github.com/dan-suciu/mystiq)
     - 技术栈: C++
     - 应用场景: 概率查询处理、不确定性数据管理

   - **ProvSQL项目**
     - 描述: PostgreSQL的溯源和概率管理扩展
     - 链接: [https://github.com/PierreSenellart/provsql](https://github.com/PierreSenellart/provsql)
     - 技术栈: C/C++
     - 应用场景: 数据溯源、概率计算

2. **企业应用案例**
   - **金融行业**
     - 应用: 风险评估、数据质量评估
     - 技术: 概率数据库、数据溯源
     - 案例: 银行数据治理、保险风险评估

   - **医疗行业**
     - 应用: 医疗数据不确定性处理、数据溯源
     - 技术: 概率数据库、ProvSQL
     - 案例: 医疗数据质量评估、数据合规审计

   - **IoT行业**
     - 应用: 传感器数据不确定性处理
     - 技术: 概率数据库、实时查询
     - 案例: 环境监测、智能城市、工业4.0

### 8.8 研究机构和实验室

#### **8.8.1 主要研究机构**

1. **University of Washington - Database Group**
   - **链接**: [https://db.cs.washington.edu/](https://db.cs.washington.edu/)
   - **研究方向**: 概率数据库、数据溯源、不确定性数据处理
   - **主要贡献**:
     - MystiQ项目：概率查询处理引擎
     - 概率数据库基础理论研究
     - 不确定性数据管理算法
   - **重要人物**: Dan Suciu, Nilesh Dalvi
   - **重要论文**:
     - "Foundations of Probabilistic Databases" (VLDB Journal 2011)
     - "Efficient Query Evaluation on Probabilistic Databases" (VLDB 2007)
   - **重要性**: ⭐⭐⭐⭐⭐ 概率数据库领域的领先研究机构

2. **University of Pennsylvania - Database Group**
   - **研究方向**: 概率数据库理论、不确定性数据模型
   - **主要贡献**:
     - 概率数据库基础理论
     - 可能世界语义研究
     - 概率查询处理算法
   - **重要人物**: Dan Suciu, Val Tannen
   - **重要论文**:
     - "Probabilistic Databases: The Diamonds and the Rough" (CIDR 2011)
   - **重要性**: ⭐⭐⭐⭐⭐ 概率数据库理论研究的先驱

3. **École normale supérieure (ENS) - Database Group**
   - **研究方向**: ProvSQL、数据溯源、概率计算
   - **主要贡献**:
     - ProvSQL项目：PostgreSQL的溯源和概率管理扩展
     - 数据溯源理论研究
     - 概率计算引擎开发
   - **重要人物**: Pierre Senellart
   - **重要论文**:
     - "ProvSQL: Provenance and Probability Management in PostgreSQL" (SIGMOD 2018)
   - **重要性**: ⭐⭐⭐⭐⭐ ProvSQL项目的开发机构

#### **8.8.2 其他重要研究机构**

1. **Stanford University - Database Group**
   - **研究方向**: 数据库系统、数据管理、不确定性数据处理
   - **主要贡献**:
     - 数据库系统理论研究
     - 数据管理技术
   - **重要性**: ⭐⭐⭐⭐ 数据库系统研究的领先机构

2. **MIT CSAIL - Database Group**
   - **研究方向**: 数据库系统、数据管理、数据科学
   - **主要贡献**:
     - 数据库系统创新研究
     - 数据管理技术
   - **重要性**: ⭐⭐⭐⭐ 数据库系统研究的重要机构

3. **Microsoft Research - Database Group**
   - **研究方向**: 数据库系统、数据管理、云数据库
   - **主要贡献**:
     - SQL Server相关研究
     - 云数据库技术
   - **重要性**: ⭐⭐⭐⭐ 企业数据库研究的重要机构

#### **8.8.3 研究实验室**

1. **UW Database Lab**
   - **机构**: University of Washington
   - **研究方向**: 概率数据库、数据溯源、不确定性数据处理
   - **项目**: MystiQ, Probabilistic Database Systems
   - **重要性**: ⭐⭐⭐⭐⭐ 概率数据库研究的核心实验室

2. **ENS Database Lab**
   - **机构**: École normale supérieure
   - **研究方向**: ProvSQL、数据溯源、概率计算
   - **项目**: ProvSQL, Data Provenance Systems
   - **重要性**: ⭐⭐⭐⭐⭐ ProvSQL项目的核心实验室

3. **UPenn Database Lab**
   - **机构**: University of Pennsylvania
   - **研究方向**: 概率数据库理论、不确定性数据模型
   - **项目**: Probabilistic Database Theory
   - **重要性**: ⭐⭐⭐⭐⭐ 概率数据库理论研究的重要实验室

#### **8.8.4 研究项目和实验室链接**

1. **MystiQ项目**
   - GitHub: [https://github.com/dan-suciu/mystiq](https://github.com/dan-suciu/mystiq)
   - 机构: University of Washington
   - 状态: ✅ 活跃维护

2. **ProvSQL项目**
   - GitHub: [https://github.com/PierreSenellart/provsql](https://github.com/PierreSenellart/provsql)
   - 机构: École normale supérieure
   - 状态: ✅ 活跃维护

3. **概率数据库研究资源**
   - 研究论文: [DBLP - Probabilistic Databases](https://dblp.org/search?q=probabilistic+database)
   - 会议: VLDB, SIGMOD, ICDE, PODS
   - 重要性: ⭐⭐⭐⭐⭐ 概率数据库研究资源

### 8.9 参考资源使用建议

#### **8.9.1 不同角色的学习路径**

**📚 初学者学习路径**：

1. **第一步：理解基础概念**
   - 阅读本文档的"1. 概述"和"2. 技术原理"章节
   - 理解概率数据库的基本概念和原理
   - 了解不确定性数据的来源和处理方法

2. **第二步：学习理论基础**
   - 阅读"8.1 学术论文"中的基础理论论文
   - 理解可能世界语义和概率查询处理
   - 学习概率数据库的数学基础

3. **第三步：实践操作**
   - 按照"4. ProvSQL集成"章节安装和配置ProvSQL
   - 运行"9. 完整代码示例"中的示例代码
   - 尝试实现简单的概率查询

4. **推荐资源**：
   - 本文档的"1. 概述"和"2. 技术原理"章节
   - "8.1.1 概率数据库基础理论"中的经典论文
   - "8.2 官方文档"中的PostgreSQL和ProvSQL文档
   - "8.6 视频教程"中的入门教程

**🔧 开发者学习路径**：

1. **第一步：快速上手**
   - 阅读"4. ProvSQL集成"章节，了解ProvSQL的安装和配置
   - 参考"9. 完整代码示例"，快速实现基础功能
   - 理解ProvSQL的API和接口

2. **第二步：深入学习**
   - 阅读"3. PostgreSQL实现方案"，了解扩展开发
   - 学习"3.3 查询处理扩展"，实现自定义功能
   - 参考"7. 最佳实践"，遵循最佳实践

3. **第三步：性能优化**
   - 阅读"6. 性能分析"，了解性能影响因素
   - 学习"6.4 性能优化建议"，优化查询性能
   - 参考"7.3 性能优化"，应用优化技巧

4. **推荐资源**：
   - "4. ProvSQL集成"章节
   - "9. 完整代码示例"章节
   - "8.2.1 PostgreSQL文档"中的扩展开发文档
   - "8.2.2 ProvSQL文档"中的API文档
   - "8.6.3 PostgreSQL扩展开发教程"

**🏢 企业用户学习路径**：

1. **第一步：了解应用场景**
   - 阅读"1.3.2 应用场景"，了解适用场景
   - 阅读"5. 实际应用案例"，了解实际应用
   - 阅读"7.1 使用场景"，了解适用和不适用场景

2. **第二步：评估和规划**
   - 阅读"7.6 实施建议"，制定实施计划
   - 参考"6. 性能分析"，评估性能影响
   - 阅读"7.4 常见问题和解决方案"，了解常见问题

3. **第三步：实施和优化**
   - 按照"7.6.1 渐进式实施"逐步实施
   - 参考"7.2 设计原则"，设计数据模型
   - 应用"7.3 性能优化"，优化性能

4. **推荐资源**：
   - "5. 实际应用案例"章节
   - "7. 最佳实践"章节
   - "8.7 技术博客和案例"中的企业应用案例
   - "8.5 书籍推荐"中的数据治理相关书籍

**🔬 研究者学习路径**：

1. **第一步：理论基础**
   - 阅读"8.1 学术论文"中的所有论文
   - 深入理解概率数据库的理论基础
   - 学习可能世界语义和概率查询处理算法

2. **第二步：研究前沿**
   - 关注"8.8 研究机构和实验室"的最新研究
   - 阅读最新的学术论文
   - 了解概率数据库的最新进展

3. **第三步：实践研究**
   - 使用"8.3 开源项目和工具"进行研究
   - 参考"8.8 研究机构和实验室"的研究方法
   - 贡献开源项目或发表研究成果

4. **推荐资源**：
   - "8.1 学术论文"中的所有论文
   - "8.8 研究机构和实验室"的研究资源
   - "8.5.1 概率数据库相关书籍"中的理论书籍
   - "8.5.4 数据库理论相关书籍"中的经典教材

#### **8.9.2 学习资源优先级**

**高优先级资源**（必须阅读）：

1. 本文档的"1. 概述"和"2. 技术原理"章节
2. "4. ProvSQL集成"章节
3. "9. 完整代码示例"章节
4. ProvSQL官方文档

**中优先级资源**（推荐阅读）：

1. "5. 实际应用案例"章节
2. "7. 最佳实践"章节
3. "6. 性能分析"章节
4. "8.1.1 概率数据库基础理论"中的经典论文

**低优先级资源**（可选阅读）：

1. "8.5 书籍推荐"中的相关书籍
2. "8.6 视频教程"中的视频
3. "8.7 技术博客和案例"中的博客文章
4. "8.8 研究机构和实验室"的研究资源

#### **8.9.3 学习时间规划**

**初学者**：

- 第1周：阅读基础概念和理论（"1. 概述"和"2. 技术原理"）
- 第2周：安装和配置ProvSQL（"4. ProvSQL集成"）
- 第3周：实践代码示例（"9. 完整代码示例"）
- 第4周：深入学习理论（"8.1 学术论文"）

**开发者**：

- 第1周：快速上手ProvSQL（"4. ProvSQL集成"和"9. 完整代码示例"）
- 第2周：学习扩展开发（"3. PostgreSQL实现方案"）
- 第3周：性能优化（"6. 性能分析"和"7.3 性能优化"）
- 第4周：最佳实践（"7. 最佳实践"）

**企业用户**：

- 第1周：了解应用场景（"1.3.2 应用场景"和"5. 实际应用案例"）
- 第2周：评估和规划（"7.6 实施建议"和"6. 性能分析"）
- 第3周：实施准备（"7.2 设计原则"和"7.4 常见问题和解决方案"）
- 第4周：开始实施（"7.6.1 渐进式实施"）

**研究者**：

- 第1-2周：理论基础（"8.1 学术论文"和"8.5.1 概率数据库相关书籍"）
- 第3-4周：研究前沿（"8.8 研究机构和实验室"的最新研究）
- 第5-6周：实践研究（"8.3 开源项目和工具"）

#### **8.9.4 学习建议**

1. **循序渐进**：按照学习路径逐步学习，不要跳跃
2. **理论与实践结合**：理论学习后立即实践，加深理解
3. **多参考资源**：不要只依赖单一资源，多参考不同资源
4. **持续学习**：关注最新进展，持续更新知识
5. **实践为主**：多写代码，多实践，多总结
6. **社区参与**：参与社区讨论，分享经验，获取帮助

---

## 9. 完整代码示例

### 9.1 概率数据类型实现

本节提供完整的概率数据类型实现代码，包括类型定义、表创建、数据插入、查询和操作函数。

#### **9.1.1 创建概率数据类型**

**概率值类型定义**：

```sql
-- 创建概率值类型（带错误处理）
DO $$
BEGIN
    BEGIN
        IF EXISTS (SELECT 1 FROM pg_type WHERE typname = 'probability_value') THEN
            RAISE NOTICE '类型 probability_value 已存在';
        ELSE
            CREATE TYPE probability_value AS (
                value NUMERIC,
                probability NUMERIC
            );
            RAISE NOTICE '类型 probability_value 已创建';
        END IF;
    EXCEPTION
        WHEN duplicate_object THEN
            RAISE WARNING '类型 probability_value 已存在';
        WHEN OTHERS THEN
            RAISE WARNING '创建类型 probability_value 失败: %', SQLERRM;
            RAISE;
    END;
END $$;
```

**类型说明**：

- **value**：实际数据值（NUMERIC类型）
- **probability**：该值的概率/置信度（NUMERIC类型，范围[0, 1]）

#### **9.1.2 创建概率数据表**

**传感器数据表示例**：

```sql
-- 创建概率表（带错误处理）
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'probability_value') THEN
            RAISE WARNING '类型 probability_value 不存在，请先创建该类型';
            RETURN;
        END IF;

        IF EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'sensor_readings') THEN
            RAISE NOTICE '表 sensor_readings 已存在';
        ELSE
            CREATE TABLE sensor_readings (
                id SERIAL PRIMARY KEY,
                sensor_id INTEGER NOT NULL,
                temperature probability_value NOT NULL,
                humidity probability_value NOT NULL,
                timestamp TIMESTAMP DEFAULT NOW(),
                CONSTRAINT check_probability_range CHECK (
                    (temperature).probability >= 0 AND (temperature).probability <= 1 AND
                    (humidity).probability >= 0 AND (humidity).probability <= 1
                )
            );
            RAISE NOTICE '表 sensor_readings 已创建';
        END IF;
    EXCEPTION
        WHEN duplicate_table THEN
            RAISE WARNING '表 sensor_readings 已存在';
        WHEN undefined_object THEN
            RAISE WARNING '类型 probability_value 不存在';
        WHEN OTHERS THEN
            RAISE WARNING '创建表失败: %', SQLERRM;
            RAISE;
    END;
END $$;

-- 创建索引
CREATE INDEX IF NOT EXISTS idx_sensor_id ON sensor_readings(sensor_id);
CREATE INDEX IF NOT EXISTS idx_timestamp ON sensor_readings(timestamp);
CREATE INDEX IF NOT EXISTS idx_temperature_prob ON sensor_readings(((temperature).probability));
CREATE INDEX IF NOT EXISTS idx_humidity_prob ON sensor_readings(((humidity).probability));
```

#### **9.1.3 插入概率数据**

**单条数据插入**：

```sql
-- 插入概率数据（带错误处理）
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'sensor_readings') THEN
            RAISE WARNING '表 sensor_readings 不存在，无法插入数据';
            RETURN;
        END IF;

        IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'probability_value') THEN
            RAISE WARNING '类型 probability_value 不存在，无法插入数据';
            RETURN;
        END IF;

        INSERT INTO sensor_readings (sensor_id, temperature, humidity, timestamp)
        VALUES (
            1,
            ROW(25.5, 0.95)::probability_value,  -- 温度25.5度，置信度95%
            ROW(60.0, 0.90)::probability_value,  -- 湿度60%，置信度90%
            NOW()
        );

        RAISE NOTICE '数据插入成功';
    EXCEPTION
        WHEN check_violation THEN
            RAISE WARNING '概率值超出范围[0, 1]';
        WHEN OTHERS THEN
            RAISE WARNING '插入数据失败: %', SQLERRM;
            RAISE;
    END;
END $$;
```

**批量数据插入**：

```sql
-- 批量插入概率数据
INSERT INTO sensor_readings (sensor_id, temperature, humidity, timestamp)
SELECT
    generate_series(1, 100) AS sensor_id,
    ROW(
        20 + random() * 10,  -- 温度范围20-30度
        0.8 + random() * 0.2  -- 置信度范围0.8-1.0
    )::probability_value AS temperature,
    ROW(
        50 + random() * 20,  -- 湿度范围50-70%
        0.8 + random() * 0.2  -- 置信度范围0.8-1.0
    )::probability_value AS humidity,
    NOW() - (random() * INTERVAL '7 days') AS timestamp;
```

#### **9.1.4 查询概率数据**

**基础查询**：

```sql
-- 查询所有概率数据
SELECT
    id,
    sensor_id,
    (temperature).value AS temp_value,
    (temperature).probability AS temp_prob,
    (humidity).value AS hum_value,
    (humidity).probability AS hum_prob,
    timestamp
FROM sensor_readings
ORDER BY timestamp DESC;

-- 查询高置信度数据（概率>0.9）
SELECT
    id,
    sensor_id,
    (temperature).value AS temp_value,
    (temperature).probability AS temp_prob,
    (humidity).value AS hum_value,
    (humidity).probability AS hum_prob
FROM sensor_readings
WHERE (temperature).probability > 0.9 AND (humidity).probability > 0.9
ORDER BY (temperature).probability DESC, (humidity).probability DESC;
```

**概率聚合查询**：

```sql
-- 按传感器ID聚合，计算加权平均值
SELECT
    sensor_id,
    COUNT(*) AS total_readings,
    AVG((temperature).value) AS avg_temperature,
    SUM((temperature).value * (temperature).probability) /
        SUM((temperature).probability) AS weighted_avg_temperature,
    AVG((humidity).value) AS avg_humidity,
    SUM((humidity).value * (humidity).probability) /
        SUM((humidity).probability) AS weighted_avg_humidity,
    AVG((temperature).probability) AS avg_temp_confidence,
    AVG((humidity).probability) AS avg_hum_confidence
FROM sensor_readings
GROUP BY sensor_id
ORDER BY sensor_id;
```

#### **9.1.5 概率操作函数**

**概率值操作函数**：

```sql
-- 计算概率值的期望值
CREATE OR REPLACE FUNCTION expected_value(pv probability_value)
RETURNS NUMERIC AS $$
BEGIN
    RETURN (pv).value * (pv).probability;
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- 计算两个概率值的联合概率
CREATE OR REPLACE FUNCTION joint_probability(
    pv1 probability_value,
    pv2 probability_value
) RETURNS NUMERIC AS $$
BEGIN
    RETURN (pv1).probability * (pv2).probability;
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- 概率值比较函数（按期望值）
CREATE OR REPLACE FUNCTION compare_by_expected_value(
    pv1 probability_value,
    pv2 probability_value
) RETURNS INTEGER AS $$
BEGIN
    IF expected_value(pv1) > expected_value(pv2) THEN
        RETURN 1;
    ELSIF expected_value(pv1) < expected_value(pv2) THEN
        RETURN -1;
    ELSE
        RETURN 0;
    END IF;
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- 使用示例
SELECT
    sensor_id,
    expected_value(temperature) AS temp_expected_value,
    expected_value(humidity) AS hum_expected_value,
    joint_probability(temperature, humidity) AS joint_prob
FROM sensor_readings
WHERE sensor_id = 1
ORDER BY expected_value(temperature) DESC;
```

**概率过滤函数**：

```sql
-- 概率阈值过滤函数
CREATE OR REPLACE FUNCTION filter_by_probability(
    pv probability_value,
    min_prob NUMERIC DEFAULT 0.5
) RETURNS BOOLEAN AS $$
BEGIN
    RETURN (pv).probability >= min_prob;
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- 使用示例
SELECT
    id,
    sensor_id,
    (temperature).value AS temp_value,
    (temperature).probability AS temp_prob
FROM sensor_readings
WHERE filter_by_probability(temperature, 0.9)
ORDER BY (temperature).probability DESC;
```

#### **9.1.6 概率数据更新**

**更新概率值**：

```sql
-- 更新概率值（带验证）
CREATE OR REPLACE FUNCTION update_probability_value(
    table_name TEXT,
    record_id INTEGER,
    column_name TEXT,
    new_value NUMERIC,
    new_probability NUMERIC
) RETURNS BOOLEAN AS $$
DECLARE
    sql_text TEXT;
BEGIN
    -- 验证概率值范围
    IF new_probability < 0 OR new_probability > 1 THEN
        RAISE EXCEPTION '概率值必须在[0, 1]范围内';
    END IF;

    -- 构建更新SQL
    sql_text := format(
        'UPDATE %I SET %I = ROW(%s, %s)::probability_value WHERE id = %s',
        table_name,
        column_name,
        new_value,
        new_probability,
        record_id
    );

    EXECUTE sql_text;

    RETURN TRUE;
EXCEPTION
    WHEN OTHERS THEN
        RAISE WARNING '更新失败: %', SQLERRM;
        RETURN FALSE;
END;
$$ LANGUAGE plpgsql;

-- 使用示例
SELECT update_probability_value('sensor_readings', 1, 'temperature', 26.0, 0.95);
```

#### **9.1.7 概率数据验证**

**数据质量检查函数**：

```sql
-- 检查概率数据质量
CREATE OR REPLACE FUNCTION check_probability_data_quality()
RETURNS TABLE (
    check_name TEXT,
    status TEXT,
    issue_count BIGINT,
    details TEXT
) AS $$
BEGIN
    -- 检查概率值范围
    RETURN QUERY
    SELECT
        'Probability Range Check'::TEXT,
        CASE
            WHEN COUNT(*) FILTER (WHERE (temperature).probability < 0 OR (temperature).probability > 1 OR
                                         (humidity).probability < 0 OR (humidity).probability > 1) > 0
            THEN 'FAIL'
            ELSE 'PASS'
        END,
        COUNT(*) FILTER (WHERE (temperature).probability < 0 OR (temperature).probability > 1 OR
                                (humidity).probability < 0 OR (humidity).probability > 1),
        format('Found %s records with invalid probability values',
               COUNT(*) FILTER (WHERE (temperature).probability < 0 OR (temperature).probability > 1 OR
                                       (humidity).probability < 0 OR (humidity).probability > 1))
    FROM sensor_readings;

    -- 检查NULL值
    RETURN QUERY
    SELECT
        'NULL Value Check'::TEXT,
        CASE
            WHEN COUNT(*) FILTER (WHERE temperature IS NULL OR humidity IS NULL) > 0
            THEN 'WARNING'
            ELSE 'PASS'
        END,
        COUNT(*) FILTER (WHERE temperature IS NULL OR humidity IS NULL),
        format('Found %s records with NULL probability values',
               COUNT(*) FILTER (WHERE temperature IS NULL OR humidity IS NULL))
    FROM sensor_readings;
END;
$$ LANGUAGE plpgsql;

-- 使用示例
SELECT * FROM check_probability_data_quality();
```

#### **9.1.8 完整使用示例**

**完整的工作流程**：

```sql
-- 1. 创建类型和表
-- （使用上面的代码）

-- 2. 插入测试数据
INSERT INTO sensor_readings (sensor_id, temperature, humidity, timestamp)
VALUES
    (1, ROW(25.5, 0.95)::probability_value, ROW(60.0, 0.90)::probability_value, NOW()),
    (1, ROW(26.0, 0.88)::probability_value, ROW(61.0, 0.85)::probability_value, NOW()),
    (2, ROW(24.5, 0.92)::probability_value, ROW(58.0, 0.88)::probability_value, NOW());

-- 3. 查询高置信度数据
SELECT
    sensor_id,
    (temperature).value AS temp,
    (temperature).probability AS temp_conf,
    (humidity).value AS hum,
    (humidity).probability AS hum_conf
FROM sensor_readings
WHERE (temperature).probability > 0.9 AND (humidity).probability > 0.85
ORDER BY (temperature).probability DESC;

-- 4. 概率聚合分析
SELECT
    sensor_id,
    COUNT(*) AS readings,
    AVG((temperature).value) AS avg_temp,
    SUM((temperature).value * (temperature).probability) /
        SUM((temperature).probability) AS weighted_avg_temp,
    AVG((temperature).probability) AS avg_confidence
FROM sensor_readings
GROUP BY sensor_id;

-- 5. 数据质量检查
SELECT * FROM check_probability_data_quality();
```

```

### 9.2 Python 概率数据库客户端

**完整的概率数据库操作类（带错误处理、连接池、事务管理）**：

```python
# probabilistic_client.py
import psycopg2
from psycopg2 import pool
from psycopg2.extras import RealDictCursor
from typing import List, Tuple, Dict, Optional
from dataclasses import dataclass
from contextlib import contextmanager
import logging
from datetime import datetime

# 配置日志
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ProbabilityValue:
    """概率值数据类"""
    value: float
    probability: float

    def __post_init__(self):
        """验证概率值范围"""
        if not 0 <= self.probability <= 1:
            raise ValueError(f"概率值必须在[0, 1]范围内，当前值: {self.probability}")

class ProbabilisticDBClient:
    """概率数据库客户端（完整版）"""

    def __init__(self, conn_str: str, min_conn: int = 1, max_conn: int = 10):
        """
        初始化概率数据库客户端

        Args:
            conn_str: PostgreSQL连接字符串
            min_conn: 连接池最小连接数
            max_conn: 连接池最大连接数
        """
        try:
            self.conn_pool = psycopg2.pool.ThreadedConnectionPool(
                min_conn, max_conn, conn_str
            )
            logger.info("数据库连接池创建成功")
        except Exception as e:
            logger.error(f"创建连接池失败: {e}")
            raise

    @contextmanager
    def get_connection(self):
        """获取数据库连接的上下文管理器"""
        conn = None
        try:
            conn = self.conn_pool.getconn()
            yield conn
            conn.commit()
        except Exception as e:
            if conn:
                conn.rollback()
            logger.error(f"数据库操作失败: {e}")
            raise
        finally:
            if conn:
                self.conn_pool.putconn(conn)

    def insert_probabilistic_data(
        self,
        table_name: str,
        data: Dict,
        batch_size: int = 1000
    ) -> int:
        """
        插入概率数据（支持批量插入）

        Args:
            table_name: 表名
            data: 数据字典或数据列表
            batch_size: 批量插入大小

        Returns:
            插入的行数
        """
        if isinstance(data, dict):
            data = [data]

        inserted_count = 0

        with self.get_connection() as conn:
            with conn.cursor() as cur:
                try:
                    # 批量插入
                    for i in range(0, len(data), batch_size):
                        batch = data[i:i + batch_size]
                        placeholders = []
                        values_list = []

                        for row in batch:
                            cols = []
                            vals = []
                            for col, val in row.items():
                                if isinstance(val, ProbabilityValue):
                                    cols.append(col)
                                    vals.append(f"ROW(%s, %s)::probability_value")
                                    placeholders.append((val.value, val.probability))
                                elif col == 'timestamp' and val == 'NOW()':
                                    cols.append(col)
                                    vals.append('NOW()')
                                else:
                                    cols.append(col)
                                    vals.append('%s')
                                    placeholders.append(val)

                            columns_str = ', '.join(cols)
                            values_str = ', '.join(vals)

                            query = f"""
                                INSERT INTO {table_name} ({columns_str})
                                VALUES ({values_str})
                            """

                            cur.execute(query, tuple(placeholders))
                            inserted_count += 1

                        conn.commit()
                        logger.info(f"批量插入 {len(batch)} 条数据成功")

                except psycopg2.Error as e:
                    logger.error(f"插入数据失败: {e}")
                    conn.rollback()
                    raise

        return inserted_count

    def query_with_confidence(
        self,
        query: str,
        min_confidence: float = 0.8,
        params: Optional[Tuple] = None
    ) -> List[Dict]:
        """
        执行带置信度过滤的查询

        Args:
            query: SQL查询语句
            min_confidence: 最小置信度阈值
            params: 查询参数

        Returns:
            查询结果列表
        """
        if not 0 <= min_confidence <= 1:
            raise ValueError(f"置信度必须在[0, 1]范围内，当前值: {min_confidence}")

        filtered_query = f"""
            SELECT *
            FROM ({query}) AS subquery
            WHERE probability >= %s
        """

        with self.get_connection() as conn:
            with conn.cursor(cursor_factory=RealDictCursor) as cur:
                try:
                    if params:
                        cur.execute(filtered_query, params + (min_confidence,))
                    else:
                        cur.execute(filtered_query, (min_confidence,))

                    results = cur.fetchall()
                    logger.info(f"查询返回 {len(results)} 条结果")
                    return [dict(row) for row in results]

                except psycopg2.Error as e:
                    logger.error(f"查询失败: {e}")
                    raise

    def aggregate_probabilistic(
        self,
        table_name: str,
        column: str,
        filters: Optional[Dict] = None
    ) -> Dict:
        """
        概率聚合查询

        Args:
            table_name: 表名
            column: 列名（概率值列）
            filters: 过滤条件字典

        Returns:
            聚合结果字典
        """
        where_clause = ""
        params = []

        if filters:
            conditions = []
            for key, value in filters.items():
                conditions.append(f"{key} = %s")
                params.append(value)
            where_clause = "WHERE " + " AND ".join(conditions)

        query = f"""
            SELECT
                AVG(({column}).value) AS avg_value,
                AVG(({column}).probability) AS avg_confidence,
                MIN(({column}).value) AS min_value,
                MAX(({column}).value) AS max_value,
                COUNT(*) AS count
            FROM {table_name}
            {where_clause}
        """

        with self.get_connection() as conn:
            with conn.cursor(cursor_factory=RealDictCursor) as cur:
                try:
                    cur.execute(query, tuple(params))
                    result = cur.fetchone()
                    return dict(result) if result else {}

                except psycopg2.Error as e:
                    logger.error(f"聚合查询失败: {e}")
                    raise

    def update_probability(
        self,
        table_name: str,
        row_id: int,
        column: str,
        new_probability: float
    ) -> bool:
        """
        更新概率值

        Args:
            table_name: 表名
            row_id: 行ID
            column: 列名
            new_probability: 新的概率值

        Returns:
            是否更新成功
        """
        if not 0 <= new_probability <= 1:
            raise ValueError(f"概率值必须在[0, 1]范围内，当前值: {new_probability}")

        query = f"""
            UPDATE {table_name}
            SET {column} = ROW(({column}).value, %s)::probability_value
            WHERE id = %s
        """

        with self.get_connection() as conn:
            with conn.cursor() as cur:
                try:
                    cur.execute(query, (new_probability, row_id))
                    affected = cur.rowcount
                    logger.info(f"更新 {affected} 行数据")
                    return affected > 0

                except psycopg2.Error as e:
                    logger.error(f"更新概率值失败: {e}")
                    raise

    def batch_update_probability(
        self,
        table_name: str,
        updates: List[Tuple[int, str, float]]
    ) -> int:
        """
        批量更新概率值

        Args:
            table_name: 表名
            updates: 更新列表，每个元素为(row_id, column, new_probability)

        Returns:
            更新的行数
        """
        updated_count = 0

        with self.get_connection() as conn:
            with conn.cursor() as cur:
                try:
                    for row_id, column, new_probability in updates:
                        if not 0 <= new_probability <= 1:
                            logger.warning(f"跳过无效概率值: {new_probability}")
                            continue

                        query = f"""
                            UPDATE {table_name}
                            SET {column} = ROW(({column}).value, %s)::probability_value
                            WHERE id = %s
                        """

                        cur.execute(query, (new_probability, row_id))
                        updated_count += cur.rowcount

                    conn.commit()
                    logger.info(f"批量更新 {updated_count} 行数据")
                    return updated_count

                except psycopg2.Error as e:
                    logger.error(f"批量更新失败: {e}")
                    conn.rollback()
                    raise

    def close(self):
        """关闭连接池"""
        if self.conn_pool:
            self.conn_pool.closeall()
            logger.info("连接池已关闭")

# 使用示例
if __name__ == "__main__":
    # 初始化客户端
    client = ProbabilisticDBClient(
        "host=localhost dbname=testdb user=postgres password=secret",
        min_conn=2,
        max_conn=10
    )

    try:
        # 插入概率数据
        client.insert_probabilistic_data('sensor_readings', {
            'sensor_id': 1,
            'temperature': ProbabilityValue(25.5, 0.95),
            'humidity': ProbabilityValue(60.0, 0.90),
            'timestamp': 'NOW()'
        })

        # 批量插入
        batch_data = [
            {
                'sensor_id': i,
                'temperature': ProbabilityValue(25.0 + i * 0.1, 0.9),
                'humidity': ProbabilityValue(60.0 + i * 0.5, 0.85),
                'timestamp': 'NOW()'
            }
            for i in range(1, 11)
        ]
        client.insert_probabilistic_data('sensor_readings', batch_data)

        # 查询高置信度数据
        results = client.query_with_confidence("""
            SELECT * FROM sensor_readings
            WHERE sensor_id = 1
        """, min_confidence=0.85)

        print(f"查询结果: {len(results)} 条")

        # 概率聚合
        stats = client.aggregate_probabilistic(
            'sensor_readings',
            'temperature',
            filters={'sensor_id': 1}
        )
        print(f"统计结果: {stats}")

        # 更新概率值
        client.update_probability('sensor_readings', 1, 'temperature', 0.98)

        # 批量更新概率值
        updates = [
            (i, 'temperature', 0.95) for i in range(1, 6)
        ]
        client.batch_update_probability('sensor_readings', updates)

    finally:
        # 关闭连接池
        client.close()
```

### 9.3 ProvSQL 概率查询集成

#### **9.3.1 ProvSQL集成完整示例**

**使用 ProvSQL 进行概率查询**：

```sql
-- 启用 ProvSQL（带错误处理）
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM pg_extension WHERE extname = 'provsql') THEN
            CREATE EXTENSION provsql;
            RAISE NOTICE 'ProvSQL扩展已创建';
        ELSE
            RAISE NOTICE 'ProvSQL扩展已存在';
        END IF;
    EXCEPTION
        WHEN undefined_file THEN
            RAISE WARNING 'ProvSQL扩展文件不存在，请确保已正确安装';
        WHEN insufficient_privilege THEN
            RAISE WARNING '权限不足，无法创建扩展';
        WHEN OTHERS THEN
            RAISE WARNING '创建ProvSQL扩展失败: %', SQLERRM;
            RAISE;
    END;
END $$;

-- 创建概率表（带溯源）
CREATE TABLE IF NOT EXISTS uncertain_data (
    id SERIAL PRIMARY KEY,
    value NUMERIC NOT NULL,
    confidence NUMERIC CHECK (confidence >= 0 AND confidence <= 1),
    data_source VARCHAR(100),
    created_at TIMESTAMP DEFAULT NOW()
) WITH PROVENANCE;

-- 创建索引优化查询性能
CREATE INDEX IF NOT EXISTS idx_confidence ON uncertain_data(confidence);
CREATE INDEX IF NOT EXISTS idx_data_source ON uncertain_data(data_source);

-- 插入数据（带错误处理）
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'uncertain_data') THEN
            RAISE WARNING '表 uncertain_data 不存在，无法插入数据';
            RETURN;
        END IF;

        INSERT INTO uncertain_data (value, confidence, data_source)
        VALUES
            (10.5, 0.9, 'sensor_1'),
            (11.2, 0.85, 'sensor_2'),
            (9.8, 0.95, 'sensor_1'),
            (10.1, 0.88, 'sensor_3'),
            (11.5, 0.92, 'sensor_2');

        RAISE NOTICE '已插入5条概率数据';
    EXCEPTION
        WHEN check_violation THEN
            RAISE WARNING '概率值超出范围[0, 1]';
        WHEN OTHERS THEN
            RAISE WARNING '插入数据失败: %', SQLERRM;
            RAISE;
    END;
END $$;

-- 概率查询：计算加权平均值
SELECT
    data_source,
    COUNT(*) AS record_count,
    SUM(value * confidence) / SUM(confidence) AS weighted_avg,
    AVG(confidence) AS avg_confidence,
    MIN(confidence) AS min_confidence,
    MAX(confidence) AS max_confidence
FROM uncertain_data
GROUP BY data_source
ORDER BY avg_confidence DESC;

-- 使用 ProvSQL 溯源概率计算
SELECT
    provsql_provenance_of(
        SELECT SUM(value * confidence) / SUM(confidence)
        FROM uncertain_data
        WHERE confidence > 0.8
    ) AS provenance,
    SUM(value * confidence) / SUM(confidence) AS weighted_avg
FROM uncertain_data
WHERE confidence > 0.8;
```

#### **9.3.2 ProvSQL概率查询函数**

**创建概率查询函数**：

```sql
-- 概率加权平均函数
CREATE OR REPLACE FUNCTION provsql_weighted_avg(
    table_name TEXT,
    value_column TEXT,
    confidence_column TEXT,
    filter_condition TEXT DEFAULT 'TRUE'
) RETURNS TABLE (
    weighted_avg NUMERIC,
    avg_confidence NUMERIC,
    provenance JSONB
) AS $$
DECLARE
    query_text TEXT;
BEGIN
    query_text := format('
        SELECT
            SUM(%I * %I) / SUM(%I) AS weighted_avg,
            AVG(%I) AS avg_confidence,
            provsql_provenance_of(
                SELECT SUM(%I * %I) / SUM(%I)
                FROM %I
                WHERE %s
            )::JSONB AS provenance
        FROM %I
        WHERE %s
    ',
        value_column, confidence_column, confidence_column,
        confidence_column,
        value_column, confidence_column, confidence_column,
        table_name, filter_condition,
        table_name, filter_condition
    );

    RETURN QUERY EXECUTE query_text;
END;
$$ LANGUAGE plpgsql;

-- 使用概率查询函数
SELECT * FROM provsql_weighted_avg(
    'uncertain_data',
    'value',
    'confidence',
    'confidence > 0.8'
);
```

#### **9.3.3 ProvSQL概率查询Python集成**

**Python客户端集成ProvSQL**：

```python
# provsql_integration.py
from probabilistic_client import ProbabilisticDBClient
import json

class ProvSQLClient(ProbabilisticDBClient):
    """ProvSQL概率查询客户端"""

    def provsql_weighted_avg(
        self,
        table_name: str,
        value_column: str,
        confidence_column: str,
        filter_condition: str = 'TRUE'
    ) -> dict:
        """
        使用ProvSQL计算概率加权平均值

        Args:
            table_name: 表名
            value_column: 值列名
            confidence_column: 置信度列名
            filter_condition: 过滤条件

        Returns:
            包含加权平均值、平均置信度和溯源信息的字典
        """
        query = f"""
            SELECT * FROM provsql_weighted_avg(
                '{table_name}',
                '{value_column}',
                '{confidence_column}',
                '{filter_condition}'
            )
        """

        with self.get_connection() as conn:
            with conn.cursor(cursor_factory=RealDictCursor) as cur:
                cur.execute(query)
                result = cur.fetchone()

                if result:
                    return {
                        'weighted_avg': float(result['weighted_avg']),
                        'avg_confidence': float(result['avg_confidence']),
                        'provenance': json.loads(result['provenance']) if isinstance(result['provenance'], str) else result['provenance']
                    }
                return None

    def get_provenance(self, table_name: str, record_id: int) -> dict:
        """
        获取记录的溯源信息

        Args:
            table_name: 表名
            record_id: 记录ID

        Returns:
            溯源信息字典
        """
        query = f"""
            SELECT provsql_provenance_of(
                SELECT * FROM {table_name} WHERE id = %s
            ) AS provenance
        """

        with self.get_connection() as conn:
            with conn.cursor(cursor_factory=RealDictCursor) as cur:
                cur.execute(query, (record_id,))
                result = cur.fetchone()

                if result:
                    provenance = result['provenance']
                    return json.loads(provenance) if isinstance(provenance, str) else provenance
                return None

# 使用示例
if __name__ == '__main__':
    client = ProvSQLClient("host=localhost dbname=testdb user=postgres password=secret")

    # 计算概率加权平均值
    result = client.provsql_weighted_avg(
        'uncertain_data',
        'value',
        'confidence',
        'confidence > 0.8'
    )

    print(f"加权平均值: {result['weighted_avg']:.2f}")
    print(f"平均置信度: {result['avg_confidence']:.2%}")
    print(f"溯源信息: {json.dumps(result['provenance'], indent=2)}")

    # 获取溯源信息
    provenance = client.get_provenance('uncertain_data', 1)
    print(f"记录1的溯源信息: {json.dumps(provenance, indent=2)}")
```

### 9.4 Docker Compose 部署配置

#### **9.4.1 完整Docker Compose配置**

**docker-compose.yml**：

```yaml
version: '3.8'

services:
  postgresql:
    image: postgres:18
    container_name: probabilistic_postgres
    environment:
      POSTGRES_DB: testdb
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: secret
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --locale=C"
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init_probabilistic.sql:/docker-entrypoint-initdb.d/init.sql
      - ./provsql:/usr/local/share/postgresql/extension
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - probabilistic_network

  python-client:
    image: python:3.11
    container_name: probabilistic_client
    volumes:
      - ./client:/app
    working_dir: /app
    command: python probabilistic_client.py
    depends_on:
      postgresql:
        condition: service_healthy
    environment:
      DB_HOST: postgresql
      DB_PORT: 5432
      DB_NAME: testdb
      DB_USER: postgres
      DB_PASSWORD: secret
    networks:
      - probabilistic_network
    restart: unless-stopped

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: probabilistic_pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@example.com
      PGADMIN_DEFAULT_PASSWORD: admin
      PGADMIN_CONFIG_SERVER_MODE: 'False'
    ports:
      - "5050:80"
    depends_on:
      - postgresql
    networks:
      - probabilistic_network
    restart: unless-stopped

volumes:
  postgres_data:
    driver: local

networks:
  probabilistic_network:
    driver: bridge
```

#### **9.4.2 初始化SQL脚本**

**init_probabilistic.sql**：

```sql
-- 初始化概率数据库
-- 创建扩展
CREATE EXTENSION IF NOT EXISTS provsql;

-- 创建概率数据类型
CREATE TYPE probability_value AS (
    value NUMERIC,
    confidence NUMERIC
);

-- 创建概率表
CREATE TABLE IF NOT EXISTS uncertain_data (
    id SERIAL PRIMARY KEY,
    value NUMERIC NOT NULL,
    confidence NUMERIC CHECK (confidence >= 0 AND confidence <= 1),
    data_source VARCHAR(100),
    created_at TIMESTAMP DEFAULT NOW()
) WITH PROVENANCE;

-- 创建索引
CREATE INDEX IF NOT EXISTS idx_confidence ON uncertain_data(confidence);
CREATE INDEX IF NOT EXISTS idx_data_source ON uncertain_data(data_source);
CREATE INDEX IF NOT EXISTS idx_created_at ON uncertain_data(created_at);

-- 插入示例数据
INSERT INTO uncertain_data (value, confidence, data_source)
VALUES
    (10.5, 0.9, 'sensor_1'),
    (11.2, 0.85, 'sensor_2'),
    (9.8, 0.95, 'sensor_1'),
    (10.1, 0.88, 'sensor_3'),
    (11.5, 0.92, 'sensor_2');

-- 创建概率查询函数
CREATE OR REPLACE FUNCTION provsql_weighted_avg(
    table_name TEXT,
    value_column TEXT,
    confidence_column TEXT,
    filter_condition TEXT DEFAULT 'TRUE'
) RETURNS TABLE (
    weighted_avg NUMERIC,
    avg_confidence NUMERIC,
    provenance JSONB
) AS $$
DECLARE
    query_text TEXT;
BEGIN
    query_text := format('
        SELECT
            SUM(%I * %I) / SUM(%I) AS weighted_avg,
            AVG(%I) AS avg_confidence,
            provsql_provenance_of(
                SELECT SUM(%I * %I) / SUM(%I)
                FROM %I
                WHERE %s
            )::JSONB AS provenance
        FROM %I
        WHERE %s
    ',
        value_column, confidence_column, confidence_column,
        confidence_column,
        value_column, confidence_column, confidence_column,
        table_name, filter_condition,
        table_name, filter_condition
    );

    RETURN QUERY EXECUTE query_text;
END;
$$ LANGUAGE plpgsql;
```

#### **9.4.3 Python客户端配置**

**client/probabilistic_client.py**：

```python
# probabilistic_client.py
import os
import psycopg2
from psycopg2 import pool
from psycopg2.extras import RealDictCursor
from contextlib import contextmanager
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ProbabilisticDBClient:
    """概率数据库客户端"""

    def __init__(self):
        """从环境变量读取数据库配置"""
        self.conn_pool = psycopg2.pool.ThreadedConnectionPool(
            1, 10,
            host=os.getenv('DB_HOST', 'localhost'),
            port=os.getenv('DB_PORT', '5432'),
            database=os.getenv('DB_NAME', 'testdb'),
            user=os.getenv('DB_USER', 'postgres'),
            password=os.getenv('DB_PASSWORD', 'secret')
        )

    @contextmanager
    def get_connection(self):
        """获取数据库连接的上下文管理器"""
        conn = None
        try:
            conn = self.conn_pool.getconn()
            yield conn
            conn.commit()
        except Exception as e:
            if conn:
                conn.rollback()
            logger.error(f"数据库操作失败: {e}")
            raise
        finally:
            if conn:
                self.conn_pool.putconn(conn)

    def test_connection(self):
        """测试数据库连接"""
        with self.get_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("SELECT version();")
                version = cur.fetchone()
                logger.info(f"PostgreSQL版本: {version[0]}")
                return version[0]

if __name__ == '__main__':
    client = ProbabilisticDBClient()
    client.test_connection()
    logger.info("概率数据库客户端测试成功")
```

#### **9.4.4 部署和使用说明**

**部署步骤**：

1. **准备文件**：

   ```bash
   mkdir -p probabilistic_db
   cd probabilistic_db
   # 创建docker-compose.yml
   # 创建init_probabilistic.sql
   # 创建client/probabilistic_client.py
   ```

2. **启动服务**：

   ```bash
   docker-compose up -d
   ```

3. **检查服务状态**：

   ```bash
   docker-compose ps
   docker-compose logs postgresql
   docker-compose logs python-client
   ```

4. **访问PgAdmin**：
   - 打开浏览器访问: <http://localhost:5050>
   - 登录: <admin@example.com> / admin
   - 添加服务器: postgresql (host: postgresql, port: 5432)

5. **停止服务**：

   ```bash
   docker-compose down
   ```

6. **清理数据**：

   ```bash
   docker-compose down -v
   ```

**使用说明**：

- **数据库连接**: localhost:5432
- **数据库名**: testdb
- **用户名**: postgres
- **密码**: secret
- **PgAdmin**: <http://localhost:5050>
- **Python客户端**: 自动运行，检查日志查看结果

---

**最后更新**: 2025年1月
**维护状态**: ✅ 持续更新
