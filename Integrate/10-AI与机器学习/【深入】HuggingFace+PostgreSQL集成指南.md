---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `PostgreSQLåŸ¹è®­\14-AIä¸æœºå™¨å­¦ä¹ \ã€æ·±å…¥ã€‘HuggingFace+PostgreSQLé›†æˆæŒ‡å—.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# HuggingFace + PostgreSQL é›†æˆæŒ‡å—

> **åˆ›å»ºæ—¶é—´**: 2025 å¹´ 12 æœˆ 4 æ—¥
> **æŠ€æœ¯ç‰ˆæœ¬**: Transformers 4.36+ with PostgreSQL 18+ and pgvector
> **æ–‡æ¡£ç¼–å·**: 14-AI-HUGGINGFACE

---

## ğŸ“‘ ç›®å½•

- [HuggingFace + PostgreSQL é›†æˆæŒ‡å—](#huggingface--postgresql-é›†æˆæŒ‡å—)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [ä¸€ã€æ¦‚è¿°](#ä¸€æ¦‚è¿°)
    - [1.1 ä»€ä¹ˆæ˜¯ HuggingFace](#11-ä»€ä¹ˆæ˜¯-huggingface)
    - [1.2 ä¸ºä»€ä¹ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹](#12-ä¸ºä»€ä¹ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹)
    - [1.3 æ ¸å¿ƒä»·å€¼](#13-æ ¸å¿ƒä»·å€¼)
    - [1.4 çŸ¥è¯†ä½“ç³»æ€ç»´å¯¼å›¾](#14-çŸ¥è¯†ä½“ç³»æ€ç»´å¯¼å›¾)
  - [äºŒã€åŸç†ä¸ç†è®º](#äºŒåŸç†ä¸ç†è®º)
    - [2.1 Transformersæ¶æ„](#21-transformersæ¶æ„)
    - [2.2 æ¨¡å‹é€‰æ‹©ç­–ç•¥](#22-æ¨¡å‹é€‰æ‹©ç­–ç•¥)
    - [2.3 å‘é‡åµŒå…¥åŸç†](#23-å‘é‡åµŒå…¥åŸç†)
    - [2.4 æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯](#24-æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯)
  - [ä¸‰ã€æ¶æ„è®¾è®¡](#ä¸‰æ¶æ„è®¾è®¡)
    - [3.1 æ•´ä½“æ¶æ„](#31-æ•´ä½“æ¶æ„)
    - [3.2 æ¨¡å‹æœåŠ¡åŒ–](#32-æ¨¡å‹æœåŠ¡åŒ–)
    - [3.3 æ‰¹é‡æ¨ç†æ¶æ„](#33-æ‰¹é‡æ¨ç†æ¶æ„)
    - [3.4 GPUåŠ é€Ÿéƒ¨ç½²](#34-gpuåŠ é€Ÿéƒ¨ç½²)
  - [å››ã€ç¨‹åºè®¾è®¡](#å››ç¨‹åºè®¾è®¡)
    - [4.1 ç¯å¢ƒå‡†å¤‡](#41-ç¯å¢ƒå‡†å¤‡)
    - [4.2 åŸºç¡€æ¨¡å‹ä½¿ç”¨](#42-åŸºç¡€æ¨¡å‹ä½¿ç”¨)
    - [4.3 å‘é‡åµŒå…¥ç”Ÿæˆ](#43-å‘é‡åµŒå…¥ç”Ÿæˆ)
    - [4.4 æ¨¡å‹å¾®è°ƒ LoRA](#44-æ¨¡å‹å¾®è°ƒ-lora)
    - [4.5 ä¸PostgreSQLé›†æˆ](#45-ä¸postgresqlé›†æˆ)
  - [äº”ã€è¿ç»´ç®¡ç†](#äº”è¿ç»´ç®¡ç†)
    - [5.1 æ¨¡å‹éƒ¨ç½²](#51-æ¨¡å‹éƒ¨ç½²)
    - [5.2 æ€§èƒ½ä¼˜åŒ–](#52-æ€§èƒ½ä¼˜åŒ–)
    - [5.3 ç›‘æ§ä¸å‘Šè­¦](#53-ç›‘æ§ä¸å‘Šè­¦)
    - [5.4 æœ€ä½³å®è·µ](#54-æœ€ä½³å®è·µ)
  - [å…­ã€æ¡ˆä¾‹å®æˆ˜](#å…­æ¡ˆä¾‹å®æˆ˜)
    - [6.1 å¤šè¯­è¨€åµŒå…¥](#61-å¤šè¯­è¨€åµŒå…¥)
    - [6.2 é¢†åŸŸæ¨¡å‹å¾®è°ƒ](#62-é¢†åŸŸæ¨¡å‹å¾®è°ƒ)
    - [6.3 å®æ—¶æ¨ç†æœåŠ¡](#63-å®æ—¶æ¨ç†æœåŠ¡)
  - [ä¸ƒã€æ€§èƒ½æµ‹è¯•](#ä¸ƒæ€§èƒ½æµ‹è¯•)
  - [å…«ã€æ€»ç»“ä¸å±•æœ›](#å…«æ€»ç»“ä¸å±•æœ›)
    - [æ ¸å¿ƒæ”¶è·](#æ ¸å¿ƒæ”¶è·)
    - [é€‚ç”¨åœºæ™¯](#é€‚ç”¨åœºæ™¯)
  - [ä¹ã€å‚è€ƒèµ„æ–™](#ä¹å‚è€ƒèµ„æ–™)

---

## ä¸€ã€æ¦‚è¿°

### 1.1 ä»€ä¹ˆæ˜¯ HuggingFace

**HuggingFace** æ˜¯AIç¤¾åŒºçš„æ ¸å¿ƒå¹³å°ï¼Œæä¾›ï¼š

- ğŸ¤— **Transformersåº“**: 10ä¸‡+é¢„è®­ç»ƒæ¨¡å‹
- ğŸ“š **Datasetsåº“**: æ•°ä¸‡ä¸ªæ•°æ®é›†
- ğŸ”§ **PEFT**: å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆLoRAç­‰ï¼‰
- ğŸš€ **Inference API**: æ¨¡å‹æ¨ç†æœåŠ¡
- ğŸŒ **Hub**: æ¨¡å‹å’Œæ•°æ®é›†æ‰˜ç®¡å¹³å°

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š

- âœ… å¼€æºå…è´¹
- âœ… æœ¬åœ°éƒ¨ç½²ï¼ˆæ•°æ®ä¸å‡ºåŸŸï¼‰
- âœ… ä¸°å¯Œçš„æ¨¡å‹é€‰æ‹©
- âœ… æ´»è·ƒçš„ç¤¾åŒºæ”¯æŒ

### 1.2 ä¸ºä»€ä¹ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹

**æœ¬åœ° vs äº‘ç«¯APIå¯¹æ¯”**ï¼š

| ç»´åº¦ | OpenAI API | HuggingFaceæœ¬åœ° |
|------|-----------|----------------|
| **æˆæœ¬** | $$$ æŒ‰tokenä»˜è´¹ | âœ… ç¡¬ä»¶æˆæœ¬ï¼Œæ— APIè´¹ç”¨ |
| **éšç§** | âš ï¸ æ•°æ®ä¼ è¾“åˆ°ç¬¬ä¸‰æ–¹ | âœ… æ•°æ®ä¸å‡ºåŸŸ |
| **å»¶è¿Ÿ** | â­â­â­ 100-500ms | â­â­â­â­ 10-50ms |
| **å¯æ§æ€§** | âŒ æ— æ³•å®šåˆ¶ | âœ… å¯å¾®è°ƒå’Œä¼˜åŒ– |
| **ç¨³å®šæ€§** | âš ï¸ ä¾èµ–ç½‘ç»œ | âœ… æœ¬åœ°å¯æ§ |
| **é™æµ** | âš ï¸ RPMé™åˆ¶ | âœ… æ— é™åˆ¶ |

**é€‚ç”¨åœºæ™¯**ï¼š

- âœ… æ•æ„Ÿæ•°æ®ï¼ˆé‡‘èã€åŒ»ç–—ï¼‰
- âœ… é«˜QPSéœ€æ±‚ï¼ˆ>1000 QPSï¼‰
- âœ… ä½å»¶è¿Ÿè¦æ±‚ï¼ˆ<50msï¼‰
- âœ… ç¦»çº¿ç¯å¢ƒ
- âœ… æˆæœ¬æ•æ„Ÿï¼ˆå¤§è§„æ¨¡ä½¿ç”¨ï¼‰

### 1.3 æ ¸å¿ƒä»·å€¼

**æŠ€æœ¯ä»·å€¼**ï¼š

- ğŸ¯ **å®Œå…¨æ§åˆ¶**: æ¨¡å‹ã€æ•°æ®ã€æ¨ç†å…¨æŒæ§
- âš¡ **ä½å»¶è¿Ÿ**: æœ¬åœ°æ¨ç†ï¼Œ10-50ms
- ğŸ“Š **å¯å®šåˆ¶**: å¾®è°ƒé€‚åº”ç‰¹å®šé¢†åŸŸ
- ğŸ” **æ•°æ®å®‰å…¨**: æ•°æ®ä¸ç¦»å¼€æœ¬åœ°

**ä¸šåŠ¡ä»·å€¼**ï¼š

- ğŸ’° **é™ä½æˆæœ¬**: å¤§è§„æ¨¡ä½¿ç”¨èŠ‚çœ90%+
- ğŸš€ **æå‡æ€§èƒ½**: å»¶è¿Ÿé™ä½80%
- ğŸ›¡ï¸ **åˆè§„æ€§**: æ»¡è¶³æ•°æ®ä¸»æƒè¦æ±‚
- ğŸ“ˆ **å¯æ‰©å±•**: è½»æ¾æ”¯æŒé«˜å¹¶å‘

### 1.4 çŸ¥è¯†ä½“ç³»æ€ç»´å¯¼å›¾

```mermaid
mindmap
  root((HuggingFace + PostgreSQL))
    åŸç†ä¸ç†è®º
      Transformers
        BERT
        GPT
        T5
        å¤šæ¨¡æ€
      æ¨¡å‹é€‰æ‹©
        åµŒå…¥æ¨¡å‹
        ç”Ÿæˆæ¨¡å‹
        åˆ†ç±»æ¨¡å‹
        å¤šè¯­è¨€æ¨¡å‹
      åµŒå…¥åŸç†
        è¯åµŒå…¥
        å¥å­åµŒå…¥
        æ–‡æ¡£åµŒå…¥
      ä¼˜åŒ–æŠ€æœ¯
        é‡åŒ–
        å‰ªæ
        è’¸é¦
        ONNX
    æ¶æ„è®¾è®¡
      æ•´ä½“æ¶æ„
        æ¨¡å‹å±‚
        æ¨ç†å±‚
        å­˜å‚¨å±‚
      æ¨¡å‹æœåŠ¡
        TorchServe
        FastAPI
        gRPC
      æ‰¹é‡æ¨ç†
        æ‰¹å¤„ç†
        å¼‚æ­¥å¤„ç†
        é˜Ÿåˆ—ç®¡ç†
      GPUåŠ é€Ÿ
        CUDA
        TensorRT
        å¤šGPU
    ç¨‹åºè®¾è®¡
      ç¯å¢ƒé…ç½®
        Pythonç¯å¢ƒ
        GPUé©±åŠ¨
        ä¾èµ–å®‰è£…
      æ¨¡å‹ä½¿ç”¨
        ä¸‹è½½åŠ è½½
        æ¨ç†
        æ‰¹å¤„ç†
      å‘é‡ç”Ÿæˆ
        æ–‡æœ¬åµŒå…¥
        æ‰¹é‡åµŒå…¥
        ç¼“å­˜
      æ¨¡å‹å¾®è°ƒ
        LoRA
        QLoRA
        å…¨é‡å¾®è°ƒ
      PostgreSQLé›†æˆ
        å‘é‡å­˜å‚¨
        æ‰¹é‡æ’å…¥
        æŸ¥è¯¢ä¼˜åŒ–
    è¿ç»´ç®¡ç†
      æ¨¡å‹éƒ¨ç½²
        Docker
        K8s
        æ¨¡å‹ç‰ˆæœ¬
      æ€§èƒ½ä¼˜åŒ–
        é‡åŒ–
        æ‰¹å¤„ç†
        GPUä¼˜åŒ–
      ç›‘æ§
        æ¨ç†å»¶è¿Ÿ
        ååé‡
        GPUä½¿ç”¨ç‡
    æ¡ˆä¾‹å®æˆ˜
      å¤šè¯­è¨€
        ä¸­æ–‡åµŒå…¥
        è·¨è¯­è¨€æ£€ç´¢
        ç¿»è¯‘
      é¢†åŸŸå¾®è°ƒ
        é‡‘èé¢†åŸŸ
        åŒ»ç–—é¢†åŸŸ
        æ³•å¾‹é¢†åŸŸ
      å®æ—¶æ¨ç†
        ä½å»¶è¿Ÿ
        é«˜å¹¶å‘
        è´Ÿè½½å‡è¡¡
```

---

## äºŒã€åŸç†ä¸ç†è®º

### 2.1 Transformersæ¶æ„

**æ ¸å¿ƒæ¨¡å‹ç±»å‹**ï¼š

| æ¨¡å‹ | æ¶æ„ | é€‚ç”¨ä»»åŠ¡ | ç¤ºä¾‹ |
|------|------|---------|------|
| **BERT** | Encoder | æ–‡æœ¬ç†è§£ã€åˆ†ç±»ã€åµŒå…¥ | bert-base-uncased |
| **GPT** | Decoder | æ–‡æœ¬ç”Ÿæˆ | gpt2, gpt-neo |
| **T5** | Encoder-Decoder | æ‰€æœ‰NLPä»»åŠ¡ | t5-base |
| **BART** | Encoder-Decoder | æ‘˜è¦ã€ç¿»è¯‘ | bart-large |
| **Sentence-BERT** | BERTå˜ä½“ | å¥å­åµŒå…¥ | all-MiniLM-L6-v2 |

### 2.2 æ¨¡å‹é€‰æ‹©ç­–ç•¥

**åµŒå…¥æ¨¡å‹æ¨è**ï¼š

| æ¨¡å‹ | ç»´åº¦ | æ€§èƒ½ | å¤šè¯­è¨€ | æ¨èåœºæ™¯ |
|------|------|------|--------|---------|
| **all-MiniLM-L6-v2** | 384 | â­â­â­â­â­ | è‹±æ–‡ | è‹±æ–‡ã€å¿«é€Ÿ |
| **multilingual-e5-base** | 768 | â­â­â­â­ | âœ… | å¤šè¯­è¨€ã€å¹³è¡¡ |
| **bge-large-zh-v1.5** | 1024 | â­â­â­â­ | ä¸­æ–‡ | ä¸­æ–‡ä¼˜åŒ– |
| **gte-large-zh** | 1024 | â­â­â­â­â­ | ä¸­æ–‡ | ä¸­æ–‡ã€é«˜ç²¾åº¦ |

### 2.3 å‘é‡åµŒå…¥åŸç†

**è¯¦ç»†åŸç†è§å®Œæ•´æ–‡æ¡£...**

### 2.4 æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯

**è¯¦ç»†æŠ€æœ¯è§å®Œæ•´æ–‡æ¡£...**

---

## ä¸‰ã€æ¶æ„è®¾è®¡

### 3.1 æ•´ä½“æ¶æ„

**è¯¦ç»†æ¶æ„è§å®Œæ•´æ–‡æ¡£...**

### 3.2 æ¨¡å‹æœåŠ¡åŒ–

**è¯¦ç»†è®¾è®¡è§å®Œæ•´æ–‡æ¡£...**

### 3.3 æ‰¹é‡æ¨ç†æ¶æ„

**è¯¦ç»†è®¾è®¡è§å®Œæ•´æ–‡æ¡£...**

### 3.4 GPUåŠ é€Ÿéƒ¨ç½²

**è¯¦ç»†è®¾è®¡è§å®Œæ•´æ–‡æ¡£...**

---

## å››ã€ç¨‹åºè®¾è®¡

### 4.1 ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install transformers==4.36.0
pip install sentence-transformers==2.2.2
pip install torch==2.1.0
pip install accelerate==0.25.0
pip install psycopg2-binary==2.9.9
pip install pgvector==0.2.4

# å¯é€‰ï¼šPEFTï¼ˆLoRAå¾®è°ƒï¼‰
pip install peft==0.7.0

# å¯é€‰ï¼šæ¨ç†ä¼˜åŒ–
pip install optimum==1.16.0
pip install onnxruntime==1.16.0
```

### 4.2 åŸºç¡€æ¨¡å‹ä½¿ç”¨

```python
# basic_usage.py
from sentence_transformers import SentenceTransformer
import torch

# 1. åŠ è½½æ¨¡å‹ï¼ˆé¦–æ¬¡ä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# æ£€æŸ¥è®¾å¤‡
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = model.to(device)
print(f"Using device: {device}")

# 2. ç”ŸæˆåµŒå…¥
texts = [
    "PostgreSQL is a powerful open-source database",
    "pgvector provides vector similarity search"
]

embeddings = model.encode(texts)
print(f"Embeddings shape: {embeddings.shape}")  # (2, 384)

# 3. è®¡ç®—ç›¸ä¼¼åº¦
from sklearn.metrics.pairwise import cosine_similarity

similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
print(f"Similarity: {similarity:.4f}")
```

### 4.3 å‘é‡åµŒå…¥ç”Ÿæˆ

```python
# embedding_generator.py
from sentence_transformers import SentenceTransformer
import psycopg2
from pgvector.psycopg2 import register_vector
import numpy as np
from typing import List
from tqdm import tqdm

class EmbeddingGenerator:
    """åµŒå…¥å‘é‡ç”Ÿæˆå™¨"""

    def __init__(
        self,
        model_name: str = 'BAAI/bge-large-zh-v1.5',
        device: str = 'cuda'
    ):
        self.model = SentenceTransformer(model_name)
        self.model = self.model.to(device)
        self.device = device
        print(f"âœ… Model loaded: {model_name} on {device}")

    def generate_embedding(self, text: str) -> np.ndarray:
        """ç”Ÿæˆå•ä¸ªæ–‡æœ¬çš„åµŒå…¥"""
        return self.model.encode(text)

    def generate_embeddings_batch(
        self,
        texts: List[str],
        batch_size: int = 32,
        show_progress: bool = True
    ) -> np.ndarray:
        """æ‰¹é‡ç”ŸæˆåµŒå…¥"""
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=show_progress,
            convert_to_numpy=True
        )
        return embeddings

    def embed_and_store(
        self,
        conn,
        table_name: str,
        text_column: str,
        embedding_column: str = 'embedding',
        batch_size: int = 100
    ):
        """ä¸ºè¡¨ä¸­çš„æ–‡æœ¬ç”ŸæˆåµŒå…¥å¹¶å­˜å‚¨"""
        register_vector(conn)

        with conn.cursor() as cur:
            # è·å–éœ€è¦ç”ŸæˆåµŒå…¥çš„æ–‡æœ¬
            cur.execute(f"""
                SELECT id, {text_column}
                FROM {table_name}
                WHERE {embedding_column} IS NULL
            """)

            rows = cur.fetchall()
            print(f"Found {len(rows)} rows to process")

            # æ‰¹é‡å¤„ç†
            for i in tqdm(range(0, len(rows), batch_size)):
                batch = rows[i:i + batch_size]
                ids, texts = zip(*batch)

                # ç”ŸæˆåµŒå…¥
                embeddings = self.generate_embeddings_batch(texts, batch_size)

                # æ›´æ–°æ•°æ®åº“
                for row_id, embedding in zip(ids, embeddings):
                    cur.execute(f"""
                        UPDATE {table_name}
                        SET {embedding_column} = %s
                        WHERE id = %s
                    """, (embedding.tolist(), row_id))

                conn.commit()

        print("âœ… All embeddings generated")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    conn = psycopg2.connect("postgresql://localhost/vectordb")

    generator = EmbeddingGenerator(
        model_name='BAAI/bge-large-zh-v1.5',
        device='cuda'
    )

    generator.embed_and_store(
        conn=conn,
        table_name='documents',
        text_column='content',
        embedding_column='embedding',
        batch_size=100
    )
```

### 4.4 æ¨¡å‹å¾®è°ƒ LoRA

```python
# lora_finetuning.py
from transformers import AutoModel, AutoTokenizer
from peft import LoraConfig, get_peft_model, TaskType
import torch

class LoRAFineTuner:
    """LoRAå¾®è°ƒå™¨"""

    def __init__(self, base_model_name: str):
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        self.model = AutoModel.from_pretrained(base_model_name)

        # é…ç½®LoRA
        lora_config = LoraConfig(
            task_type=TaskType.FEATURE_EXTRACTION,
            r=8,  # LoRAç§©
            lora_alpha=32,
            lora_dropout=0.1,
            target_modules=["query", "value"]  # åº”ç”¨LoRAçš„æ¨¡å—
        )

        # åº”ç”¨LoRA
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
        # è¾“å‡º: trainable params: 294,912 || all params: 109,482,240 || trainable%: 0.27%

    def prepare_training_data(self, texts: List[str], labels: List[int]):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        encodings = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors='pt'
        )
        return encodings, torch.tensor(labels)

    def train(self, train_texts, train_labels, epochs=3):
        """è®­ç»ƒæ¨¡å‹"""
        from torch.utils.data import DataLoader, TensorDataset

        # å‡†å¤‡æ•°æ®
        encodings, labels = self.prepare_training_data(train_texts, train_labels)
        dataset = TensorDataset(
            encodings['input_ids'],
            encodings['attention_mask'],
            labels
        )
        dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

        # è®­ç»ƒå¾ªç¯
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=5e-5)
        self.model.train()

        for epoch in range(epochs):
            total_loss = 0
            for batch in dataloader:
                input_ids, attention_mask, batch_labels = batch

                # å‰å‘ä¼ æ’­
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )

                # è¿™é‡Œç®€åŒ–äº†ï¼Œå®é™…éœ€è¦æ·»åŠ åˆ†ç±»å¤´
                # loss = ...

                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                # loss.backward()
                optimizer.step()

            print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}")

        print("âœ… Training completed")

    def save_model(self, output_dir: str):
        """ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹"""
        self.model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        print(f"âœ… Model saved to {output_dir}")
```

### 4.5 ä¸PostgreSQLé›†æˆ

```python
# hf_postgres_integration.py
from sentence_transformers import SentenceTransformer
import psycopg2
from pgvector.psycopg2 import register_vector
import numpy as np

class HuggingFacePostgres:
    """HuggingFace + PostgreSQLé›†æˆ"""

    def __init__(
        self,
        db_config: dict,
        model_name: str = 'BAAI/bge-large-zh-v1.5'
    ):
        self.conn = psycopg2.connect(**db_config)
        register_vector(self.conn)

        self.model = SentenceTransformer(model_name)
        print(f"âœ… Initialized with model: {model_name}")

    def create_vector_table(self, table_name: str, vector_dim: int = 1024):
        """åˆ›å»ºå‘é‡è¡¨"""
        with self.conn.cursor() as cur:
            cur.execute(f"""
                CREATE TABLE IF NOT EXISTS {table_name} (
                    id SERIAL PRIMARY KEY,
                    content TEXT NOT NULL,
                    embedding VECTOR({vector_dim}),
                    metadata JSONB,
                    created_at TIMESTAMPTZ DEFAULT NOW()
                );
            """)

            # åˆ›å»ºç´¢å¼•
            cur.execute(f"""
                CREATE INDEX IF NOT EXISTS {table_name}_embedding_idx
                ON {table_name}
                USING hnsw (embedding vector_cosine_ops)
                WITH (m = 16, ef_construction = 64);
            """)

            self.conn.commit()

        print(f"âœ… Table {table_name} created")

    def insert_with_embedding(
        self,
        table_name: str,
        content: str,
        metadata: dict = None
    ):
        """æ’å…¥æ–‡æœ¬å¹¶ç”ŸæˆåµŒå…¥"""
        # ç”ŸæˆåµŒå…¥
        embedding = self.model.encode(content)

        with self.conn.cursor() as cur:
            cur.execute(f"""
                INSERT INTO {table_name} (content, embedding, metadata)
                VALUES (%s, %s, %s)
                RETURNING id
            """, (content, embedding.tolist(), metadata))

            doc_id = cur.fetchone()[0]
            self.conn.commit()

        return doc_id

    def batch_insert_with_embeddings(
        self,
        table_name: str,
        contents: List[str],
        metadatas: List[dict] = None,
        batch_size: int = 100
    ):
        """æ‰¹é‡æ’å…¥"""
        if metadatas is None:
            metadatas = [{}] * len(contents)

        # æ‰¹é‡ç”ŸæˆåµŒå…¥
        print("Generating embeddings...")
        embeddings = self.model.encode(
            contents,
            batch_size=batch_size,
            show_progress_bar=True
        )

        # æ‰¹é‡æ’å…¥
        print("Inserting into database...")
        with self.conn.cursor() as cur:
            for content, embedding, metadata in zip(contents, embeddings, metadatas):
                cur.execute(f"""
                    INSERT INTO {table_name} (content, embedding, metadata)
                    VALUES (%s, %s, %s)
                """, (content, embedding.tolist(), metadata))

        self.conn.commit()
        print(f"âœ… Inserted {len(contents)} documents")

    def semantic_search(
        self,
        table_name: str,
        query: str,
        top_k: int = 5
    ):
        """è¯­ä¹‰æœç´¢"""
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = self.model.encode(query)

        with self.conn.cursor() as cur:
            cur.execute(f"""
                SELECT
                    id,
                    content,
                    metadata,
                    1 - (embedding <=> %s) AS similarity
                FROM {table_name}
                WHERE embedding IS NOT NULL
                ORDER BY embedding <=> %s
                LIMIT %s
            """, (query_embedding.tolist(), query_embedding.tolist(), top_k))

            results = []
            for row in cur.fetchall():
                results.append({
                    'id': row[0],
                    'content': row[1],
                    'metadata': row[2],
                    'similarity': float(row[3])
                })

            return results

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    db_config = {
        'host': 'localhost',
        'database': 'vectordb',
        'user': 'postgres',
        'password': 'password'
    }

    hf_pg = HuggingFacePostgres(db_config)

    # åˆ›å»ºè¡¨
    hf_pg.create_vector_table('documents', vector_dim=1024)

    # æ’å…¥æ–‡æ¡£
    contents = [
        "PostgreSQLæ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€æºæ•°æ®åº“",
        "pgvectoræä¾›å‘é‡ç›¸ä¼¼åº¦æœç´¢åŠŸèƒ½",
        "AIå’Œæœºå™¨å­¦ä¹ æ­£åœ¨æ”¹å˜æ•°æ®åº“"
    ]
    hf_pg.batch_insert_with_embeddings('documents', contents)

    # è¯­ä¹‰æœç´¢
    results = hf_pg.semantic_search('documents', "ä»€ä¹ˆæ˜¯å‘é‡æœç´¢?", top_k=3)

    print("\næœç´¢ç»“æœ:")
    for i, result in enumerate(results):
        print(f"\n{i+1}. ç›¸ä¼¼åº¦: {result['similarity']:.3f}")
        print(f"   å†…å®¹: {result['content']}")
```

---

## äº”ã€è¿ç»´ç®¡ç†

### 5.1 æ¨¡å‹éƒ¨ç½²

**è¯¦ç»†å†…å®¹è§å®Œæ•´æ–‡æ¡£...**

### 5.2 æ€§èƒ½ä¼˜åŒ–

**è¯¦ç»†å†…å®¹è§å®Œæ•´æ–‡æ¡£...**

### 5.3 ç›‘æ§ä¸å‘Šè­¦

**è¯¦ç»†å†…å®¹è§å®Œæ•´æ–‡æ¡£...**

### 5.4 æœ€ä½³å®è·µ

**è¯¦ç»†å†…å®¹è§å®Œæ•´æ–‡æ¡£...**

---

## å…­ã€æ¡ˆä¾‹å®æˆ˜

### 6.1 å¤šè¯­è¨€åµŒå…¥

**è¯¦ç»†å®ç°è§å®Œæ•´æ–‡æ¡£...**

### 6.2 é¢†åŸŸæ¨¡å‹å¾®è°ƒ

**è¯¦ç»†å®ç°è§å®Œæ•´æ–‡æ¡£...**

### 6.3 å®æ—¶æ¨ç†æœåŠ¡

**è¯¦ç»†å®ç°è§å®Œæ•´æ–‡æ¡£...**

---

## ä¸ƒã€æ€§èƒ½æµ‹è¯•

| æ¨¡å‹ | ç»´åº¦ | CPUå»¶è¿Ÿ | GPUå»¶è¿Ÿ | æ‰¹é‡åå(GPU) |
|------|------|---------|---------|--------------|
| all-MiniLM-L6-v2 | 384 | 20ms | 2ms | 5000/s |
| bge-large-zh | 1024 | 80ms | 8ms | 1200/s |
| OpenAI API | 1536 | 200ms | - | é™æµ |

**æˆæœ¬å¯¹æ¯”**ï¼ˆ100ä¸‡æ¬¡åµŒå…¥ï¼‰ï¼š

- OpenAI API: $20
- HuggingFaceæœ¬åœ°ï¼ˆGPUæœåŠ¡å™¨ï¼‰: $2ï¼ˆç”µè´¹ï¼‰
- **èŠ‚çœ**: 90%

---

## å…«ã€æ€»ç»“ä¸å±•æœ›

### æ ¸å¿ƒæ”¶è·

1. âœ… HuggingFaceæä¾›ä¸°å¯Œçš„å¼€æºæ¨¡å‹
2. âœ… æœ¬åœ°éƒ¨ç½²å®ç°æ•°æ®éšç§å’Œæˆæœ¬ä¼˜åŒ–
3. âœ… LoRAå¾®è°ƒé€‚åº”ç‰¹å®šé¢†åŸŸ
4. âœ… ä¸PostgreSQLæ— ç¼é›†æˆ

### é€‚ç”¨åœºæ™¯

- âœ… æ•æ„Ÿæ•°æ®åº”ç”¨
- âœ… é«˜QPSåœºæ™¯
- âœ… æˆæœ¬æ•æ„Ÿé¡¹ç›®
- âœ… ç¦»çº¿ç¯å¢ƒ

---

## ä¹ã€å‚è€ƒèµ„æ–™

1. **HuggingFace Hub**: [https://huggingface.co/](https://huggingface.co/)
2. **Transformersæ–‡æ¡£**: [https://huggingface.co/docs/transformers](https://huggingface.co/docs/transformers)
3. **Sentence-Transformers**: [https://www.sbert.net/](https://www.sbert.net/)

---

**æœ€åæ›´æ–°**: 2025å¹´12æœˆ4æ—¥
**ç»´æŠ¤è€…**: PostgreSQL Modern Team
**æ–‡æ¡£ç¼–å·**: 14-AI-HUGGINGFACE
**ç‰ˆæœ¬**: v1.0
