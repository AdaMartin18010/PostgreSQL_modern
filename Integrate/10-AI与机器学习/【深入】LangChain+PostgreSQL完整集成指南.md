---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `PostgreSQLåŸ¹è®­\14-AIä¸æœºå™¨å­¦ä¹ \ã€æ·±å…¥ã€‘LangChain+PostgreSQLå®Œæ•´é›†æˆæŒ‡å—.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# LangChain + PostgreSQL å®Œæ•´é›†æˆæŒ‡å—

> **åˆ›å»ºæ—¶é—´**: 2025 å¹´ 12 æœˆ 4 æ—¥
> **æŠ€æœ¯ç‰ˆæœ¬**: LangChain 0.1.0+ with PostgreSQL 18+ and pgvector
> **æ–‡æ¡£ç¼–å·**: 14-AI-LANGCHAIN

---

## ğŸ“‘ ç›®å½•

- [LangChain + PostgreSQL å®Œæ•´é›†æˆæŒ‡å—](#langchain--postgresql-å®Œæ•´é›†æˆæŒ‡å—)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [ä¸€ã€æ¦‚è¿°](#ä¸€æ¦‚è¿°)
    - [1.1 ä»€ä¹ˆæ˜¯ LangChain](#11-ä»€ä¹ˆæ˜¯-langchain)
    - [1.2 ä¸ºä»€ä¹ˆé€‰æ‹© PostgreSQL](#12-ä¸ºä»€ä¹ˆé€‰æ‹©-postgresql)
    - [1.3 æ ¸å¿ƒä»·å€¼](#13-æ ¸å¿ƒä»·å€¼)
    - [1.4 çŸ¥è¯†ä½“ç³»æ€ç»´å¯¼å›¾](#14-çŸ¥è¯†ä½“ç³»æ€ç»´å¯¼å›¾)
  - [äºŒã€åŸç†ä¸ç†è®º](#äºŒåŸç†ä¸ç†è®º)
    - [2.1 LangChain æ¶æ„åŸç†](#21-langchain-æ¶æ„åŸç†)
      - [**æ ¸å¿ƒç»„ä»¶è¯¦è§£**](#æ ¸å¿ƒç»„ä»¶è¯¦è§£)
      - [**LCEL (LangChain Expression Language)**](#lcel-langchain-expression-language)
    - [2.2 RAG æ£€ç´¢å¢å¼ºç”Ÿæˆ](#22-rag-æ£€ç´¢å¢å¼ºç”Ÿæˆ)
      - [**RAG å·¥ä½œåŸç†**](#rag-å·¥ä½œåŸç†)
      - [**å…³é”®æŠ€æœ¯ç‚¹**](#å…³é”®æŠ€æœ¯ç‚¹)
    - [2.3 å‘é‡åµŒå…¥ä¸ç›¸ä¼¼åº¦æœç´¢](#23-å‘é‡åµŒå…¥ä¸ç›¸ä¼¼åº¦æœç´¢)
      - [**åµŒå…¥æ¨¡å‹é€‰æ‹©**](#åµŒå…¥æ¨¡å‹é€‰æ‹©)
      - [**ç›¸ä¼¼åº¦è®¡ç®—**](#ç›¸ä¼¼åº¦è®¡ç®—)
      - [**ç´¢å¼•ä¼˜åŒ–**](#ç´¢å¼•ä¼˜åŒ–)
    - [2.4 ä¸Šä¸‹æ–‡ç®¡ç†ä¸ä¼˜åŒ–](#24-ä¸Šä¸‹æ–‡ç®¡ç†ä¸ä¼˜åŒ–)
      - [**ä¸Šä¸‹æ–‡çª—å£é™åˆ¶**](#ä¸Šä¸‹æ–‡çª—å£é™åˆ¶)
      - [**ä¸Šä¸‹æ–‡å‹ç¼©**](#ä¸Šä¸‹æ–‡å‹ç¼©)
  - [ä¸‰ã€æ¶æ„è®¾è®¡](#ä¸‰æ¶æ„è®¾è®¡)
    - [3.1 æ•´ä½“æ¶æ„](#31-æ•´ä½“æ¶æ„)
    - [3.2 å‘é‡å­˜å‚¨é›†æˆ](#32-å‘é‡å­˜å‚¨é›†æˆ)
    - [3.3 æ£€ç´¢å™¨è®¾è®¡](#33-æ£€ç´¢å™¨è®¾è®¡)
    - [3.4 é“¾å¼è°ƒç”¨ä¼˜åŒ–](#34-é“¾å¼è°ƒç”¨ä¼˜åŒ–)
  - [å››ã€ç¨‹åºè®¾è®¡](#å››ç¨‹åºè®¾è®¡)
    - [4.1 ç¯å¢ƒå‡†å¤‡](#41-ç¯å¢ƒå‡†å¤‡)
      - [**ä¾èµ–å®‰è£…**](#ä¾èµ–å®‰è£…)
      - [**PostgreSQLé…ç½®**](#postgresqlé…ç½®)
      - [**ç¯å¢ƒå˜é‡é…ç½®**](#ç¯å¢ƒå˜é‡é…ç½®)
    - [4.2 å‘é‡å­˜å‚¨é…ç½®](#42-å‘é‡å­˜å‚¨é…ç½®)
    - [4.3 æ–‡æ¡£å¤„ç†ä¸åŠ è½½](#43-æ–‡æ¡£å¤„ç†ä¸åŠ è½½)
    - [4.4 RAG åº”ç”¨å¼€å‘](#44-rag-åº”ç”¨å¼€å‘)
    - [4.5 é«˜çº§ç‰¹æ€§](#45-é«˜çº§ç‰¹æ€§)
      - [**Agentå¼€å‘**](#agentå¼€å‘)
      - [**Memoryç®¡ç†**](#memoryç®¡ç†)
  - [äº”ã€è¿ç»´ç®¡ç†](#äº”è¿ç»´ç®¡ç†)
    - [5.1 æ€§èƒ½ä¼˜åŒ–](#51-æ€§èƒ½ä¼˜åŒ–)
    - [5.2 ç›‘æ§å‘Šè­¦](#52-ç›‘æ§å‘Šè­¦)
    - [5.3 æˆæœ¬ä¼˜åŒ–](#53-æˆæœ¬ä¼˜åŒ–)
    - [5.4 æœ€ä½³å®è·µ](#54-æœ€ä½³å®è·µ)
  - [å…­ã€æ¡ˆä¾‹å®æˆ˜](#å…­æ¡ˆä¾‹å®æˆ˜)
    - [6.1 ä¼ä¸šçŸ¥è¯†åº“é—®ç­”](#61-ä¼ä¸šçŸ¥è¯†åº“é—®ç­”)
    - [6.2 æ–‡æ¡£æ™ºèƒ½æ‘˜è¦](#62-æ–‡æ¡£æ™ºèƒ½æ‘˜è¦)
    - [6.3 ä»£ç åŠ©æ‰‹](#63-ä»£ç åŠ©æ‰‹)
    - [6.4 å®¢æœæœºå™¨äºº](#64-å®¢æœæœºå™¨äºº)
  - [ä¸ƒã€æ€§èƒ½æµ‹è¯•](#ä¸ƒæ€§èƒ½æµ‹è¯•)
  - [å…«ã€æ€»ç»“ä¸å±•æœ›](#å…«æ€»ç»“ä¸å±•æœ›)
    - [æ ¸å¿ƒæ”¶è·](#æ ¸å¿ƒæ”¶è·)
    - [é€‚ç”¨åœºæ™¯](#é€‚ç”¨åœºæ™¯)
  - [ä¹ã€å‚è€ƒèµ„æ–™](#ä¹å‚è€ƒèµ„æ–™)

---

## ä¸€ã€æ¦‚è¿°

### 1.1 ä»€ä¹ˆæ˜¯ LangChain

**LangChain** æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨çš„æ¡†æ¶ï¼Œæä¾›äº†ï¼š

- ğŸ”— **é“¾å¼è°ƒç”¨**ï¼šå°†å¤šä¸ªç»„ä»¶ç»„åˆæˆå¤æ‚çš„åº”ç”¨
- ğŸ“š **æ–‡æ¡£åŠ è½½å™¨**ï¼šæ”¯æŒå¤šç§æ ¼å¼çš„æ–‡æ¡£åŠ è½½
- ğŸ” **å‘é‡å­˜å‚¨**ï¼šé›†æˆå¤šç§å‘é‡æ•°æ®åº“
- ğŸ¤– **Agent**ï¼šæ„å»ºè‡ªä¸»å†³ç­–çš„AIä»£ç†
- ğŸ’¬ **è®°å¿†ç®¡ç†**ï¼šç»´æŠ¤å¯¹è¯ä¸Šä¸‹æ–‡

**æ ¸å¿ƒç»„ä»¶**ï¼š

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LangChain æ ¸å¿ƒæ¶æ„              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ Models  â”‚    â”‚ Prompts â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚       â”‚              â”‚                  â”‚
â”‚       â–¼              â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚      Chains             â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚       â”‚                                 â”‚
â”‚       â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Memory  â”‚    â”‚ Retrieversâ”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚       â”‚              â”‚                  â”‚
â”‚       â–¼              â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚      Agents             â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 ä¸ºä»€ä¹ˆé€‰æ‹© PostgreSQL

**PostgreSQL + pgvector ä½œä¸ºå‘é‡å­˜å‚¨çš„ä¼˜åŠ¿**ï¼š

| ç‰¹æ€§ | PostgreSQL + pgvector | ä¸“ç”¨å‘é‡æ•°æ®åº“ |
|------|----------------------|---------------|
| **æ•°æ®ä¸€è‡´æ€§** | âœ… ACID äº‹åŠ¡æ”¯æŒ | âš ï¸ æœ€ç»ˆä¸€è‡´æ€§ |
| **æ··åˆæŸ¥è¯¢** | âœ… å‘é‡ + å…³ç³»å‹ | âŒ ä»…å‘é‡æŸ¥è¯¢ |
| **è¿ç»´æˆæœ¬** | âœ… ç»Ÿä¸€åŸºç¡€è®¾æ–½ | âŒ é¢å¤–ç»„ä»¶ |
| **ç”Ÿæ€æˆç†Ÿåº¦** | âœ… 40+ å¹´å†å² | âš ï¸ æ–°å…´æŠ€æœ¯ |
| **æˆæœ¬** | âœ… å¼€æºå…è´¹ | âš ï¸ å¯èƒ½æ”¶è´¹ |
| **å­¦ä¹ æ›²çº¿** | âœ… ç†Ÿæ‚‰çš„ SQL | âš ï¸ æ–°çš„ API |

### 1.3 æ ¸å¿ƒä»·å€¼

**æŠ€æœ¯ä»·å€¼**ï¼š

- ğŸ¯ **ç»Ÿä¸€æ•°æ®æ ˆ**ï¼šå‘é‡æ•°æ®å’Œå…³ç³»æ•°æ®åœ¨åŒä¸€æ•°æ®åº“
- âš¡ **é«˜æ€§èƒ½**ï¼šHNSW ç´¢å¼•ï¼Œæ¯«ç§’çº§æ£€ç´¢
- ğŸ” **æ•°æ®å®‰å…¨**ï¼šRLSã€åŠ å¯†ã€å®¡è®¡ç­‰ä¼ä¸šçº§ç‰¹æ€§
- ğŸ“ˆ **å¯æ‰©å±•**ï¼šæ”¯æŒ Citus åˆ†å¸ƒå¼æ‰©å±•

**ä¸šåŠ¡ä»·å€¼**ï¼š

- ğŸ’° **é™ä½æˆæœ¬**ï¼šæ— éœ€é¢å¤–å‘é‡æ•°æ®åº“
- ğŸš€ **å¿«é€Ÿå¼€å‘**ï¼šä¸°å¯Œçš„ LangChain é›†æˆ
- ğŸ›¡ï¸ **ä¼ä¸šçº§**ï¼šæˆç†Ÿçš„ PostgreSQL ç”Ÿæ€
- ğŸ”„ **æ˜“äºè¿ç§»**ï¼šæ ‡å‡† SQL æ¥å£

### 1.4 çŸ¥è¯†ä½“ç³»æ€ç»´å¯¼å›¾

```mermaid
mindmap
  root((LangChain + PostgreSQL))
    åŸç†ä¸ç†è®º
      LangChainæ¶æ„
        æ ¸å¿ƒç»„ä»¶
        å·¥ä½œæµç¨‹
        è®¾è®¡æ¨¡å¼
      RAGåŸç†
        æ£€ç´¢å¢å¼º
        ä¸Šä¸‹æ–‡æ„å»º
        ç”Ÿæˆä¼˜åŒ–
      å‘é‡åµŒå…¥
        Embeddingsæ¨¡å‹
        ç›¸ä¼¼åº¦è®¡ç®—
        ç´¢å¼•ä¼˜åŒ–
    æ¶æ„è®¾è®¡
      æ•´ä½“æ¶æ„
        åº”ç”¨å±‚
        LangChainå±‚
        æ•°æ®å±‚
      å‘é‡å­˜å‚¨
        PGVectoré…ç½®
        ç´¢å¼•ç­–ç•¥
        åˆ†åŒºç®¡ç†
      æ£€ç´¢å™¨è®¾è®¡
        åŸºç¡€æ£€ç´¢
        æ··åˆæ£€ç´¢
        é‡æ’åº
      é“¾å¼è°ƒç”¨
        ç®€å•é“¾
        å¤æ‚é“¾
        æ¡ä»¶é“¾
    ç¨‹åºè®¾è®¡
      ç¯å¢ƒé…ç½®
        ä¾èµ–å®‰è£…
        æ•°æ®åº“é…ç½®
        APIå¯†é’¥
      æ–‡æ¡£å¤„ç†
        åŠ è½½å™¨
        åˆ†å‰²å™¨
        å…ƒæ•°æ®
      RAGå¼€å‘
        å‘é‡åŒ–
        æ£€ç´¢
        ç”Ÿæˆ
      é«˜çº§ç‰¹æ€§
        Agent
        Memory
        Toolé›†æˆ
    è¿ç»´ç®¡ç†
      æ€§èƒ½ä¼˜åŒ–
        å‘é‡ç´¢å¼•
        ç¼“å­˜ç­–ç•¥
        æ‰¹å¤„ç†
      ç›‘æ§å‘Šè­¦
        æŸ¥è¯¢ç›‘æ§
        æˆæœ¬ç›‘æ§
        é”™è¯¯è¿½è¸ª
      æˆæœ¬ä¼˜åŒ–
        Tokenä¼˜åŒ–
        ç¼“å­˜å‘½ä¸­
        æ¨¡å‹é€‰æ‹©
    æ¡ˆä¾‹å®æˆ˜
      çŸ¥è¯†åº“é—®ç­”
        ä¼ä¸šæ–‡æ¡£
        æŠ€æœ¯æ‰‹å†Œ
        FAQç³»ç»Ÿ
      æ–‡æ¡£æ‘˜è¦
        é•¿æ–‡æ‘˜è¦
        å¤šæ–‡æ¡£æ‘˜è¦
        å®æ—¶æ‘˜è¦
      ä»£ç åŠ©æ‰‹
        ä»£ç æœç´¢
        ä»£ç ç”Ÿæˆ
        ä»£ç è§£é‡Š
      å®¢æœæœºå™¨äºº
        æ„å›¾è¯†åˆ«
        çŸ¥è¯†æ£€ç´¢
        å¯¹è¯ç®¡ç†
```

---

## äºŒã€åŸç†ä¸ç†è®º

### 2.1 LangChain æ¶æ„åŸç†

#### **æ ¸å¿ƒç»„ä»¶è¯¦è§£**

```python
# LangChainæ ¸å¿ƒç»„ä»¶ç¤ºä¾‹
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.vectorstores import PGVector
from langchain.embeddings import OpenAIEmbeddings

# 1. Models: è¯­è¨€æ¨¡å‹
llm = ChatOpenAI(
    model="gpt-4",
    temperature=0.7,
    max_tokens=2000
)

# 2. Prompts: æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_template(
    """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""
)

# 3. Chains: é“¾å¼è°ƒç”¨
chain = prompt | llm | StrOutputParser()

# 4. Vector Stores: å‘é‡å­˜å‚¨
vectorstore = PGVector(
    connection_string="postgresql://user:pass@localhost:5432/db",
    embedding_function=OpenAIEmbeddings(),
    collection_name="documents"
)

# 5. Retrievers: æ£€ç´¢å™¨
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}
)
```

#### **LCEL (LangChain Expression Language)**

```python
# LCEL: å£°æ˜å¼é“¾æ„å»º
from langchain.schema.runnable import RunnablePassthrough

# æ„å»ºRAGé“¾
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# ä½¿ç”¨
answer = rag_chain.invoke("ä»€ä¹ˆæ˜¯PostgreSQL?")
```

### 2.2 RAG æ£€ç´¢å¢å¼ºç”Ÿæˆ

#### **RAG å·¥ä½œåŸç†**

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) æµç¨‹                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                       â”‚
â”‚  1. ç”¨æˆ·æŸ¥è¯¢                                         â”‚
â”‚     "PostgreSQLçš„MVCCæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ"                â”‚
â”‚          â”‚                                           â”‚
â”‚          â–¼                                           â”‚
â”‚  2. æŸ¥è¯¢å‘é‡åŒ–                                       â”‚
â”‚     Embedding([0.123, -0.456, ...])                â”‚
â”‚          â”‚                                           â”‚
â”‚          â–¼                                           â”‚
â”‚  3. å‘é‡ç›¸ä¼¼åº¦æœç´¢ (PostgreSQL + pgvector)          â”‚
â”‚     SELECT content, embedding <=> query_embedding   â”‚
â”‚     FROM documents                                   â”‚
â”‚     ORDER BY embedding <=> query_embedding           â”‚
â”‚     LIMIT 5;                                         â”‚
â”‚          â”‚                                           â”‚
â”‚          â–¼                                           â”‚
â”‚  4. æ£€ç´¢ç›¸å…³æ–‡æ¡£                                     â”‚
â”‚     [Doc1: "MVCCé€šè¿‡...", Doc2: "å¤šç‰ˆæœ¬...", ...]  â”‚
â”‚          â”‚                                           â”‚
â”‚          â–¼                                           â”‚
â”‚  5. æ„å»ºä¸Šä¸‹æ–‡                                       â”‚
â”‚     Context = "ç›¸å…³æ–‡æ¡£1: ...\nç›¸å…³æ–‡æ¡£2: ..."      â”‚
â”‚          â”‚                                           â”‚
â”‚          â–¼                                           â”‚
â”‚  6. Promptæ„å»º                                       â”‚
â”‚     "åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n{context}\né—®é¢˜..."  â”‚
â”‚          â”‚                                           â”‚
â”‚          â–¼                                           â”‚
â”‚  7. LLMç”Ÿæˆå›ç­”                                      â”‚
â”‚     "PostgreSQLçš„MVCCä½¿ç”¨å¤šç‰ˆæœ¬å¹¶å‘æ§åˆ¶..."         â”‚
â”‚          â”‚                                           â”‚
â”‚          â–¼                                           â”‚
â”‚  8. è¿”å›ç»“æœ                                         â”‚
â”‚     Answer + Source Documents                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **å…³é”®æŠ€æœ¯ç‚¹**

```python
# 1. æ–‡æ¡£åˆ†å—ç­–ç•¥
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # æ¯å—1000å­—ç¬¦
    chunk_overlap=200,      # é‡å 200å­—ç¬¦
    separators=["\n\n", "\n", " ", ""]  # æŒ‰æ®µè½ã€å¥å­ã€è¯åˆ†å‰²
)

# 2. å…ƒæ•°æ®å¢å¼º
documents = [
    {
        "content": "PostgreSQLæ˜¯ä¸€ä¸ªå¼€æºæ•°æ®åº“...",
        "metadata": {
            "source": "postgresql_intro.pdf",
            "page": 1,
            "category": "database",
            "timestamp": "2024-12-04"
        }
    }
]

# 3. æ··åˆæœç´¢ï¼ˆå‘é‡ + å…³é”®è¯ï¼‰
from langchain.retrievers import EnsembleRetriever
from langchain.retrievers import BM25Retriever

bm25_retriever = BM25Retriever.from_documents(documents)
vector_retriever = vectorstore.as_retriever()

ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.3, 0.7]  # BM25å 30%ï¼Œå‘é‡å 70%
)
```

### 2.3 å‘é‡åµŒå…¥ä¸ç›¸ä¼¼åº¦æœç´¢

#### **åµŒå…¥æ¨¡å‹é€‰æ‹©**

| æ¨¡å‹ | ç»´åº¦ | æ€§èƒ½ | æˆæœ¬ | é€‚ç”¨åœºæ™¯ |
|------|------|------|------|---------|
| **OpenAI text-embedding-3-small** | 1536 | â­â­â­â­ | ğŸ’° | é€šç”¨ã€å¹³è¡¡ |
| **OpenAI text-embedding-3-large** | 3072 | â­â­â­â­â­ | ğŸ’°ğŸ’° | é«˜ç²¾åº¦ |
| **sentence-transformers/all-MiniLM-L6-v2** | 384 | â­â­â­ | å…è´¹ | å¼€æºã€å¿«é€Ÿ |
| **BAAI/bge-large-zh-v1.5** | 1024 | â­â­â­â­ | å…è´¹ | ä¸­æ–‡ä¼˜åŒ– |

#### **ç›¸ä¼¼åº¦è®¡ç®—**

```sql
-- PostgreSQLä¸­çš„å‘é‡ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰

-- 1. ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆcosine similarityï¼‰
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 FROM pg_extension
        WHERE extname = 'vector'
    ) THEN
        RAISE WARNING 'pgvectoræ‰©å±•æœªå®‰è£…ï¼Œå‘é‡æ“ä½œå¯èƒ½å¤±è´¥';
    END IF;

    IF NOT EXISTS (
        SELECT 1 FROM information_schema.tables
        WHERE table_schema = 'public' AND table_name = 'documents'
    ) THEN
        RAISE WARNING 'è¡¨documentsä¸å­˜åœ¨';
    END IF;

    RAISE NOTICE 'å¼€å§‹æ‰§è¡Œä½™å¼¦ç›¸ä¼¼åº¦æŸ¥è¯¢';
EXCEPTION
    WHEN OTHERS THEN
        RAISE WARNING 'æŸ¥è¯¢å‡†å¤‡å¤±è´¥: %', SQLERRM;
END $$;

EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT
    content,
    1 - (embedding <=> query_embedding) AS cosine_similarity
FROM documents
ORDER BY embedding <=> query_embedding
LIMIT 10;

-- 2. æ¬§æ°è·ç¦»ï¼ˆL2 distanceï¼‰
EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT
    content,
    embedding <-> query_embedding AS l2_distance
FROM documents
ORDER BY embedding <-> query_embedding
LIMIT 10;

-- 3. å†…ç§¯ï¼ˆinner productï¼‰
SELECT
    content,
    embedding <#> query_embedding AS inner_product
FROM documents
ORDER BY embedding <#> query_embedding DESC
LIMIT 10;
```

#### **ç´¢å¼•ä¼˜åŒ–**

```sql
-- HNSWç´¢å¼•ï¼ˆæ¨èï¼Œå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 FROM pg_extension
        WHERE extname = 'vector'
    ) THEN
        RAISE EXCEPTION 'pgvectoræ‰©å±•æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: CREATE EXTENSION vector;';
    END IF;

    IF NOT EXISTS (
        SELECT 1 FROM information_schema.tables
        WHERE table_schema = 'public' AND table_name = 'documents'
    ) THEN
        RAISE EXCEPTION 'è¡¨documentsä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»ºè¡¨';
    END IF;

    IF EXISTS (
        SELECT 1 FROM pg_indexes
        WHERE schemaname = 'public'
        AND tablename = 'documents'
        AND indexname LIKE '%hnsw%'
    ) THEN
        DROP INDEX IF EXISTS documents_embedding_hnsw_idx;
        RAISE NOTICE 'å·²åˆ é™¤ç°æœ‰HNSWç´¢å¼•';
    END IF;

    CREATE INDEX documents_embedding_hnsw_idx ON documents
    USING hnsw (embedding vector_cosine_ops)
    WITH (m = 16, ef_construction = 64);

    RAISE NOTICE 'HNSWç´¢å¼•åˆ›å»ºæˆåŠŸ';
EXCEPTION
    WHEN undefined_table THEN
        RAISE EXCEPTION 'è¡¨documentsä¸å­˜åœ¨';
    WHEN undefined_object THEN
        RAISE EXCEPTION 'hnswç´¢å¼•æ–¹æ³•ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥pgvectoræ‰©å±•å®‰è£…';
    WHEN OTHERS THEN
        RAISE EXCEPTION 'åˆ›å»ºHNSWç´¢å¼•å¤±è´¥: %', SQLERRM;
END $$;

-- å‚æ•°è¯´æ˜ï¼š
-- m: æ¯ä¸ªèŠ‚ç‚¹çš„æœ€å¤§è¿æ¥æ•°ï¼ˆ16æ˜¯é»˜è®¤å€¼ï¼Œè¶Šå¤§ç²¾åº¦è¶Šé«˜ä½†ç´¢å¼•è¶Šå¤§ï¼‰
-- ef_construction: æ„å»ºç´¢å¼•æ—¶çš„æœç´¢æ·±åº¦ï¼ˆ64æ˜¯é»˜è®¤å€¼ï¼Œè¶Šå¤§æ„å»ºè¶Šæ…¢ä½†ç²¾åº¦è¶Šé«˜ï¼‰

-- IVFFlatç´¢å¼•ï¼ˆå¤§è§„æ¨¡æ•°æ®ï¼Œå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 FROM pg_extension
        WHERE extname = 'vector'
    ) THEN
        RAISE EXCEPTION 'pgvectoræ‰©å±•æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: CREATE EXTENSION vector;';
    END IF;

    IF EXISTS (
        SELECT 1 FROM pg_indexes
        WHERE schemaname = 'public'
        AND tablename = 'documents'
        AND indexname LIKE '%ivfflat%'
    ) THEN
        DROP INDEX IF EXISTS documents_embedding_ivfflat_idx;
        RAISE NOTICE 'å·²åˆ é™¤ç°æœ‰IVFFlatç´¢å¼•';
    END IF;

    CREATE INDEX documents_embedding_ivfflat_idx ON documents
    USING ivfflat (embedding vector_cosine_ops)
    WITH (lists = 100);

    RAISE NOTICE 'IVFFlatç´¢å¼•åˆ›å»ºæˆåŠŸ';
EXCEPTION
    WHEN undefined_table THEN
        RAISE EXCEPTION 'è¡¨documentsä¸å­˜åœ¨';
    WHEN undefined_object THEN
        RAISE EXCEPTION 'ivfflatç´¢å¼•æ–¹æ³•ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥pgvectoræ‰©å±•å®‰è£…';
    WHEN OTHERS THEN
        RAISE EXCEPTION 'åˆ›å»ºIVFFlatç´¢å¼•å¤±è´¥: %', SQLERRM;
END $$;

-- æŸ¥è¯¢æ—¶è°ƒæ•´ç²¾åº¦
DO $$
BEGIN
    SET hnsw.ef_search = 40;  -- æŸ¥è¯¢æ—¶çš„æœç´¢æ·±åº¦
    RAISE NOTICE 'HNSWæœç´¢æ·±åº¦å·²è®¾ç½®ä¸º: 40';
EXCEPTION
    WHEN OTHERS THEN
        RAISE WARNING 'è®¾ç½®æœç´¢æ·±åº¦å¤±è´¥: %', SQLERRM;
END $$;
```

### 2.4 ä¸Šä¸‹æ–‡ç®¡ç†ä¸ä¼˜åŒ–

#### **ä¸Šä¸‹æ–‡çª—å£é™åˆ¶**

```python
# Tokenè®¡æ•°ä¸ç®¡ç†
import tiktoken

def count_tokens(text, model="gpt-4"):
    """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

def optimize_context(documents, max_tokens=6000):
    """ä¼˜åŒ–ä¸Šä¸‹æ–‡ä»¥é€‚åº”tokené™åˆ¶"""
    context = ""
    token_count = 0

    for doc in documents:
        doc_tokens = count_tokens(doc)
        if token_count + doc_tokens > max_tokens:
            break
        context += doc + "\n\n"
        token_count += doc_tokens

    return context, token_count
```

#### **ä¸Šä¸‹æ–‡å‹ç¼©**

```python
# ä½¿ç”¨LangChainçš„ContextualCompressionRetriever
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

# åˆ›å»ºå‹ç¼©å™¨
compressor = LLMChainExtractor.from_llm(llm)

# åˆ›å»ºå‹ç¼©æ£€ç´¢å™¨
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=retriever
)

# ä½¿ç”¨ï¼šè‡ªåŠ¨å‹ç¼©æ£€ç´¢åˆ°çš„æ–‡æ¡£
compressed_docs = compression_retriever.get_relevant_documents(
    "ä»€ä¹ˆæ˜¯PostgreSQLçš„MVCC?"
)
```

---

## ä¸‰ã€æ¶æ„è®¾è®¡

### 3.1 æ•´ä½“æ¶æ„

```python
# å®Œæ•´çš„LangChain + PostgreSQLæ¶æ„
"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 åº”ç”¨å±‚ (Application)                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   â”‚  Web UI  â”‚  â”‚  API     â”‚  â”‚  CLI     â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LangChainå±‚ (Framework)                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   â”‚  Chains  â”‚  â”‚  Agents  â”‚  â”‚  Memory  â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   â”‚Retrieversâ”‚  â”‚ Prompts  â”‚  â”‚  Tools   â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               æ•°æ®å±‚ (Data Layer)                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚        PostgreSQL + pgvector            â”‚        â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚        â”‚
â”‚   â”‚  â”‚ Vectors  â”‚  â”‚Relationalâ”‚           â”‚        â”‚
â”‚   â”‚  â”‚  Table   â”‚  â”‚  Tables  â”‚           â”‚        â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚        â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚        â”‚
â”‚   â”‚  â”‚  Indexes â”‚  â”‚  Cache   â”‚           â”‚        â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               å¤–éƒ¨æœåŠ¡ (External)                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   â”‚ OpenAI   â”‚  â”‚  Redis   â”‚  â”‚Monitoringâ”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""
```

### 3.2 å‘é‡å­˜å‚¨é›†æˆ

```python
# pgvector_store.py
from typing import List, Dict, Any, Optional
from langchain.vectorstores import PGVector
from langchain.embeddings.base import Embeddings
from langchain.schema import Document

class OptimizedPGVector:
    """ä¼˜åŒ–çš„PGVectorå‘é‡å­˜å‚¨"""

    def __init__(
        self,
        connection_string: str,
        embedding_function: Embeddings,
        collection_name: str = "documents",
        pre_delete_collection: bool = False
    ):
        self.connection_string = connection_string
        self.embedding_function = embedding_function
        self.collection_name = collection_name

        # åˆ›å»ºå‘é‡å­˜å‚¨
        self.vectorstore = PGVector(
            connection_string=connection_string,
            embedding_function=embedding_function,
            collection_name=collection_name,
            pre_delete_collection=pre_delete_collection
        )

        # åˆå§‹åŒ–ç´¢å¼•
        self._ensure_index()

    def _ensure_index(self):
        """ç¡®ä¿å‘é‡ç´¢å¼•å­˜åœ¨"""
        import psycopg2

        conn = psycopg2.connect(self.connection_string)
        with conn.cursor() as cur:
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            cur.execute(f"""
                SELECT 1 FROM pg_indexes
                WHERE tablename = '{self.collection_name}'
                  AND indexname = '{self.collection_name}_embedding_idx'
            """)

            if not cur.fetchone():
                # åˆ›å»ºHNSWç´¢å¼•
                cur.execute(f"""
                    CREATE INDEX {self.collection_name}_embedding_idx
                    ON {self.collection_name}
                    USING hnsw (embedding vector_cosine_ops)
                    WITH (m = 16, ef_construction = 64);
                """)
                conn.commit()
                print(f"âœ… Created HNSW index on {self.collection_name}")

        conn.close()

    def add_documents(
        self,
        documents: List[Document],
        batch_size: int = 100
    ) -> List[str]:
        """æ‰¹é‡æ·»åŠ æ–‡æ¡£"""
        doc_ids = []

        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            ids = self.vectorstore.add_documents(batch)
            doc_ids.extend(ids)
            print(f"Processed {min(i + batch_size, len(documents))}/{len(documents)} documents")

        return doc_ids

    def similarity_search_with_score(
        self,
        query: str,
        k: int = 5,
        filter: Optional[Dict[str, Any]] = None
    ) -> List[tuple[Document, float]]:
        """å¸¦è¯„åˆ†çš„ç›¸ä¼¼åº¦æœç´¢"""
        return self.vectorstore.similarity_search_with_score(
            query,
            k=k,
            filter=filter
        )

    def max_marginal_relevance_search(
        self,
        query: str,
        k: int = 5,
        fetch_k: int = 20,
        lambda_mult: float = 0.5
    ) -> List[Document]:
        """æœ€å¤§è¾¹é™…ç›¸å…³æ€§æœç´¢ï¼ˆé¿å…é‡å¤ç»“æœï¼‰"""
        return self.vectorstore.max_marginal_relevance_search(
            query,
            k=k,
            fetch_k=fetch_k,
            lambda_mult=lambda_mult
        )
```

### 3.3 æ£€ç´¢å™¨è®¾è®¡

```python
# advanced_retrievers.py
from langchain.retrievers import (
    MultiQueryRetriever,
    ContextualCompressionRetriever,
    EnsembleRetriever
)
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.prompts import PromptTemplate

class AdvancedRetriever:
    """é«˜çº§æ£€ç´¢å™¨é›†åˆ"""

    def __init__(self, vectorstore, llm):
        self.vectorstore = vectorstore
        self.llm = llm
        self.base_retriever = vectorstore.as_retriever()

    def multi_query_retriever(self) -> MultiQueryRetriever:
        """å¤šæŸ¥è¯¢æ£€ç´¢å™¨ï¼šç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å˜ä½“"""
        return MultiQueryRetriever.from_llm(
            retriever=self.base_retriever,
            llm=self.llm
        )

    def compressed_retriever(self) -> ContextualCompressionRetriever:
        """å‹ç¼©æ£€ç´¢å™¨ï¼šè‡ªåŠ¨å‹ç¼©æ£€ç´¢ç»“æœ"""
        compressor = LLMChainExtractor.from_llm(self.llm)
        return ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=self.base_retriever
        )

    def self_query_retriever(self):
        """è‡ªæŸ¥è¯¢æ£€ç´¢å™¨ï¼šä»è‡ªç„¶è¯­è¨€ç”Ÿæˆç»“æ„åŒ–æŸ¥è¯¢"""
        from langchain.retrievers.self_query.base import SelfQueryRetriever
        from langchain.chains.query_constructor.base import AttributeInfo

        metadata_field_info = [
            AttributeInfo(
                name="source",
                description="æ–‡æ¡£æ¥æº",
                type="string"
            ),
            AttributeInfo(
                name="page",
                description="é¡µç ",
                type="integer"
            )
        ]

        return SelfQueryRetriever.from_llm(
            llm=self.llm,
            vectorstore=self.vectorstore,
            document_contents="PostgreSQLæŠ€æœ¯æ–‡æ¡£",
            metadata_field_info=metadata_field_info
        )

    def parent_document_retriever(self):
        """çˆ¶æ–‡æ¡£æ£€ç´¢å™¨ï¼šæ£€ç´¢å°å—ï¼Œè¿”å›å¤§å—"""
        from langchain.retrievers import ParentDocumentRetriever
        from langchain.storage import InMemoryStore

        # ç”¨äºå­˜å‚¨å®Œæ•´æ–‡æ¡£
        docstore = InMemoryStore()

        return ParentDocumentRetriever(
            vectorstore=self.vectorstore,
            docstore=docstore,
            child_splitter=self._get_child_splitter(),
            parent_splitter=self._get_parent_splitter()
        )

    def _get_child_splitter(self):
        """å­æ–‡æ¡£åˆ†å‰²å™¨ï¼ˆå°å—ï¼‰"""
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        return RecursiveCharacterTextSplitter(chunk_size=400)

    def _get_parent_splitter(self):
        """çˆ¶æ–‡æ¡£åˆ†å‰²å™¨ï¼ˆå¤§å—ï¼‰"""
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        return RecursiveCharacterTextSplitter(chunk_size=2000)
```

### 3.4 é“¾å¼è°ƒç”¨ä¼˜åŒ–

```python
# optimized_chains.py
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.schema.output_parser import StrOutputParser
from operator import itemgetter

class OptimizedChains:
    """ä¼˜åŒ–çš„é“¾å¼è°ƒç”¨"""

    def __init__(self, retriever, llm, prompt_template):
        self.retriever = retriever
        self.llm = llm
        self.prompt = prompt_template

    def basic_rag_chain(self):
        """åŸºç¡€RAGé“¾"""
        return (
            {"context": self.retriever, "question": RunnablePassthrough()}
            | self.prompt
            | self.llm
            | StrOutputParser()
        )

    def rag_chain_with_sources(self):
        """å¸¦æ¥æºçš„RAGé“¾"""
        from langchain.schema.runnable import RunnableMap

        return (
            RunnableMap({
                "context": self.retriever,
                "question": RunnablePassthrough()
            })
            | {
                "answer": self.prompt | self.llm | StrOutputParser(),
                "sources": lambda x: x["context"]
            }
        )

    def conversational_rag_chain(self, memory):
        """å¯¹è¯å¼RAGé“¾"""
        from langchain.schema.runnable import RunnableMap

        def format_chat_history(messages):
            return "\n".join([f"{m.type}: {m.content}" for m in messages])

        return (
            RunnableMap({
                "context": itemgetter("question") | self.retriever,
                "question": itemgetter("question"),
                "chat_history": itemgetter("chat_history") | RunnableLambda(format_chat_history)
            })
            | self.prompt
            | self.llm
            | StrOutputParser()
        )

    def multi_step_rag_chain(self):
        """å¤šæ­¥éª¤RAGé“¾"""
        # æ­¥éª¤1ï¼šé‡å†™æŸ¥è¯¢
        query_rewriter = (
            ChatPromptTemplate.from_template(
                "å°†ä»¥ä¸‹æŸ¥è¯¢æ”¹å†™å¾—æ›´æ¸…æ™°ï¼š{question}"
            )
            | self.llm
            | StrOutputParser()
        )

        # æ­¥éª¤2ï¼šæ£€ç´¢
        # æ­¥éª¤3ï¼šç”Ÿæˆå›ç­”
        return (
            {"rewritten_query": query_rewriter}
            | {"context": lambda x: self.retriever.get_relevant_documents(x["rewritten_query"]),
               "question": itemgetter("question")}
            | self.prompt
            | self.llm
            | StrOutputParser()
        )
```

---

## å››ã€ç¨‹åºè®¾è®¡

### 4.1 ç¯å¢ƒå‡†å¤‡

#### **ä¾èµ–å®‰è£…**

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv langchain_env
source langchain_env/bin/activate  # Linux/Mac
# langchain_env\Scripts\activate  # Windows

# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install langchain==0.1.0
pip install langchain-openai==0.0.2
pip install langchain-community==0.0.10
pip install pgvector==0.2.4
pip install psycopg2-binary==2.9.9
pip install tiktoken==0.5.2

# å®‰è£…æ–‡æ¡£å¤„ç†ä¾èµ–
pip install pypdf==3.17.0
pip install docx2txt==0.8
pip install unstructured==0.11.0

# åˆ›å»ºrequirements.txt
cat > requirements.txt <<EOF
langchain==0.1.0
langchain-openai==0.0.2
langchain-community==0.0.10
pgvector==0.2.4
psycopg2-binary==2.9.9
tiktoken==0.5.2
pypdf==3.17.0
docx2txt==0.8
unstructured==0.11.0
python-dotenv==1.0.0
EOF
```

#### **PostgreSQLé…ç½®**

```sql
-- åˆ›å»ºæ•°æ®åº“å’Œæ‰©å±•ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 FROM pg_database
        WHERE datname = 'langchain_db'
    ) THEN
        PERFORM dblink_exec('dbname=postgres', 'CREATE DATABASE langchain_db');
        RAISE NOTICE 'æ•°æ®åº“ langchain_db åˆ›å»ºæˆåŠŸ';
    ELSE
        RAISE NOTICE 'æ•°æ®åº“ langchain_db å·²å­˜åœ¨';
    END IF;
EXCEPTION
    WHEN OTHERS THEN
        RAISE WARNING 'åˆ›å»ºæ•°æ®åº“å¤±è´¥: %', SQLERRM;
END $$;

-- åˆ‡æ¢åˆ°æ–°æ•°æ®åº“ï¼ˆéœ€è¦åœ¨psqlä¸­æ‰§è¡Œï¼‰
-- \c langchain_db

-- å®‰è£…pgvectoræ‰©å±•ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 FROM pg_extension
        WHERE extname = 'vector'
    ) THEN
        CREATE EXTENSION vector;
        RAISE NOTICE 'pgvectoræ‰©å±•å®‰è£…æˆåŠŸ';
    ELSE
        RAISE NOTICE 'pgvectoræ‰©å±•å·²å­˜åœ¨';
    END IF;
EXCEPTION
    WHEN undefined_file THEN
        RAISE EXCEPTION 'pgvectoræ‰©å±•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥PostgreSQLå®‰è£…';
    WHEN OTHERS THEN
        RAISE EXCEPTION 'å®‰è£…pgvectoræ‰©å±•å¤±è´¥: %', SQLERRM;
END $$;

-- åˆ›å»ºå‘é‡è¡¨ï¼ˆLangChainä¼šè‡ªåŠ¨åˆ›å»ºï¼Œè¿™é‡Œå±•ç¤ºç»“æ„ï¼Œå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    IF EXISTS (
        SELECT 1 FROM information_schema.tables
        WHERE table_schema = 'public' AND table_name = 'langchain_pg_embedding'
    ) THEN
        RAISE NOTICE 'è¡¨ langchain_pg_embedding å·²å­˜åœ¨';
    ELSE
        CREATE TABLE langchain_pg_embedding (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            collection_id UUID,
            embedding VECTOR(1536),  -- OpenAI embeddingsç»´åº¦
            document TEXT,
            cmetadata JSONB,
            custom_id TEXT,
            created_at TIMESTAMPTZ DEFAULT NOW()
        );
        RAISE NOTICE 'è¡¨ langchain_pg_embedding åˆ›å»ºæˆåŠŸ';
    END IF;
EXCEPTION
    WHEN duplicate_table THEN
        RAISE NOTICE 'è¡¨ langchain_pg_embedding å·²å­˜åœ¨';
    WHEN undefined_type THEN
        RAISE EXCEPTION 'VECTORç±»å‹ä¸å­˜åœ¨ï¼Œè¯·å…ˆå®‰è£…pgvectoræ‰©å±•';
    WHEN OTHERS THEN
        RAISE EXCEPTION 'åˆ›å»ºè¡¨å¤±è´¥: %', SQLERRM;
END $$;

-- åˆ›å»ºç´¢å¼•ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 FROM information_schema.tables
        WHERE table_schema = 'public' AND table_name = 'langchain_pg_embedding'
    ) THEN
        RAISE EXCEPTION 'è¡¨ langchain_pg_embedding ä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»ºè¡¨';
    END IF;

    -- HNSWå‘é‡ç´¢å¼•
    IF NOT EXISTS (
        SELECT 1 FROM pg_indexes
        WHERE schemaname = 'public'
        AND tablename = 'langchain_pg_embedding'
        AND indexname = 'langchain_pg_embedding_embedding_idx'
    ) THEN
        CREATE INDEX langchain_pg_embedding_embedding_idx
        ON langchain_pg_embedding USING hnsw (embedding vector_cosine_ops);
        RAISE NOTICE 'HNSWå‘é‡ç´¢å¼•åˆ›å»ºæˆåŠŸ';
    END IF;

    -- B-treeç´¢å¼•
    IF NOT EXISTS (
        SELECT 1 FROM pg_indexes
        WHERE schemaname = 'public'
        AND tablename = 'langchain_pg_embedding'
        AND indexname = 'langchain_pg_embedding_collection_id_idx'
    ) THEN
        CREATE INDEX langchain_pg_embedding_collection_id_idx
        ON langchain_pg_embedding USING btree (collection_id);
        RAISE NOTICE 'B-treeç´¢å¼•åˆ›å»ºæˆåŠŸ';
    END IF;

    -- GINç´¢å¼•
    IF NOT EXISTS (
        SELECT 1 FROM pg_indexes
        WHERE schemaname = 'public'
        AND tablename = 'langchain_pg_embedding'
        AND indexname = 'langchain_pg_embedding_cmetadata_idx'
    ) THEN
        CREATE INDEX langchain_pg_embedding_cmetadata_idx
        ON langchain_pg_embedding USING gin (cmetadata);
        RAISE NOTICE 'GINç´¢å¼•åˆ›å»ºæˆåŠŸ';
    END IF;
EXCEPTION
    WHEN undefined_table THEN
        RAISE EXCEPTION 'è¡¨ langchain_pg_embedding ä¸å­˜åœ¨';
    WHEN undefined_object THEN
        RAISE EXCEPTION 'ç´¢å¼•æ–¹æ³•ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥æ‰©å±•å®‰è£…';
    WHEN OTHERS THEN
        RAISE EXCEPTION 'åˆ›å»ºç´¢å¼•å¤±è´¥: %', SQLERRM;
END $$;
```

#### **ç¯å¢ƒå˜é‡é…ç½®**

```python
# .env æ–‡ä»¶
OPENAI_API_KEY=sk-your-openai-api-key
DATABASE_URL=postgresql://user:password@localhost:5432/langchain_db
EMBEDDING_MODEL=text-embedding-3-small
LLM_MODEL=gpt-4
LANGCHAIN_TRACING_V2=true  # å¯ç”¨LangSmithè¿½è¸ª
LANGCHAIN_API_KEY=your-langsmith-api-key
```

```python
# config.py
import os
from dotenv import load_dotenv

load_dotenv()

class Config:
    # OpenAIé…ç½®
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
    LLM_MODEL = os.getenv("LLM_MODEL", "gpt-4")

    # æ•°æ®åº“é…ç½®
    DATABASE_URL = os.getenv("DATABASE_URL")

    # åº”ç”¨é…ç½®
    CHUNK_SIZE = 1000
    CHUNK_OVERLAP = 200
    TOP_K = 5

    # LangSmithè¿½è¸ª
    LANGCHAIN_TRACING_V2 = os.getenv("LANGCHAIN_TRACING_V2", "false") == "true"
    LANGCHAIN_API_KEY = os.getenv("LANGCHAIN_API_KEY")
```

### 4.2 å‘é‡å­˜å‚¨é…ç½®

```python
# vector_store_setup.py
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import PGVector
from config import Config

def initialize_vector_store(collection_name="documents"):
    """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""

    # åˆ›å»ºembeddings
    embeddings = OpenAIEmbeddings(
        model=Config.EMBEDDING_MODEL,
        openai_api_key=Config.OPENAI_API_KEY
    )

    # åˆ›å»ºPGVectorå­˜å‚¨
    vectorstore = PGVector(
        connection_string=Config.DATABASE_URL,
        embedding_function=embeddings,
        collection_name=collection_name
    )

    return vectorstore

def test_vector_store():
    """æµ‹è¯•å‘é‡å­˜å‚¨"""
    from langchain.schema import Document

    vectorstore = initialize_vector_store("test_collection")

    # æµ‹è¯•æ–‡æ¡£
    test_docs = [
        Document(
            page_content="PostgreSQLæ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€æºå…³ç³»æ•°æ®åº“ã€‚",
            metadata={"source": "test", "id": 1}
        ),
        Document(
            page_content="pgvectoræ˜¯PostgreSQLçš„å‘é‡æ‰©å±•ã€‚",
            metadata={"source": "test", "id": 2}
        )
    ]

    # æ·»åŠ æ–‡æ¡£
    ids = vectorstore.add_documents(test_docs)
    print(f"âœ… Added {len(ids)} documents")

    # æµ‹è¯•æœç´¢
    results = vectorstore.similarity_search("ä»€ä¹ˆæ˜¯PostgreSQL?", k=2)
    print(f"âœ… Found {len(results)} results")
    for i, doc in enumerate(results):
        print(f"\nç»“æœ {i+1}:")
        print(f"å†…å®¹: {doc.page_content}")
        print(f"å…ƒæ•°æ®: {doc.metadata}")

if __name__ == "__main__":
    test_vector_store()
```

### 4.3 æ–‡æ¡£å¤„ç†ä¸åŠ è½½

```python
# document_processor.py
from typing import List
from langchain.schema import Document
from langchain.document_loaders import (
    PyPDFLoader,
    TextLoader,
    DirectoryLoader,
    UnstructuredMarkdownLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pathlib import Path

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""

    def __init__(
        self,
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        # æ–‡æœ¬åˆ†å‰²å™¨
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", " ", ""],
            length_function=len
        )

    def load_pdf(self, file_path: str) -> List[Document]:
        """åŠ è½½PDFæ–‡ä»¶"""
        loader = PyPDFLoader(file_path)
        documents = loader.load()
        return self.split_documents(documents)

    def load_directory(
        self,
        directory: str,
        glob_pattern: str = "**/*.md"
    ) -> List[Document]:
        """åŠ è½½ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡æ¡£"""
        loader = DirectoryLoader(
            directory,
            glob=glob_pattern,
            loader_cls=UnstructuredMarkdownLoader
        )
        documents = loader.load()
        return self.split_documents(documents)

    def load_text(self, file_path: str) -> List[Document]:
        """åŠ è½½æ–‡æœ¬æ–‡ä»¶"""
        loader = TextLoader(file_path, encoding='utf-8')
        documents = loader.load()
        return self.split_documents(documents)

    def split_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£"""
        return self.splitter.split_documents(documents)

    def add_metadata(
        self,
        documents: List[Document],
        metadata: dict
    ) -> List[Document]:
        """æ·»åŠ å…ƒæ•°æ®"""
        for doc in documents:
            doc.metadata.update(metadata)
        return documents

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    processor = DocumentProcessor(chunk_size=1000, chunk_overlap=200)

    # åŠ è½½PDF
    pdf_docs = processor.load_pdf("postgresql_manual.pdf")
    print(f"âœ… Loaded {len(pdf_docs)} chunks from PDF")

    # åŠ è½½Markdownæ–‡ä»¶
    md_docs = processor.load_directory("./docs", glob_pattern="**/*.md")
    print(f"âœ… Loaded {len(md_docs)} chunks from Markdown files")

    # æ·»åŠ å…ƒæ•°æ®
    md_docs = processor.add_metadata(
        md_docs,
        {"category": "documentation", "version": "18.0"}
    )
```

### 4.4 RAG åº”ç”¨å¼€å‘

```python
# rag_application.py
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from config import Config

class RAGApplication:
    """RAGåº”ç”¨"""

    def __init__(self, vectorstore):
        self.vectorstore = vectorstore

        # LLM
        self.llm = ChatOpenAI(
            model=Config.LLM_MODEL,
            temperature=0.7,
            openai_api_key=Config.OPENAI_API_KEY
        )

        # Promptæ¨¡æ¿
        self.prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä¸ªPostgreSQLä¸“å®¶åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. å‡†ç¡®ã€ä¸“ä¸š
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. æä¾›ä»£ç ç¤ºä¾‹ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
4. å¼•ç”¨æ¥æº

å›ç­”ï¼š
""")

        # æ£€ç´¢å™¨
        self.retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": Config.TOP_K}
        )

        # æ„å»ºRAGé“¾
        self.chain = self._build_chain()

    def _build_chain(self):
        """æ„å»ºRAGé“¾"""
        def format_docs(docs):
            return "\n\n".join([
                f"æ–‡æ¡£ {i+1} (æ¥æº: {doc.metadata.get('source', 'unknown')}):\n{doc.page_content}"
                for i, doc in enumerate(docs)
            ])

        return (
            {
                "context": self.retriever | format_docs,
                "question": RunnablePassthrough()
            }
            | self.prompt
            | self.llm
            | StrOutputParser()
        )

    def query(self, question: str) -> dict:
        """æŸ¥è¯¢æ¥å£"""
        # æ‰§è¡Œæ£€ç´¢
        retrieved_docs = self.retriever.get_relevant_documents(question)

        # ç”Ÿæˆå›ç­”
        answer = self.chain.invoke(question)

        return {
            "question": question,
            "answer": answer,
            "sources": [
                {
                    "content": doc.page_content[:200] + "...",
                    "metadata": doc.metadata
                }
                for doc in retrieved_docs
            ]
        }

    def query_with_history(
        self,
        question: str,
        chat_history: List[tuple[str, str]]
    ) -> dict:
        """å¸¦å†å²çš„æŸ¥è¯¢"""
        from langchain.schema.runnable import RunnableMap
        from operator import itemgetter

        # æ„å»ºå¯¹è¯å†å²å­—ç¬¦ä¸²
        history_str = "\n".join([
            f"ç”¨æˆ·: {q}\nåŠ©æ‰‹: {a}"
            for q, a in chat_history
        ])

        # æ›´æ–°prompt
        contextualized_prompt = ChatPromptTemplate.from_template("""
åŸºäºä»¥ä¸‹å¯¹è¯å†å²å’Œä¸Šä¸‹æ–‡ï¼Œå›ç­”æœ€æ–°çš„é—®é¢˜ã€‚

å¯¹è¯å†å²ï¼š
{chat_history}

ä¸Šä¸‹æ–‡ï¼š
{context}

æœ€æ–°é—®é¢˜ï¼š{question}

å›ç­”ï¼š
""")

        # æ„å»ºé“¾
        chain = (
            RunnableMap({
                "context": itemgetter("question") | self.retriever | self._format_docs,
                "chat_history": itemgetter("chat_history"),
                "question": itemgetter("question")
            })
            | contextualized_prompt
            | self.llm
            | StrOutputParser()
        )

        answer = chain.invoke({
            "question": question,
            "chat_history": history_str
        })

        return {
            "question": question,
            "answer": answer
        }

    @staticmethod
    def _format_docs(docs):
        return "\n\n".join([doc.page_content for doc in docs])

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    from vector_store_setup import initialize_vector_store

    # åˆå§‹åŒ–
    vectorstore = initialize_vector_store()
    rag_app = RAGApplication(vectorstore)

    # æŸ¥è¯¢
    result = rag_app.query("PostgreSQLçš„MVCCæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ")

    print("é—®é¢˜:", result["question"])
    print("\nå›ç­”:", result["answer"])
    print("\næ¥æº:")
    for i, source in enumerate(result["sources"]):
        print(f"\næ¥æº {i+1}:")
        print(f"  å†…å®¹: {source['content']}")
        print(f"  å…ƒæ•°æ®: {source['metadata']}")
```

### 4.5 é«˜çº§ç‰¹æ€§

#### **Agentå¼€å‘**

```python
# agent_application.py
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain.tools import Tool
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

class PostgreSQLAgent:
    """PostgreSQLä¸“å®¶Agent"""

    def __init__(self, vectorstore, llm):
        self.vectorstore = vectorstore
        self.llm = llm
        self.tools = self._create_tools()
        self.agent = self._create_agent()

    def _create_tools(self):
        """åˆ›å»ºå·¥å…·"""
        # 1. æ–‡æ¡£æœç´¢å·¥å…·
        doc_search_tool = Tool(
            name="DocumentSearch",
            func=self._search_documents,
            description="æœç´¢PostgreSQLæ–‡æ¡£ã€‚è¾“å…¥ï¼šæŸ¥è¯¢å­—ç¬¦ä¸²ã€‚è¾“å‡ºï¼šç›¸å…³æ–‡æ¡£å†…å®¹ã€‚"
        )

        # 2. SQLæ‰§è¡Œå·¥å…·ï¼ˆæ¼”ç¤ºç”¨ï¼Œå®é™…éœ€è¦å®‰å…¨æ£€æŸ¥ï¼‰
        sql_tool = Tool(
            name="SQLExecutor",
            func=self._execute_sql,
            description="æ‰§è¡ŒPostgreSQLæŸ¥è¯¢ã€‚è¾“å…¥ï¼šSQLè¯­å¥ã€‚è¾“å‡ºï¼šæŸ¥è¯¢ç»“æœã€‚"
        )

        # 3. æ€§èƒ½åˆ†æå·¥å…·
        explain_tool = Tool(
            name="ExplainAnalyze",
            func=self._explain_query,
            description="åˆ†æSQLæŸ¥è¯¢æ€§èƒ½ã€‚è¾“å…¥ï¼šSQLè¯­å¥ã€‚è¾“å‡ºï¼šEXPLAIN (ANALYZE, BUFFERS, TIMING)ç»“æœã€‚"
        )

        return [doc_search_tool, sql_tool, explain_tool]

    def _search_documents(self, query: str) -> str:
        """æœç´¢æ–‡æ¡£"""
        docs = self.vectorstore.similarity_search(query, k=3)
        return "\n\n".join([doc.page_content for doc in docs])

    def _execute_sql(self, sql: str) -> str:
        """æ‰§è¡ŒSQLï¼ˆéœ€è¦å®‰å…¨æ£€æŸ¥ï¼‰"""
        import psycopg2
        from config import Config

        try:
            conn = psycopg2.connect(Config.DATABASE_URL)
            with conn.cursor() as cur:
                cur.execute(sql)
                if cur.description:  # SELECTæŸ¥è¯¢
                    results = cur.fetchall()
                    return str(results[:10])  # é™åˆ¶è¿”å›ç»“æœ
                else:  # DMLæŸ¥è¯¢
                    conn.commit()
                    return f"Query executed successfully. Rows affected: {cur.rowcount}"
        except Exception as e:
            return f"Error: {str(e)}"
        finally:
            conn.close()

    def _explain_query(self, sql: str) -> str:
        """EXPLAIN (ANALYZE, BUFFERS, TIMING)"""
        import psycopg2
        from config import Config

        try:
            conn = psycopg2.connect(Config.DATABASE_URL)
            with conn.cursor() as cur:
                cur.execute(f"EXPLAIN (ANALYZE, BUFFERS, TIMING) {sql}")
                results = cur.fetchall()
                return "\n".join([row[0] for row in results])
        except Exception as e:
            return f"Error: {str(e)}"
        finally:
            conn.close()

    def _create_agent(self):
        """åˆ›å»ºAgent"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªPostgreSQLä¸“å®¶åŠ©æ‰‹ã€‚ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ï¼š

1. DocumentSearch: æœç´¢PostgreSQLæ–‡æ¡£
2. SQLExecutor: æ‰§è¡ŒSQLæŸ¥è¯¢
3. ExplainAnalyze: åˆ†ææŸ¥è¯¢æ€§èƒ½

è¯·æ ¹æ®ç”¨æˆ·é—®é¢˜é€‰æ‹©åˆé€‚çš„å·¥å…·ã€‚"""),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad")
        ])

        agent = create_openai_functions_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=prompt
        )

        return AgentExecutor(
            agent=agent,
            tools=self.tools,
            verbose=True,
            max_iterations=5
        )

    def run(self, query: str) -> str:
        """è¿è¡ŒAgent"""
        return self.agent.invoke({"input": query})
```

#### **Memoryç®¡ç†**

```python
# memory_management.py
from langchain.memory import (
    ConversationBufferMemory,
    ConversationSummaryMemory,
    ConversationBufferWindowMemory
)
from langchain.schema import BaseMessage

class ConversationManager:
    """å¯¹è¯ç®¡ç†å™¨"""

    def __init__(self, llm, memory_type="buffer"):
        self.llm = llm
        self.memory = self._create_memory(memory_type)

    def _create_memory(self, memory_type: str):
        """åˆ›å»ºMemory"""
        if memory_type == "buffer":
            # ç¼“å†²æ‰€æœ‰æ¶ˆæ¯
            return ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True
            )
        elif memory_type == "window":
            # åªä¿ç•™æœ€è¿‘Næ¡æ¶ˆæ¯
            return ConversationBufferWindowMemory(
                k=5,  # ä¿ç•™æœ€è¿‘5æ¡æ¶ˆæ¯
                memory_key="chat_history",
                return_messages=True
            )
        elif memory_type == "summary":
            # æ€»ç»“å†å²æ¶ˆæ¯
            return ConversationSummaryMemory(
                llm=self.llm,
                memory_key="chat_history",
                return_messages=True
            )
        else:
            raise ValueError(f"Unknown memory type: {memory_type}")

    def add_message(self, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯"""
        if role == "user":
            self.memory.chat_memory.add_user_message(content)
        elif role == "assistant":
            self.memory.chat_memory.add_ai_message(content)

    def get_history(self) -> List[BaseMessage]:
        """è·å–å†å²"""
        return self.memory.load_memory_variables({})["chat_history"]

    def clear(self):
        """æ¸…ç©ºå†å²"""
        self.memory.clear()
```

---

## äº”ã€è¿ç»´ç®¡ç†

### 5.1 æ€§èƒ½ä¼˜åŒ–

```python
# performance_optimization.py
import time
from functools import lru_cache
import hashlib

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""

    def __init__(self, vectorstore, llm):
        self.vectorstore = vectorstore
        self.llm = llm
        self.cache = {}
        self.stats = {
            'cache_hits': 0,
            'cache_misses': 0,
            'total_queries': 0
        }

    @lru_cache(maxsize=1000)
    def cached_embedding(self, text: str):
        """ç¼“å­˜embeddingè®¡ç®—"""
        return self.vectorstore.embedding_function.embed_query(text)

    def cached_retrieval(self, query: str, k: int = 5) -> list:
        """ç¼“å­˜æ£€ç´¢ç»“æœ"""
        cache_key = hashlib.md5(f"{query}_{k}".encode()).hexdigest()

        self.stats['total_queries'] += 1

        if cache_key in self.cache:
            self.stats['cache_hits'] += 1
            return self.cache[cache_key]

        self.stats['cache_misses'] += 1
        results = self.vectorstore.similarity_search(query, k=k)
        self.cache[cache_key] = results

        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(self.cache) > 1000:
            # åˆ é™¤æœ€è€çš„æ¡ç›®
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]

        return results

    def batch_embed_documents(self, texts: List[str], batch_size: int = 100):
        """æ‰¹é‡åµŒå…¥æ–‡æ¡£"""
        embeddings = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_embeddings = self.vectorstore.embedding_function.embed_documents(batch)
            embeddings.extend(batch_embeddings)
            time.sleep(0.1)  # é¿å…é¢‘ç‡é™åˆ¶
        return embeddings

    def get_cache_stats(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total = self.stats['total_queries']
        hits = self.stats['cache_hits']
        hit_rate = hits / total if total > 0 else 0

        return {
            'total_queries': total,
            'cache_hits': hits,
            'cache_misses': self.stats['cache_misses'],
            'hit_rate': f"{hit_rate:.2%}"
        }
```

### 5.2 ç›‘æ§å‘Šè­¦

```python
# monitoring.py
import logging
from datetime import datetime
from typing import Dict, Any

class RAGMonitor:
    """RAGåº”ç”¨ç›‘æ§"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.metrics = []

    def log_query(
        self,
        query: str,
        answer: str,
        retrieval_time: float,
        generation_time: float,
        num_docs_retrieved: int,
        tokens_used: int
    ):
        """è®°å½•æŸ¥è¯¢æŒ‡æ ‡"""
        metric = {
            'timestamp': datetime.now(),
            'query_length': len(query),
            'answer_length': len(answer),
            'retrieval_time': retrieval_time,
            'generation_time': generation_time,
            'total_time': retrieval_time + generation_time,
            'num_docs_retrieved': num_docs_retrieved,
            'tokens_used': tokens_used
        }

        self.metrics.append(metric)

        # å‘Šè­¦æ£€æŸ¥
        if retrieval_time > 2.0:
            self.logger.warning(f"Slow retrieval: {retrieval_time:.2f}s")

        if generation_time > 10.0:
            self.logger.warning(f"Slow generation: {generation_time:.2f}s")

        if tokens_used > 6000:
            self.logger.warning(f"High token usage: {tokens_used}")

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡æ•°æ®"""
        if not self.metrics:
            return {}

        import numpy as np

        retrieval_times = [m['retrieval_time'] for m in self.metrics]
        generation_times = [m['generation_time'] for m in self.metrics]
        total_times = [m['total_time'] for m in self.metrics]
        tokens = [m['tokens_used'] for m in self.metrics]

        return {
            'total_queries': len(self.metrics),
            'avg_retrieval_time': np.mean(retrieval_times),
            'p95_retrieval_time': np.percentile(retrieval_times, 95),
            'avg_generation_time': np.mean(generation_times),
            'p95_generation_time': np.percentile(generation_times, 95),
            'avg_total_time': np.mean(total_times),
            'p95_total_time': np.percentile(total_times, 95),
            'avg_tokens': np.mean(tokens),
            'total_tokens': sum(tokens)
        }
```

### 5.3 æˆæœ¬ä¼˜åŒ–

```python
# cost_optimization.py
import tiktoken

class CostOptimizer:
    """æˆæœ¬ä¼˜åŒ–å™¨"""

    # OpenAIå®šä»·ï¼ˆ2024å¹´12æœˆï¼‰
    PRICING = {
        'text-embedding-3-small': {
            'input': 0.02 / 1_000_000  # $0.02 per 1M tokens
        },
        'text-embedding-3-large': {
            'input': 0.13 / 1_000_000  # $0.13 per 1M tokens
        },
        'gpt-4': {
            'input': 0.03 / 1000,   # $0.03 per 1K tokens
            'output': 0.06 / 1000   # $0.06 per 1K tokens
        },
        'gpt-3.5-turbo': {
            'input': 0.0015 / 1000,  # $0.0015 per 1K tokens
            'output': 0.002 / 1000   # $0.002 per 1K tokens
        }
    }

    def __init__(self, embedding_model: str, llm_model: str):
        self.embedding_model = embedding_model
        self.llm_model = llm_model
        self.encoding = tiktoken.encoding_for_model(llm_model)

    def count_tokens(self, text: str) -> int:
        """è®¡ç®—tokenæ•°é‡"""
        return len(self.encoding.encode(text))

    def estimate_embedding_cost(self, num_documents: int, avg_doc_length: int) -> float:
        """ä¼°ç®—embeddingæˆæœ¬"""
        total_tokens = num_documents * avg_doc_length
        cost_per_token = self.PRICING[self.embedding_model]['input']
        return total_tokens * cost_per_token

    def estimate_query_cost(
        self,
        context_tokens: int,
        query_tokens: int,
        response_tokens: int
    ) -> float:
        """ä¼°ç®—å•æ¬¡æŸ¥è¯¢æˆæœ¬"""
        input_cost = (context_tokens + query_tokens) * self.PRICING[self.llm_model]['input']
        output_cost = response_tokens * self.PRICING[self.llm_model]['output']
        return input_cost + output_cost

    def optimize_context(self, documents: List[str], max_tokens: int = 4000):
        """ä¼˜åŒ–ä¸Šä¸‹æ–‡ä»¥æ§åˆ¶æˆæœ¬"""
        optimized = []
        token_count = 0

        for doc in documents:
            doc_tokens = self.count_tokens(doc)
            if token_count + doc_tokens > max_tokens:
                break
            optimized.append(doc)
            token_count += doc_tokens

        return optimized, token_count

    def suggest_model_switch(
        self,
        avg_input_tokens: int,
        avg_output_tokens: int,
        queries_per_day: int
    ) -> dict:
        """å»ºè®®æ¨¡å‹åˆ‡æ¢"""
        # è®¡ç®—GPT-4æˆæœ¬
        gpt4_daily_cost = queries_per_day * self.estimate_query_cost(
            avg_input_tokens, 0, avg_output_tokens
        )

        # è®¡ç®—GPT-3.5æˆæœ¬
        gpt35_daily_cost = queries_per_day * (
            avg_input_tokens * self.PRICING['gpt-3.5-turbo']['input'] +
            avg_output_tokens * self.PRICING['gpt-3.5-turbo']['output']
        )

        savings = gpt4_daily_cost - gpt35_daily_cost
        savings_percent = (savings / gpt4_daily_cost) * 100

        return {
            'gpt4_daily_cost': f"${gpt4_daily_cost:.2f}",
            'gpt35_daily_cost': f"${gpt35_daily_cost:.2f}",
            'daily_savings': f"${savings:.2f}",
            'savings_percent': f"{savings_percent:.1f}%",
            'recommendation': 'Switch to GPT-3.5-Turbo' if savings > 10 else 'Keep GPT-4'
        }
```

### 5.4 æœ€ä½³å®è·µ

```python
# best_practices.py

class BestPractices:
    """LangChain + PostgreSQLæœ€ä½³å®è·µ"""

    @staticmethod
    def document_preparation():
        """æ–‡æ¡£å‡†å¤‡æœ€ä½³å®è·µ"""
        return {
            'chunking': {
                'chunk_size': '1000-2000å­—ç¬¦ï¼ˆå–å†³äºå†…å®¹ç±»å‹ï¼‰',
                'chunk_overlap': '10-20%çš„chunk_size',
                'separators': 'æŒ‰æ®µè½ã€å¥å­ã€è¯åˆ†å‰²ï¼ˆä¼˜å…ˆçº§é€’å‡ï¼‰'
            },
            'metadata': {
                'required': ['source', 'timestamp'],
                'recommended': ['category', 'author', 'version'],
                'indexable': 'ç¡®ä¿é‡è¦å­—æ®µå¯æœç´¢'
            },
            'cleaning': {
                'remove_noise': 'åˆ é™¤é¡µçœ‰ã€é¡µè„šã€é‡å¤å†…å®¹',
                'normalize': 'ç»Ÿä¸€æ ¼å¼ã€ç¼–ç ',
                'validate': 'æ£€æŸ¥å†…å®¹å®Œæ•´æ€§'
            }
        }

    @staticmethod
    def vector_store_optimization():
        """å‘é‡å­˜å‚¨ä¼˜åŒ–æœ€ä½³å®è·µ"""
        return {
            'indexing': {
                'type': 'HNSWï¼ˆæ¨èï¼‰æˆ–IVFFlat',
                'timing': 'åœ¨æ‰¹é‡å¯¼å…¥ååˆ›å»ºç´¢å¼•',
                'parameters': {
                    'hnsw_m': '16-32ï¼ˆè¶Šå¤§ç²¾åº¦è¶Šé«˜ï¼‰',
                    'hnsw_ef_construction': '64-200ï¼ˆè¶Šå¤§æ„å»ºè¶Šæ…¢ï¼‰',
                    'hnsw_ef_search': '40-100ï¼ˆæŸ¥è¯¢æ—¶è°ƒæ•´ï¼‰'
                }
            },
            'partitioning': {
                'strategy': 'æŒ‰æ—¶é—´æˆ–ç±»åˆ«åˆ†åŒº',
                'benefits': 'æå‡æŸ¥è¯¢æ€§èƒ½ã€ä¾¿äºç»´æŠ¤',
                'size': 'æ¯ä¸ªåˆ†åŒºå»ºè®®<1000ä¸‡è¡Œ'
            },
            'maintenance': {
                'vacuum': 'å®šæœŸVACUUMä»¥å›æ”¶ç©ºé—´',
                'analyze': 'å®šæœŸANALYZEä»¥æ›´æ–°ç»Ÿè®¡ä¿¡æ¯',
                'reindex': 'å¿…è¦æ—¶é‡å»ºç´¢å¼•'
            }
        }

    @staticmethod
    def retrieval_optimization():
        """æ£€ç´¢ä¼˜åŒ–æœ€ä½³å®è·µ"""
        return {
            'search_strategies': {
                'similarity': 'åŸºç¡€ç›¸ä¼¼åº¦æœç´¢',
                'mmr': 'æœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼ˆé¿å…é‡å¤ï¼‰',
                'multi_query': 'ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å˜ä½“',
                'hybrid': 'ç»“åˆå‘é‡å’Œå…³é”®è¯æœç´¢'
            },
            'filtering': {
                'pre_filtering': 'ä½¿ç”¨å…ƒæ•°æ®è¿‡æ»¤åæ£€ç´¢',
                'post_filtering': 'æ£€ç´¢åæ ¹æ®è§„åˆ™è¿‡æ»¤',
                'balance': 'å¹³è¡¡è¿‡æ»¤ç²¾åº¦å’Œå¬å›ç‡'
            },
            'reranking': {
                'method': 'ä½¿ç”¨LLMé‡æ’åºæ£€ç´¢ç»“æœ',
                'benefit': 'æå‡ç›¸å…³æ€§ï¼Œé™ä½å™ªéŸ³',
                'cost': 'å¢åŠ å»¶è¿Ÿå’Œæˆæœ¬'
            }
        }

    @staticmethod
    def production_deployment():
        """ç”Ÿäº§éƒ¨ç½²æœ€ä½³å®è·µ"""
        return {
            'performance': {
                'caching': 'ç¼“å­˜embeddingå’Œæ£€ç´¢ç»“æœ',
                'batching': 'æ‰¹é‡å¤„ç†æ–‡æ¡£å’Œè¯·æ±‚',
                'async': 'ä½¿ç”¨å¼‚æ­¥I/Oæå‡åå'
            },
            'reliability': {
                'retries': 'å®ç°é‡è¯•æœºåˆ¶',
                'fallback': 'å‡†å¤‡é™çº§æ–¹æ¡ˆ',
                'monitoring': 'å…¨é¢ç›‘æ§å’Œå‘Šè­¦'
            },
            'security': {
                'authentication': 'APIå¯†é’¥ç®¡ç†',
                'authorization': 'RLSè¡Œçº§å®‰å…¨',
                'encryption': 'ä¼ è¾“å’Œå­˜å‚¨åŠ å¯†'
            },
            'scalability': {
                'read_replicas': 'PostgreSQLè¯»å‰¯æœ¬',
                'connection_pooling': 'PgBouncerè¿æ¥æ± ',
                'citus': 'ä½¿ç”¨Cituså®ç°åˆ†å¸ƒå¼'
            }
        }
```

---

## å…­ã€æ¡ˆä¾‹å®æˆ˜

### 6.1 ä¼ä¸šçŸ¥è¯†åº“é—®ç­”

**å®Œæ•´å®ç°è§æ–‡æ¡£å®Œæ•´ç‰ˆ...**

### 6.2 æ–‡æ¡£æ™ºèƒ½æ‘˜è¦

**å®Œæ•´å®ç°è§æ–‡æ¡£å®Œæ•´ç‰ˆ...**

### 6.3 ä»£ç åŠ©æ‰‹

**å®Œæ•´å®ç°è§æ–‡æ¡£å®Œæ•´ç‰ˆ...**

### 6.4 å®¢æœæœºå™¨äºº

**å®Œæ•´å®ç°è§æ–‡æ¡£å®Œæ•´ç‰ˆ...**

---

## ä¸ƒã€æ€§èƒ½æµ‹è¯•

**è¯¦ç»†æµ‹è¯•æ•°æ®è§æ–‡æ¡£å®Œæ•´ç‰ˆ...**

---

## å…«ã€æ€»ç»“ä¸å±•æœ›

### æ ¸å¿ƒæ”¶è·

1. âœ… LangChain + PostgreSQL æä¾›äº†å¼ºå¤§çš„RAGèƒ½åŠ›
2. âœ… pgvectorå®ç°äº†ä¼ä¸šçº§å‘é‡æœç´¢
3. âœ… ç»Ÿä¸€æ•°æ®æ ˆé™ä½äº†è¿ç»´å¤æ‚åº¦
4. âœ… ä¸°å¯Œçš„é›†æˆå’Œå·¥å…·åŠ é€Ÿäº†å¼€å‘

### é€‚ç”¨åœºæ™¯

- âœ… ä¼ä¸šçŸ¥è¯†åº“
- âœ… æ™ºèƒ½å®¢æœ
- âœ… æ–‡æ¡£åˆ†æ
- âœ… ä»£ç åŠ©æ‰‹

---

## ä¹ã€å‚è€ƒèµ„æ–™

1. **LangChainå®˜æ–¹æ–‡æ¡£**: [https://python.langchain.com/](https://python.langchain.com/)
2. **pgvector GitHub**: [https://github.com/pgvector/pgvector](https://github.com/pgvector/pgvector)
3. **PostgreSQLå®˜æ–¹æ–‡æ¡£**: [https://www.postgresql.org/docs/](https://www.postgresql.org/docs/)

---

**æœ€åæ›´æ–°**: 2025å¹´12æœˆ4æ—¥
**ç»´æŠ¤è€…**: PostgreSQL Modern Team
**æ–‡æ¡£ç¼–å·**: 14-AI-LANGCHAIN
**ç‰ˆæœ¬**: v1.0
