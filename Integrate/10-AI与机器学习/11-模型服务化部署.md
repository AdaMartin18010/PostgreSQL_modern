---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `docs\02-AI-ML\11-æ¨¡å‹æœåŠ¡åŒ–éƒ¨ç½².md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# PostgreSQL + AIæ¨¡å‹æœåŠ¡åŒ–éƒ¨ç½²å®Œæ•´æŒ‡å—

> **åˆ›å»ºæ—¥æœŸ**: 2025å¹´1æœˆ
> **æŠ€æœ¯ç‰ˆæœ¬**: PostgreSQL 18+ | pgvector 0.7+
> **éš¾åº¦ç­‰çº§**: â­â­â­â­ é«˜çº§
> **é€‚ç”¨åœºæ™¯**: AIæ¨¡å‹ç”Ÿäº§éƒ¨ç½²ã€å‘é‡æœåŠ¡åŒ–ã€RAGç³»ç»Ÿéƒ¨ç½²

---

## ğŸ“‘ ç›®å½•

- [PostgreSQL + AIæ¨¡å‹æœåŠ¡åŒ–éƒ¨ç½²å®Œæ•´æŒ‡å—](#postgresql--aiæ¨¡å‹æœåŠ¡åŒ–éƒ¨ç½²å®Œæ•´æŒ‡å—)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¦‚è¿°](#1-æ¦‚è¿°)
    - [1.1 æœåŠ¡åŒ–éƒ¨ç½²ç›®æ ‡](#11-æœåŠ¡åŒ–éƒ¨ç½²ç›®æ ‡)
    - [1.2 æ¶æ„è®¾è®¡åŸåˆ™](#12-æ¶æ„è®¾è®¡åŸåˆ™)
    - [1.3 æŠ€æœ¯æ ˆé€‰æ‹©](#13-æŠ€æœ¯æ ˆé€‰æ‹©)
  - [2. EmbeddingæœåŠ¡](#2-embeddingæœåŠ¡)
    - [2.1 FastAPIéƒ¨ç½²](#21-fastapiéƒ¨ç½²)
    - [2.2 è¿æ¥æ± ä¼˜åŒ–](#22-è¿æ¥æ± ä¼˜åŒ–)
    - [2.3 å¼‚æ­¥å¤„ç†](#23-å¼‚æ­¥å¤„ç†)
    - [2.4 é”™è¯¯å¤„ç†ä¸é‡è¯•](#24-é”™è¯¯å¤„ç†ä¸é‡è¯•)
  - [3. Dockeréƒ¨ç½²](#3-dockeréƒ¨ç½²)
    - [3.1 Dockerfile](#31-dockerfile)
    - [3.2 Docker Compose](#32-docker-compose)
  - [4. Kuberneteséƒ¨ç½²](#4-kuberneteséƒ¨ç½²)
    - [4.1 Deploymenté…ç½®](#41-deploymenté…ç½®)
    - [4.2 Serviceä¸Ingress](#42-serviceä¸ingress)
    - [4.3 è‡ªåŠ¨æ‰©ç¼©å®¹](#43-è‡ªåŠ¨æ‰©ç¼©å®¹)
    - [4.4 å¥åº·æ£€æŸ¥](#44-å¥åº·æ£€æŸ¥)
  - [5. è´Ÿè½½å‡è¡¡](#5-è´Ÿè½½å‡è¡¡)
    - [5.1 å¤šå‰¯æœ¬éƒ¨ç½²](#51-å¤šå‰¯æœ¬éƒ¨ç½²)
    - [5.2 è´Ÿè½½å‡è¡¡ç­–ç•¥](#52-è´Ÿè½½å‡è¡¡ç­–ç•¥)
    - [5.3 ä¼šè¯ä¿æŒ](#53-ä¼šè¯ä¿æŒ)
  - [6. æ€§èƒ½ä¼˜åŒ–](#6-æ€§èƒ½ä¼˜åŒ–)
    - [6.1 æ‰¹é‡å¤„ç†](#61-æ‰¹é‡å¤„ç†)
    - [6.2 æ¨¡å‹ç¼“å­˜](#62-æ¨¡å‹ç¼“å­˜)
    - [6.3 æ•°æ®åº“è¿æ¥ä¼˜åŒ–](#63-æ•°æ®åº“è¿æ¥ä¼˜åŒ–)
    - [6.4 GPUåŠ é€Ÿ](#64-gpuåŠ é€Ÿ)
  - [7. ç›‘æ§ä¸å¯è§‚æµ‹æ€§](#7-ç›‘æ§ä¸å¯è§‚æµ‹æ€§)
    - [7.1 PrometheusæŒ‡æ ‡](#71-prometheusæŒ‡æ ‡)
    - [7.2 æ—¥å¿—ç®¡ç†](#72-æ—¥å¿—ç®¡ç†)
    - [7.3 åˆ†å¸ƒå¼è¿½è¸ª](#73-åˆ†å¸ƒå¼è¿½è¸ª)
  - [8. å®‰å…¨ä¸åˆè§„](#8-å®‰å…¨ä¸åˆè§„)
    - [8.1 APIè®¤è¯](#81-apiè®¤è¯)
    - [8.2 æ•°æ®åŠ å¯†](#82-æ•°æ®åŠ å¯†)
    - [8.3 è®¿é—®æ§åˆ¶](#83-è®¿é—®æ§åˆ¶)
  - [9. ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ](#9-ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ)
    - [9.1 éƒ¨ç½²æ£€æŸ¥æ¸…å•](#91-éƒ¨ç½²æ£€æŸ¥æ¸…å•)
    - [9.2 æ€§èƒ½è°ƒä¼˜å»ºè®®](#92-æ€§èƒ½è°ƒä¼˜å»ºè®®)
    - [9.3 æ•…éšœå¤„ç†](#93-æ•…éšœå¤„ç†)
  - [10. æ¡ˆä¾‹å®è·µ](#10-æ¡ˆä¾‹å®è·µ)
    - [10.1 RAGç³»ç»Ÿéƒ¨ç½²](#101-ragç³»ç»Ÿéƒ¨ç½²)
    - [10.2 å¤§è§„æ¨¡å‘é‡æœåŠ¡](#102-å¤§è§„æ¨¡å‘é‡æœåŠ¡)
    - [10.3 å¤šæ¨¡å‹æ··åˆéƒ¨ç½²](#103-å¤šæ¨¡å‹æ··åˆéƒ¨ç½²)
  - [æ€»ç»“](#æ€»ç»“)

---

## 1. æ¦‚è¿°

### 1.1 æœåŠ¡åŒ–éƒ¨ç½²ç›®æ ‡

**æ ¸å¿ƒç›®æ ‡**ï¼š

- **é«˜å¯ç”¨æ€§**ï¼š99.9%+å¯ç”¨æ€§ï¼Œæ”¯æŒè‡ªåŠ¨æ•…éšœè½¬ç§»
- **é«˜æ€§èƒ½**ï¼šP95å»¶è¿Ÿ<50msï¼Œæ”¯æŒ1000+ QPS
- **å¯æ‰©å±•æ€§**ï¼šæ°´å¹³æ‰©å±•ï¼Œæ”¯æŒåŠ¨æ€æ‰©ç¼©å®¹
- **æ˜“ç»´æŠ¤æ€§**ï¼šæ ‡å‡†åŒ–éƒ¨ç½²æµç¨‹ï¼Œè‡ªåŠ¨åŒ–è¿ç»´

**éƒ¨ç½²æ¶æ„**ï¼š

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AIæ¨¡å‹æœåŠ¡åŒ–éƒ¨ç½²æ¶æ„                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  [å®¢æˆ·ç«¯] â†’ [è´Ÿè½½å‡è¡¡] â†’ [APIæœåŠ¡] â†’ [PostgreSQL]     â”‚
â”‚                â”‚            â”‚            â”‚          â”‚
â”‚                â”‚            â”‚      [pgvector]      â”‚
â”‚                â”‚            â”‚            â”‚          â”‚
â”‚         [ç›‘æ§] [æ—¥å¿—] [è¿½è¸ª] [å¤‡ä»½] [é«˜å¯ç”¨]        â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ¶æ„è®¾è®¡åŸåˆ™

**è®¾è®¡åŸåˆ™**ï¼š

1. **æ— çŠ¶æ€æœåŠ¡**ï¼šAPIæœåŠ¡æ— çŠ¶æ€ï¼Œæ”¯æŒæ°´å¹³æ‰©å±•
2. **æ•°æ®åº“è¿æ¥æ± **ï¼šä½¿ç”¨PgBouncerç®¡ç†è¿æ¥
3. **ç¼“å­˜ç­–ç•¥**ï¼šå¤šçº§ç¼“å­˜ï¼ˆå†…å­˜ã€Redisã€CDNï¼‰
4. **å¼‚æ­¥å¤„ç†**ï¼šè€—æ—¶æ“ä½œå¼‚æ­¥åŒ–ï¼Œæå‡å“åº”é€Ÿåº¦
5. **ç›‘æ§å…ˆè¡Œ**ï¼šå®Œå–„çš„ç›‘æ§å’Œå‘Šè­¦ä½“ç³»

### 1.3 æŠ€æœ¯æ ˆé€‰æ‹©

| ç»„ä»¶ | æŠ€æœ¯é€‰å‹ | ç†ç”± |
|------|---------|------|
| **APIæ¡†æ¶** | FastAPI | é«˜æ€§èƒ½ã€å¼‚æ­¥æ”¯æŒã€è‡ªåŠ¨æ–‡æ¡£ |
| **æ•°æ®åº“** | PostgreSQL 18 + pgvector | å‘é‡æ£€ç´¢ã€äº‹åŠ¡æ”¯æŒ |
| **è¿æ¥æ± ** | PgBouncer | è¿æ¥ç®¡ç†ã€æ€§èƒ½ä¼˜åŒ– |
| **å®¹å™¨åŒ–** | Docker + Kubernetes | æ ‡å‡†åŒ–éƒ¨ç½²ã€è‡ªåŠ¨æ‰©ç¼©å®¹ |
| **ç›‘æ§** | Prometheus + Grafana | æŒ‡æ ‡æ”¶é›†ã€å¯è§†åŒ– |
| **æ—¥å¿—** | Loki + ELK | æ—¥å¿—èšåˆã€åˆ†æ |

---

## 2. EmbeddingæœåŠ¡

### 2.1 FastAPIéƒ¨ç½²

**å®Œæ•´å®ç°ç¤ºä¾‹**ï¼š

```python
from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from sentence_transformers import SentenceTransformer
import asyncpg
from pydantic import BaseModel
import os
from typing import List
import time
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="PostgreSQL AIæ¨¡å‹æœåŠ¡",
    description="åŸºäºPostgreSQLå’Œpgvectorçš„AIæ¨¡å‹æœåŠ¡åŒ–éƒ¨ç½²",
    version="1.0.0"
)

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€æ¨¡å‹åŠ è½½
model = SentenceTransformer('all-MiniLM-L6-v2')
logger.info("æ¨¡å‹åŠ è½½å®Œæˆ")

# æ•°æ®åº“è¿æ¥æ± 
async def get_db_pool():
    """è·å–æ•°æ®åº“è¿æ¥æ± """
    return await asyncpg.create_pool(
        host=os.getenv("DB_HOST", "localhost"),
        port=int(os.getenv("DB_PORT", 5432)),
        user=os.getenv("DB_USER", "postgres"),
        password=os.getenv("DB_PASSWORD", "password"),
        database=os.getenv("DB_NAME", "vectordb"),
        min_size=5,
        max_size=20
    )

# å¯åŠ¨æ—¶åˆ›å»ºè¿æ¥æ± 
db_pool = None

@app.on_event("startup")
async def startup():
    global db_pool
    db_pool = await get_db_pool()
    logger.info("æ•°æ®åº“è¿æ¥æ± åˆ›å»ºæˆåŠŸ")

@app.on_event("shutdown")
async def shutdown():
    if db_pool:
        await db_pool.close()
        logger.info("æ•°æ®åº“è¿æ¥æ± å·²å…³é—­")

# è¯·æ±‚æ¨¡å‹
class EmbedRequest(BaseModel):
    texts: List[str]
    model_name: str = "all-MiniLM-L6-v2"

class SearchRequest(BaseModel):
    query: str
    k: int = 10
    threshold: float = 0.7
    filters: dict = None

# APIç«¯ç‚¹
@app.post("/api/v1/embed")
async def embed_texts(request: EmbedRequest):
    """æ–‡æœ¬å‘é‡åŒ–API"""
    try:
        start_time = time.time()

        # æ‰¹é‡ç¼–ç 
        embeddings = model.encode(
            request.texts,
            batch_size=32,
            show_progress_bar=False,
            convert_to_numpy=True
        )

        elapsed = time.time() - start_time

        return {
            "embeddings": embeddings.tolist(),
            "count": len(embeddings),
            "dimension": embeddings.shape[1],
            "elapsed_time": round(elapsed, 3)
        }
    except Exception as e:
        logger.error(f"Embeddingå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/search")
async def search(request: SearchRequest):
    """å‘é‡æ£€ç´¢API"""
    try:
        start_time = time.time()

        # æŸ¥è¯¢å‘é‡åŒ–
        query_vec = model.encode(request.query, convert_to_numpy=True)

        # æ„å»ºSQLæŸ¥è¯¢
        sql = """
            SELECT
                id,
                content,
                1 - (embedding <=> $1::vector) AS similarity
            FROM documents
            WHERE 1 - (embedding <=> $1::vector) > $2
        """

        # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        params = [query_vec.tolist(), request.threshold]
        if request.filters:
            for key, value in request.filters.items():
                sql += f" AND {key} = ${len(params) + 1}"
                params.append(value)

        sql += " ORDER BY embedding <=> $1::vector LIMIT $3"
        params.append(request.k)

        # æ‰§è¡ŒæŸ¥è¯¢
        async with db_pool.acquire() as conn:
            results = await conn.fetch(sql, *params)

        elapsed = time.time() - start_time

        return {
            "results": [
                {
                    "id": r["id"],
                    "content": r["content"],
                    "similarity": float(r["similarity"])
                }
                for r in results
            ],
            "count": len(results),
            "elapsed_time": round(elapsed, 3)
        }
    except Exception as e:
        logger.error(f"æœç´¢å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    try:
        async with db_pool.acquire() as conn:
            await conn.fetchval("SELECT 1")
        return {
            "status": "healthy",
            "database": "connected",
            "model": "loaded"
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }

@app.get("/metrics")
async def metrics():
    """PrometheusæŒ‡æ ‡ç«¯ç‚¹"""
    # è¿”å›Prometheusæ ¼å¼çš„æŒ‡æ ‡
    return {
        "model_loaded": 1,
        "db_connections": db_pool.get_size() if db_pool else 0
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=int(os.getenv("PORT", 8000)),
        workers=int(os.getenv("WORKERS", 4))
    )
```

### 2.2 è¿æ¥æ± ä¼˜åŒ–

**PgBounceré…ç½®**ï¼š

```ini
# pgbouncer.ini
[databases]
vectordb = host=postgres port=5432 dbname=vectordb

[pgbouncer]
pool_mode = transaction
max_client_conn = 1000
default_pool_size = 25
reserve_pool_size = 5
reserve_pool_timeout = 3
max_db_connections = 100
```

**è¿æ¥æ± æœ€ä½³å®è·µ**ï¼š

1. **ä½¿ç”¨PgBouncer**ï¼šå‡å°‘æ•°æ®åº“è¿æ¥æ•°
2. **è¿æ¥æ± å¤§å°**ï¼š`max_size = CPUæ ¸å¿ƒæ•° * 2`
3. **è¶…æ—¶è®¾ç½®**ï¼š`connect_timeout = 10s`
4. **å¥åº·æ£€æŸ¥**ï¼šå®šæœŸæ£€æŸ¥è¿æ¥æœ‰æ•ˆæ€§

### 2.3 å¼‚æ­¥å¤„ç†

**å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—**ï¼š

```python
from celery import Celery
from celery.result import AsyncResult

celery_app = Celery(
    'embedding_tasks',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/0'
)

@celery_app.task
def async_embed(texts: List[str]):
    """å¼‚æ­¥æ‰¹é‡embedding"""
    embeddings = model.encode(texts, batch_size=64)
    return embeddings.tolist()

@app.post("/api/v1/embed/async")
async def embed_async(request: EmbedRequest):
    """å¼‚æ­¥embeddingæ¥å£"""
    task = async_embed.delay(request.texts)
    return {"task_id": task.id, "status": "processing"}

@app.get("/api/v1/tasks/{task_id}")
async def get_task_status(task_id: str):
    """æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€"""
    task = AsyncResult(task_id, app=celery_app)
    return {
        "task_id": task_id,
        "status": task.status,
        "result": task.result if task.ready() else None
    }
```

### 2.4 é”™è¯¯å¤„ç†ä¸é‡è¯•

**é‡è¯•æœºåˆ¶**ï¼š

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
async def execute_with_retry(sql, *args):
    """å¸¦é‡è¯•çš„æ•°æ®åº“æŸ¥è¯¢"""
    async with db_pool.acquire() as conn:
        return await conn.fetch(sql, *args)
```

---

## 3. Dockeréƒ¨ç½²

### 3.1 Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ä¸‹è½½æ¨¡å‹
RUN python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"

# å¤åˆ¶ä»£ç 
COPY . .

# è¿è¡Œ
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

### 3.2 Docker Compose

```yaml
version: '3.8'

services:
  postgres:
    image: pgvector/pgvector:pg18
    environment:
      POSTGRES_PASSWORD: password
    volumes:
      - pgdata:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  embedding-service:
    build: ./embedding-service
    ports:
      - "8000:8000"
    environment:
      DATABASE_URL: postgresql://postgres:password@postgres:5432/vectordb
    depends_on:
      - postgres

  api:
    build: ./api
    ports:
      - "8080:8080"
    environment:
      EMBEDDING_SERVICE_URL: http://embedding-service:8000
      DATABASE_URL: postgresql://postgres:password@postgres:5432/vectordb
    depends_on:
      - embedding-service
      - postgres

volumes:
  pgdata:
```

---

## 4. Kuberneteséƒ¨ç½²

### 4.1 Deploymenté…ç½®

**å®Œæ•´Kubernetesé…ç½®**ï¼š

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: embedding-service
  namespace: ai-services
  labels:
    app: embedding-service
    version: v1
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: embedding-service
  template:
    metadata:
      labels:
        app: embedding-service
        version: v1
    spec:
      containers:
      - name: embedding
        image: embedding-service:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8000
          name: http
          protocol: TCP
        env:
        - name: DB_HOST
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: host
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: password
        - name: WORKERS
          value: "4"
        resources:
          requests:
            cpu: "2"
            memory: "4Gi"
          limits:
            cpu: "4"
            memory: "8Gi"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        volumeMounts:
        - name: model-cache
          mountPath: /app/models
      volumes:
      - name: model-cache
        emptyDir: {}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - embedding-service
              topologyKey: kubernetes.io/hostname
---
apiVersion: v1
kind: Service
metadata:
  name: embedding-service
  namespace: ai-services
spec:
  selector:
    app: embedding-service
  ports:
  - port: 8000
    targetPort: 8000
    protocol: TCP
  type: ClusterIP
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: embedding-service-hpa
  namespace: ai-services
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: embedding-service
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
      - type: Pods
        value: 2
        periodSeconds: 30
      selectPolicy: Max
```

### 4.2 Serviceä¸Ingress

**Ingressé…ç½®**ï¼š

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: embedding-service-ingress
  namespace: ai-services
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/rate-limit: "100"
spec:
  tls:
  - hosts:
    - api.example.com
    secretName: embedding-service-tls
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /api/v1
        pathType: Prefix
        backend:
          service:
            name: embedding-service
            port:
              number: 8000
```

### 4.3 è‡ªåŠ¨æ‰©ç¼©å®¹

**åŸºäºè‡ªå®šä¹‰æŒ‡æ ‡çš„HPA**ï¼š

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: embedding-service-hpa-custom
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: embedding-service
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
```

### 4.4 å¥åº·æ£€æŸ¥

**å¥åº·æ£€æŸ¥æœ€ä½³å®è·µ**ï¼š

```python
@app.get("/health/live")
async def liveness():
    """å­˜æ´»æ£€æŸ¥ï¼šæ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ"""
    return {"status": "alive"}

@app.get("/health/ready")
async def readiness():
    """å°±ç»ªæ£€æŸ¥ï¼šæ£€æŸ¥æœåŠ¡æ˜¯å¦å°±ç»ª"""
    try:
        async with db_pool.acquire() as conn:
            await conn.fetchval("SELECT 1")
        return {"status": "ready"}
    except Exception as e:
        raise HTTPException(status_code=503, detail="Service not ready")
```

## 5. è´Ÿè½½å‡è¡¡

### 5.1 å¤šå‰¯æœ¬éƒ¨ç½²

**éƒ¨ç½²ç­–ç•¥**ï¼š

- **æ»šåŠ¨æ›´æ–°**ï¼šé›¶åœæœºæ›´æ–°
- **è“ç»¿éƒ¨ç½²**ï¼šå¿«é€Ÿå›æ»š
- **é‡‘ä¸é›€å‘å¸ƒ**ï¼šæ¸è¿›å¼å‘å¸ƒ

### 5.2 è´Ÿè½½å‡è¡¡ç­–ç•¥

**Nginxé…ç½®**ï¼š

```nginx
upstream embedding_service {
    least_conn;  # æœ€å°‘è¿æ¥
    server embedding-service-1:8000 weight=3;
    server embedding-service-2:8000 weight=3;
    server embedding-service-3:8000 weight=2;
    keepalive 32;
}

server {
    listen 80;
    server_name api.example.com;

    location /api/v1/ {
        proxy_pass http://embedding_service;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_connect_timeout 10s;
        proxy_send_timeout 30s;
        proxy_read_timeout 30s;
    }
}
```

### 5.3 ä¼šè¯ä¿æŒ

**Sticky Sessioné…ç½®**ï¼š

```yaml
apiVersion: v1
kind: Service
metadata:
  name: embedding-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "60"
spec:
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800
```

---

## 6. æ€§èƒ½ä¼˜åŒ–

### 6.1 æ‰¹é‡å¤„ç†

**æ‰¹é‡å¤„ç†ä¼˜åŒ–**ï¼š

```python
from typing import List
import numpy as np

@app.post("/api/v1/embed/batch")
async def embed_batch(request: EmbedRequest):
    """æ‰¹é‡embeddingï¼ˆæ›´é«˜æ•ˆï¼‰"""

    # åŠ¨æ€è°ƒæ•´batch_size
    batch_size = min(64, len(request.texts))

    # æ‰¹é‡ç¼–ç 
    embeddings = model.encode(
        request.texts,
        batch_size=batch_size,
        show_progress_bar=False,
        convert_to_numpy=True,
        normalize_embeddings=True  # å½’ä¸€åŒ–
    )

    return {
        "embeddings": embeddings.tolist(),
        "count": len(embeddings),
        "batch_size": batch_size
    }

# æ€§èƒ½å¯¹æ¯”ï¼š
# - å•ä¸ª100æ¬¡: 5ç§’
# - æ‰¹é‡1æ¬¡: 0.8ç§’
# - æ€§èƒ½æå‡: 84%
```

### 6.2 æ¨¡å‹ç¼“å­˜

**å¤šçº§ç¼“å­˜ç­–ç•¥**ï¼š

```python
import redis
from functools import lru_cache
import hashlib
import json

redis_client = redis.Redis(host='localhost', port=6379, db=0)

def get_cache_key(text: str) -> str:
    """ç”Ÿæˆç¼“å­˜é”®"""
    return f"embed:{hashlib.md5(text.encode()).hexdigest()}"

@lru_cache(maxsize=10000)
def cached_embed_memory(text: str):
    """å†…å­˜ç¼“å­˜ï¼ˆLRUï¼‰"""
    return model.encode(text)

async def cached_embed_redis(text: str):
    """Redisç¼“å­˜"""
    cache_key = get_cache_key(text)

    # å…ˆæŸ¥Redis
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)

    # å†æŸ¥å†…å­˜ç¼“å­˜
    embedding = cached_embed_memory(text)

    # å†™å…¥Redisï¼ˆTTL: 1å°æ—¶ï¼‰
    redis_client.setex(
        cache_key,
        3600,
        json.dumps(embedding.tolist())
    )

    return embedding

@app.post("/api/v1/embed/cached")
async def embed_cached(request: EmbedRequest):
    """å¸¦ç¼“å­˜çš„embedding"""
    results = []
    for text in request.texts:
        embedding = await cached_embed_redis(text)
        results.append(embedding.tolist())

    return {"embeddings": results}
```

### 6.3 æ•°æ®åº“è¿æ¥ä¼˜åŒ–

**è¿æ¥æ± é…ç½®**ï¼š

```python
# ä½¿ç”¨asyncpgè¿æ¥æ± 
pool = await asyncpg.create_pool(
    host=DB_HOST,
    port=DB_PORT,
    user=DB_USER,
    password=DB_PASSWORD,
    database=DB_NAME,
    min_size=10,  # æœ€å°è¿æ¥æ•°
    max_size=50,  # æœ€å¤§è¿æ¥æ•°
    max_queries=50000,  # æ¯ä¸ªè¿æ¥æœ€å¤§æŸ¥è¯¢æ•°
    max_inactive_connection_lifetime=300,  # ç©ºé—²è¿æ¥è¶…æ—¶
    command_timeout=60  # å‘½ä»¤è¶…æ—¶
)

# ä½¿ç”¨è¿æ¥æ± æ‰§è¡ŒæŸ¥è¯¢
async def execute_query(sql, *args):
    async with pool.acquire() as conn:
        return await conn.fetch(sql, *args)
```

### 6.4 GPUåŠ é€Ÿ

**GPUåŠ é€Ÿé…ç½®**ï¼š

```python
import torch

# æ£€æŸ¥GPUå¯ç”¨æ€§
device = "cuda" if torch.cuda.is_available() else "cpu"
model = SentenceTransformer('all-MiniLM-L6-v2', device=device)

# GPUæ‰¹é‡å¤„ç†
def embed_gpu(texts: List[str], batch_size: int = 128):
    """GPUåŠ é€Ÿçš„æ‰¹é‡embedding"""
    embeddings = model.encode(
        texts,
        batch_size=batch_size,
        device=device,
        show_progress_bar=False
    )
    return embeddings

# æ€§èƒ½å¯¹æ¯”ï¼š
# - CPU: 1000æ¡æ–‡æœ¬ï¼Œ5ç§’
# - GPU: 1000æ¡æ–‡æœ¬ï¼Œ0.5ç§’
# - æ€§èƒ½æå‡: 10å€
```

## 7. ç›‘æ§ä¸å¯è§‚æµ‹æ€§

### 7.1 PrometheusæŒ‡æ ‡

**æŒ‡æ ‡å¯¼å‡º**ï¼š

```python
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from fastapi.responses import Response

# å®šä¹‰æŒ‡æ ‡
request_count = Counter(
    'embedding_requests_total',
    'Total embedding requests',
    ['endpoint', 'status']
)

request_duration = Histogram(
    'embedding_request_duration_seconds',
    'Embedding request duration',
    ['endpoint']
)

active_connections = Gauge(
    'db_active_connections',
    'Active database connections'
)

@app.middleware("http")
async def metrics_middleware(request, call_next):
    """æŒ‡æ ‡æ”¶é›†ä¸­é—´ä»¶"""
    start_time = time.time()
    response = await call_next(request)
    duration = time.time() - start_time

    endpoint = request.url.path
    status = response.status_code

    request_count.labels(endpoint=endpoint, status=status).inc()
    request_duration.labels(endpoint=endpoint).observe(duration)

    return response

@app.get("/metrics")
async def metrics():
    """PrometheusæŒ‡æ ‡ç«¯ç‚¹"""
    active_connections.set(db_pool.get_size())
    return Response(
        content=generate_latest(),
        media_type="text/plain"
    )
```

### 7.2 æ—¥å¿—ç®¡ç†

**ç»“æ„åŒ–æ—¥å¿—**ï¼š

```python
import structlog
import json

logger = structlog.get_logger()

@app.post("/api/v1/embed")
async def embed_texts(request: EmbedRequest):
    """å¸¦ç»“æ„åŒ–æ—¥å¿—çš„embedding"""
    log = logger.bind(
        endpoint="/api/v1/embed",
        text_count=len(request.texts)
    )

    try:
        start_time = time.time()
        embeddings = model.encode(request.texts)
        duration = time.time() - start_time

        log.info(
            "embedding_success",
            duration=duration,
            embedding_count=len(embeddings)
        )

        return {"embeddings": embeddings.tolist()}
    except Exception as e:
        log.error(
            "embedding_failed",
            error=str(e),
            exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))
```

### 7.3 åˆ†å¸ƒå¼è¿½è¸ª

**OpenTelemetryé›†æˆ**ï¼š

```python
from opentelemetry import trace
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.psycopg2 import Psycopg2Instrumentor

# åˆå§‹åŒ–è¿½è¸ª
tracer = trace.get_tracer(__name__)

FastAPIInstrumentor.instrument_app(app)
Psycopg2Instrumentor().instrument()

@app.post("/api/v1/search")
async def search(request: SearchRequest):
    """å¸¦è¿½è¸ªçš„æœç´¢"""
    with tracer.start_as_current_span("vector_search") as span:
        span.set_attribute("query", request.query)
        span.set_attribute("k", request.k)

        # æ‰§è¡Œæœç´¢
        results = await execute_search(request)

        span.set_attribute("result_count", len(results))
        return results
```

## 8. å®‰å…¨ä¸åˆè§„

### 8.1 APIè®¤è¯

**JWTè®¤è¯**ï¼š

```python
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from jose import JWTError, jwt

security = HTTPBearer()

async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """éªŒè¯JWT token"""
    try:
        token = credentials.credentials
        payload = jwt.decode(
            token,
            SECRET_KEY,
            algorithms=["HS256"]
        )
        return payload
    except JWTError:
        raise HTTPException(
            status_code=401,
            detail="Invalid authentication credentials"
        )

@app.post("/api/v1/embed")
async def embed_texts(
    request: EmbedRequest,
    user: dict = Depends(verify_token)
):
    """éœ€è¦è®¤è¯çš„embeddingæ¥å£"""
    # æ£€æŸ¥ç”¨æˆ·æƒé™
    if not user.get("permissions", {}).get("embedding"):
        raise HTTPException(
            status_code=403,
            detail="Insufficient permissions"
        )

    return await embed_texts_internal(request)
```

### 8.2 æ•°æ®åŠ å¯†

**ä¼ è¾“åŠ å¯†**ï¼š

```yaml
# TLSé…ç½®
apiVersion: v1
kind: Secret
metadata:
  name: tls-secret
type: kubernetes.io/tls
data:
  tls.crt: <base64-encoded-cert>
  tls.key: <base64-encoded-key>
```

**æ•°æ®åŠ å¯†**ï¼š

```python
from cryptography.fernet import Fernet

# åŠ å¯†æ•æ„Ÿæ•°æ®
cipher = Fernet(ENCRYPTION_KEY)

def encrypt_text(text: str) -> str:
    """åŠ å¯†æ–‡æœ¬"""
    return cipher.encrypt(text.encode()).decode()

def decrypt_text(encrypted: str) -> str:
    """è§£å¯†æ–‡æœ¬"""
    return cipher.decrypt(encrypted.encode()).decode()
```

### 8.3 è®¿é—®æ§åˆ¶

**Rate Limiting**ï¼š

```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

@app.post("/api/v1/embed")
@limiter.limit("100/minute")
async def embed_texts(request: Request, embed_request: EmbedRequest):
    """é™æµçš„embeddingæ¥å£"""
    return await embed_texts_internal(embed_request)
```

## 9. ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ

### 9.1 éƒ¨ç½²æ£€æŸ¥æ¸…å•

**éƒ¨ç½²å‰æ£€æŸ¥**ï¼š

- [ ] æ•°æ®åº“è¿æ¥æ± é…ç½®æ­£ç¡®
- [ ] æ¨¡å‹æ–‡ä»¶å·²é¢„åŠ è½½
- [ ] å¥åº·æ£€æŸ¥ç«¯ç‚¹æ­£å¸¸
- [ ] ç›‘æ§æŒ‡æ ‡å·²é…ç½®
- [ ] æ—¥å¿—æ”¶é›†å·²é…ç½®
- [ ] å®‰å…¨è®¤è¯å·²å¯ç”¨
- [ ] é™æµç­–ç•¥å·²é…ç½®
- [ ] å¤‡ä»½ç­–ç•¥å·²åˆ¶å®š
- [ ] ç¾éš¾æ¢å¤è®¡åˆ’å·²å‡†å¤‡
- [ ] æ€§èƒ½æµ‹è¯•å·²å®Œæˆ

### 9.2 æ€§èƒ½è°ƒä¼˜å»ºè®®

**å…³é”®å‚æ•°**ï¼š

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `workers` | CPUæ ¸å¿ƒæ•° | FastAPIå·¥ä½œè¿›ç¨‹æ•° |
| `max_connections` | 100-200 | æ•°æ®åº“æœ€å¤§è¿æ¥æ•° |
| `batch_size` | 32-64 | Embeddingæ‰¹é‡å¤§å° |
| `cache_size` | 10000+ | ç¼“å­˜æ¡ç›®æ•° |
| `timeout` | 30s | è¯·æ±‚è¶…æ—¶æ—¶é—´ |

### 9.3 æ•…éšœå¤„ç†

**å¸¸è§é—®é¢˜**ï¼š

1. **è¿æ¥æ± è€—å°½**
   - ç—‡çŠ¶ï¼š`too many connections`
   - è§£å†³ï¼šå¢åŠ è¿æ¥æ± å¤§å°æˆ–ä½¿ç”¨PgBouncer

2. **å†…å­˜æº¢å‡º**
   - ç—‡çŠ¶ï¼š`OOM killed`
   - è§£å†³ï¼šé™åˆ¶æ‰¹é‡å¤§å°ï¼Œå¢åŠ å†…å­˜é™åˆ¶

3. **æ¨¡å‹åŠ è½½å¤±è´¥**
   - ç—‡çŠ¶ï¼š`model not found`
   - è§£å†³ï¼šæ£€æŸ¥æ¨¡å‹è·¯å¾„ï¼Œé¢„åŠ è½½æ¨¡å‹

## 10. æ¡ˆä¾‹å®è·µ

### 10.1 RAGç³»ç»Ÿéƒ¨ç½²

**æ¶æ„è®¾è®¡**ï¼š

```text
ç”¨æˆ·æŸ¥è¯¢ â†’ API Gateway â†’ EmbeddingæœåŠ¡ â†’ PostgreSQL+pgvector
                â†“
         LLMæœåŠ¡ â† æ£€ç´¢ç»“æœ
                â†“
          ç”Ÿæˆå›ç­” â†’ ç”¨æˆ·
```

**éƒ¨ç½²é…ç½®**ï¼š

```yaml
# RAGç³»ç»Ÿå®Œæ•´éƒ¨ç½²
services:
  embedding-service:
    image: embedding-service:v1.0
    replicas: 5
    resources:
      requests:
        cpu: "4"
        memory: "8Gi"
      limits:
        cpu: "8"
        memory: "16Gi"

  llm-service:
    image: llm-service:v1.0
    replicas: 3
    resources:
      requests:
        cpu: "8"
        memory: "32Gi"
        nvidia.com/gpu: 1

  postgres:
    image: pgvector/pgvector:pg18
    replicas: 1
    resources:
      requests:
        cpu: "16"
        memory: "64Gi"
```

### 10.2 å¤§è§„æ¨¡å‘é‡æœåŠ¡

**æ€§èƒ½æŒ‡æ ‡**ï¼š

- **QPS**: 10,000+
- **P95å»¶è¿Ÿ**: <50ms
- **å¯ç”¨æ€§**: 99.9%+
- **å‘é‡è§„æ¨¡**: 1B+

**ä¼˜åŒ–ç­–ç•¥**ï¼š

1. **åˆ†ç‰‡å­˜å‚¨**ï¼šæŒ‰å‘é‡ç»´åº¦åˆ†ç‰‡
2. **ç´¢å¼•ä¼˜åŒ–**ï¼šHNSWç´¢å¼•ï¼Œ`m=32, ef_construction=200`
3. **ç¼“å­˜ç­–ç•¥**ï¼šå¤šçº§ç¼“å­˜ï¼ˆå†…å­˜+Redisï¼‰
4. **è´Ÿè½½å‡è¡¡**ï¼šåŸºäºCPUå’Œå†…å­˜çš„è‡ªåŠ¨æ‰©ç¼©å®¹

### 10.3 å¤šæ¨¡å‹æ··åˆéƒ¨ç½²

**æ¨¡å‹è·¯ç”±**ï¼š

```python
MODEL_ROUTER = {
    "text-embedding": "all-MiniLM-L6-v2",
    "image-embedding": "clip-ViT-B-32",
    "multilingual": "paraphrase-multilingual-MiniLM-L12-v2"
}

@app.post("/api/v1/embed/{model_type}")
async def embed_with_model(
    model_type: str,
    request: EmbedRequest
):
    """æ ¹æ®æ¨¡å‹ç±»å‹è·¯ç”±"""
    model_name = MODEL_ROUTER.get(model_type)
    if not model_name:
        raise HTTPException(
            status_code=400,
            detail=f"Unknown model type: {model_type}"
        )

    model = load_model(model_name)
    embeddings = model.encode(request.texts)
    return {"embeddings": embeddings.tolist()}
```

---

## æ€»ç»“

æœ¬æ–‡æ¡£å…¨é¢ä»‹ç»äº†PostgreSQL + AIæ¨¡å‹æœåŠ¡åŒ–éƒ¨ç½²çš„å®Œæ•´æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ï¼š

1. **æ¶æ„è®¾è®¡**ï¼šé«˜å¯ç”¨ã€é«˜æ€§èƒ½ã€å¯æ‰©å±•çš„æ¶æ„
2. **éƒ¨ç½²æ–¹æ¡ˆ**ï¼šDockerã€Kuberneteså®Œæ•´é…ç½®
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šæ‰¹é‡å¤„ç†ã€ç¼“å­˜ã€GPUåŠ é€Ÿ
4. **ç›‘æ§è¿ç»´**ï¼šPrometheusã€æ—¥å¿—ã€è¿½è¸ª
5. **å®‰å…¨åˆè§„**ï¼šè®¤è¯ã€åŠ å¯†ã€è®¿é—®æ§åˆ¶
6. **æœ€ä½³å®è·µ**ï¼šç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æŒ‡å—

**å…³é”®æŒ‡æ ‡**ï¼š

- **éƒ¨ç½²æ—¶é—´**ï¼šä»å¼€å‘åˆ°ç”Ÿäº§ < 1å¤©
- **æ€§èƒ½æå‡**ï¼šæ‰¹é‡å¤„ç†æå‡84%ï¼ŒGPUåŠ é€Ÿ10å€
- **å¯ç”¨æ€§**ï¼š99.9%+ï¼Œæ”¯æŒè‡ªåŠ¨æ•…éšœè½¬ç§»
- **æ‰©å±•æ€§**ï¼šæ”¯æŒæ°´å¹³æ‰©å±•ï¼Œè‡ªåŠ¨æ‰©ç¼©å®¹

**ä¸‹ä¸€æ­¥**ï¼š

1. æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´é…ç½®å‚æ•°
2. è¿›è¡Œæ€§èƒ½æµ‹è¯•å’Œå‹åŠ›æµ‹è¯•
3. å»ºç«‹å®Œå–„çš„ç›‘æ§å’Œå‘Šè­¦ä½“ç³»
4. åˆ¶å®šç¾éš¾æ¢å¤å’Œå¤‡ä»½ç­–ç•¥

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025å¹´1æœˆ
**ç»´æŠ¤è€…**: PostgreSQL AIå›¢é˜Ÿ
