---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `docs\02-AI-ML\10-RAGç³»ç»Ÿå®Œæ•´å®ç°.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# PostgreSQL 18 RAGç³»ç»Ÿå®Œæ•´å®ç°

## ğŸ“‘ ç›®å½•

- [PostgreSQL 18 RAGç³»ç»Ÿå®Œæ•´å®ç°](#postgresql-18-ragç³»ç»Ÿå®Œæ•´å®ç°)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. RAGæ¶æ„è®¾è®¡](#1-ragæ¶æ„è®¾è®¡)
  - [2. æ•°æ®åº“Schema](#2-æ•°æ®åº“schema)
  - [3. Pythonå®ç°](#3-pythonå®ç°)
  - [4. é«˜çº§ç‰¹æ€§](#4-é«˜çº§ç‰¹æ€§)
    - [4.1 æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å…³é”®è¯ï¼‰](#41-æ··åˆæ£€ç´¢å‘é‡å…³é”®è¯)
    - [4.2 é‡æ’åºï¼ˆRerankï¼‰](#42-é‡æ’åºrerank)
  - [5. æ€§èƒ½ä¼˜åŒ–](#5-æ€§èƒ½ä¼˜åŒ–)
    - [5.1 ç¼“å­˜ç­–ç•¥](#51-ç¼“å­˜ç­–ç•¥)
    - [5.2 æ‰¹é‡embedding](#52-æ‰¹é‡embedding)

## 1. RAGæ¶æ„è®¾è®¡

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         RAG (Retrieval Augmented Generation)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  ç”¨æˆ·é—®é¢˜                                       â”‚
â”‚     â†“                                          â”‚
â”‚  [Embedding Model]                             â”‚
â”‚     â†“                                          â”‚
â”‚  é—®é¢˜å‘é‡                                       â”‚
â”‚     â†“                                          â”‚
â”‚  [Vector Search] â† PostgreSQL + pgvector       â”‚
â”‚     â†“                                          â”‚
â”‚  ç›¸å…³æ–‡æ¡£Top-K                                  â”‚
â”‚     â†“                                          â”‚
â”‚  [æ„å»ºPrompt]                                  â”‚
â”‚  Context + Question                            â”‚
â”‚     â†“                                          â”‚
â”‚  [LLM]                                         â”‚
â”‚     â†“                                          â”‚
â”‚  ç”Ÿæˆç­”æ¡ˆ                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. æ•°æ®åº“Schema

```sql
-- æ–‡æ¡£è¡¨ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 FROM pg_extension
        WHERE extname = 'vector'
    ) THEN
        RAISE EXCEPTION 'pgvectoræ‰©å±•æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: CREATE EXTENSION vector;';
    END IF;

    IF EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'documents') THEN
        DROP TABLE documents CASCADE;
        RAISE NOTICE 'å·²åˆ é™¤ç°æœ‰è¡¨: documents';
    END IF;

    CREATE TABLE documents (
        doc_id BIGSERIAL PRIMARY KEY,
        source_id VARCHAR(100),
        doc_type VARCHAR(50),
        title TEXT,
        content TEXT,
        metadata JSONB,
        created_at TIMESTAMPTZ DEFAULT now()
    );

    RAISE NOTICE 'è¡¨åˆ›å»ºæˆåŠŸ: documents';
EXCEPTION
    WHEN duplicate_table THEN
        RAISE WARNING 'è¡¨documentså·²å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE EXCEPTION 'åˆ›å»ºè¡¨å¤±è´¥: %', SQLERRM;
END $$;

-- æ–‡æ¡£å—è¡¨ï¼ˆChunkï¼Œå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'documents') THEN
        RAISE EXCEPTION 'è¡¨documentsä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»º';
    END IF;

    IF EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'document_chunks') THEN
        DROP TABLE document_chunks;
        RAISE NOTICE 'å·²åˆ é™¤ç°æœ‰è¡¨: document_chunks';
    END IF;

    CREATE TABLE document_chunks (
        chunk_id BIGSERIAL PRIMARY KEY,
        doc_id BIGINT REFERENCES documents(doc_id) ON DELETE CASCADE,
        chunk_index INT,
        chunk_text TEXT,
        embedding vector(768),
        token_count INT,
        metadata JSONB,
        created_at TIMESTAMPTZ DEFAULT now(),
        UNIQUE (doc_id, chunk_index)
    );

    RAISE NOTICE 'è¡¨åˆ›å»ºæˆåŠŸ: document_chunks';
EXCEPTION
    WHEN undefined_table THEN
        RAISE EXCEPTION 'è¡¨documentsä¸å­˜åœ¨';
    WHEN undefined_type THEN
        RAISE EXCEPTION 'vectorç±»å‹ä¸å­˜åœ¨ï¼Œè¯·å…ˆå®‰è£…pgvectoræ‰©å±•';
    WHEN duplicate_table THEN
        RAISE WARNING 'è¡¨document_chunkså·²å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE EXCEPTION 'åˆ›å»ºè¡¨å¤±è´¥: %', SQLERRM;
END $$;

-- HNSWç´¢å¼•ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'document_chunks') THEN
        RAISE EXCEPTION 'è¡¨document_chunksä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»º';
    END IF;

    IF NOT EXISTS (
        SELECT 1 FROM pg_extension
        WHERE extname = 'vector'
    ) THEN
        RAISE EXCEPTION 'pgvectoræ‰©å±•æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…';
    END IF;

    IF EXISTS (
        SELECT 1 FROM pg_indexes
        WHERE schemaname = 'public'
        AND tablename = 'document_chunks'
        AND indexname = 'idx_chunks_embedding'
    ) THEN
        DROP INDEX idx_chunks_embedding;
        RAISE NOTICE 'å·²åˆ é™¤ç°æœ‰HNSWç´¢å¼•';
    END IF;

    CREATE INDEX idx_chunks_embedding ON document_chunks
    USING hnsw (embedding vector_cosine_ops)
    WITH (m = 16, ef_construction = 64);

    RAISE NOTICE 'HNSWç´¢å¼•åˆ›å»ºæˆåŠŸ: idx_chunks_embedding';
EXCEPTION
    WHEN undefined_table THEN
        RAISE EXCEPTION 'è¡¨document_chunksä¸å­˜åœ¨';
    WHEN undefined_object THEN
        RAISE EXCEPTION 'hnswç´¢å¼•æ–¹æ³•ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥pgvectorç‰ˆæœ¬ï¼ˆéœ€è¦0.7+ï¼‰';
    WHEN OTHERS THEN
        RAISE EXCEPTION 'åˆ›å»ºHNSWç´¢å¼•å¤±è´¥: %', SQLERRM;
END $$;

-- æŸ¥è¯¢æ—¥å¿—ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    IF EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'query_logs') THEN
        DROP TABLE query_logs;
        RAISE NOTICE 'å·²åˆ é™¤ç°æœ‰è¡¨: query_logs';
    END IF;

    CREATE TABLE query_logs (
        query_id BIGSERIAL PRIMARY KEY,
        user_id BIGINT,
        question TEXT,
        retrieved_chunks BIGINT[],
        answer TEXT,
        latency_ms FLOAT,
        created_at TIMESTAMPTZ DEFAULT now()
    );

    RAISE NOTICE 'è¡¨åˆ›å»ºæˆåŠŸ: query_logs';
EXCEPTION
    WHEN duplicate_table THEN
        RAISE WARNING 'è¡¨query_logså·²å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE EXCEPTION 'åˆ›å»ºè¡¨å¤±è´¥: %', SQLERRM;
END $$;
```

---

## 3. Pythonå®ç°

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.llms import OpenAI
from langchain.vectorstores.pgvector import PGVector
import psycopg2
from psycopg2.extras import RealDictCursor

class RAGSystem:
    """RAGç³»ç»Ÿ"""

    def __init__(self, conn_str: str, openai_api_key: str):
        self.conn = psycopg2.connect(conn_str, cursor_factory=RealDictCursor)
        self.cursor = self.conn.cursor()

        # åˆå§‹åŒ–ç»„ä»¶
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=openai_api_key
        )
        self.llm = OpenAI(temperature=0, openai_api_key=openai_api_key)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )

    def index_document(self, doc_id: int, title: str, content: str, metadata: dict = None):
        """ç´¢å¼•æ–‡æ¡£"""

        # 1. ä¿å­˜æ–‡æ¡£
        self.cursor.execute("""
            INSERT INTO documents (doc_id, title, content, metadata)
            VALUES (%s, %s, %s, %s)
            ON CONFLICT (doc_id) DO UPDATE
            SET title = EXCLUDED.title,
                content = EXCLUDED.content,
                metadata = EXCLUDED.metadata;
        """, (doc_id, title, content, metadata or {}))

        # 2. åˆ†å—
        chunks = self.text_splitter.split_text(content)

        # 3. ç”Ÿæˆembedding
        embeddings = self.embeddings.embed_documents(chunks)

        # 4. ä¿å­˜chunks
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            self.cursor.execute("""
                INSERT INTO document_chunks
                (doc_id, chunk_index, chunk_text, embedding, token_count)
                VALUES (%s, %s, %s, %s, %s)
                ON CONFLICT (doc_id, chunk_index) DO UPDATE
                SET chunk_text = EXCLUDED.chunk_text,
                    embedding = EXCLUDED.embedding;
            """, (doc_id, i, chunk, embedding, len(chunk.split())))

        self.conn.commit()
        print(f"âœ… ç´¢å¼•æ–‡æ¡£{doc_id}: {len(chunks)}ä¸ªchunks")

    def search(self, question: str, k: int = 5):
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""

        # 1. é—®é¢˜å‘é‡åŒ–
        question_vec = self.embeddings.embed_query(question)

        # 2. å‘é‡æ£€ç´¢
        self.cursor.execute("""
            SELECT
                dc.chunk_id,
                dc.doc_id,
                d.title,
                dc.chunk_text,
                dc.embedding <=> %s::vector AS distance,
                1 - (dc.embedding <=> %s::vector) AS similarity
            FROM document_chunks dc
            JOIN documents d ON dc.doc_id = d.doc_id
            ORDER BY dc.embedding <=> %s::vector
            LIMIT %s;
        """, (question_vec, question_vec, question_vec, k))

        results = self.cursor.fetchall()

        return [
            {
                'chunk_id': r['chunk_id'],
                'doc_id': r['doc_id'],
                'title': r['title'],
                'content': r['chunk_text'],
                'similarity': float(r['similarity'])
            }
            for r in results
        ]

    def generate_answer(self, question: str, k: int = 5):
        """ç”Ÿæˆç­”æ¡ˆ"""

        import time
        start_time = time.time()

        # 1. æ£€ç´¢
        contexts = self.search(question, k)

        # 2. æ„å»ºprompt
        context_text = "\n\n".join([
            f"æ–‡æ¡£: {ctx['title']}\nå†…å®¹: {ctx['content']}"
            for ctx in contexts
        ])

        prompt = f"""
åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœæ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè¯·è¯´"æˆ‘ä¸çŸ¥é“"ã€‚

ä¸Šä¸‹æ–‡:
{context_text}

é—®é¢˜: {question}

å›ç­”:
"""

        # 3. LLMç”Ÿæˆ
        answer = self.llm(prompt)

        latency = (time.time() - start_time) * 1000

        # 4. è®°å½•æ—¥å¿—
        self.cursor.execute("""
            INSERT INTO query_logs
            (question, retrieved_chunks, answer, latency_ms)
            VALUES (%s, %s, %s, %s)
            RETURNING query_id;
        """, (
            question,
            [ctx['chunk_id'] for ctx in contexts],
            answer,
            latency
        ))

        query_id = self.cursor.fetchone()['query_id']
        self.conn.commit()

        return {
            'query_id': query_id,
            'question': question,
            'answer': answer.strip(),
            'sources': contexts,
            'latency_ms': latency
        }

    def batch_index_documents(self, documents: list, batch_size: int = 10):
        """æ‰¹é‡ç´¢å¼•"""

        for i in range(0, len(documents), batch_size):
            batch = documents[i:i+batch_size]

            for doc in batch:
                self.index_document(
                    doc['id'],
                    doc['title'],
                    doc['content'],
                    doc.get('metadata')
                )

            print(f"å·²ç´¢å¼• {min(i+batch_size, len(documents))}/{len(documents)}")

# ä½¿ç”¨
rag = RAGSystem(
    conn_str="postgresql://localhost/ragdb",
    openai_api_key="your-key"
)

# ç´¢å¼•æ–‡æ¡£
rag.index_document(
    doc_id=1,
    title="PostgreSQL MVCCåŸç†",
    content="å¤šç‰ˆæœ¬å¹¶å‘æ§åˆ¶..."
)

# é—®ç­”
result = rag.generate_answer("ä»€ä¹ˆæ˜¯MVCCï¼Ÿ")
print(f"ç­”æ¡ˆ: {result['answer']}")
print(f"å»¶è¿Ÿ: {result['latency_ms']:.1f}ms")
```

---

## 4. é«˜çº§ç‰¹æ€§

### 4.1 æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å…³é”®è¯ï¼‰

```sql
-- æ·»åŠ å…¨æ–‡æœç´¢å­—æ®µ
ALTER TABLE document_chunks ADD COLUMN ts_vector tsvector;

UPDATE document_chunks
SET ts_vector = to_tsvector('english', chunk_text);

CREATE INDEX idx_chunks_fulltext ON document_chunks USING gin(ts_vector);

-- æ··åˆæ£€ç´¢
WITH vector_results AS (
    SELECT
        chunk_id,
        1 - (embedding <=> query_vec) AS vec_score
    FROM document_chunks
    ORDER BY embedding <=> query_vec
    LIMIT 100
),
text_results AS (
    SELECT
        chunk_id,
        ts_rank(ts_vector, query) AS text_score
    FROM document_chunks
    WHERE ts_vector @@ to_tsquery('postgresql & mvcc')
)
SELECT
    dc.chunk_id,
    dc.chunk_text,
    COALESCE(vr.vec_score, 0) * 0.7 + COALESCE(tr.text_score, 0) * 0.3 AS final_score
FROM document_chunks dc
LEFT JOIN vector_results vr ON dc.chunk_id = vr.chunk_id
LEFT JOIN text_results tr ON dc.chunk_id = tr.chunk_id
WHERE vr.chunk_id IS NOT NULL OR tr.chunk_id IS NOT NULL
ORDER BY final_score DESC
LIMIT 5;
```

### 4.2 é‡æ’åºï¼ˆRerankï¼‰

```python
def search_with_rerank(self, question: str, k: int = 5, candidates: int = 20):
    """å‘é‡å¬å›+é‡æ’åº"""

    # 1. å‘é‡å¬å›ï¼ˆå¿«é€Ÿï¼Œå¬å›æ›´å¤šï¼‰
    contexts = self.search(question, k=candidates)

    # 2. è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆç²¾ç¡®ï¼‰
    from sentence_transformers import CrossEncoder
    reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')

    # é‡æ–°æ‰“åˆ†
    pairs = [(question, ctx['content']) for ctx in contexts]
    scores = reranker.predict(pairs)

    # 3. æŒ‰æ–°åˆ†æ•°æ’åº
    for ctx, score in zip(contexts, scores):
        ctx['rerank_score'] = float(score)

    contexts.sort(key=lambda x: x['rerank_score'], reverse=True)

    return contexts[:k]
```

---

## 5. æ€§èƒ½ä¼˜åŒ–

### 5.1 ç¼“å­˜ç­–ç•¥

```python
import redis

class CachedRAG(RAGSystem):
    """å¸¦ç¼“å­˜çš„RAG"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.redis_client = redis.Redis(host='localhost', decode_responses=True)
        self.cache_ttl = 3600  # 1å°æ—¶

    def generate_answer(self, question: str, k: int = 5):
        """å¸¦ç¼“å­˜çš„ç­”æ¡ˆç”Ÿæˆ"""

        # ç”Ÿæˆç¼“å­˜key
        import hashlib
        cache_key = f"rag:answer:{hashlib.md5(question.encode()).hexdigest()}"

        # æ£€æŸ¥ç¼“å­˜
        cached = self.redis_client.get(cache_key)
        if cached:
            import json
            return json.loads(cached)

        # ç”Ÿæˆç­”æ¡ˆ
        result = super().generate_answer(question, k)

        # å†™å…¥ç¼“å­˜
        import json
        self.redis_client.setex(cache_key, self.cache_ttl, json.dumps(result))

        return result
```

### 5.2 æ‰¹é‡embedding

```python
def batch_embed_documents(docs: list, batch_size: int = 100):
    """æ‰¹é‡ç”Ÿæˆembedding"""

    all_embeddings = []

    for i in range(0, len(docs), batch_size):
        batch = docs[i:i+batch_size]

        # æ‰¹é‡è°ƒç”¨embedding API
        embeddings = embeddings_model.embed_documents(batch)
        all_embeddings.extend(embeddings)

        print(f"å·²å¤„ç† {min(i+batch_size, len(docs))}/{len(docs)}")

    return all_embeddings

# æ€§èƒ½: å•ä¸ªè°ƒç”¨100æ¬¡ vs æ‰¹é‡è°ƒç”¨1æ¬¡
# æ—¶é—´: 50ç§’ vs 5ç§’ (-90%)
```

---

**å®Œæˆ**: PostgreSQL 18 RAGç³»ç»Ÿå®Œæ•´å®ç°
**å­—æ•°**: ~8,000å­—
**æ¶µç›–**: æ¶æ„ã€Schemaã€Pythonå®ç°ã€æ··åˆæ£€ç´¢ã€é‡æ’åºã€ç¼“å­˜ä¼˜åŒ–
