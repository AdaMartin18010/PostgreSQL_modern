---

> **📋 文档来源**: `docs\02-AI-ML\04-模型优化完整指南.md`
> **📅 复制日期**: 2025-12-22
> **⚠️ 注意**: 本文档为复制版本，原文件保持不变

---

# AI模型优化完整指南（PostgreSQL集成）

> **创建日期**: 2025年12月4日
> **适用场景**: 边缘部署、成本优化、性能提升
> **文档状态**: 🚧 深度创建中

---

## 📑 目录

- [AI模型优化完整指南（PostgreSQL集成）](#ai模型优化完整指南postgresql集成)
  - [📑 目录](#-目录)
  - [一、模型优化概述](#一模型优化概述)
    - [1.1 为什么需要优化](#11-为什么需要优化)
    - [1.2 优化方法对比](#12-优化方法对比)
  - [二、量化（Quantization）](#二量化quantization)
    - [2.1 量化原理](#21-量化原理)
    - [2.2 INT8量化实战](#22-int8量化实战)
    - [2.3 INT4量化实战](#23-int4量化实战)
  - [三、剪枝（Pruning）](#三剪枝pruning)
    - [3.1 剪枝原理](#31-剪枝原理)
    - [3.2 结构化剪枝](#32-结构化剪枝)
  - [四、蒸馏（Distillation）](#四蒸馏distillation)
    - [4.1 知识蒸馏原理](#41-知识蒸馏原理)
    - [4.2 实战：GPT-4 → 小模型](#42-实战gpt-4--小模型)
  - [五、PostgreSQL存储优化模型](#五postgresql存储优化模型)
    - [5.1 模型版本管理](#51-模型版本管理)
    - [5.2 A/B测试](#52-ab测试)
  - [六、生产案例](#六生产案例)
    - [案例1：Embedding模型量化](#案例1embedding模型量化)
    - [案例2：边缘部署优化](#案例2边缘部署优化)

---

## 一、模型优化概述

### 1.1 为什么需要优化

**原始模型的问题**：

```text
以LLaMA-2-7B为例：
- 模型大小：13.5GB（FP32）
- 内存占用：14GB+
- 推理速度：20 tokens/秒（CPU）
- 成本：高昂的GPU/内存

问题：
❌ 部署成本高
❌ 推理速度慢
❌ 无法边缘部署
❌ 扩展困难
```

**优化后的效果**：

```text
LLaMA-2-7B量化（INT4）：
- 模型大小：3.5GB（-74%）⭐
- 内存占用：4GB+（-71%）⭐
- 推理速度：50 tokens/秒（+150%）⭐
- 精度损失：<2%（可接受）✅

优势：
✅ 部署成本降低70%
✅ 推理速度提升2.5倍
✅ 可以边缘部署
✅ 扩展性提升
```

### 1.2 优化方法对比

| 方法 | 模型大小减少 | 速度提升 | 精度损失 | 难度 |
|------|------------|---------|---------|------|
| **量化（INT8）** | 75% | 2-3倍 | <1% | 简单 ⭐ |
| **量化（INT4）** | 87.5% | 3-4倍 | 2-3% | 简单 ⭐ |
| **剪枝** | 50-70% | 1.5-2倍 | 1-3% | 中等 |
| **蒸馏** | 70-90% | 5-10倍 | 3-5% | 困难 |
| **组合** | 95% | 10-20倍 | 5-8% | 困难 |

---

## 二、量化（Quantization）

### 2.1 量化原理

**将高精度权重转换为低精度**：

```text
原始权重（FP32，32位）：
  3.14159265 → 0 10000000 10010010000111111011011
  │            │    │        │
  符号位        指数  尾数

量化后（INT8，8位）：
  3.14159265 → 缩放到[-128, 127]
  scale = max_value / 127
  quantized = round(value / scale)

  优势：
  - 存储：32位 → 8位（-75%）
  - 计算：INT8运算比FP32快
```

### 2.2 INT8量化实战

**使用ONNX Runtime量化**：

```python
import onnx
from onnxruntime.quantization import quantize_dynamic, QuantType

# 加载模型
model_path = "model.onnx"

# 动态量化（INT8）
quantized_model_path = "model_int8.onnx"
quantize_dynamic(
    model_input=model_path,
    model_output=quantized_model_path,
    weight_type=QuantType.QInt8,  # INT8量化
    optimize_model=True
)

# 保存到PostgreSQL
import psycopg2

conn = psycopg2.connect("dbname=mydb")
cur = conn.cursor()

# 读取量化模型
with open(quantized_model_path, 'rb') as f:
    model_bytes = f.read()

# 存储到数据库
cur.execute("""
    INSERT INTO ml_models (name, version, model_type, model_data, created_at)
    VALUES (%s, %s, %s, %s, NOW())
""", ("embedding_model", "v1.0_int8", "onnx", psycopg2.Binary(model_bytes)))

conn.commit()
```

**性能对比（embedding模型）**：

| 精度 | 大小 | 推理速度 | 精度 |
|------|------|---------|------|
| FP32 | 420MB | 50ms | 100% |
| FP16 | 210MB | 35ms | 99.98% |
| INT8 | 105MB | **20ms** | 99.5% ⭐ |

### 2.3 INT4量化实战

**使用bitsandbytes量化**：

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# INT4量化配置
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# 加载量化模型
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=bnb_config,
    device_map="auto"
)

print(f"Model memory: {model.get_memory_footprint() / 1e9:.2f} GB")
# 输出：~3.5GB（vs 13.5GB原始）
```

**性能测试（LLaMA-2-7B）**：

| 量化 | 大小 | 内存 | 速度 | 精度 |
|------|------|------|------|------|
| 无 | 13.5GB | 14GB | 基准 | 100% |
| INT8 | 6.8GB | 7GB | +150% | 99.5% |
| INT4 | 3.5GB | 4GB | +200% | 97% ⭐ |

---

## 三、剪枝（Pruning）

### 3.1 剪枝原理

**移除不重要的权重**：

```text
神经网络权重重要性分布：
┌─────────────────────────────┐
│ 重要权重（20%）              │ ← 保留
├─────────────────────────────┤
│ 中等重要（30%）              │ ← 保留
├─────────────────────────────┤
│ 不重要权重（50%）            │ ← 剪枝掉
└─────────────────────────────┘

剪枝后：
- 模型大小：-50-70%
- 推理速度：+50-100%
- 精度损失：1-3%
```

### 3.2 结构化剪枝

**使用PyTorch剪枝**：

```python
import torch
import torch.nn.utils.prune as prune

# 加载模型
model = ...

# 剪枝策略：移除50%的权重
for name, module in model.named_modules():
    if isinstance(module, torch.nn.Linear):
        prune.l1_unstructured(module, name='weight', amount=0.5)

# 永久移除剪枝权重
for name, module in model.named_modules():
    if isinstance(module, torch.nn.Linear):
        prune.remove(module, 'weight')

# 保存剪枝模型
torch.save(model.state_dict(), "model_pruned.pt")
```

---

## 四、蒸馏（Distillation）

### 4.1 知识蒸馏原理

**大模型（教师）→ 小模型（学生）**：

```text
┌────────────────────────────────────┐
│       知识蒸馏流程                  │
├────────────────────────────────────┤
│                                      │
│  教师模型（GPT-4，175B参数）         │
│    ├─ 预测：[0.7, 0.2, 0.1, ...]   │
│    └─ 软标签（概率分布）             │
│          ↓                           │
│  学生模型（1B参数）                  │
│    ├─ 学习教师的预测分布             │
│    ├─ 学习硬标签（真实标签）         │
│    └─ 综合学习                       │
│          ↓                           │
│  结果：小模型接近大模型性能           │
└────────────────────────────────────┘
```

### 4.2 实战：GPT-4 → 小模型

**收集训练数据**：

```python
import openai
import psycopg2

# 从PostgreSQL获取训练数据
conn = psycopg2.connect("dbname=mydb")
cur = conn.cursor()

cur.execute("""
    SELECT question, context
    FROM training_data
    LIMIT 10000
""")

training_data = []
for question, context in cur.fetchall():
    # 使用GPT-4生成回答（教师）
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "Answer based on context."},
            {"role": "user", "content": f"Context: {context}\n\nQuestion: {question}"}
        ]
    )

    answer = response['choices'][0]['message']['content']

    training_data.append({
        "question": question,
        "context": context,
        "teacher_answer": answer
    })

    # 保存到数据库
    cur.execute("""
        INSERT INTO distillation_data (question, context, teacher_answer)
        VALUES (%s, %s, %s)
    """, (question, context, answer))

conn.commit()
```

**训练学生模型**（使用Hugging Face）：

```python
from transformers import Trainer, TrainingArguments

# 配置训练
training_args = TrainingArguments(
    output_dir="./student_model",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    learning_rate=5e-5,
    logging_steps=100
)

# 训练（代码简化）
trainer = Trainer(
    model=student_model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

trainer.train()
```

**效果对比**：

| 模型 | 大小 | 速度 | 成本/1M tokens | 精度 |
|------|------|------|---------------|------|
| GPT-4（教师）| - | 基准 | $30 | 100% |
| 学生模型（1B）| 2GB | 50倍 | $0.1 | 92% ⭐ |

**ROI**：

- 成本降低：300倍
- 速度提升：50倍
- 可自部署
- 数据隐私保护

---

## 五、PostgreSQL存储优化模型

### 5.1 模型版本管理

**Schema设计**：

```sql
-- 模型存储表（带错误处理）
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ml_models') THEN
            CREATE TABLE ml_models (
                id BIGSERIAL PRIMARY KEY,
                name TEXT NOT NULL,
                version TEXT NOT NULL,
                model_type TEXT,  -- 'onnx', 'pytorch', 'tensorflow'
                optimization TEXT,  -- 'fp32', 'int8', 'int4', 'pruned'
                model_data BYTEA,  -- 模型二进制数据
                config JSONB,  -- 配置参数
                metrics JSONB,  -- 性能指标
                created_at TIMESTAMPTZ DEFAULT NOW(),
                UNIQUE(name, version)
            );
            RAISE NOTICE '表 ml_models 创建成功';
        ELSE
            RAISE NOTICE '表 ml_models 已存在';
        END IF;
    EXCEPTION
        WHEN duplicate_table THEN
            RAISE NOTICE '表 ml_models 已存在';
        WHEN OTHERS THEN
            RAISE WARNING '创建表失败: %', SQLERRM;
            RAISE;
    END;
END $$;

-- 插入模型（带错误处理）
DO $$
BEGIN
    BEGIN
        INSERT INTO ml_models (name, version, model_type, optimization, model_data, metrics)
        VALUES (
            'embedding_model',
            'v1.0_int8',
            'onnx',
    'int8',
    pg_read_binary_file('/path/to/model_int8.onnx'),
    '{"size_mb": 105, "inference_ms": 20, "accuracy": 0.995}'::jsonb
);

-- 查询最优模型
SELECT name, version, metrics
FROM ml_models
WHERE name = 'embedding_model'
ORDER BY (metrics->>'inference_ms')::float ASC
LIMIT 1;
```

### 5.2 A/B测试

**模型A/B测试**：

```sql
-- A/B测试表
CREATE TABLE model_ab_tests (
    id BIGSERIAL PRIMARY KEY,
    test_name TEXT,
    model_a_id BIGINT REFERENCES ml_models(id),
    model_b_id BIGINT REFERENCES ml_models(id),
    traffic_split NUMERIC(3, 2) DEFAULT 0.5,  -- 50/50
    start_time TIMESTAMPTZ,
    end_time TIMESTAMPTZ,
    status TEXT DEFAULT 'running'
);

-- 结果统计表
CREATE TABLE model_test_results (
    id BIGSERIAL PRIMARY KEY,
    test_id BIGINT REFERENCES model_ab_tests(id),
    model_id BIGINT,
    request_id UUID,
    query TEXT,
    response TEXT,
    latency_ms INT,
    user_feedback INT,  -- 1-5星
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- 分析A/B测试结果
SELECT
    model_id,
    COUNT(*) as requests,
    AVG(latency_ms) as avg_latency,
    AVG(user_feedback) as avg_rating,
    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY latency_ms) as p95_latency
FROM model_test_results
WHERE test_id = 1
GROUP BY model_id;
```

---

## 六、生产案例

### 案例1：Embedding模型量化

**场景**：

- 公司：某搜索公司
- 模型：BERT-base embedding（440MB）
- QPS需求：1000+

**优化前**：

```python
# FP32模型
model_size = 440MB
inference_time = 45ms
max_qps = 220（单机）
cost_per_month = $5000（多台服务器）
```

**优化：INT8量化**:

```python
# 量化后
model_size = 110MB（-75%）
inference_time = 18ms（+150%）
max_qps = 550（单机）⭐
cost_per_month = $2000（-60%）

# 精度：99.7%（vs 100%，损失0.3%）
```

**存储到PostgreSQL**：

```sql
-- 存储两个版本
INSERT INTO ml_models (name, version, model_type, model_data, metrics)
VALUES
    ('bert_embedding', 'v1.0_fp32', 'onnx', ..., '{"size": 440, "latency": 45}'),
    ('bert_embedding', 'v1.0_int8', 'onnx', ..., '{"size": 110, "latency": 18}');

-- 默认使用INT8版本
```

**ROI**：

- 服务器成本：-60%
- QPS：+150%
- 精度损失：0.3%（可接受）
- 年节省：$36,000

---

### 案例2：边缘部署优化

**场景**：

- 部署到边缘设备（Raspberry Pi 4，4GB内存）
- 模型：LLaMA-2-7B
- 挑战：内存和计算受限

**解决方案：INT4量化 + 剪枝**:

```python
# 1. INT4量化
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=BitsAndBytesConfig(load_in_4bit=True)
)

# 模型大小：3.5GB（可装入4GB设备）

# 2. 转换为ONNX并进一步优化
import onnx
from onnxruntime.quantization import quantize_dynamic

# 导出ONNX
torch.onnx.export(model, ...)

# 优化
optimized_model = onnx_optimizer.optimize(model)
```

**效果**：

- 可在Raspberry Pi运行 ✅
- 推理速度：5 tokens/秒（可接受）
- 成本：$100/设备（vs $5000 GPU服务器）

---

**最后更新**: 2025年12月4日
**文档编号**: P5-4-MODEL-OPTIMIZATION
**版本**: v1.0
**状态**: ✅ 完成
