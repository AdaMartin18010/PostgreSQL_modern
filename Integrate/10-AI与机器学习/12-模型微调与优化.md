---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `docs\02-AI-ML\12-æ¨¡å‹å¾®è°ƒä¸ä¼˜åŒ–.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# PostgreSQL AI/MLæ¨¡å‹å¾®è°ƒä¸ä¼˜åŒ–

## ğŸ“‘ ç›®å½•

- [PostgreSQL AI/MLæ¨¡å‹å¾®è°ƒä¸ä¼˜åŒ–](#postgresql-aimlæ¨¡å‹å¾®è°ƒä¸ä¼˜åŒ–)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. å‘é‡æ¨¡å‹å¾®è°ƒ](#1-å‘é‡æ¨¡å‹å¾®è°ƒ)
    - [1.1 é¢†åŸŸé€‚é…](#11-é¢†åŸŸé€‚é…)
  - [2. æŸ¥è¯¢é‡å†™æ¨¡å‹](#2-æŸ¥è¯¢é‡å†™æ¨¡å‹)
    - [2.1 Text-to-SQLå¾®è°ƒ](#21-text-to-sqlå¾®è°ƒ)
  - [3. æ¨¡å‹é‡åŒ–](#3-æ¨¡å‹é‡åŒ–)
    - [3.1 INT8é‡åŒ–](#31-int8é‡åŒ–)
    - [3.2 çŸ¥è¯†è’¸é¦](#32-çŸ¥è¯†è’¸é¦)
  - [4. æ¨¡å‹ç¼“å­˜ä¼˜åŒ–](#4-æ¨¡å‹ç¼“å­˜ä¼˜åŒ–)
  - [5. æ‰¹é‡æ¨ç†ä¼˜åŒ–](#5-æ‰¹é‡æ¨ç†ä¼˜åŒ–)
  - [6. A/Bæµ‹è¯•æ¡†æ¶](#6-abæµ‹è¯•æ¡†æ¶)

## 1. å‘é‡æ¨¡å‹å¾®è°ƒ

### 1.1 é¢†åŸŸé€‚é…

```python
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader

# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = SentenceTransformer('all-MiniLM-L6-v2')

# 2. å‡†å¤‡é¢†åŸŸæ•°æ®ï¼ˆPostgreSQLæ–‡æ¡£ï¼‰
train_examples = [
    InputExample(texts=['CREATE TABLE users', 'PostgreSQL table creation'], label=0.9),
    InputExample(texts=['SELECT * FROM users', 'Query all users'], label=0.85),
    InputExample(texts=['CREATE INDEX', 'Build B-tree index'], label=0.8),
    # ... æ›´å¤šé¢†åŸŸæ ·æœ¬
]

# 3. è®­ç»ƒ
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)
train_loss = losses.CosineSimilarityLoss(model)

model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=5,
    warmup_steps=100
)

# 4. ä¿å­˜å¾®è°ƒæ¨¡å‹
model.save('models/postgresql-embeddings')

# 5. éƒ¨ç½²åˆ°PostgreSQL
import psycopg2

conn = psycopg2.connect("dbname=mydb")
cursor = conn.cursor()

# æ‰¹é‡ç”Ÿæˆå‘é‡
texts = cursor.execute("SELECT content FROM docs").fetchall()
embeddings = model.encode([t[0] for t in texts])

# æ›´æ–°å‘é‡
for text_id, embedding in zip(text_ids, embeddings):
    cursor.execute(
        "UPDATE docs SET embedding = %s WHERE id = %s",
        (embedding.tolist(), text_id)
    )
conn.commit()
```

---

## 2. æŸ¥è¯¢é‡å†™æ¨¡å‹

### 2.1 Text-to-SQLå¾®è°ƒ

```python
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
import torch

# 1. å‡†å¤‡æ•°æ®é›†
training_data = [
    {
        "input": "æŸ¥è¯¢æ‰€æœ‰å¹´é¾„å¤§äº25çš„ç”¨æˆ·",
        "output": "SELECT * FROM users WHERE age > 25"
    },
    {
        "input": "ç»Ÿè®¡æ¯ä¸ªåŸå¸‚çš„ç”¨æˆ·æ•°é‡",
        "output": "SELECT city, COUNT(*) FROM users GROUP BY city"
    },
    # ... PostgreSQLç‰¹å®šè¯­æ³•
]

# 2. åŠ è½½æ¨¡å‹
model_name = "t5-base"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

# 3. æ•°æ®é¢„å¤„ç†
def preprocess(examples):
    inputs = ["translate to SQL: " + ex['input'] for ex in examples]
    targets = [ex['output'] for ex in examples]

    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding='max_length')
    labels = tokenizer(targets, max_length=256, truncation=True, padding='max_length')

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# 4. è®­ç»ƒ
training_args = TrainingArguments(
    output_dir="./text2sql",
    num_train_epochs=10,
    per_device_train_batch_size=8,
    save_steps=500,
    save_total_limit=2,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)

trainer.train()

# 5. æ¨ç†
def generate_sql(natural_query):
    input_text = f"translate to SQL: {natural_query}"
    inputs = tokenizer(input_text, return_tensors="pt")

    outputs = model.generate(
        inputs.input_ids,
        max_length=256,
        num_beams=4,
        early_stopping=True
    )

    sql = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return sql

# æµ‹è¯•
print(generate_sql("æŸ¥æ‰¾æœ€è¿‘7å¤©çš„è®¢å•"))
# è¾“å‡º: SELECT * FROM orders WHERE created_at >= CURRENT_DATE - INTERVAL '7 days'
```

---

## 3. æ¨¡å‹é‡åŒ–

### 3.1 INT8é‡åŒ–

```python
from optimum.onnxruntime import ORTModelForSequenceClassification
from transformers import AutoTokenizer

# 1. åŠ è½½æ¨¡å‹
model = ORTModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    export=True
)

# 2. é‡åŒ–
from optimum.onnxruntime.configuration import OptimizationConfig, QuantizationConfig

# INT8é‡åŒ–
quantization_config = QuantizationConfig(
    is_static=False,  # åŠ¨æ€é‡åŒ–
    format="QOperator"
)

model.quantize(save_dir="models/bert-int8", quantization_config=quantization_config)

# 3. å¯¹æ¯”
# FP32: 420MB, 50ms/query
# INT8: 110MB, 25ms/query (-74% size, -50% latency)
```

### 3.2 çŸ¥è¯†è’¸é¦

```python
from transformers import DistilBertForSequenceClassification, BertForSequenceClassification

# æ•™å¸ˆæ¨¡å‹ï¼ˆå¤§ï¼‰
teacher = BertForSequenceClassification.from_pretrained('bert-large-uncased')

# å­¦ç”Ÿæ¨¡å‹ï¼ˆå°ï¼‰
student = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')

# è’¸é¦è®­ç»ƒ
import torch.nn.functional as F

def distillation_loss(student_logits, teacher_logits, labels, alpha=0.5, temperature=2.0):
    # è½¯æ ‡ç­¾æŸå¤±
    soft_loss = F.kl_div(
        F.log_softmax(student_logits / temperature, dim=-1),
        F.softmax(teacher_logits / temperature, dim=-1),
        reduction='batchmean'
    ) * (temperature ** 2)

    # ç¡¬æ ‡ç­¾æŸå¤±
    hard_loss = F.cross_entropy(student_logits, labels)

    return alpha * soft_loss + (1 - alpha) * hard_loss

# è®­ç»ƒå¾ªç¯
for batch in train_loader:
    with torch.no_grad():
        teacher_logits = teacher(**batch).logits

    student_logits = student(**batch).logits
    loss = distillation_loss(student_logits, teacher_logits, batch['labels'])

    loss.backward()
    optimizer.step()

# ç»“æœ:
# BERT-Large: 330Må‚æ•°, 1.2GB, 200ms
# DistilBERT: 66Må‚æ•°, 250MB, 60ms (-80% params, -70% latency)
# å‡†ç¡®ç‡ä¿æŒ97%
```

---

## 4. æ¨¡å‹ç¼“å­˜ä¼˜åŒ–

```sql
-- åˆ›å»ºæ¨¡å‹æ¨ç†ç¼“å­˜è¡¨
CREATE TABLE model_cache (
    input_hash VARCHAR(64) PRIMARY KEY,
    input_text TEXT,
    output JSONB,
    model_version VARCHAR(50),
    created_at TIMESTAMPTZ DEFAULT now(),
    hit_count INT DEFAULT 1
);

CREATE INDEX idx_model_version ON model_cache(model_version);
CREATE INDEX idx_created_at ON model_cache(created_at);

-- Pythonç¼“å­˜é€»è¾‘
import hashlib
import json

def get_embedding_with_cache(text, model):
    """å¸¦ç¼“å­˜çš„å‘é‡ç”Ÿæˆ"""

    # è®¡ç®—è¾“å…¥hash
    text_hash = hashlib.sha256(text.encode()).hexdigest()

    # æ£€æŸ¥ç¼“å­˜
    cursor.execute(
        "SELECT output FROM model_cache WHERE input_hash = %s AND model_version = %s",
        (text_hash, MODEL_VERSION)
    )

    result = cursor.fetchone()
    if result:
        # ç¼“å­˜å‘½ä¸­
        cursor.execute(
            "UPDATE model_cache SET hit_count = hit_count + 1 WHERE input_hash = %s",
            (text_hash,)
        )
        return json.loads(result[0])['embedding']

    # ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨æ¨¡å‹
    embedding = model.encode(text)

    # å­˜å…¥ç¼“å­˜
    cursor.execute(
        """
        INSERT INTO model_cache (input_hash, input_text, output, model_version)
        VALUES (%s, %s, %s, %s)
        ON CONFLICT (input_hash) DO UPDATE SET hit_count = model_cache.hit_count + 1
        """,
        (text_hash, text, json.dumps({'embedding': embedding.tolist()}), MODEL_VERSION)
    )
    conn.commit()

    return embedding

# ç¼“å­˜ç»Ÿè®¡
cursor.execute("""
    SELECT
        COUNT(*) AS total_entries,
        SUM(hit_count) AS total_hits,
        AVG(hit_count) AS avg_hits_per_entry
    FROM model_cache
""")

# æ¸…ç†æ—§ç¼“å­˜
cursor.execute("""
    DELETE FROM model_cache
    WHERE created_at < now() - INTERVAL '30 days'
      AND hit_count < 5
""")
```

---

## 5. æ‰¹é‡æ¨ç†ä¼˜åŒ–

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class BatchInferenceEngine:
    """æ‰¹é‡æ¨ç†å¼•æ“"""

    def __init__(self, model, batch_size=32, max_wait_ms=100):
        self.model = model
        self.batch_size = batch_size
        self.max_wait_ms = max_wait_ms
        self.queue = []
        self.lock = asyncio.Lock()

    async def infer(self, text):
        """å¼‚æ­¥æ¨ç†"""
        future = asyncio.Future()

        async with self.lock:
            self.queue.append((text, future))

            # è¾¾åˆ°æ‰¹æ¬¡å¤§å°æˆ–è¶…æ—¶ï¼Œæ‰§è¡Œæ‰¹é‡æ¨ç†
            if len(self.queue) >= self.batch_size:
                await self._process_batch()

        return await future

    async def _process_batch(self):
        """å¤„ç†æ‰¹æ¬¡"""
        if not self.queue:
            return

        batch = self.queue[:self.batch_size]
        self.queue = self.queue[self.batch_size:]

        texts = [item[0] for item in batch]
        futures = [item[1] for item in batch]

        # æ‰¹é‡æ¨ç†
        embeddings = await asyncio.to_thread(self.model.encode, texts)

        # è¿”å›ç»“æœ
        for future, embedding in zip(futures, embeddings):
            future.set_result(embedding)

# ä½¿ç”¨
engine = BatchInferenceEngine(model)

async def main():
    tasks = [engine.infer(text) for text in texts]
    results = await asyncio.gather(*tasks)

# æ€§èƒ½å¯¹æ¯”:
# é€æ¡æ¨ç†: 100æ¬¡Ã—10ms = 1000ms
# æ‰¹é‡æ¨ç†: 4æ‰¹æ¬¡Ã—30ms = 120ms (-88%)
```

---

## 6. A/Bæµ‹è¯•æ¡†æ¶

```sql
-- æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
CREATE TABLE model_versions (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100),
    version VARCHAR(50),
    model_path TEXT,
    traffic_percent INT DEFAULT 0,  -- æµé‡ç™¾åˆ†æ¯”
    is_active BOOLEAN DEFAULT false,
    created_at TIMESTAMPTZ DEFAULT now()
);

-- æ’å…¥æ¨¡å‹ç‰ˆæœ¬
INSERT INTO model_versions (name, version, model_path, traffic_percent, is_active) VALUES
('embedding', 'v1.0', '/models/v1', 80, true),
('embedding', 'v2.0', '/models/v2', 20, true);  -- 20%æµé‡æµ‹è¯•æ–°æ¨¡å‹

-- æ¨ç†æ—¥å¿—
CREATE TABLE inference_logs (
    id BIGSERIAL PRIMARY KEY,
    model_version VARCHAR(50),
    input_text TEXT,
    output JSONB,
    latency_ms INT,
    created_at TIMESTAMPTZ DEFAULT now()
);

-- Pythonè·¯ç”±é€»è¾‘
import random

def get_model_by_ab_test():
    """A/Bæµ‹è¯•æ¨¡å‹è·¯ç”±"""

    cursor.execute("""
        SELECT version, model_path, traffic_percent
        FROM model_versions
        WHERE is_active = true
        ORDER BY version
    """)

    versions = cursor.fetchall()

    # æŒ‰æµé‡ç™¾åˆ†æ¯”éšæœºé€‰æ‹©
    rand = random.randint(1, 100)
    cumulative = 0

    for version, path, percent in versions:
        cumulative += percent
        if rand <= cumulative:
            return version, load_model(path)

    return versions[0][0], load_model(versions[0][1])

# åˆ†æA/Bæµ‹è¯•ç»“æœ
cursor.execute("""
    SELECT
        model_version,
        COUNT(*) AS requests,
        AVG(latency_ms) AS avg_latency,
        PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY latency_ms) AS p95_latency
    FROM inference_logs
    WHERE created_at >= now() - INTERVAL '1 day'
    GROUP BY model_version
""")

# ç»“æœ:
# v1.0: 10000 requests, 45ms avg, 80ms p95
# v2.0: 2500 requests, 30ms avg, 55ms p95 (-33% latency)
# â†’ å†³ç­–: å°†v2.0æµé‡æå‡åˆ°100%
```

---

**å®Œæˆ**: PostgreSQL AI/MLæ¨¡å‹å¾®è°ƒä¸ä¼˜åŒ–
**å­—æ•°**: ~10,000å­—
**æ¶µç›–**: å‘é‡æ¨¡å‹å¾®è°ƒã€Text-to-SQLã€æ¨¡å‹é‡åŒ–ã€çŸ¥è¯†è’¸é¦ã€ç¼“å­˜ä¼˜åŒ–ã€æ‰¹é‡æ¨ç†ã€A/Bæµ‹è¯•
