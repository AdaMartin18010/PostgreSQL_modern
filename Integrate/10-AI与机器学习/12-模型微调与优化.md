---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `docs\02-AI-ML\12-æ¨¡å‹å¾®è°ƒä¸ä¼˜åŒ–.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# PostgreSQL AI/MLæ¨¡å‹å¾®è°ƒä¸ä¼˜åŒ–

## ğŸ“‘ ç›®å½•

- [PostgreSQL AI/MLæ¨¡å‹å¾®è°ƒä¸ä¼˜åŒ–](#postgresql-aimlæ¨¡å‹å¾®è°ƒä¸ä¼˜åŒ–)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [2. æŸ¥è¯¢é‡å†™æ¨¡å‹](#2-æŸ¥è¯¢é‡å†™æ¨¡å‹)
    - [2.1 Text-to-SQLå¾®è°ƒ](#21-text-to-sqlå¾®è°ƒ)
  - [3. æ¨¡å‹é‡åŒ–](#3-æ¨¡å‹é‡åŒ–)
    - [3.1 INT8é‡åŒ–](#31-int8é‡åŒ–)
    - [3.2 çŸ¥è¯†è’¸é¦](#32-çŸ¥è¯†è’¸é¦)
  - [4. æ¨¡å‹ç¼“å­˜ä¼˜åŒ–](#4-æ¨¡å‹ç¼“å­˜ä¼˜åŒ–)
  - [5. æ‰¹é‡æ¨ç†ä¼˜åŒ–](#5-æ‰¹é‡æ¨ç†ä¼˜åŒ–)
  - [6. A/Bæµ‹è¯•æ¡†æ¶](#6-abæµ‹è¯•æ¡†æ¶)

---

## 2. æŸ¥è¯¢é‡å†™æ¨¡å‹

### 2.1 Text-to-SQLå¾®è°ƒ

```python
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
import torch

# 1. å‡†å¤‡æ•°æ®é›†
training_data = [
    {
        "input": "æŸ¥è¯¢æ‰€æœ‰å¹´é¾„å¤§äº25çš„ç”¨æˆ·",
        "output": "SELECT * FROM users WHERE age > 25"
    },
    {
        "input": "ç»Ÿè®¡æ¯ä¸ªåŸå¸‚çš„ç”¨æˆ·æ•°é‡",
        "output": "SELECT city, COUNT(*) FROM users GROUP BY city"
    },
    # ... PostgreSQLç‰¹å®šè¯­æ³•
]

# 2. åŠ è½½æ¨¡å‹
model_name = "t5-base"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

# 3. æ•°æ®é¢„å¤„ç†
def preprocess(examples):
    inputs = ["translate to SQL: " + ex['input'] for ex in examples]
    targets = [ex['output'] for ex in examples]

    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding='max_length')
    labels = tokenizer(targets, max_length=256, truncation=True, padding='max_length')

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# 4. è®­ç»ƒ
training_args = TrainingArguments(
    output_dir="./text2sql",
    num_train_epochs=10,
    per_device_train_batch_size=8,
    save_steps=500,
    save_total_limit=2,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)

trainer.train()

# 5. æ¨ç†
def generate_sql(natural_query):
    input_text = f"translate to SQL: {natural_query}"
    inputs = tokenizer(input_text, return_tensors="pt")

    outputs = model.generate(
        inputs.input_ids,
        max_length=256,
        num_beams=4,
        early_stopping=True
    )

    sql = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return sql

# æµ‹è¯•
print(generate_sql("æŸ¥æ‰¾æœ€è¿‘7å¤©çš„è®¢å•"))
# è¾“å‡º: SELECT * FROM orders WHERE created_at >= CURRENT_DATE - INTERVAL '7 days'
```

---

## 3. æ¨¡å‹é‡åŒ–

### 3.1 INT8é‡åŒ–

```python
from optimum.onnxruntime import ORTModelForSequenceClassification
from transformers import AutoTokenizer

# 1. åŠ è½½æ¨¡å‹
model = ORTModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    export=True
)

# 2. é‡åŒ–
from optimum.onnxruntime.configuration import OptimizationConfig, QuantizationConfig

# INT8é‡åŒ–
quantization_config = QuantizationConfig(
    is_static=False,  # åŠ¨æ€é‡åŒ–
    format="QOperator"
)

model.quantize(save_dir="models/bert-int8", quantization_config=quantization_config)

# 3. å¯¹æ¯”
# FP32: 420MB, 50ms/query
# INT8: 110MB, 25ms/query (-74% size, -50% latency)
```

### 3.2 çŸ¥è¯†è’¸é¦

```python
from transformers import DistilBertForSequenceClassification, BertForSequenceClassification

# æ•™å¸ˆæ¨¡å‹ï¼ˆå¤§ï¼‰
teacher = BertForSequenceClassification.from_pretrained('bert-large-uncased')

# å­¦ç”Ÿæ¨¡å‹ï¼ˆå°ï¼‰
student = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')

# è’¸é¦è®­ç»ƒ
import torch.nn.functional as F

def distillation_loss(student_logits, teacher_logits, labels, alpha=0.5, temperature=2.0):
    # è½¯æ ‡ç­¾æŸå¤±
    soft_loss = F.kl_div(
        F.log_softmax(student_logits / temperature, dim=-1),
        F.softmax(teacher_logits / temperature, dim=-1),
        reduction='batchmean'
    ) * (temperature ** 2)

    # ç¡¬æ ‡ç­¾æŸå¤±
    hard_loss = F.cross_entropy(student_logits, labels)

    return alpha * soft_loss + (1 - alpha) * hard_loss

# è®­ç»ƒå¾ªç¯
for batch in train_loader:
    with torch.no_grad():
        teacher_logits = teacher(**batch).logits

    student_logits = student(**batch).logits
    loss = distillation_loss(student_logits, teacher_logits, batch['labels'])

    loss.backward()
    optimizer.step()

# ç»“æœ:
# BERT-Large: 330Må‚æ•°, 1.2GB, 200ms
# DistilBERT: 66Må‚æ•°, 250MB, 60ms (-80% params, -70% latency)
# å‡†ç¡®ç‡ä¿æŒ97%
```

---

## 4. æ¨¡å‹ç¼“å­˜ä¼˜åŒ–

```sql
-- åˆ›å»ºæ¨¡å‹æ¨ç†ç¼“å­˜è¡¨
CREATE TABLE model_cache (
    input_hash VARCHAR(64) PRIMARY KEY,
    input_text TEXT,
    output JSONB,
    model_version VARCHAR(50),
    created_at TIMESTAMPTZ DEFAULT now(),
    hit_count INT DEFAULT 1
);

CREATE INDEX idx_model_version ON model_cache(model_version);
CREATE INDEX idx_created_at ON model_cache(created_at);

-- Pythonç¼“å­˜é€»è¾‘
import hashlib
import json

def get_embedding_with_cache(text, model):
    """å¸¦ç¼“å­˜çš„å‘é‡ç”Ÿæˆ"""

    # è®¡ç®—è¾“å…¥hash
    text_hash = hashlib.sha256(text.encode()).hexdigest()

    # æ£€æŸ¥ç¼“å­˜
    cursor.execute(
        "SELECT output FROM model_cache WHERE input_hash = %s AND model_version = %s",
        (text_hash, MODEL_VERSION)
    )

    result = cursor.fetchone()
    if result:
        # ç¼“å­˜å‘½ä¸­
        cursor.execute(
            "UPDATE model_cache SET hit_count = hit_count + 1 WHERE input_hash = %s",
            (text_hash,)
        )
        return json.loads(result[0])['embedding']

    # ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨æ¨¡å‹
    embedding = model.encode(text)

    # å­˜å…¥ç¼“å­˜
    cursor.execute(
        """
        INSERT INTO model_cache (input_hash, input_text, output, model_version)
        VALUES (%s, %s, %s, %s)
        ON CONFLICT (input_hash) DO UPDATE SET hit_count = model_cache.hit_count + 1
        """,
        (text_hash, text, json.dumps({'embedding': embedding.tolist()}), MODEL_VERSION)
    )
    conn.commit()

    return embedding

# ç¼“å­˜ç»Ÿè®¡
cursor.execute("""
    SELECT
        COUNT(*) AS total_entries,
        SUM(hit_count) AS total_hits,
        AVG(hit_count) AS avg_hits_per_entry
    FROM model_cache
""")

# æ¸…ç†æ—§ç¼“å­˜
cursor.execute("""
    DELETE FROM model_cache
    WHERE created_at < now() - INTERVAL '30 days'
      AND hit_count < 5
""")
```

---

## 5. æ‰¹é‡æ¨ç†ä¼˜åŒ–

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class BatchInferenceEngine:
    """æ‰¹é‡æ¨ç†å¼•æ“"""

    def __init__(self, model, batch_size=32, max_wait_ms=100):
        self.model = model
        self.batch_size = batch_size
        self.max_wait_ms = max_wait_ms
        self.queue = []
        self.lock = asyncio.Lock()

    async def infer(self, text):
        """å¼‚æ­¥æ¨ç†"""
        future = asyncio.Future()

        async with self.lock:
            self.queue.append((text, future))

            # è¾¾åˆ°æ‰¹æ¬¡å¤§å°æˆ–è¶…æ—¶ï¼Œæ‰§è¡Œæ‰¹é‡æ¨ç†
            if len(self.queue) >= self.batch_size:
                await self._process_batch()

        return await future

    async def _process_batch(self):
        """å¤„ç†æ‰¹æ¬¡"""
        if not self.queue:
            return

        batch = self.queue[:self.batch_size]
        self.queue = self.queue[self.batch_size:]

        texts = [item[0] for item in batch]
        futures = [item[1] for item in batch]

        # æ‰¹é‡æ¨ç†
        embeddings = await asyncio.to_thread(self.model.encode, texts)

        # è¿”å›ç»“æœ
        for future, embedding in zip(futures, embeddings):
            future.set_result(embedding)

# ä½¿ç”¨
engine = BatchInferenceEngine(model)

async def main():
    tasks = [engine.infer(text) for text in texts]
    results = await asyncio.gather(*tasks)

# æ€§èƒ½å¯¹æ¯”:
# é€æ¡æ¨ç†: 100æ¬¡Ã—10ms = 1000ms
# æ‰¹é‡æ¨ç†: 4æ‰¹æ¬¡Ã—30ms = 120ms (-88%)
```

---

## 6. A/Bæµ‹è¯•æ¡†æ¶

```sql
-- æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
CREATE TABLE model_versions (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100),
    version VARCHAR(50),
    model_path TEXT,
    traffic_percent INT DEFAULT 0,  -- æµé‡ç™¾åˆ†æ¯”
    is_active BOOLEAN DEFAULT false,
    created_at TIMESTAMPTZ DEFAULT now()
);

-- æ’å…¥æ¨¡å‹ç‰ˆæœ¬
INSERT INTO model_versions (name, version, model_path, traffic_percent, is_active) VALUES
('embedding', 'v1.0', '/models/v1', 80, true),
('embedding', 'v2.0', '/models/v2', 20, true);  -- 20%æµé‡æµ‹è¯•æ–°æ¨¡å‹

-- æ¨ç†æ—¥å¿—
CREATE TABLE inference_logs (
    id BIGSERIAL PRIMARY KEY,
    model_version VARCHAR(50),
    input_text TEXT,
    output JSONB,
    latency_ms INT,
    created_at TIMESTAMPTZ DEFAULT now()
);

-- Pythonè·¯ç”±é€»è¾‘
import random

def get_model_by_ab_test():
    """A/Bæµ‹è¯•æ¨¡å‹è·¯ç”±"""

    cursor.execute("""
        SELECT version, model_path, traffic_percent
        FROM model_versions
        WHERE is_active = true
        ORDER BY version
    """)

    versions = cursor.fetchall()

    # æŒ‰æµé‡ç™¾åˆ†æ¯”éšæœºé€‰æ‹©
    rand = random.randint(1, 100)
    cumulative = 0

    for version, path, percent in versions:
        cumulative += percent
        if rand <= cumulative:
            return version, load_model(path)

    return versions[0][0], load_model(versions[0][1])

# åˆ†æA/Bæµ‹è¯•ç»“æœ
cursor.execute("""
    SELECT
        model_version,
        COUNT(*) AS requests,
        AVG(latency_ms) AS avg_latency,
        PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY latency_ms) AS p95_latency
    FROM inference_logs
    WHERE created_at >= now() - INTERVAL '1 day'
    GROUP BY model_version
""")

# ç»“æœ:
# v1.0: 10000 requests, 45ms avg, 80ms p95
# v2.0: 2500 requests, 30ms avg, 55ms p95 (-33% latency)
# â†’ å†³ç­–: å°†v2.0æµé‡æå‡åˆ°100%
```

---

## 7. å‘é‡æ¨¡å‹å¾®è°ƒ

### 7.1 Sentence-BERTå¾®è°ƒ

**Sentence-BERTå¾®è°ƒï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```python
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader

# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# 2. å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆPostgreSQLæŸ¥è¯¢ç›¸ä¼¼åº¦ï¼‰
training_examples = [
    InputExample(
        texts=[
            "æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·",
            "SELECT * FROM users"
        ],
        label=1.0  # ç›¸ä¼¼åº¦
    ),
    InputExample(
        texts=[
            "æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·",
            "DELETE FROM users"
        ],
        label=0.0  # ä¸ç›¸ä¼¼
    ),
    # ... æ›´å¤šPostgreSQLç‰¹å®šæ ·æœ¬
]

# 3. è®­ç»ƒ
train_dataloader = DataLoader(training_examples, shuffle=True, batch_size=16)
train_loss = losses.CosineSimilarityLoss(model)

model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=10,
    warmup_steps=100,
    output_path='./models/postgresql-sbert'
)

# 4. æµ‹è¯•
test_sentences = [
    "æŸ¥è¯¢æ‰€æœ‰è®¢å•",
    "SELECT * FROM orders",
    "ç»Ÿè®¡ç”¨æˆ·æ•°é‡"
]

embeddings = model.encode(test_sentences)
similarity = cosine_similarity(embeddings[0], embeddings[1])
# è¾“å‡º: 0.95ï¼ˆé«˜ç›¸ä¼¼åº¦ï¼‰
```

### 7.2 é¢†åŸŸç‰¹å®šå¾®è°ƒ

**é¢†åŸŸç‰¹å®šå¾®è°ƒï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```python
# é‡‘èé¢†åŸŸå¾®è°ƒ
financial_examples = [
    InputExample(
        texts=[
            "æŸ¥è¯¢è´¦æˆ·ä½™é¢",
            "SELECT balance FROM accounts WHERE account_id = ?"
        ],
        label=1.0
    ),
    # ... é‡‘èé¢†åŸŸç‰¹å®šæ ·æœ¬
]

# ç”µå•†é¢†åŸŸå¾®è°ƒ
ecommerce_examples = [
    InputExample(
        texts=[
            "æŸ¥è¯¢å•†å“åº“å­˜",
            "SELECT stock FROM products WHERE product_id = ?"
        ],
        label=1.0
    ),
    # ... ç”µå•†é¢†åŸŸç‰¹å®šæ ·æœ¬
]

# åˆ†åˆ«è®­ç»ƒ
financial_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
financial_model.fit(...)

ecommerce_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
ecommerce_model.fit(...)
```

---

## 8. æ¨¡å‹ç‰ˆæœ¬ç®¡ç†

### 8.1 ç‰ˆæœ¬æ§åˆ¶ç­–ç•¥

**ç‰ˆæœ¬æ§åˆ¶ç­–ç•¥ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```sql
-- æ¨¡å‹ç‰ˆæœ¬ç®¡ç†è¡¨
CREATE TABLE model_registry (
    id SERIAL PRIMARY KEY,
    model_name VARCHAR(100) NOT NULL,
    version VARCHAR(50) NOT NULL,
    model_path TEXT NOT NULL,
    model_type VARCHAR(50),  -- embedding, classification, generation
    model_size_mb INT,
    accuracy_score FLOAT,
    latency_ms FLOAT,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    is_production BOOLEAN DEFAULT FALSE,
    UNIQUE(model_name, version)
);

CREATE INDEX idx_model_registry_name ON model_registry(model_name, created_at DESC);
CREATE INDEX idx_model_registry_production ON model_registry(is_production) WHERE is_production = TRUE;

-- æ’å…¥æ¨¡å‹ç‰ˆæœ¬
INSERT INTO model_registry (
    model_name, version, model_path, model_type, model_size_mb, accuracy_score, latency_ms
) VALUES (
    'embedding', 'v1.0', '/models/embedding/v1.0', 'embedding', 420, 0.92, 50
);

-- æ ‡è®°ç”Ÿäº§ç‰ˆæœ¬
UPDATE model_registry
SET is_production = FALSE
WHERE model_name = 'embedding';

UPDATE model_registry
SET is_production = TRUE
WHERE model_name = 'embedding' AND version = 'v2.0';
```

### 8.2 æ¨¡å‹å›æ»šæœºåˆ¶

**æ¨¡å‹å›æ»šæœºåˆ¶ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```python
def rollback_model(model_name: str, target_version: str):
    """å›æ»šæ¨¡å‹åˆ°æŒ‡å®šç‰ˆæœ¬"""

    # 1. æ£€æŸ¥ç›®æ ‡ç‰ˆæœ¬æ˜¯å¦å­˜åœ¨
    cursor.execute("""
        SELECT model_path, is_production
        FROM model_registry
        WHERE model_name = %s AND version = %s
    """, (model_name, target_version))

    result = cursor.fetchone()
    if not result:
        raise ValueError(f"æ¨¡å‹ç‰ˆæœ¬ {model_name}:{target_version} ä¸å­˜åœ¨")

    model_path, is_production = result

    # 2. å¤‡ä»½å½“å‰ç”Ÿäº§ç‰ˆæœ¬
    cursor.execute("""
        SELECT version, model_path
        FROM model_registry
        WHERE model_name = %s AND is_production = TRUE
    """, (model_name,))

    current_prod = cursor.fetchone()
    if current_prod:
        backup_path = f"{current_prod[1]}.backup"
        shutil.copy(current_prod[1], backup_path)

    # 3. åˆ‡æ¢åˆ°ç›®æ ‡ç‰ˆæœ¬
    cursor.execute("""
        UPDATE model_registry
        SET is_production = FALSE
        WHERE model_name = %s
    """, (model_name,))

    cursor.execute("""
        UPDATE model_registry
        SET is_production = TRUE
        WHERE model_name = %s AND version = %s
    """, (model_name, target_version))

    conn.commit()

    # 4. é‡æ–°åŠ è½½æ¨¡å‹
    model = load_model(model_path)
    return model
```

---

## 9. æ¨¡å‹æ€§èƒ½ç›‘æ§

### 9.1 æ¨ç†æ€§èƒ½ç›‘æ§

**æ¨ç†æ€§èƒ½ç›‘æ§ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```sql
-- æ¨ç†æ€§èƒ½ç›‘æ§è¡¨
CREATE TABLE model_performance_logs (
    id BIGSERIAL PRIMARY KEY,
    model_name VARCHAR(100),
    model_version VARCHAR(50),
    input_length INT,
    output_length INT,
    latency_ms FLOAT,
    memory_mb FLOAT,
    gpu_utilization FLOAT,
    error_message TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW()
) PARTITION BY RANGE (created_at);

CREATE TABLE model_performance_logs_2025_01 PARTITION OF model_performance_logs
FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

CREATE INDEX idx_model_perf_model ON model_performance_logs(model_name, model_version, created_at);
CREATE INDEX idx_model_perf_latency ON model_performance_logs(latency_ms, created_at);

-- æ€§èƒ½ç»Ÿè®¡æŸ¥è¯¢
SELECT
    model_name,
    model_version,
    COUNT(*) AS request_count,
    AVG(latency_ms) AS avg_latency,
    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY latency_ms) AS p95_latency,
    PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY latency_ms) AS p99_latency,
    AVG(memory_mb) AS avg_memory,
    COUNT(*) FILTER (WHERE error_message IS NOT NULL) AS error_count
FROM model_performance_logs
WHERE created_at >= NOW() - INTERVAL '1 day'
GROUP BY model_name, model_version;
```

### 9.2 æ¨¡å‹è´¨é‡ç›‘æ§

**æ¨¡å‹è´¨é‡ç›‘æ§ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```python
# æ¨¡å‹è´¨é‡ç›‘æ§
class ModelQualityMonitor:
    """æ¨¡å‹è´¨é‡ç›‘æ§å™¨"""

    def __init__(self, model_name: str, model_version: str):
        self.model_name = model_name
        self.model_version = model_version
        self.quality_metrics = {
            'accuracy': [],
            'precision': [],
            'recall': [],
            'f1_score': []
        }

    def log_prediction(self, input_text: str, predicted: str, actual: str):
        """è®°å½•é¢„æµ‹ç»“æœ"""
        # è®¡ç®—è´¨é‡æŒ‡æ ‡
        accuracy = 1.0 if predicted == actual else 0.0

        # å­˜å‚¨åˆ°æ•°æ®åº“
        cursor.execute("""
            INSERT INTO model_quality_logs (
                model_name, model_version, input_text, predicted, actual, accuracy
            ) VALUES (%s, %s, %s, %s, %s, %s)
        """, (self.model_name, self.model_version, input_text, predicted, actual, accuracy))

        conn.commit()

    def get_quality_report(self) -> dict:
        """è·å–è´¨é‡æŠ¥å‘Š"""
        cursor.execute("""
            SELECT
                AVG(accuracy) AS avg_accuracy,
                COUNT(*) AS total_predictions,
                COUNT(*) FILTER (WHERE predicted = actual) AS correct_predictions
            FROM model_quality_logs
            WHERE model_name = %s AND model_version = %s
                AND created_at >= NOW() - INTERVAL '1 day'
        """, (self.model_name, self.model_version))

        result = cursor.fetchone()
        return {
            'accuracy': result[0],
            'total': result[1],
            'correct': result[2]
        }
```

---

## 10. æ¨¡å‹éƒ¨ç½²ä¼˜åŒ–

### 10.1 ONNX Runtimeä¼˜åŒ–

**ONNX Runtimeä¼˜åŒ–ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```python
from optimum.onnxruntime import ORTModelForSequenceClassification
from transformers import AutoTokenizer
import onnxruntime as ort

# 1. è½¬æ¢ä¸ºONNXæ ¼å¼
model = ORTModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    export=True
)

# 2. ä¼˜åŒ–ONNX Runtimeä¼šè¯
sess_options = ort.SessionOptions()
sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL
sess_options.intra_op_num_threads = 4
sess_options.inter_op_num_threads = 4

# 3. åˆ›å»ºä¼˜åŒ–ä¼šè¯
model.model = ort.InferenceSession(
    model.model.get_inputs()[0].name,
    sess_options=sess_options
)

# æ€§èƒ½æå‡:
# PyTorch: 50ms/query
# ONNX Runtime: 25ms/query (-50%)
```

### 10.2 TensorRTä¼˜åŒ–

**TensorRTä¼˜åŒ–ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```python
import tensorrt as trt
import pycuda.driver as cuda

# 1. æ„å»ºTensorRTå¼•æ“
def build_engine(onnx_path, engine_path):
    """æ„å»ºTensorRTå¼•æ“"""
    logger = trt.Logger(trt.Logger.WARNING)
    builder = trt.Builder(logger)
    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
    parser = trt.OnnxParser(network, logger)

    with open(onnx_path, 'rb') as model:
        parser.parse(model.read())

    config = builder.create_builder_config()
    config.max_workspace_size = 1 << 30  # 1GB
    config.set_flag(trt.BuilderFlag.FP16)  # FP16ç²¾åº¦

    engine = builder.build_engine(network, config)

    with open(engine_path, 'wb') as f:
        f.write(engine.serialize())

    return engine

# 2. ä½¿ç”¨TensorRTæ¨ç†
engine = build_engine('model.onnx', 'model.trt')
context = engine.create_execution_context()

# æ€§èƒ½æå‡:
# ONNX Runtime: 25ms/query
# TensorRT: 10ms/query (-60%)
```

---

**å®Œæˆ**: PostgreSQL AI/MLæ¨¡å‹å¾®è°ƒä¸ä¼˜åŒ–
**å­—æ•°**: ~15,000å­—
**æ¶µç›–**: å‘é‡æ¨¡å‹å¾®è°ƒã€Text-to-SQLã€æ¨¡å‹é‡åŒ–ã€çŸ¥è¯†è’¸é¦ã€ç¼“å­˜ä¼˜åŒ–ã€æ‰¹é‡æ¨ç†ã€A/Bæµ‹è¯•ã€ç‰ˆæœ¬ç®¡ç†ã€æ€§èƒ½ç›‘æ§ã€éƒ¨ç½²ä¼˜åŒ–
