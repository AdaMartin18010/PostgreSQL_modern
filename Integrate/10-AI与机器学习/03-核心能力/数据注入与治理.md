---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `PostgreSQL_AI\03-æ ¸å¿ƒèƒ½åŠ›\æ•°æ®æ³¨å…¥ä¸æ²»ç†.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# æ•°æ®æ³¨å…¥ä¸æ²»ç†

> **æ–‡æ¡£ç¼–å·**: AI-03-06
> **æœ€åæ›´æ–°**: 2025å¹´1æœˆ
> **ä¸»é¢˜**: 03-æ ¸å¿ƒèƒ½åŠ›
> **å­ä¸»é¢˜**: 06-æ•°æ®æ³¨å…¥ä¸æ²»ç†

## ğŸ“‘ ç›®å½•

- [æ•°æ®æ³¨å…¥ä¸æ²»ç†](#æ•°æ®æ³¨å…¥ä¸æ²»ç†)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ•°æ®æ³¨å…¥æ¦‚è¿°](#1-æ•°æ®æ³¨å…¥æ¦‚è¿°)
    - [1.1 æ•°æ®æ³¨å…¥æ¶æ„](#11-æ•°æ®æ³¨å…¥æ¶æ„)
    - [1.2 æ•°æ®æ³¨å…¥æ–¹å¼](#12-æ•°æ®æ³¨å…¥æ–¹å¼)
  - [2. æ•°æ®æ³¨å…¥å®ç°](#2-æ•°æ®æ³¨å…¥å®ç°)
    - [2.1 CDCå˜æ›´æ•°æ®æ•è·](#21-cdcå˜æ›´æ•°æ®æ•è·)
    - [2.2 æ‰¹é‡ETLå¤„ç†](#22-æ‰¹é‡etlå¤„ç†)
    - [2.3 å®æ—¶æµå¼å¤„ç†](#23-å®æ—¶æµå¼å¤„ç†)
  - [3. æ•°æ®æ²»ç†](#3-æ•°æ®æ²»ç†)
    - [3.1 æ•°æ®éªŒè¯](#31-æ•°æ®éªŒè¯)
    - [3.2 æ•°æ®æ¸…æ´—](#32-æ•°æ®æ¸…æ´—)
    - [3.3 æ•°æ®æ ‡å‡†åŒ–](#33-æ•°æ®æ ‡å‡†åŒ–)
  - [4. æ•°æ®ç‰ˆæœ¬ç®¡ç†](#4-æ•°æ®ç‰ˆæœ¬ç®¡ç†)
    - [4.1 ç‰ˆæœ¬æ§åˆ¶ç­–ç•¥](#41-ç‰ˆæœ¬æ§åˆ¶ç­–ç•¥)
    - [4.2 æ•°æ®å¿«ç…§](#42-æ•°æ®å¿«ç…§)
    - [4.3 æ•°æ®å›æ»š](#43-æ•°æ®å›æ»š)
  - [5. è‡ªåŠ¨åŒ–å‘é‡åŒ–](#5-è‡ªåŠ¨åŒ–å‘é‡åŒ–)
    - [5.1 pg\_ai Vectorizer](#51-pg_ai-vectorizer)
    - [5.2 æ‰¹é‡å¤„ç†](#52-æ‰¹é‡å¤„ç†)
    - [5.3 å¢é‡æ›´æ–°](#53-å¢é‡æ›´æ–°)
  - [6. ç›‘æ§ä¸å‘Šè­¦](#6-ç›‘æ§ä¸å‘Šè­¦)
    - [6.1 æ•°æ®è´¨é‡ç›‘æ§](#61-æ•°æ®è´¨é‡ç›‘æ§)
    - [6.2 å¤„ç†è¿›åº¦ç›‘æ§](#62-å¤„ç†è¿›åº¦ç›‘æ§)
    - [6.3 å¼‚å¸¸å‘Šè­¦](#63-å¼‚å¸¸å‘Šè­¦)

---

## 1. æ•°æ®æ³¨å…¥æ¦‚è¿°

### 1.1 æ•°æ®æ³¨å…¥æ¶æ„

**æ•°æ®æ³¨å…¥æµç¨‹**ï¼š

```mermaid
graph TB
    subgraph "æ•°æ®æº"
        Source1[å…³ç³»æ•°æ®åº“]
        Source2[NoSQLæ•°æ®åº“]
        Source3[æ–‡ä»¶ç³»ç»Ÿ]
        Source4[APIæ¥å£]
        Source5[æ¶ˆæ¯é˜Ÿåˆ—]
    end

    subgraph "æ•°æ®é‡‡é›†å±‚"
        CDC[Debezium CDC]
        ETL[ETLå·¥å…·]
        Stream[æµå¼å¤„ç†]
    end

    subgraph "æ•°æ®å¤„ç†å±‚"
        Transform[æ•°æ®è½¬æ¢]
        Validate[æ•°æ®éªŒè¯]
        Clean[æ•°æ®æ¸…æ´—]
    end

    subgraph "PostgreSQLå­˜å‚¨"
        PG[(PostgreSQL)]
        Vector[å‘é‡åŒ–]
        Index[ç´¢å¼•æ„å»º]
    end

    Source1 --> CDC
    Source2 --> ETL
    Source3 --> ETL
    Source4 --> Stream
    Source5 --> Stream

    CDC --> Transform
    ETL --> Transform
    Stream --> Transform

    Transform --> Validate
    Validate --> Clean
    Clean --> PG
    PG --> Vector
    Vector --> Index

    style PG fill:#4a90e2,color:#fff
    style Vector fill:#50c878,color:#fff
```

### 1.2 æ•°æ®æ³¨å…¥æ–¹å¼

**æ•°æ®æ³¨å…¥æ–¹å¼å¯¹æ¯”**ï¼š

| æ–¹å¼ | å»¶è¿Ÿ | ååé‡ | é€‚ç”¨åœºæ™¯ |
|------|------|--------|---------|
| **COPYå‘½ä»¤** | ä½ | 100ä¸‡è¡Œ/s | æ‰¹é‡å¯¼å…¥ |
| **INSERTæ‰¹é‡** | ä¸­ | 10ä¸‡è¡Œ/s | å°æ‰¹é‡å¯¼å…¥ |
| **é€»è¾‘å¤åˆ¶** | ä½ | å®æ—¶ | å®æ—¶åŒæ­¥ |
| **CDCï¼ˆDebeziumï¼‰** | ä½ | 5ä¸‡æ¡/s | å˜æ›´æ•è· |
| **æµå¼å¤„ç†ï¼ˆFlinkï¼‰** | ä½ | 10ä¸‡+QPS | å®æ—¶è®¡ç®— |

---

## 2. æ•°æ®æ³¨å…¥å®ç°

### 2.1 CDCå˜æ›´æ•°æ®æ•è·

**Debeziumé…ç½®**ï¼š

```sql
-- 1. é…ç½®é€»è¾‘å¤åˆ¶
ALTER SYSTEM SET wal_level = logical;
ALTER SYSTEM SET max_replication_slots = 10;
SELECT pg_reload_conf();

-- 2. åˆ›å»ºå¤åˆ¶æ§½
SELECT pg_create_logical_replication_slot('debezium_slot', 'pgoutput');

-- 3. Debeziumè‡ªåŠ¨æ•è·å˜æ›´å¹¶å†™å…¥PostgreSQL
-- é…ç½®è‡ªåŠ¨åˆ›å»ºå‘é‡è¡¨ç»“æ„
CREATE TABLE user_behavior_vectors (
    id SERIAL PRIMARY KEY,
    user_id INT REFERENCES users(id),
    behavior_vec vector(1536),
    created_at TIMESTAMPTZ DEFAULT NOW()
);
```

**å®æ—¶æ•°æ®åŒæ­¥**ï¼š

```sql
-- ä½¿ç”¨é€»è¾‘å¤åˆ¶å®æ—¶åŒæ­¥
CREATE PUBLICATION my_publication FOR TABLE users, items, behaviors;

-- è®¢é˜…ç«¯
CREATE SUBSCRIPTION my_subscription
CONNECTION 'host=source_db port=5432 dbname=mydb'
PUBLICATION my_publication;
```

### 2.2 æ‰¹é‡ETLå¤„ç†

**COPYå‘½ä»¤æ‰¹é‡å¯¼å…¥**ï¼š

```sql
-- 1. ä»CSVæ–‡ä»¶å¯¼å…¥
COPY documents(title, content, category_id)
FROM '/path/to/data.csv'
WITH (FORMAT csv, HEADER true);

-- 2. æ‰¹é‡ç”Ÿæˆå‘é‡
UPDATE documents
SET embedding = ai.embedding_openai(
    'text-embedding-3-small',
    title || ' ' || content
)
WHERE embedding IS NULL;

-- 3. æ‰¹é‡åˆ›å»ºç´¢å¼•
CREATE INDEX CONCURRENTLY ON documents
USING hnsw(embedding vector_cosine_ops);
```

**Airflow ETLç®¡é“**ï¼š

```python
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.operators.python import PythonOperator

def generate_embeddings(**context):
    # æ‰¹é‡ç”Ÿæˆembedding
    docs = fetch_unprocessed_docs()
    vectors = openai.Embedding.create(input=docs)
    # æ‰¹é‡å†™å…¥PostgreSQL
    postgres_hook.insert_rows("document_vectors", vectors)

with DAG('ai_data_pipeline') as dag:
    extract = PostgresOperator(
        sql="SELECT * FROM raw_documents WHERE processed=false"
    )
    embed = PythonOperator(python_callable=generate_embeddings)
    load = PostgresOperator(
        sql="UPDATE raw_documents SET processed=true"
    )
    extract >> embed >> load
```

### 2.3 å®æ—¶æµå¼å¤„ç†

**Flinkæµå¼å¤„ç†**ï¼š

```sql
-- Flinkå®æ—¶å†™å…¥PostgreSQL
-- ä½¿ç”¨UPSERTè¯­ä¹‰ä¿è¯exactly-once
INSERT INTO documents (id, content, embedding)
VALUES (?, ?, ?)
ON CONFLICT (id) DO UPDATE
SET content = EXCLUDED.content,
    embedding = EXCLUDED.embedding;
```

**Kafka Connecté›†æˆ**ï¼š

```sql
-- Kafka Connect JDBC Sinké…ç½®
-- æ‰¹é‡å†™å…¥PostgreSQL
INSERT INTO documents (content, embedding)
SELECT content, embedding
FROM kafka_messages
WHERE processed = false
LIMIT 1000;
```

---

## 3. æ•°æ®æ²»ç†

### 3.1 æ•°æ®éªŒè¯

**æ•°æ®è´¨é‡æ£€æŸ¥**ï¼š

```sql
-- 1. æ£€æŸ¥ç¼ºå¤±å€¼
SELECT
    COUNT(*) AS total_rows,
    COUNT(embedding) AS non_null_embedding,
    COUNT(*) - COUNT(embedding) AS missing_embedding
FROM documents;

-- 2. æ£€æŸ¥å‘é‡ç»´åº¦
SELECT
    id,
    CASE
        WHEN array_length(embedding::float[], 1) != 1536 THEN 'Wrong dimension'
        ELSE 'OK'
    END AS dimension_check
FROM documents
WHERE embedding IS NOT NULL;

-- 3. æ£€æŸ¥æ•°æ®èŒƒå›´
SELECT
    COUNT(*) AS total,
    COUNT(*) FILTER (WHERE rating BETWEEN 0 AND 5) AS valid_rating,
    COUNT(*) FILTER (WHERE rating NOT BETWEEN 0 AND 5) AS invalid_rating
FROM items;
```

### 3.2 æ•°æ®æ¸…æ´—

**æ•°æ®æ¸…æ´—SQL**ï¼š

```sql
-- 1. æ¸…ç†HTMLæ ‡ç­¾
UPDATE documents
SET content = regexp_replace(content, '<[^>]+>', '', 'g')
WHERE content ~ '<[^>]+>';

-- 2. æ ‡å‡†åŒ–ç©ºæ ¼
UPDATE documents
SET content = regexp_replace(content, '\s+', ' ', 'g');

-- 3. ç§»é™¤ç‰¹æ®Šå­—ç¬¦
UPDATE documents
SET content = regexp_replace(content, '[^\w\s\u4e00-\u9fa5]', '', 'g');

-- 4. æˆªæ–­è¿‡é•¿æ–‡æœ¬
UPDATE documents
SET content = left(content, 2000)
WHERE length(content) > 2000;
```

### 3.3 æ•°æ®æ ‡å‡†åŒ–

**æ•°æ®æ ‡å‡†åŒ–å¤„ç†**ï¼š

```sql
-- 1. æ–‡æœ¬æ ‡å‡†åŒ–
CREATE OR REPLACE FUNCTION normalize_text(text_content TEXT)
RETURNS TEXT AS $$
BEGIN
    -- ç§»é™¤HTMLæ ‡ç­¾
    text_content = regexp_replace(text_content, '<[^>]+>', '', 'g');
    -- æ ‡å‡†åŒ–ç©ºæ ¼
    text_content = regexp_replace(text_content, '\s+', ' ', 'g');
    -- æˆªæ–­è¿‡é•¿æ–‡æœ¬
    IF length(text_content) > 2000 THEN
        text_content = left(text_content, 2000);
    END IF;
    RETURN trim(text_content);
END;
$$ LANGUAGE plpgsql;

-- 2. ä½¿ç”¨æ ‡å‡†åŒ–å‡½æ•°
UPDATE documents
SET content = normalize_text(content)
WHERE content IS NOT NULL;
```

---

## 4. æ•°æ®ç‰ˆæœ¬ç®¡ç†

### 4.1 ç‰ˆæœ¬æ§åˆ¶ç­–ç•¥

**æ•°æ®ç‰ˆæœ¬è¡¨è®¾è®¡**ï¼š

```sql
-- 1. ä¸»æ•°æ®è¡¨
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    content TEXT NOT NULL,
    embedding vector(1536),
    version INT DEFAULT 1,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- 2. ç‰ˆæœ¬å†å²è¡¨
CREATE TABLE document_versions (
    id SERIAL PRIMARY KEY,
    document_id INT REFERENCES documents(id),
    content TEXT,
    embedding vector(1536),
    version INT,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- 3. ç‰ˆæœ¬è§¦å‘å™¨
CREATE OR REPLACE FUNCTION save_document_version()
RETURNS TRIGGER AS $$
BEGIN
    INSERT INTO document_versions (document_id, content, embedding, version)
    VALUES (OLD.id, OLD.content, OLD.embedding, OLD.version);
    NEW.version = OLD.version + 1;
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER document_version_trigger
BEFORE UPDATE ON documents
FOR EACH ROW
EXECUTE FUNCTION save_document_version();
```

### 4.2 æ•°æ®å¿«ç…§

**æ•°æ®å¿«ç…§åˆ›å»º**ï¼š

```sql
-- 1. åˆ›å»ºå¿«ç…§è¡¨
CREATE TABLE documents_snapshot_20250101 AS
SELECT * FROM documents;

-- 2. ä½¿ç”¨æ—¶é—´ç‚¹æ¢å¤
-- é…ç½®WALå½’æ¡£
ALTER SYSTEM SET archive_mode = on;
ALTER SYSTEM SET archive_command = 'cp %p /backup/wal/%f';

-- 3. åˆ›å»ºåŸºç¡€å¤‡ä»½
SELECT pg_start_backup('backup_label');
-- å¤åˆ¶æ•°æ®ç›®å½•
SELECT pg_stop_backup();
```

### 4.3 æ•°æ®å›æ»š

**æ•°æ®å›æ»šå®ç°**ï¼š

```sql
-- 1. å›æ»šåˆ°æŒ‡å®šç‰ˆæœ¬
UPDATE documents d
SET
    content = dv.content,
    embedding = dv.embedding,
    version = dv.version
FROM document_versions dv
WHERE d.id = dv.document_id
  AND dv.version = 5;  -- å›æ»šåˆ°ç‰ˆæœ¬5

-- 2. å›æ»šåˆ°æŒ‡å®šæ—¶é—´ç‚¹
UPDATE documents d
SET
    content = dv.content,
    embedding = dv.embedding
FROM document_versions dv
WHERE d.id = dv.document_id
  AND dv.created_at = (
      SELECT MAX(created_at)
      FROM document_versions
      WHERE document_id = d.id
        AND created_at <= '2025-01-01 00:00:00'
  );
```

---

## 5. è‡ªåŠ¨åŒ–å‘é‡åŒ–

### 5.1 pg_ai Vectorizer

**è‡ªåŠ¨å‘é‡åŒ–é…ç½®**ï¼š

```sql
-- 1. åˆ›å»ºè‡ªåŠ¨å‘é‡åŒ–å™¨
SELECT ai.create_vectorizer(
    'documents'::regclass,
    destination => 'document_chunks',
    embedding => ai.embedding_openai('text-embedding-3-small', 'content'),
    chunking => ai.chunking_recursive_character_text_splitter(
        'content',
        chunk_size => 500,
        chunk_overlap => 100
    )
);

-- 2. æ’å…¥æ–‡æ¡£ï¼Œè‡ªåŠ¨å‘é‡åŒ–
INSERT INTO documents(title, content)
VALUES ('New Document', 'Content here...');
-- è‡ªåŠ¨ç”Ÿæˆchunkså’Œembeddings
```

### 5.2 æ‰¹é‡å¤„ç†

**æ‰¹é‡å‘é‡åŒ–**ï¼š

```sql
-- 1. æ‰¹é‡ç”Ÿæˆå‘é‡
WITH batch AS (
    SELECT id, content
    FROM documents
    WHERE embedding IS NULL
    LIMIT 1000
)
UPDATE documents d
SET embedding = ai.embedding_openai('text-embedding-3-small', d.content)
FROM batch b
WHERE d.id = b.id;

-- 2. ä½¿ç”¨COPYæ‰¹é‡å¯¼å…¥
COPY documents(title, content)
FROM '/path/to/data.csv'
WITH (FORMAT csv, HEADER true);

-- 3. æ‰¹é‡åˆ›å»ºç´¢å¼•
CREATE INDEX CONCURRENTLY ON documents
USING hnsw(embedding vector_cosine_ops);
```

### 5.3 å¢é‡æ›´æ–°

**å¢é‡å‘é‡åŒ–**ï¼š

```sql
-- 1. å¢é‡æ›´æ–°å‘é‡
CREATE OR REPLACE FUNCTION incremental_vectorize()
RETURNS void AS $$
BEGIN
    UPDATE documents
    SET embedding = ai.embedding_openai('text-embedding-3-small', content)
    WHERE embedding IS NULL
      AND updated_at > NOW() - INTERVAL '1 day'
    LIMIT 1000;
END;
$$ LANGUAGE plpgsql;

-- 2. å®šæ—¶ä»»åŠ¡ï¼ˆä½¿ç”¨pg_cronï¼‰
SELECT cron.schedule(
    'incremental-vectorize',
    '*/5 * * * *',  -- æ¯5åˆ†é’Ÿ
    'SELECT incremental_vectorize();'
);
```

---

## 6. ç›‘æ§ä¸å‘Šè­¦

### 6.1 æ•°æ®è´¨é‡ç›‘æ§

**æ•°æ®è´¨é‡æ£€æŸ¥**ï¼š

```sql
-- 1. æ£€æŸ¥å‘é‡å®Œæ•´æ€§
SELECT
    COUNT(*) AS total_documents,
    COUNT(embedding) AS documents_with_embedding,
    COUNT(*) - COUNT(embedding) AS missing_embeddings,
    ROUND(COUNT(embedding)::float / COUNT(*) * 100, 2) AS completeness_pct
FROM documents;

-- 2. æ£€æŸ¥å‘é‡è´¨é‡
SELECT
    id,
    CASE
        WHEN array_length(embedding::float[], 1) != 1536 THEN 'Wrong dimension'
        WHEN embedding IS NULL THEN 'Missing'
        ELSE 'OK'
    END AS quality_status
FROM documents
WHERE quality_status != 'OK';

-- 3. æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
SELECT
    COUNT(*) AS total_chunks,
    COUNT(DISTINCT document_id) AS unique_documents,
    COUNT(*) - COUNT(DISTINCT document_id) AS duplicate_chunks
FROM document_chunks;
```

### 6.2 å¤„ç†è¿›åº¦ç›‘æ§

**å¤„ç†è¿›åº¦è·Ÿè¸ª**ï¼š

```sql
-- 1. åˆ›å»ºå¤„ç†è¿›åº¦è¡¨
CREATE TABLE processing_status (
    table_name TEXT PRIMARY KEY,
    total_rows BIGINT,
    processed_rows BIGINT,
    last_processed_at TIMESTAMPTZ,
    status TEXT  -- processing, completed, error
);

-- 2. æ›´æ–°å¤„ç†è¿›åº¦
UPDATE processing_status
SET
    processed_rows = (
        SELECT COUNT(*) FROM documents WHERE embedding IS NOT NULL
    ),
    last_processed_at = NOW()
WHERE table_name = 'documents';

-- 3. æŸ¥çœ‹å¤„ç†è¿›åº¦
SELECT
    table_name,
    total_rows,
    processed_rows,
    ROUND(processed_rows::float / total_rows * 100, 2) AS progress_pct,
    last_processed_at
FROM processing_status;
```

### 6.3 å¼‚å¸¸å‘Šè­¦

**å¼‚å¸¸æ£€æµ‹ä¸å‘Šè­¦**ï¼š

```sql
-- 1. æ£€æµ‹å¼‚å¸¸æ•°æ®
CREATE OR REPLACE FUNCTION check_data_quality()
RETURNS TABLE(issue TEXT, count BIGINT) AS $$
BEGIN
    RETURN QUERY
    SELECT 'Missing embeddings'::TEXT, COUNT(*)
    FROM documents
    WHERE embedding IS NULL
    UNION ALL
    SELECT 'Wrong dimension'::TEXT, COUNT(*)
    FROM documents
    WHERE array_length(embedding::float[], 1) != 1536
    UNION ALL
    SELECT 'Duplicate content'::TEXT, COUNT(*)
    FROM (
        SELECT content, COUNT(*)
        FROM documents
        GROUP BY content
        HAVING COUNT(*) > 1
    ) AS dupes;
END;
$$ LANGUAGE plpgsql;

-- 2. å®šæ—¶æ£€æŸ¥ï¼ˆä½¿ç”¨pg_cronï¼‰
SELECT cron.schedule(
    'data-quality-check',
    '0 * * * *',  -- æ¯å°æ—¶
    $$
    DO $$
    DECLARE
        issue_count INT;
    BEGIN
        SELECT COUNT(*) INTO issue_count FROM check_data_quality();
        IF issue_count > 0 THEN
            RAISE WARNING 'Data quality issues detected: %', issue_count;
        END IF;
    END;
    $$;
    $$
);
```

---

**æœ€åæ›´æ–°**: 2025å¹´1æœˆ
**ç»´æŠ¤è€…**: PostgreSQL Modern Team
**æ–‡æ¡£ç¼–å·**: AI-03-06
