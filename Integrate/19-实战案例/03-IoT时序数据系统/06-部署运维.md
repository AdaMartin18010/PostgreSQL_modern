---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `DataBaseTheory\19-åœºæ™¯æ¡ˆä¾‹åº“\03-IoTæ—¶åºæ•°æ®ç³»ç»Ÿ\06-éƒ¨ç½²è¿ç»´.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# æ¡ˆä¾‹3ï¼šIoTæ—¶åºæ•°æ®ç³»ç»Ÿ - éƒ¨ç½²è¿ç»´

## ğŸ“‹ ç›®å½•

- [æ¡ˆä¾‹3ï¼šIoTæ—¶åºæ•°æ®ç³»ç»Ÿ - éƒ¨ç½²è¿ç»´](#æ¡ˆä¾‹3iotæ—¶åºæ•°æ®ç³»ç»Ÿ---éƒ¨ç½²è¿ç»´)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [å…ƒæ•°æ®](#å…ƒæ•°æ®)
  - [1. éƒ¨ç½²é…ç½®](#1-éƒ¨ç½²é…ç½®)
    - [1.1 Docker Compose](#11-docker-compose)
  - [2. PostgreSQLä¼˜åŒ–é…ç½®](#2-postgresqlä¼˜åŒ–é…ç½®)
    - [2.1 IoTæ—¶åºä¼˜åŒ–](#21-iotæ—¶åºä¼˜åŒ–)
  - [3. æ•°æ®ä¿ç•™ç­–ç•¥](#3-æ•°æ®ä¿ç•™ç­–ç•¥)
    - [3.1 è‡ªåŠ¨åˆ é™¤æ—§æ•°æ®](#31-è‡ªåŠ¨åˆ é™¤æ—§æ•°æ®)
  - [4. ç›‘æ§å‘Šè­¦](#4-ç›‘æ§å‘Šè­¦)
    - [4.1 å…³é”®ç›‘æ§æŒ‡æ ‡](#41-å…³é”®ç›‘æ§æŒ‡æ ‡)
  - [5. è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬](#5-è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬)
    - [5.1 åˆ†åŒºç®¡ç†è‡ªåŠ¨åŒ–](#51-åˆ†åŒºç®¡ç†è‡ªåŠ¨åŒ–)
    - [5.2 æ•°æ®å‹ç¼©è‡ªåŠ¨åŒ–](#52-æ•°æ®å‹ç¼©è‡ªåŠ¨åŒ–)
  - [6. ç›‘æ§å‘Šè­¦é…ç½®](#6-ç›‘æ§å‘Šè­¦é…ç½®)
    - [6.1 Prometheusç›‘æ§é…ç½®](#61-prometheusç›‘æ§é…ç½®)
    - [6.2 Grafanaä»ªè¡¨æ¿é…ç½®](#62-grafanaä»ªè¡¨æ¿é…ç½®)
  - [7. æ•…éšœæ¢å¤æµç¨‹](#7-æ•…éšœæ¢å¤æµç¨‹)
    - [7.1 æ•…éšœæ¢å¤è„šæœ¬](#71-æ•…éšœæ¢å¤è„šæœ¬)
  - [8. æ€§èƒ½è°ƒä¼˜å»ºè®®](#8-æ€§èƒ½è°ƒä¼˜å»ºè®®)
    - [8.1 å†™å…¥æ€§èƒ½ä¼˜åŒ–](#81-å†™å…¥æ€§èƒ½ä¼˜åŒ–)
    - [8.2 æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–](#82-æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–)

## å…ƒæ•°æ®

- **åˆ›å»ºæ—¥æœŸ**: 2025-12-04
- **éƒ¨ç½²æ–¹å¼**: Docker + TimescaleDB
- **ç›‘æ§**: Prometheus + Grafana

---

## 1. éƒ¨ç½²é…ç½®

### 1.1 Docker Compose

```yaml
version: '3.8'

services:
  timescaledb:
    image: timescale/timescaledb:latest-pg18
    container_name: iot-timescaledb
    environment:
      POSTGRES_DB: iot_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - timescale-data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    command:
      - "postgres"
      - "-c"
      - "shared_buffers=2GB"
      - "-c"
      - "work_mem=128MB"
      - "-c"
      - "maintenance_work_mem=512MB"
      - "-c"
      - "max_connections=500"
      - "-c"
      - "effective_cache_size=6GB"

  collector:
    build: ./collector
    container_name: iot-collector
    depends_on:
      - timescaledb
    environment:
      DB_HOST: timescaledb
      DB_NAME: iot_db

  api:
    build: ./api
    container_name: iot-api
    depends_on:
      - timescaledb
    ports:
      - "8001:8001"

volumes:
  timescale-data:
```

---

## 2. PostgreSQLä¼˜åŒ–é…ç½®

### 2.1 IoTæ—¶åºä¼˜åŒ–

```ini
# postgresql.conf - IoTæ—¶åºä¼˜åŒ–

# å†…å­˜é…ç½®ï¼ˆå†™ä¼˜åŒ–ï¼‰
shared_buffers = 2GB
work_mem = 128MB
maintenance_work_mem = 512MB

# WALé…ç½®ï¼ˆé«˜é¢‘å†™å…¥ï¼‰
wal_buffers = 16MB
max_wal_size = 4GB
checkpoint_timeout = 15min
checkpoint_completion_target = 0.9

# å¼‚æ­¥I/O (PostgreSQL 18)
io_direct = data
io_combine_limit = 128kB

# è¿æ¥
max_connections = 500

# ç»Ÿè®¡
track_io_timing = on
```

---

## 3. æ•°æ®ä¿ç•™ç­–ç•¥

### 3.1 è‡ªåŠ¨åˆ é™¤æ—§æ•°æ®

```sql
-- TimescaleDBæ•°æ®ä¿ç•™ç­–ç•¥
SELECT add_retention_policy('iot_data', INTERVAL '365 days');

-- æ‰‹åŠ¨åˆ é™¤ï¼ˆå¦‚æœä¸ä½¿ç”¨TimescaleDBï¼‰
DELETE FROM iot_data
WHERE timestamp < NOW() - INTERVAL '365 days';
```

---

## 4. ç›‘æ§å‘Šè­¦

### 4.1 å…³é”®ç›‘æ§æŒ‡æ ‡

```text
å†™å…¥æ€§èƒ½:
â”œâ”€ å†™å…¥ååé‡ (points/ç§’)
â”œâ”€ å†™å…¥å»¶è¿Ÿ
â””â”€ æ‰¹æ¬¡å¤§å°

æŸ¥è¯¢æ€§èƒ½:
â”œâ”€ æŸ¥è¯¢å»¶è¿Ÿ
â”œâ”€ ç¼“å­˜å‘½ä¸­ç‡
â””â”€ å¹¶å‘æŸ¥è¯¢æ•°

å­˜å‚¨:
â”œâ”€ æ•°æ®åº“å¤§å°
â”œâ”€ å‹ç¼©ç‡
â”œâ”€ ç£ç›˜ä½¿ç”¨ç‡
â””â”€ åˆ†åŒºæ•°é‡

ç³»ç»Ÿ:
â”œâ”€ CPUä½¿ç”¨ç‡
â”œâ”€ å†…å­˜ä½¿ç”¨ç‡
â””â”€ ç£ç›˜I/O
```

---

---

## 5. è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬

### 5.1 åˆ†åŒºç®¡ç†è‡ªåŠ¨åŒ–

**åˆ†åŒºç®¡ç†è‡ªåŠ¨åŒ–è„šæœ¬ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```bash
#!/bin/bash
# è‡ªåŠ¨åˆ†åŒºç®¡ç†è„šæœ¬

# é…ç½®
DB_NAME="iot_db"
DB_USER="postgres"
RETENTION_DAYS=365
PARTITION_PREFIX="sensor_data"

# åˆ›å»ºæœªæ¥åˆ†åŒº
psql -d $DB_NAME -U $DB_USER <<EOF
-- åˆ›å»ºæœªæ¥30å¤©çš„åˆ†åŒº
SELECT create_daily_partitions('$PARTITION_PREFIX', 30);
EOF

# æ¸…ç†æ—§åˆ†åŒº
psql -d $DB_NAME -U $DB_USER <<EOF
-- æ¸…ç†è¶…è¿‡ä¿ç•™æœŸçš„åˆ†åŒº
SELECT cleanup_old_partitions('$PARTITION_PREFIX', $RETENTION_DAYS);
EOF

echo "âœ… åˆ†åŒºç®¡ç†å®Œæˆ"
```

### 5.2 æ•°æ®å‹ç¼©è‡ªåŠ¨åŒ–

**æ•°æ®å‹ç¼©è‡ªåŠ¨åŒ–è„šæœ¬ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```sql
-- æ•°æ®å‹ç¼©è‡ªåŠ¨åŒ–å‡½æ•°
CREATE OR REPLACE FUNCTION auto_compress_old_data(
    p_hypertable_name TEXT,
    p_compress_after_days INT DEFAULT 7
)
RETURNS TABLE (
    chunks_compressed INT,
    total_size_before TEXT,
    total_size_after TEXT,
    compression_ratio NUMERIC
) AS $$
DECLARE
    compressed_count INT := 0;
    size_before BIGINT := 0;
    size_after BIGINT := 0;
    chunk_record RECORD;
BEGIN
    -- å‹ç¼©æ—§chunks
    FOR chunk_record IN
        SELECT c.chunk_schema || '.' || c.chunk_name AS chunk_full_name,
               c.total_bytes
        FROM timescaledb_information.chunks c
        WHERE c.hypertable_name = p_hypertable_name
          AND c.range_end < NOW() - (p_compress_after_days || ' days')::INTERVAL
          AND NOT c.is_compressed
    LOOP
        -- è®°å½•å‹ç¼©å‰å¤§å°
        size_before := size_before + chunk_record.total_bytes;

        -- å‹ç¼©chunk
        PERFORM compress_chunk(chunk_record.chunk_full_name);

        compressed_count := compressed_count + 1;
    END LOOP;

    -- è®¡ç®—å‹ç¼©åå¤§å°
    SELECT SUM(total_bytes)
    INTO size_after
    FROM timescaledb_information.chunks
    WHERE hypertable_name = p_hypertable_name
      AND is_compressed;

    RETURN QUERY SELECT
        compressed_count,
        pg_size_pretty(size_before),
        pg_size_pretty(size_after),
        CASE
            WHEN size_before > 0 THEN ROUND(size_before::NUMERIC / NULLIF(size_after, 0), 2)
            ELSE 0
        END;

    RETURN;

EXCEPTION
    WHEN OTHERS THEN
        RAISE EXCEPTION 'è‡ªåŠ¨å‹ç¼©å¤±è´¥: %', SQLERRM;
END;
$$ LANGUAGE plpgsql;
```

---

## 6. ç›‘æ§å‘Šè­¦é…ç½®

### 6.1 Prometheusç›‘æ§é…ç½®

**Prometheusç›‘æ§é…ç½®ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'postgresql'
    static_configs:
      - targets: ['postgres-exporter:9187']

  - job_name: 'iot-collector'
    static_configs:
      - targets: ['collector:8000']

  - job_name: 'iot-api'
    static_configs:
      - targets: ['api:8001']
```

### 6.2 Grafanaä»ªè¡¨æ¿é…ç½®

**Grafanaä»ªè¡¨æ¿é…ç½®ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```json
{
  "dashboard": {
    "title": "IoTæ—¶åºæ•°æ®ç³»ç»Ÿç›‘æ§",
    "panels": [
      {
        "title": "å†™å…¥ååé‡",
        "targets": [
          {
            "expr": "rate(sensor_data_points_total[5m])",
            "legendFormat": "points/sec"
          }
        ]
      },
      {
        "title": "æŸ¥è¯¢å»¶è¿Ÿ",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(pg_stat_statements_mean_exec_time_bucket[5m]))",
            "legendFormat": "P95 latency"
          }
        ]
      },
      {
        "title": "æ•°æ®åº“å¤§å°",
        "targets": [
          {
            "expr": "pg_database_size_bytes",
            "legendFormat": "Database size"
          }
        ]
      }
    ]
  }
}
```

---

## 7. æ•…éšœæ¢å¤æµç¨‹

### 7.1 æ•…éšœæ¢å¤è„šæœ¬

**æ•…éšœæ¢å¤è„šæœ¬ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```bash
#!/bin/bash
# æ•…éšœæ¢å¤è„šæœ¬

set -e

DB_NAME="iot_db"
DB_USER="postgres"
BACKUP_DIR="/backup"

echo "å¼€å§‹æ•…éšœæ¢å¤..."

# 1. æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
if ! pg_isready -U $DB_USER -d $DB_NAME; then
    echo "æ•°æ®åº“æœªå°±ç»ªï¼Œå°è¯•å¯åŠ¨..."
    pg_ctl start -D /var/lib/postgresql/data
    sleep 5
fi

# 2. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
psql -d $DB_NAME -U $DB_USER <<EOF
SELECT verify_data_integrity();
EOF

# 3. æ£€æŸ¥åˆ†åŒºçŠ¶æ€
psql -d $DB_NAME -U $DB_USER <<EOF
SELECT * FROM timescaledb_information.chunks
WHERE hypertable_name = 'sensor_data'
ORDER BY range_start DESC
LIMIT 10;
EOF

# 4. æ£€æŸ¥å‹ç¼©çŠ¶æ€
psql -d $DB_NAME -U $DB_USER <<EOF
SELECT
    COUNT(*) FILTER (WHERE is_compressed) AS compressed_chunks,
    COUNT(*) FILTER (WHERE NOT is_compressed) AS uncompressed_chunks
FROM timescaledb_information.chunks
WHERE hypertable_name = 'sensor_data';
EOF

echo "âœ… æ•…éšœæ¢å¤æ£€æŸ¥å®Œæˆ"
```

---

## 8. æ€§èƒ½è°ƒä¼˜å»ºè®®

### 8.1 å†™å…¥æ€§èƒ½ä¼˜åŒ–

**å†™å…¥æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```sql
-- 1. æ‰¹é‡å†™å…¥ï¼ˆä½¿ç”¨COPYï¼‰
COPY sensor_data (device_id, metric_id, timestamp, value, quality)
FROM STDIN WITH (FORMAT csv);

-- 2. ä½¿ç”¨UNLOGGEDè¡¨ï¼ˆä¸´æ—¶æ•°æ®ï¼‰
CREATE UNLOGGED TABLE sensor_data_temp (LIKE sensor_data);
-- æ‰¹é‡æ’å…¥åï¼Œå†COPYåˆ°æ­£å¼è¡¨

-- 3. è°ƒæ•´WALé…ç½®ï¼ˆé«˜é¢‘å†™å…¥ï¼‰
ALTER SYSTEM SET wal_buffers = '16MB';
ALTER SYSTEM SET max_wal_size = '4GB';
ALTER SYSTEM SET checkpoint_timeout = '15min';
ALTER SYSTEM SET checkpoint_completion_target = 0.9;

-- 4. PostgreSQL 18å¼‚æ­¥I/O
ALTER SYSTEM SET io_direct = 'data,wal';
ALTER SYSTEM SET effective_io_concurrency = 200;
```

### 8.2 æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–

**æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```sql
-- 1. ä½¿ç”¨BRINç´¢å¼•ï¼ˆæ—¶åºæ•°æ®ï¼‰
CREATE INDEX idx_sensor_data_time_brin
ON sensor_data USING BRIN (timestamp)
WITH (pages_per_range = 128);

-- 2. ä½¿ç”¨è¿ç»­èšåˆï¼ˆé¢„è®¡ç®—ï¼‰
CREATE MATERIALIZED VIEW sensor_data_hourly
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 hour', timestamp) AS hour,
    device_id,
    metric_id,
    AVG(value) AS avg_value,
    MAX(value) AS max_value,
    MIN(value) AS min_value
FROM sensor_data
GROUP BY hour, device_id, metric_id;

-- 3. åˆ†åŒºè£å‰ªä¼˜åŒ–
-- ç¡®ä¿æŸ¥è¯¢æ¡ä»¶åŒ…å«æ—¶é—´èŒƒå›´
SELECT * FROM sensor_data
WHERE timestamp BETWEEN $1 AND $2  -- å¿…é¡»åŒ…å«æ—¶é—´èŒƒå›´
  AND device_id = $3;
```

---

**å®Œæˆæ—¥æœŸ**: 2025-12-04
**éƒ¨ç½²æ–¹å¼**: Docker Compose + TimescaleDB
**ç›‘æ§**: Prometheus + Grafana

**è¿”å›**: [æ¡ˆä¾‹3ä¸»é¡µ](./README.md)
