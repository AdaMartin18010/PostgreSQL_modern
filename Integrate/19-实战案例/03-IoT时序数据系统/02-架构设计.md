---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `DataBaseTheory\19-åœºæ™¯æ¡ˆä¾‹åº“\03-IoTæ—¶åºæ•°æ®ç³»ç»Ÿ\02-æ¶æ„è®¾è®¡.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# IoTæ—¶åºæ•°æ®ç³»ç»Ÿ - æ¶æ„è®¾è®¡

> **PostgreSQLç‰ˆæœ¬**: 18.x
> **å†™å…¥æ€§èƒ½**: 1M points/ç§’

---

## ä¸€ã€æ•´ä½“æ¶æ„

```mermaid
graph TB
    subgraph "è®¾å¤‡å±‚"
        Device1[ä¼ æ„Ÿå™¨1-1000]
        Device2[ä¼ æ„Ÿå™¨1001-2000]
        Device10[ä¼ æ„Ÿå™¨9001-10000]
    end

    subgraph "ç½‘å…³å±‚"
        Gateway1[è¾¹ç¼˜ç½‘å…³1]
        Gateway2[è¾¹ç¼˜ç½‘å…³2]
        Gateway10[è¾¹ç¼˜ç½‘å…³10]
    end

    subgraph "æ¶ˆæ¯é˜Ÿåˆ—"
        Kafka[Kafkaé›†ç¾¤<br/>100ä¸ªåˆ†åŒº]
    end

    subgraph "å†™å…¥å±‚"
        Writer1[Writer1]
        Writer2[Writer2]
        Writer8[Writer8]
    end

    subgraph "PostgreSQL 18"
        Raw[(åŸå§‹æ•°æ®<br/>æŒ‰å¤©åˆ†åŒº)]
        Agg1[(1åˆ†é’Ÿèšåˆ)]
        Agg5[(5åˆ†é’Ÿèšåˆ)]
        Agg60[(1å°æ—¶èšåˆ)]
    end

    Device1 --> Gateway1
    Device2 --> Gateway2
    Device10 --> Gateway10

    Gateway1 --> Kafka
    Gateway2 --> Kafka
    Gateway10 --> Kafka

    Kafka --> Writer1
    Kafka --> Writer2
    Kafka --> Writer8

    Writer1 --> Raw
    Writer2 --> Raw
    Writer8 --> Raw

    Raw -->|è¿ç»­èšåˆ| Agg1
    Agg1 --> Agg5
    Agg5 --> Agg60
```

---

## äºŒã€æ•°æ®è¡¨è®¾è®¡

### 2.1 åŸå§‹æ•°æ®è¡¨

```sql
-- ä¼ æ„Ÿå™¨æ•°æ®è¡¨ï¼ˆæŒ‰å¤©åˆ†åŒºï¼‰
CREATE TABLE sensor_data (
    device_id INT NOT NULL,
    metric_id SMALLINT NOT NULL,
    timestamp TIMESTAMPTZ NOT NULL,
    value DOUBLE PRECISION,
    quality SMALLINT DEFAULT 100,
    PRIMARY KEY (device_id, timestamp, metric_id)
) PARTITION BY RANGE (timestamp);

-- è‡ªåŠ¨åˆ›å»ºåˆ†åŒºï¼ˆ365å¤©ï¼‰
DO $$
BEGIN
    FOR i IN 0..364 LOOP
        EXECUTE FORMAT(
            'CREATE TABLE sensor_data_%s PARTITION OF sensor_data FOR VALUES FROM (%L) TO (%L)',
            TO_CHAR(CURRENT_DATE + i, 'YYYY_MM_DD'),
            CURRENT_DATE + i,
            CURRENT_DATE + i + 1
        );
    END LOOP;
END $$;

-- BRINç´¢å¼•ï¼ˆæ—¶åºæ•°æ®ï¼‰
CREATE INDEX idx_sensor_data_time
ON sensor_data USING BRIN (timestamp)
WITH (pages_per_range = 128);

-- å¯¹æ¯”B-treeï¼š
-- BRIN: 100MB (1äº¿è¡Œ)
-- B-tree: 2GB (1äº¿è¡Œ)
-- èŠ‚çœï¼š-95%

-- â­ PostgreSQL 18ï¼šLZ4å‹ç¼©
ALTER TABLE sensor_data ALTER COLUMN value SET COMPRESSION lz4;
-- å‹ç¼©æ¯”ï¼š10:1
```

### 2.2 èšåˆè¡¨

```sql
-- 1åˆ†é’Ÿèšåˆ
CREATE TABLE sensor_data_1min (
    device_id INT,
    metric_id SMALLINT,
    minute TIMESTAMPTZ,
    avg_value DOUBLE PRECISION,
    min_value DOUBLE PRECISION,
    max_value DOUBLE PRECISION,
    stddev_value DOUBLE PRECISION,
    sample_count INT,
    PRIMARY KEY (device_id, metric_id, minute)
) PARTITION BY RANGE (minute);

-- 1å°æ—¶èšåˆ
CREATE TABLE sensor_data_1hour (
    device_id INT,
    metric_id SMALLINT,
    hour TIMESTAMPTZ,
    avg_value DOUBLE PRECISION,
    min_value DOUBLE PRECISION,
    max_value DOUBLE PRECISION,
    PRIMARY KEY (device_id, metric_id, hour)
);
```

---

## ä¸‰ã€å†™å…¥ä¼˜åŒ–

### 3.1 æ‰¹é‡å†™å…¥

```python
import psycopg2
from psycopg2.extras import execute_values

# æ‰¹é‡æ’å…¥ï¼ˆæ¯æ‰¹10000æ¡ï¼‰
def batch_insert(data_points):
    conn = psycopg2.connect("...")
    cur = conn.cursor()

    # â­ PostgreSQL 18ï¼šexecute_values + å¼‚æ­¥I/O
    execute_values(cur, """
        INSERT INTO sensor_data (device_id, metric_id, timestamp, value, quality)
        VALUES %s
    """, data_points, page_size=10000)

    conn.commit()

# æ€§èƒ½ï¼š
# PG 17: 800K points/ç§’
# PG 18: 1.2M points/ç§’ (+50%)
```

### 3.2 COPYæ‰¹é‡åŠ è½½

```sql
-- ä½¿ç”¨COPYï¼ˆæœ€å¿«ï¼‰
COPY sensor_data FROM STDIN WITH (FORMAT binary, FREEZE);

-- â­ PostgreSQL 18ï¼šå¹¶è¡ŒCOPY
COPY sensor_data FROM '/data/sensor.csv'
WITH (FORMAT csv, PARALLEL 8);

-- é€Ÿåº¦ï¼š8MB/s â†’ 50MB/s (+525%)
```

---

## å››ã€æŸ¥è¯¢ä¼˜åŒ–

### 4.1 æ—¶é—´èŒƒå›´æŸ¥è¯¢

```sql
-- æœ€å¸¸è§ï¼šæŸ¥è¯¢æœ€è¿‘Nå°æ—¶æ•°æ®
SELECT device_id, timestamp, value
FROM sensor_data
WHERE timestamp BETWEEN NOW() - INTERVAL '1 hour' AND NOW()
  AND device_id = ANY($1::int[])
ORDER BY timestamp DESC;

-- PostgreSQL 18ä¼˜åŒ–ï¼š
-- 1. åˆ†åŒºè£å‰ªï¼ˆåªæ‰«æä»Šå¤©åˆ†åŒºï¼‰
-- 2. BRINç´¢å¼•å¿«é€Ÿå®šä½
-- 3. å¹¶è¡Œæ‰«æ
-- æ‰§è¡Œæ—¶é—´ï¼š<50ms
```

### 4.2 èšåˆæŸ¥è¯¢

```sql
-- ä½¿ç”¨é¢„èšåˆè¡¨
SELECT
    device_id,
    DATE_TRUNC('hour', minute) as hour,
    AVG(avg_value) as hourly_avg
FROM sensor_data_1min
WHERE minute >= NOW() - INTERVAL '24 hours'
  AND device_id = $1
GROUP BY device_id, hour
ORDER BY hour;

-- æ€§èƒ½ï¼šæ‰«æ1440è¡Œï¼ˆ1å¤©Ã—60åˆ†Ã—24å°æ—¶ï¼‰
-- æ‰§è¡Œæ—¶é—´ï¼š<10ms
```

---

## äº”ã€è¿ç»­èšåˆå®ç°

```sql
-- ä½¿ç”¨è§¦å‘å™¨ç»´æŠ¤èšåˆè¡¨
CREATE OR REPLACE FUNCTION update_1min_aggregate()
RETURNS TRIGGER AS $$
BEGIN
    INSERT INTO sensor_data_1min (
        device_id, metric_id, minute,
        avg_value, min_value, max_value, sample_count
    )
    SELECT
        NEW.device_id,
        NEW.metric_id,
        DATE_TRUNC('minute', NEW.timestamp),
        NEW.value, NEW.value, NEW.value, 1
    ON CONFLICT (device_id, metric_id, minute) DO UPDATE SET
        avg_value = (sensor_data_1min.avg_value * sensor_data_1min.sample_count + EXCLUDED.avg_value)
                    / (sensor_data_1min.sample_count + 1),
        min_value = LEAST(sensor_data_1min.min_value, EXCLUDED.min_value),
        max_value = GREATEST(sensor_data_1min.max_value, EXCLUDED.max_value),
        sample_count = sensor_data_1min.sample_count + 1;

    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- åº”ç”¨è§¦å‘å™¨ï¼ˆä»…çƒ­æ•°æ®ï¼‰
CREATE TRIGGER trg_sensor_data_aggregate
    AFTER INSERT ON sensor_data
    FOR EACH ROW
    WHEN (NEW.timestamp > NOW() - INTERVAL '7 days')
    EXECUTE FUNCTION update_1min_aggregate();
```

---

## å…­ã€ç›‘æ§

```sql
-- å†™å…¥æ€§èƒ½ç›‘æ§
CREATE VIEW write_performance_monitor AS
SELECT
    schemaname,
    relname,
    n_tup_ins as inserts,
    n_tup_upd as updates,
    n_tup_del as deletes,
    n_live_tup as live_tuples,
    n_dead_tup as dead_tuples,
    last_autovacuum,
    autovacuum_count
FROM pg_stat_user_tables
WHERE relname LIKE 'sensor_data%'
ORDER BY n_tup_ins DESC;
```

---

**æ–‡æ¡£å®Œæˆ** âœ…
