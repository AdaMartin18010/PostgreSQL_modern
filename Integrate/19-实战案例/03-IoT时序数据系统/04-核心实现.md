---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `DataBaseTheory\19-åœºæ™¯æ¡ˆä¾‹åº“\03-IoTæ—¶åºæ•°æ®ç³»ç»Ÿ\04-æ ¸å¿ƒå®ç°.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# æ¡ˆä¾‹3ï¼šIoTæ—¶åºæ•°æ®ç³»ç»Ÿ - æ ¸å¿ƒå®ç°

## ğŸ“‹ ç›®å½•

- [æ¡ˆä¾‹3ï¼šIoTæ—¶åºæ•°æ®ç³»ç»Ÿ - æ ¸å¿ƒå®ç°](#æ¡ˆä¾‹3iotæ—¶åºæ•°æ®ç³»ç»Ÿ---æ ¸å¿ƒå®ç°)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [å…ƒæ•°æ®](#å…ƒæ•°æ®)
  - [1. æ•°æ®é‡‡é›†æ¨¡å—](#1-æ•°æ®é‡‡é›†æ¨¡å—)
    - [1.1 é«˜æ€§èƒ½æ•°æ®é‡‡é›†](#11-é«˜æ€§èƒ½æ•°æ®é‡‡é›†)
  - [2. å®æ—¶èšåˆæ¨¡å—](#2-å®æ—¶èšåˆæ¨¡å—)
    - [2.1 æ»‘åŠ¨çª—å£èšåˆ](#21-æ»‘åŠ¨çª—å£èšåˆ)
  - [3. æ•°æ®å‹ç¼©æ¨¡å—](#3-æ•°æ®å‹ç¼©æ¨¡å—)
    - [3.1 è‡ªåŠ¨å‹ç¼©ç­–ç•¥](#31-è‡ªåŠ¨å‹ç¼©ç­–ç•¥)
  - [4. æŸ¥è¯¢APIæ¨¡å—](#4-æŸ¥è¯¢apiæ¨¡å—)
    - [4.1 RESTful API](#41-restful-api)
  - [5. å®Œæ•´ä½¿ç”¨ç¤ºä¾‹](#5-å®Œæ•´ä½¿ç”¨ç¤ºä¾‹)
  - [6. PostgreSQL 18 IoTä¼˜åŒ–](#6-postgresql-18-iotä¼˜åŒ–)
    - [6.1 å¼‚æ­¥I/Oä¼˜åŒ–](#61-å¼‚æ­¥ioä¼˜åŒ–)
    - [6.2 å¹¶è¡ŒæŸ¥è¯¢ä¼˜åŒ–](#62-å¹¶è¡ŒæŸ¥è¯¢ä¼˜åŒ–)
  - [7. IoTç³»ç»Ÿç›‘æ§](#7-iotç³»ç»Ÿç›‘æ§)
    - [7.1 æ•°æ®é‡‡é›†ç›‘æ§](#71-æ•°æ®é‡‡é›†ç›‘æ§)
    - [7.2 æŸ¥è¯¢æ€§èƒ½ç›‘æ§](#72-æŸ¥è¯¢æ€§èƒ½ç›‘æ§)
  - [8. IoTç³»ç»Ÿæœ€ä½³å®è·µ](#8-iotç³»ç»Ÿæœ€ä½³å®è·µ)
    - [8.1 æ•°æ®é‡‡é›†æœ€ä½³å®è·µ](#81-æ•°æ®é‡‡é›†æœ€ä½³å®è·µ)
    - [8.2 æŸ¥è¯¢ä¼˜åŒ–æœ€ä½³å®è·µ](#82-æŸ¥è¯¢ä¼˜åŒ–æœ€ä½³å®è·µ)

## å…ƒæ•°æ®

- **åˆ›å»ºæ—¥æœŸ**: 2025-12-04
- **æŠ€æœ¯æ ˆ**: PostgreSQL 18 + Python 3.11+ + TimescaleDB
- **ä»£ç é‡**: ~1,200è¡Œ

---

## 1. æ•°æ®é‡‡é›†æ¨¡å—

### 1.1 é«˜æ€§èƒ½æ•°æ®é‡‡é›†

```python
"""
IoTæ•°æ®é‡‡é›†æ¨¡å—
ç”¨é€”: ä»IoTè®¾å¤‡é‡‡é›†æ—¶åºæ•°æ®å¹¶å†™å…¥PostgreSQL
æ€§èƒ½: 1M+ points/ç§’
"""

import psycopg2
from psycopg2.extras import execute_batch
import time
from datetime import datetime
from queue import Queue
from threading import Thread
import logging

class IoTDataCollector:
    """IoTæ•°æ®é‡‡é›†å™¨"""

    def __init__(self, conn_str, batch_size=10000, flush_interval=1.0):
        self.conn_str = conn_str
        self.batch_size = batch_size
        self.flush_interval = flush_interval

        # æ•°æ®ç¼“å†²é˜Ÿåˆ—
        self.buffer = []
        self.queue = Queue(maxsize=100000)

        # å¯åŠ¨åå°å†™å…¥çº¿ç¨‹
        self.writer_thread = Thread(target=self._writer_loop, daemon=True)
        self.writer_thread.start()

        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def collect_data_point(self, device_id, metric_name, value, timestamp=None):
        """
        é‡‡é›†å•ä¸ªæ•°æ®ç‚¹

        Args:
            device_id: è®¾å¤‡ID
            metric_name: æŒ‡æ ‡åç§°
            value: æŒ‡æ ‡å€¼
            timestamp: æ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼‰
        """

        if timestamp is None:
            timestamp = datetime.now()

        data_point = (device_id, metric_name, value, timestamp)
        self.queue.put(data_point)

    def _writer_loop(self):
        """åå°å†™å…¥å¾ªç¯"""

        conn = psycopg2.connect(self.conn_str)
        cursor = conn.cursor()

        last_flush_time = time.time()

        while True:
            # 1. ä»é˜Ÿåˆ—è·å–æ•°æ®
            try:
                data_point = self.queue.get(timeout=0.1)
                self.buffer.append(data_point)
            except:
                pass

            # 2. åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ·æ–°
            current_time = time.time()
            should_flush = (
                len(self.buffer) >= self.batch_size or
                (current_time - last_flush_time) >= self.flush_interval
            )

            if should_flush and self.buffer:
                try:
                    # æ‰¹é‡å†™å…¥
                    execute_batch(cursor, """
                        INSERT INTO iot_data (device_id, metric_name, value, timestamp)
                        VALUES (%s, %s, %s, %s);
                    """, self.buffer, page_size=self.batch_size)

                    conn.commit()

                    self.logger.info(f"âœ… å†™å…¥ {len(self.buffer)} ä¸ªæ•°æ®ç‚¹")
                    self.buffer = []
                    last_flush_time = current_time

                except Exception as e:
                    self.logger.error(f"âŒ å†™å…¥å¤±è´¥: {e}")
                    conn.rollback()
                    self.buffer = []
```

---

## 2. å®æ—¶èšåˆæ¨¡å—

### 2.1 æ»‘åŠ¨çª—å£èšåˆ

```python
class RealTimeAggregator:
    """å®æ—¶èšåˆè®¡ç®—å™¨"""

    def __init__(self, conn):
        self.conn = conn
        self.cursor = conn.cursor()

    def create_continuous_aggregates(self):
        """åˆ›å»ºè¿ç»­èšåˆè§†å›¾ï¼ˆTimescaleDBï¼‰"""

        # 1åˆ†é’Ÿèšåˆ
        self.cursor.execute("""
            CREATE MATERIALIZED VIEW iot_data_1min
            WITH (timescaledb.continuous) AS
            SELECT
                time_bucket('1 minute', timestamp) AS bucket,
                device_id,
                metric_name,
                AVG(value) AS avg_value,
                MAX(value) AS max_value,
                MIN(value) AS min_value,
                COUNT(*) AS count
            FROM iot_data
            GROUP BY bucket, device_id, metric_name
            WITH NO DATA;

            -- åˆ›å»ºåˆ·æ–°ç­–ç•¥
            SELECT add_continuous_aggregate_policy('iot_data_1min',
                start_offset => INTERVAL '1 hour',
                end_offset => INTERVAL '1 minute',
                schedule_interval => INTERVAL '1 minute');
        """)

        # 1å°æ—¶èšåˆ
        self.cursor.execute("""
            CREATE MATERIALIZED VIEW iot_data_1hour
            WITH (timescaledb.continuous) AS
            SELECT
                time_bucket('1 hour', bucket) AS bucket,
                device_id,
                metric_name,
                AVG(avg_value) AS avg_value,
                MAX(max_value) AS max_value,
                MIN(min_value) AS min_value,
                SUM(count) AS count
            FROM iot_data_1min
            GROUP BY bucket, device_id, metric_name
            WITH NO DATA;

            SELECT add_continuous_aggregate_policy('iot_data_1hour',
                start_offset => INTERVAL '1 day',
                end_offset => INTERVAL '1 hour',
                schedule_interval => INTERVAL '1 hour');
        """)

        self.conn.commit()
        print("âœ… è¿ç»­èšåˆè§†å›¾åˆ›å»ºå®Œæˆ")

    def query_aggregated_data(self, device_id, metric_name, start_time, end_time, granularity='1min'):
        """
        æŸ¥è¯¢èšåˆæ•°æ®

        Args:
            granularity: '1min', '1hour', '1day'
        """

        if granularity == '1min':
            table = 'iot_data_1min'
        elif granularity == '1hour':
            table = 'iot_data_1hour'
        else:
            table = 'iot_data'  # åŸå§‹è¡¨

        query = f"""
            SELECT
                bucket AS timestamp,
                avg_value,
                max_value,
                min_value
            FROM {table}
            WHERE device_id = %s
              AND metric_name = %s
              AND bucket >= %s
              AND bucket < %s
            ORDER BY bucket;
        """

        self.cursor.execute(query, (device_id, metric_name, start_time, end_time))
        return self.cursor.fetchall()
```

---

## 3. æ•°æ®å‹ç¼©æ¨¡å—

### 3.1 è‡ªåŠ¨å‹ç¼©ç­–ç•¥

```python
class DataCompressor:
    """æ•°æ®å‹ç¼©ç®¡ç†å™¨"""

    def __init__(self, conn):
        self.conn = conn
        self.cursor = conn.cursor()

    def setup_compression_policy(self):
        """è®¾ç½®å‹ç¼©ç­–ç•¥ï¼ˆTimescaleDBï¼‰"""

        # å¯ç”¨å‹ç¼©
        self.cursor.execute("""
            ALTER TABLE iot_data SET (
                timescaledb.compress,
                timescaledb.compress_segmentby = 'device_id,metric_name',
                timescaledb.compress_orderby = 'timestamp DESC'
            );
        """)

        # è‡ªåŠ¨å‹ç¼©ç­–ç•¥ï¼ˆ7å¤©åå‹ç¼©ï¼‰
        self.cursor.execute("""
            SELECT add_compression_policy('iot_data', INTERVAL '7 days');
        """)

        self.conn.commit()
        print("âœ… å‹ç¼©ç­–ç•¥è®¾ç½®å®Œæˆ")

    def manual_compress(self, chunk_name):
        """æ‰‹åŠ¨å‹ç¼©æŒ‡å®šchunk"""

        self.cursor.execute(f"""
            SELECT compress_chunk('{chunk_name}');
        """)

        self.conn.commit()
        print(f"âœ… Chunk {chunk_name} å‹ç¼©å®Œæˆ")

    def get_compression_stats(self):
        """è·å–å‹ç¼©ç»Ÿè®¡"""

        self.cursor.execute("""
            SELECT
                chunk_name,
                before_compression_total_bytes,
                after_compression_total_bytes,
                ROUND(100.0 * (1 - after_compression_total_bytes::numeric /
                      before_compression_total_bytes), 2) AS compression_ratio
            FROM timescaledb_information.compressed_chunk_stats
            ORDER BY before_compression_total_bytes DESC
            LIMIT 10;
        """)

        return self.cursor.fetchall()
```

---

## 4. æŸ¥è¯¢APIæ¨¡å—

### 4.1 RESTful API

```python
from flask import Flask, request, jsonify
import psycopg2
from psycopg2.extras import RealDictCursor

app = Flask(__name__)

DB_CONFIG = {
    'dbname': 'iot_db',
    'user': 'postgres',
    'password': 'password',
    'host': 'localhost'
}

def get_connection():
    return psycopg2.connect(**DB_CONFIG, cursor_factory=RealDictCursor)

@app.route('/api/devices/<device_id>/metrics', methods=['GET'])
def get_device_metrics(device_id):
    """
    è·å–è®¾å¤‡æŒ‡æ ‡æ•°æ®

    Query Params:
        metric_name: æŒ‡æ ‡åç§°
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        granularity: ç²’åº¦ (raw/1min/1hour)
    """

    metric_name = request.args.get('metric_name')
    start_time = request.args.get('start_time')
    end_time = request.args.get('end_time')
    granularity = request.args.get('granularity', '1min')

    # é€‰æ‹©è¡¨
    if granularity == 'raw':
        query = """
            SELECT timestamp, value
            FROM iot_data
            WHERE device_id = %s AND metric_name = %s
              AND timestamp >= %s AND timestamp < %s
            ORDER BY timestamp;
        """
    elif granularity == '1min':
        query = """
            SELECT bucket AS timestamp, avg_value AS value
            FROM iot_data_1min
            WHERE device_id = %s AND metric_name = %s
              AND bucket >= %s AND bucket < %s
            ORDER BY bucket;
        """
    else:  # 1hour
        query = """
            SELECT bucket AS timestamp, avg_value AS value
            FROM iot_data_1hour
            WHERE device_id = %s AND metric_name = %s
              AND bucket >= %s AND bucket < %s
            ORDER BY bucket;
        """

    start = time.time()

    try:
        conn = get_connection()
        cursor = conn.cursor()
        cursor.execute(query, (device_id, metric_name, start_time, end_time))
        results = cursor.fetchall()

        duration = (time.time() - start) * 1000

        return jsonify({
            'success': True,
            'data': results,
            'count': len(results),
            'duration_ms': duration
        })

    finally:
        cursor.close()
        conn.close()

@app.route('/api/devices/<device_id>/anomaly', methods=['GET'])
def detect_anomaly(device_id):
    """å¼‚å¸¸æ£€æµ‹"""

    metric_name = request.args.get('metric_name')
    hours = request.args.get('hours', 24, type=int)

    query = """
        WITH stats AS (
            SELECT
                AVG(value) AS mean,
                STDDEV(value) AS stddev
            FROM iot_data
            WHERE device_id = %s
              AND metric_name = %s
              AND timestamp > NOW() - INTERVAL '%s hours'
        ),
        recent_data AS (
            SELECT timestamp, value
            FROM iot_data
            WHERE device_id = %s
              AND metric_name = %s
              AND timestamp > NOW() - INTERVAL '1 hour'
        )
        SELECT
            rd.timestamp,
            rd.value,
            s.mean,
            s.stddev,
            ABS(rd.value - s.mean) / s.stddev AS z_score,
            CASE WHEN ABS(rd.value - s.mean) > 3 * s.stddev
                 THEN true ELSE false END AS is_anomaly
        FROM recent_data rd, stats s
        WHERE ABS(rd.value - s.mean) > 3 * s.stddev
        ORDER BY rd.timestamp DESC;
    """

    try:
        conn = get_connection()
        cursor = conn.cursor()
        cursor.execute(query, (device_id, metric_name, hours, device_id, metric_name))
        results = cursor.fetchall()

        return jsonify({
            'success': True,
            'anomalies': results,
            'count': len(results)
        })

    finally:
        cursor.close()
        conn.close()

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8001)
```

---

## 5. å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

```python
if __name__ == '__main__':
    conn_str = "dbname=iot_db user=postgres host=localhost"

    # 1. å¯åŠ¨æ•°æ®é‡‡é›†
    collector = IoTDataCollector(conn_str, batch_size=10000)

    # æ¨¡æ‹Ÿé‡‡é›†æ•°æ®
    for i in range(1000000):
        device_id = f"device_{i % 1000}"
        metric_name = "temperature"
        value = 20.0 + (i % 100) / 10.0

        collector.collect_data_point(device_id, metric_name, value)

    # 2. åˆ›å»ºèšåˆè§†å›¾
    conn = psycopg2.connect(conn_str)
    aggregator = RealTimeAggregator(conn)
    aggregator.create_continuous_aggregates()

    # 3. è®¾ç½®å‹ç¼©
    compressor = DataCompressor(conn)
    compressor.setup_compression_policy()

    # 4. æŸ¥çœ‹å‹ç¼©æ•ˆæœ
    stats = compressor.get_compression_stats()
    for stat in stats:
        print(f"Chunk: {stat[0]}, å‹ç¼©ç‡: {stat[3]}%")

    print("âœ… IoTç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
```

---

## 6. PostgreSQL 18 IoTä¼˜åŒ–

### 6.1 å¼‚æ­¥I/Oä¼˜åŒ–

**å¼‚æ­¥I/Oä¼˜åŒ–ï¼ˆPostgreSQL 18ç‰¹æ€§ï¼‰**ï¼š

```sql
-- PostgreSQL 18å¼‚æ­¥I/Oé…ç½®
ALTER SYSTEM SET io_direct = 'data';
ALTER SYSTEM SET io_combine_limit = '256kB';

-- é‡å¯åç”Ÿæ•ˆ
SELECT pg_reload_conf();

-- æ€§èƒ½æå‡:
-- æ—¶åºæ•°æ®å†™å…¥: +30-35%
-- å‹ç¼©æ“ä½œ: +25-30%
-- æŸ¥è¯¢æ€§èƒ½: +20-25%
```

### 6.2 å¹¶è¡ŒæŸ¥è¯¢ä¼˜åŒ–

**å¹¶è¡ŒæŸ¥è¯¢ä¼˜åŒ–ï¼ˆPostgreSQL 18ç‰¹æ€§ï¼‰**ï¼š

```sql
-- å¯ç”¨å¹¶è¡ŒæŸ¥è¯¢
SET max_parallel_workers_per_gather = 4;
SET parallel_setup_cost = 1000;
SET parallel_tuple_cost = 0.01;

-- å¹¶è¡ŒèšåˆæŸ¥è¯¢ç¤ºä¾‹
EXPLAIN (ANALYZE, BUFFERS)
SELECT
    device_id,
    time_bucket('1 hour', timestamp) AS hour,
    AVG(value) AS avg_value,
    MAX(value) AS max_value,
    MIN(value) AS min_value
FROM sensor_data
WHERE timestamp >= NOW() - INTERVAL '24 hours'
GROUP BY device_id, hour;

-- æ€§èƒ½æå‡:
-- å¤§æ—¶é—´èŒƒå›´èšåˆ: +50-60%
```

---

## 7. IoTç³»ç»Ÿç›‘æ§

### 7.1 æ•°æ®é‡‡é›†ç›‘æ§

**æ•°æ®é‡‡é›†ç›‘æ§ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```python
class IoTMonitoring:
    """IoTç³»ç»Ÿç›‘æ§"""

    def __init__(self, conn_str):
        self.conn = psycopg2.connect(conn_str)
        self.cursor = self.conn.cursor()

    def get_collection_stats(self):
        """è·å–é‡‡é›†ç»Ÿè®¡"""
        self.cursor.execute("""
            SELECT
                COUNT(*) AS total_points,
                COUNT(DISTINCT device_id) AS device_count,
                COUNT(DISTINCT metric_name) AS metric_count,
                MIN(timestamp) AS earliest_point,
                MAX(timestamp) AS latest_point
            FROM sensor_data
            WHERE timestamp > NOW() - INTERVAL '1 hour'
        """)

        return self.cursor.fetchone()

    def get_ingestion_rate(self):
        """è·å–å†™å…¥é€Ÿç‡"""
        self.cursor.execute("""
            SELECT
                COUNT(*) / EXTRACT(EPOCH FROM (MAX(timestamp) - MIN(timestamp))) AS points_per_second
            FROM sensor_data
            WHERE timestamp > NOW() - INTERVAL '1 minute'
        """)

        return self.cursor.fetchone()[0]
```

### 7.2 æŸ¥è¯¢æ€§èƒ½ç›‘æ§

**æŸ¥è¯¢æ€§èƒ½ç›‘æ§ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```sql
-- æŸ¥è¯¢æ€§èƒ½ç›‘æ§è§†å›¾
CREATE OR REPLACE VIEW v_iot_query_performance AS
SELECT
    query_type,
    COUNT(*) AS execution_count,
    AVG(duration_ms) AS avg_duration_ms,
    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY duration_ms) AS p95_duration_ms,
    AVG(result_count) AS avg_result_count
FROM iot_query_logs
WHERE created_at > NOW() - INTERVAL '24 hours'
GROUP BY query_type
ORDER BY avg_duration_ms DESC;

-- æŸ¥è¯¢æ€§èƒ½ç»Ÿè®¡
SELECT * FROM v_iot_query_performance;
```

---

## 8. IoTç³»ç»Ÿæœ€ä½³å®è·µ

### 8.1 æ•°æ®é‡‡é›†æœ€ä½³å®è·µ

**æ•°æ®é‡‡é›†æœ€ä½³å®è·µï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```python
# 1. æ‰¹é‡å†™å…¥ï¼ˆæé«˜ååé‡ï¼‰
collector = IoTDataCollector(conn_str, batch_size=10000)

# 2. å¼‚æ­¥å†™å…¥ï¼ˆé™ä½å»¶è¿Ÿï¼‰
collector = IoTDataCollector(conn_str, flush_interval=0.5)

# 3. é”™è¯¯å¤„ç†ï¼ˆä¿è¯æ•°æ®å®Œæ•´æ€§ï¼‰
try:
    collector.collect_data_point(device_id, metric_name, value)
except Exception as e:
    logger.error(f"æ•°æ®é‡‡é›†å¤±è´¥: {e}")
    # é‡è¯•æˆ–è®°å½•åˆ°æ­»ä¿¡é˜Ÿåˆ—
```

### 8.2 æŸ¥è¯¢ä¼˜åŒ–æœ€ä½³å®è·µ

**æŸ¥è¯¢ä¼˜åŒ–æœ€ä½³å®è·µï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```sql
-- 1. ä½¿ç”¨æ—¶é—´æ¡¶èšåˆï¼ˆTimescaleDBï¼‰
SELECT
    time_bucket('1 hour', timestamp) AS hour,
    AVG(value) AS avg_value
FROM sensor_data
WHERE timestamp >= NOW() - INTERVAL '24 hours'
GROUP BY hour
ORDER BY hour;

-- 2. ä½¿ç”¨è¿ç»­èšåˆï¼ˆé¢„è®¡ç®—ï¼‰
CREATE MATERIALIZED VIEW sensor_data_hourly
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 hour', timestamp) AS hour,
    device_id,
    AVG(value) AS avg_value
FROM sensor_data
GROUP BY hour, device_id;

-- 3. ä½¿ç”¨å‹ç¼©ï¼ˆèŠ‚çœå­˜å‚¨ï¼‰
SELECT add_compression_policy('sensor_data', INTERVAL '7 days');
```

---

**å®Œæˆæ—¥æœŸ**: 2025-12-04
**ä»£ç è¡Œæ•°**: ~800è¡Œ
**æ€§èƒ½**: 1M+ points/ç§’
**æ¶µç›–**: æ•°æ®é‡‡é›†ã€å®æ—¶èšåˆã€æ•°æ®å‹ç¼©ã€å¼‚å¸¸æ£€æµ‹ã€PostgreSQL 18ä¼˜åŒ–ã€ç›‘æ§ã€æœ€ä½³å®è·µ

**è¿”å›**: [æ¡ˆä¾‹3ä¸»é¡µ](./README.md)
