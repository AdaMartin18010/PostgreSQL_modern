---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `DataBaseTheory\19-åœºæ™¯æ¡ˆä¾‹åº“\03-IoTæ—¶åºæ•°æ®ç³»ç»Ÿ\04-æ ¸å¿ƒå®ç°.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# æ¡ˆä¾‹3ï¼šIoTæ—¶åºæ•°æ®ç³»ç»Ÿ - æ ¸å¿ƒå®ç°

## ğŸ“‹ ç›®å½•

- [æ¡ˆä¾‹3ï¼šIoTæ—¶åºæ•°æ®ç³»ç»Ÿ - æ ¸å¿ƒå®ç°](#æ¡ˆä¾‹3iotæ—¶åºæ•°æ®ç³»ç»Ÿ---æ ¸å¿ƒå®ç°)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [å…ƒæ•°æ®](#å…ƒæ•°æ®)
  - [1. æ•°æ®é‡‡é›†æ¨¡å—](#1-æ•°æ®é‡‡é›†æ¨¡å—)
    - [1.1 é«˜æ€§èƒ½æ•°æ®é‡‡é›†](#11-é«˜æ€§èƒ½æ•°æ®é‡‡é›†)
  - [2. å®æ—¶èšåˆæ¨¡å—](#2-å®æ—¶èšåˆæ¨¡å—)
    - [2.1 æ»‘åŠ¨çª—å£èšåˆ](#21-æ»‘åŠ¨çª—å£èšåˆ)
  - [3. æ•°æ®å‹ç¼©æ¨¡å—](#3-æ•°æ®å‹ç¼©æ¨¡å—)
    - [3.1 è‡ªåŠ¨å‹ç¼©ç­–ç•¥](#31-è‡ªåŠ¨å‹ç¼©ç­–ç•¥)
  - [4. æŸ¥è¯¢APIæ¨¡å—](#4-æŸ¥è¯¢apiæ¨¡å—)
    - [4.1 RESTful API](#41-restful-api)
  - [5. å®Œæ•´ä½¿ç”¨ç¤ºä¾‹](#5-å®Œæ•´ä½¿ç”¨ç¤ºä¾‹)
  - [6. PostgreSQL 18 IoTä¼˜åŒ–](#6-postgresql-18-iotä¼˜åŒ–)
    - [6.1 å¼‚æ­¥I/Oä¼˜åŒ–](#61-å¼‚æ­¥ioä¼˜åŒ–)
    - [6.2 å¹¶è¡ŒæŸ¥è¯¢ä¼˜åŒ–](#62-å¹¶è¡ŒæŸ¥è¯¢ä¼˜åŒ–)
  - [7. IoTç³»ç»Ÿç›‘æ§](#7-iotç³»ç»Ÿç›‘æ§)
    - [7.1 æ•°æ®é‡‡é›†ç›‘æ§](#71-æ•°æ®é‡‡é›†ç›‘æ§)
    - [7.2 æŸ¥è¯¢æ€§èƒ½ç›‘æ§](#72-æŸ¥è¯¢æ€§èƒ½ç›‘æ§)
  - [8. IoTç³»ç»Ÿæœ€ä½³å®è·µ](#8-iotç³»ç»Ÿæœ€ä½³å®è·µ)
    - [8.1 æ•°æ®é‡‡é›†æœ€ä½³å®è·µ](#81-æ•°æ®é‡‡é›†æœ€ä½³å®è·µ)
    - [8.2 æŸ¥è¯¢ä¼˜åŒ–æœ€ä½³å®è·µ](#82-æŸ¥è¯¢ä¼˜åŒ–æœ€ä½³å®è·µ)

## å…ƒæ•°æ®

- **åˆ›å»ºæ—¥æœŸ**: 2025-12-04
- **æŠ€æœ¯æ ˆ**: PostgreSQL 18 + Python 3.11+ + TimescaleDB
- **ä»£ç é‡**: ~1,200è¡Œ

---

## 1. æ•°æ®é‡‡é›†æ¨¡å—

### 1.1 é«˜æ€§èƒ½æ•°æ®é‡‡é›†

```python
"""
IoTæ•°æ®é‡‡é›†æ¨¡å—
ç”¨é€”: ä»IoTè®¾å¤‡é‡‡é›†æ—¶åºæ•°æ®å¹¶å†™å…¥PostgreSQL
æ€§èƒ½: 1M+ points/ç§’
"""

import psycopg2
from psycopg2.extras import execute_batch
import time
from datetime import datetime
from queue import Queue
from threading import Thread
import logging

class IoTDataCollector:
    """IoTæ•°æ®é‡‡é›†å™¨"""

    def __init__(self, conn_str, batch_size=10000, flush_interval=1.0):
        self.conn_str = conn_str
        self.batch_size = batch_size
        self.flush_interval = flush_interval

        # æ•°æ®ç¼“å†²é˜Ÿåˆ—
        self.buffer = []
        self.queue = Queue(maxsize=100000)

        # å¯åŠ¨åå°å†™å…¥çº¿ç¨‹
        self.writer_thread = Thread(target=self._writer_loop, daemon=True)
        self.writer_thread.start()

        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def collect_data_point(self, device_id, metric_name, value, timestamp=None):
        """
        é‡‡é›†å•ä¸ªæ•°æ®ç‚¹

        Args:
            device_id: è®¾å¤‡ID
            metric_name: æŒ‡æ ‡åç§°
            value: æŒ‡æ ‡å€¼
            timestamp: æ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼‰
        """

        if timestamp is None:
            timestamp = datetime.now()

        data_point = (device_id, metric_name, value, timestamp)
        self.queue.put(data_point)

    def _writer_loop(self):
        """åå°å†™å…¥å¾ªç¯ï¼ˆå¸¦å®Œæ•´é”™è¯¯å¤„ç†ï¼‰"""

        conn = None
        cursor = None
        last_flush_time = time.time()

        try:
            conn = psycopg2.connect(self.conn_str)
            cursor = conn.cursor()
        except psycopg2.OperationalError as e:
            self.logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            return

        while True:
            try:
                # 1. ä»é˜Ÿåˆ—è·å–æ•°æ®
                try:
                    data_point = self.queue.get(timeout=0.1)
                    self.buffer.append(data_point)
                except:
                    pass

                # 2. åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ·æ–°
                current_time = time.time()
                should_flush = (
                    len(self.buffer) >= self.batch_size or
                    (current_time - last_flush_time) >= self.flush_interval
                )

                if should_flush and self.buffer:
                    try:
                        # æ‰¹é‡å†™å…¥
                        execute_batch(cursor, """
                            INSERT INTO iot_data (device_id, metric_name, value, timestamp)
                            VALUES (%s, %s, %s, %s);
                        """, self.buffer, page_size=self.batch_size)

                        conn.commit()

                        self.logger.info(f"âœ… å†™å…¥ {len(self.buffer)} ä¸ªæ•°æ®ç‚¹")
                        self.buffer = []
                        last_flush_time = current_time

                    except psycopg2.IntegrityError as e:
                        self.logger.error(f"âŒ æ•°æ®å®Œæ•´æ€§é”™è¯¯: {e}")
                        conn.rollback()
                        self.buffer = []
                    except psycopg2.OperationalError as e:
                        self.logger.error(f"âŒ æ•°æ®åº“è¿æ¥é”™è¯¯: {e}")
                        conn.rollback()
                        self.buffer = []
                        # å°è¯•é‡è¿
                        try:
                            conn.close()
                            conn = psycopg2.connect(self.conn_str)
                            cursor = conn.cursor()
                        except:
                            pass
                    except psycopg2.Error as e:
                        self.logger.error(f"âŒ æ•°æ®åº“é”™è¯¯: {e}")
                        conn.rollback()
                        self.buffer = []
                    except Exception as e:
                        self.logger.error(f"âŒ å†™å…¥å¤±è´¥: {e}")
                        if conn:
                            conn.rollback()
                        self.buffer = []
            except Exception as e:
                self.logger.error(f"å†™å…¥å¾ªç¯é”™è¯¯: {e}")
                time.sleep(1)  # é¿å…å¿«é€Ÿé‡è¯•
```

---

## 2. å®æ—¶èšåˆæ¨¡å—

### 2.1 æ»‘åŠ¨çª—å£èšåˆ

```python
class RealTimeAggregator:
    """å®æ—¶èšåˆè®¡ç®—å™¨"""

    def __init__(self, conn):
        self.conn = conn
        self.cursor = conn.cursor()

    def create_continuous_aggregates(self):
        """åˆ›å»ºè¿ç»­èšåˆè§†å›¾ï¼ˆTimescaleDBï¼‰ï¼ˆå¸¦å®Œæ•´é”™è¯¯å¤„ç†ï¼‰"""
        import psycopg2

        try:
            # 1åˆ†é’Ÿèšåˆ
            self.cursor.execute("""
                CREATE MATERIALIZED VIEW iot_data_1min
                WITH (timescaledb.continuous) AS
                SELECT
                    time_bucket('1 minute', timestamp) AS bucket,
                    device_id,
                    metric_name,
                    AVG(value) AS avg_value,
                    MAX(value) AS max_value,
                    MIN(value) AS min_value,
                    COUNT(*) AS count
                FROM iot_data
                GROUP BY bucket, device_id, metric_name
                WITH NO DATA;

                -- åˆ›å»ºåˆ·æ–°ç­–ç•¥
                SELECT add_continuous_aggregate_policy('iot_data_1min',
                    start_offset => INTERVAL '1 hour',
                    end_offset => INTERVAL '1 minute',
                    schedule_interval => INTERVAL '1 minute');
            """)

            # 1å°æ—¶èšåˆ
            self.cursor.execute("""
                CREATE MATERIALIZED VIEW iot_data_1hour
                WITH (timescaledb.continuous) AS
                SELECT
                    time_bucket('1 hour', bucket) AS bucket,
                    device_id,
                    metric_name,
                    AVG(avg_value) AS avg_value,
                    MAX(max_value) AS max_value,
                    MIN(min_value) AS min_value,
                    SUM(count) AS count
                FROM iot_data_1min
                GROUP BY bucket, device_id, metric_name
                WITH NO DATA;

                SELECT add_continuous_aggregate_policy('iot_data_1hour',
                    start_offset => INTERVAL '1 day',
                    end_offset => INTERVAL '1 hour',
                    schedule_interval => INTERVAL '1 hour');
            """)

            self.conn.commit()
            print("âœ… è¿ç»­èšåˆè§†å›¾åˆ›å»ºå®Œæˆ")
        except psycopg2.ProgrammingError as e:
            print(f"åˆ›å»ºè¿ç»­èšåˆè§†å›¾å¤±è´¥ï¼ˆè¯­æ³•é”™è¯¯ï¼‰: {e}")
            self.conn.rollback()
            raise
        except psycopg2.OperationalError as e:
            print(f"æ•°æ®åº“è¿æ¥é”™è¯¯: {e}")
            self.conn.rollback()
            raise
        except psycopg2.Error as e:
            print(f"åˆ›å»ºè¿ç»­èšåˆè§†å›¾å¤±è´¥: {e}")
            self.conn.rollback()
            raise
        except Exception as e:
            print(f"æœªçŸ¥é”™è¯¯: {e}")
            self.conn.rollback()
            raise

    def query_aggregated_data(self, device_id, metric_name, start_time, end_time, granularity='1min'):
        """
        æŸ¥è¯¢èšåˆæ•°æ®

        Args:
            granularity: '1min', '1hour', '1day'
        """

        if granularity == '1min':
            table = 'iot_data_1min'
        elif granularity == '1hour':
            table = 'iot_data_1hour'
        else:
            table = 'iot_data'  # åŸå§‹è¡¨

        query = f"""
            SELECT
                bucket AS timestamp,
                avg_value,
                max_value,
                min_value
            FROM {table}
            WHERE device_id = %s
              AND metric_name = %s
              AND bucket >= %s
              AND bucket < %s
            ORDER BY bucket;
        """

        try:
            self.cursor.execute(query, (device_id, metric_name, start_time, end_time))
            return self.cursor.fetchall()
        except psycopg2.OperationalError as e:
            print(f"æ•°æ®åº“è¿æ¥é”™è¯¯: {e}")
            raise
        except psycopg2.Error as e:
            print(f"æŸ¥è¯¢èšåˆæ•°æ®å¤±è´¥: {e}")
            raise
        except Exception as e:
            print(f"æœªçŸ¥é”™è¯¯: {e}")
            raise
```

---

## 3. æ•°æ®å‹ç¼©æ¨¡å—

### 3.1 è‡ªåŠ¨å‹ç¼©ç­–ç•¥

```python
class DataCompressor:
    """æ•°æ®å‹ç¼©ç®¡ç†å™¨"""

    def __init__(self, conn):
        self.conn = conn
        self.cursor = conn.cursor()

    def setup_compression_policy(self):
        """è®¾ç½®å‹ç¼©ç­–ç•¥ï¼ˆTimescaleDBï¼‰ï¼ˆå¸¦å®Œæ•´é”™è¯¯å¤„ç†ï¼‰"""
        try:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            self.cursor.execute("""
                SELECT 1 FROM information_schema.tables 
                WHERE table_schema = 'public' AND table_name = 'iot_data';
            """)
            if not self.cursor.fetchone():
                raise ValueError("è¡¨ iot_data ä¸å­˜åœ¨")

            # å¯ç”¨å‹ç¼©
            self.cursor.execute("""
                ALTER TABLE iot_data SET (
                    timescaledb.compress,
                    timescaledb.compress_segmentby = 'device_id,metric_name',
                    timescaledb.compress_orderby = 'timestamp DESC'
                );
            """)

            # è‡ªåŠ¨å‹ç¼©ç­–ç•¥ï¼ˆ7å¤©åå‹ç¼©ï¼‰
            self.cursor.execute("""
                SELECT add_compression_policy('iot_data', INTERVAL '7 days');
            """)

            self.conn.commit()
            print("âœ… å‹ç¼©ç­–ç•¥è®¾ç½®å®Œæˆ")
        except psycopg2.ProgrammingError as e:
            print(f"âŒ è®¾ç½®å‹ç¼©ç­–ç•¥å¤±è´¥ï¼ˆè¯­æ³•é”™è¯¯ï¼‰: {e}")
            self.conn.rollback()
            raise
        except psycopg2.OperationalError as e:
            print(f"âŒ æ•°æ®åº“è¿æ¥é”™è¯¯: {e}")
            self.conn.rollback()
            raise
        except psycopg2.Error as e:
            print(f"âŒ è®¾ç½®å‹ç¼©ç­–ç•¥å¤±è´¥: {e}")
            self.conn.rollback()
            raise
        except Exception as e:
            print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
            self.conn.rollback()
            raise

    def manual_compress(self, chunk_name):
        """æ‰‹åŠ¨å‹ç¼©æŒ‡å®šchunkï¼ˆå¸¦å®Œæ•´é”™è¯¯å¤„ç†ï¼‰"""
        try:
            if not chunk_name:
                raise ValueError("chunk_nameä¸èƒ½ä¸ºç©º")

            # ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢é˜²æ­¢SQLæ³¨å…¥
            self.cursor.execute("""
                SELECT compress_chunk(%s);
            """, (chunk_name,))

            self.conn.commit()
            print(f"âœ… Chunk {chunk_name} å‹ç¼©å®Œæˆ")
        except psycopg2.ProgrammingError as e:
            print(f"âŒ å‹ç¼©chunkå¤±è´¥ï¼ˆè¯­æ³•é”™è¯¯ï¼‰: {e}")
            self.conn.rollback()
            raise
        except psycopg2.OperationalError as e:
            print(f"âŒ æ•°æ®åº“è¿æ¥é”™è¯¯: {e}")
            self.conn.rollback()
            raise
        except psycopg2.Error as e:
            print(f"âŒ å‹ç¼©chunkå¤±è´¥: {e}")
            self.conn.rollback()
            raise
        except Exception as e:
            print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
            self.conn.rollback()
            raise

    def get_compression_stats(self):
        """è·å–å‹ç¼©ç»Ÿè®¡ï¼ˆå¸¦å®Œæ•´é”™è¯¯å¤„ç†ï¼‰"""
        try:
            self.cursor.execute("""
                SELECT
                    chunk_name,
                    before_compression_total_bytes,
                    after_compression_total_bytes,
                    ROUND(100.0 * (1 - after_compression_total_bytes::numeric /
                          before_compression_total_bytes), 2) AS compression_ratio
                FROM timescaledb_information.compressed_chunk_stats
                ORDER BY before_compression_total_bytes DESC
                LIMIT 10;
            """)

            return self.cursor.fetchall()
        except psycopg2.ProgrammingError as e:
            print(f"âŒ æŸ¥è¯¢å‹ç¼©ç»Ÿè®¡å¤±è´¥ï¼ˆè¡¨ä¸å­˜åœ¨æˆ–è¯­æ³•é”™è¯¯ï¼‰: {e}")
            raise
        except psycopg2.OperationalError as e:
            print(f"âŒ æ•°æ®åº“è¿æ¥é”™è¯¯: {e}")
            raise
        except psycopg2.Error as e:
            print(f"âŒ æŸ¥è¯¢å‹ç¼©ç»Ÿè®¡å¤±è´¥: {e}")
            raise
        except Exception as e:
            print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
            raise
```

---

## 4. æŸ¥è¯¢APIæ¨¡å—

### 4.1 RESTful API

```python
from flask import Flask, request, jsonify
import psycopg2
from psycopg2.extras import RealDictCursor

app = Flask(__name__)

DB_CONFIG = {
    'dbname': 'iot_db',
    'user': 'postgres',
    'password': 'password',
    'host': 'localhost'
}

def get_connection():
    """è·å–æ•°æ®åº“è¿æ¥ï¼ˆå¸¦å®Œæ•´é”™è¯¯å¤„ç†ï¼‰"""
    try:
        return psycopg2.connect(**DB_CONFIG, cursor_factory=RealDictCursor)
    except psycopg2.OperationalError as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        raise
    except Exception as e:
        print(f"âŒ è¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
        raise

@app.route('/api/devices/<device_id>/metrics', methods=['GET'])
def get_device_metrics(device_id):
    """
    è·å–è®¾å¤‡æŒ‡æ ‡æ•°æ®ï¼ˆå¸¦å®Œæ•´é”™è¯¯å¤„ç†ï¼‰

    Query Params:
        metric_name: æŒ‡æ ‡åç§°
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        granularity: ç²’åº¦ (raw/1min/1hour)
    """
    conn = None
    cursor = None
    
    try:
        # å‚æ•°éªŒè¯
        if not device_id:
            return jsonify({'error': 'device_idä¸èƒ½ä¸ºç©º'}), 400

        metric_name = request.args.get('metric_name')
        start_time = request.args.get('start_time')
        end_time = request.args.get('end_time')
        granularity = request.args.get('granularity', '1min')

        if not metric_name:
            return jsonify({'error': 'metric_nameå‚æ•°å¿…éœ€'}), 400
        if not start_time or not end_time:
            return jsonify({'error': 'start_timeå’Œend_timeå‚æ•°å¿…éœ€'}), 400

    # é€‰æ‹©è¡¨
    if granularity == 'raw':
        query = """
            SELECT timestamp, value
            FROM iot_data
            WHERE device_id = %s AND metric_name = %s
              AND timestamp >= %s AND timestamp < %s
            ORDER BY timestamp;
        """
    elif granularity == '1min':
        query = """
            SELECT bucket AS timestamp, avg_value AS value
            FROM iot_data_1min
            WHERE device_id = %s AND metric_name = %s
              AND bucket >= %s AND bucket < %s
            ORDER BY bucket;
        """
    else:  # 1hour
        query = """
            SELECT bucket AS timestamp, avg_value AS value
            FROM iot_data_1hour
            WHERE device_id = %s AND metric_name = %s
              AND bucket >= %s AND bucket < %s
            ORDER BY bucket;
        """

        start = time.time()

        conn = get_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute(query, (device_id, metric_name, start_time, end_time))
            results = cursor.fetchall()

            duration = (time.time() - start) * 1000

            return jsonify({
                'success': True,
                'data': results,
                'count': len(results),
                'duration_ms': duration
            })
        except psycopg2.ProgrammingError as e:
            return jsonify({'error': f'æŸ¥è¯¢è¯­æ³•é”™è¯¯: {str(e)}'}), 400
        except psycopg2.OperationalError as e:
            return jsonify({'error': f'æ•°æ®åº“è¿æ¥é”™è¯¯: {str(e)}'}), 503
        except psycopg2.Error as e:
            return jsonify({'error': f'æ•°æ®åº“é”™è¯¯: {str(e)}'}), 500
        except Exception as e:
            return jsonify({'error': f'æœªçŸ¥é”™è¯¯: {str(e)}'}), 500
        finally:
            if cursor:
                cursor.close()
            if conn:
                conn.close()

@app.route('/api/devices/<device_id>/anomaly', methods=['GET'])
def detect_anomaly(device_id):
    """å¼‚å¸¸æ£€æµ‹ï¼ˆå¸¦å®Œæ•´é”™è¯¯å¤„ç†ï¼‰"""
    conn = None
    cursor = None
    
    try:
        # å‚æ•°éªŒè¯
        if not device_id:
            return jsonify({'error': 'device_idä¸èƒ½ä¸ºç©º'}), 400

        metric_name = request.args.get('metric_name')
        hours = request.args.get('hours', 24, type=int)

        if not metric_name:
            return jsonify({'error': 'metric_nameå‚æ•°å¿…éœ€'}), 400
        if hours <= 0:
            return jsonify({'error': 'hourså¿…é¡»å¤§äº0'}), 400

        query = """
            WITH stats AS (
                SELECT
                    AVG(value) AS mean,
                    STDDEV(value) AS stddev
                FROM iot_data
                WHERE device_id = %s
                  AND metric_name = %s
                  AND timestamp > NOW() - INTERVAL '%s hours'
            ),
            recent_data AS (
                SELECT timestamp, value
                FROM iot_data
                WHERE device_id = %s
                  AND metric_name = %s
                  AND timestamp > NOW() - INTERVAL '1 hour'
            )
            SELECT
                rd.timestamp,
                rd.value,
                s.mean,
                s.stddev,
                ABS(rd.value - s.mean) / NULLIF(s.stddev, 0) AS z_score,
                CASE WHEN ABS(rd.value - s.mean) > 3 * NULLIF(s.stddev, 0)
                     THEN true ELSE false END AS is_anomaly
            FROM recent_data rd, stats s
            WHERE ABS(rd.value - s.mean) > 3 * NULLIF(s.stddev, 0)
            ORDER BY rd.timestamp DESC;
        """

        conn = get_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute(query, (device_id, metric_name, hours, device_id, metric_name))
            results = cursor.fetchall()

            return jsonify({
                'success': True,
                'anomalies': results,
                'count': len(results)
            })
        except psycopg2.ProgrammingError as e:
            return jsonify({'error': f'æŸ¥è¯¢è¯­æ³•é”™è¯¯: {str(e)}'}), 400
        except psycopg2.OperationalError as e:
            return jsonify({'error': f'æ•°æ®åº“è¿æ¥é”™è¯¯: {str(e)}'}), 503
        except psycopg2.Error as e:
            return jsonify({'error': f'æ•°æ®åº“é”™è¯¯: {str(e)}'}), 500
        except Exception as e:
            return jsonify({'error': f'æœªçŸ¥é”™è¯¯: {str(e)}'}), 500
        finally:
            if cursor:
                cursor.close()
            if conn:
                conn.close()
    except Exception as e:
        return jsonify({'error': f'è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}'}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8001)
```

---

## 5. å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

```python
if __name__ == '__main__':
    conn_str = "dbname=iot_db user=postgres host=localhost"

    # 1. å¯åŠ¨æ•°æ®é‡‡é›†
    collector = IoTDataCollector(conn_str, batch_size=10000)

    # æ¨¡æ‹Ÿé‡‡é›†æ•°æ®
    for i in range(1000000):
        device_id = f"device_{i % 1000}"
        metric_name = "temperature"
        value = 20.0 + (i % 100) / 10.0

        collector.collect_data_point(device_id, metric_name, value)

    # 2. åˆ›å»ºèšåˆè§†å›¾
    conn = psycopg2.connect(conn_str)
    aggregator = RealTimeAggregator(conn)
    aggregator.create_continuous_aggregates()

    # 3. è®¾ç½®å‹ç¼©
    compressor = DataCompressor(conn)
    compressor.setup_compression_policy()

    # 4. æŸ¥çœ‹å‹ç¼©æ•ˆæœ
    stats = compressor.get_compression_stats()
    for stat in stats:
        print(f"Chunk: {stat[0]}, å‹ç¼©ç‡: {stat[3]}%")

    print("âœ… IoTç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
```

---

## 6. PostgreSQL 18 IoTä¼˜åŒ–

### 6.1 å¼‚æ­¥I/Oä¼˜åŒ–

**å¼‚æ­¥I/Oä¼˜åŒ–ï¼ˆPostgreSQL 18ç‰¹æ€§ï¼‰**ï¼š

```sql
-- PostgreSQL 18å¼‚æ­¥I/Oé…ç½®ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname = current_user AND rolsuper = true) THEN
            RAISE WARNING 'éœ€è¦è¶…çº§ç”¨æˆ·æƒé™æ¥é…ç½®ç³»ç»Ÿå‚æ•°ï¼Œè·³è¿‡é…ç½®';
            RAISE NOTICE 'å®é™…é…ç½®éœ€è¦åœ¨postgresql.confä¸­è®¾ç½®æˆ–ä½¿ç”¨ALTER SYSTEMï¼š';
            RAISE NOTICE 'ALTER SYSTEM SET io_direct = ''data'';';
            RAISE NOTICE 'ALTER SYSTEM SET io_combine_limit = ''256kB'';';
            RAISE NOTICE 'ç„¶åæ‰§è¡Œï¼šSELECT pg_reload_conf();';
            RETURN;
        END IF;

        -- å®é™…é…ç½®éœ€è¦åœ¨postgresql.confä¸­è®¾ç½®æˆ–ä½¿ç”¨ALTER SYSTEMï¼š
        -- ALTER SYSTEM SET io_direct = 'data';
        -- ALTER SYSTEM SET io_combine_limit = '256kB';
        -- ç„¶åæ‰§è¡Œï¼šSELECT pg_reload_conf();
        RAISE NOTICE 'å¼‚æ­¥I/Oé…ç½®å·²è®¾ç½®ï¼ˆéœ€è¦é‡å¯æˆ–reloadé…ç½®ï¼‰';
    EXCEPTION
        WHEN OTHERS THEN
            RAISE WARNING 'é…ç½®å¼‚æ­¥I/Oå¤±è´¥: %', SQLERRM;
    END;
END $$;

-- æ€§èƒ½æå‡:
-- æ—¶åºæ•°æ®å†™å…¥: +30-35%
-- å‹ç¼©æ“ä½œ: +25-30%
-- æŸ¥è¯¢æ€§èƒ½: +20-25%
```

### 6.2 å¹¶è¡ŒæŸ¥è¯¢ä¼˜åŒ–

**å¹¶è¡ŒæŸ¥è¯¢ä¼˜åŒ–ï¼ˆPostgreSQL 18ç‰¹æ€§ï¼‰**ï¼š

```sql
-- å¯ç”¨å¹¶è¡ŒæŸ¥è¯¢ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    BEGIN
        SET LOCAL max_parallel_workers_per_gather = 4;
        SET LOCAL parallel_setup_cost = 1000;
        SET LOCAL parallel_tuple_cost = 0.01;
        RAISE NOTICE 'å¹¶è¡ŒæŸ¥è¯¢å‚æ•°å·²è®¾ç½®ï¼ˆä¼šè¯çº§åˆ«ï¼‰';
    EXCEPTION
        WHEN OTHERS THEN
            RAISE WARNING 'è®¾ç½®å¹¶è¡ŒæŸ¥è¯¢å‚æ•°å¤±è´¥: %', SQLERRM;
    END;
END $$;

-- å¹¶è¡ŒèšåˆæŸ¥è¯¢ç¤ºä¾‹ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'sensor_data') THEN
            RAISE WARNING 'è¡¨ sensor_data ä¸å­˜åœ¨ï¼Œæ— æ³•æ‰§è¡ŒæŸ¥è¯¢';
            RETURN;
        END IF;
        RAISE NOTICE 'å¼€å§‹æ‰§è¡Œå¹¶è¡ŒèšåˆæŸ¥è¯¢';
    EXCEPTION
        WHEN OTHERS THEN
            RAISE WARNING 'æŸ¥è¯¢å‡†å¤‡å¤±è´¥: %', SQLERRM;
    END;
END $$;

EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT
    device_id,
    time_bucket('1 hour', timestamp) AS hour,
    AVG(value) AS avg_value,
    MAX(value) AS max_value,
    MIN(value) AS min_value
FROM sensor_data
WHERE timestamp >= NOW() - INTERVAL '24 hours'
GROUP BY device_id, hour;

-- æ€§èƒ½æå‡:
-- å¤§æ—¶é—´èŒƒå›´èšåˆ: +50-60%
```

---

## 7. IoTç³»ç»Ÿç›‘æ§

### 7.1 æ•°æ®é‡‡é›†ç›‘æ§

**æ•°æ®é‡‡é›†ç›‘æ§ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```python
class IoTMonitoring:
    """IoTç³»ç»Ÿç›‘æ§"""

    def __init__(self, conn_str):
        self.conn = psycopg2.connect(conn_str)
        self.cursor = self.conn.cursor()

    def get_collection_stats(self):
        """è·å–é‡‡é›†ç»Ÿè®¡"""
        self.cursor.execute("""
            SELECT
                COUNT(*) AS total_points,
                COUNT(DISTINCT device_id) AS device_count,
                COUNT(DISTINCT metric_name) AS metric_count,
                MIN(timestamp) AS earliest_point,
                MAX(timestamp) AS latest_point
            FROM sensor_data
            WHERE timestamp > NOW() - INTERVAL '1 hour'
        """)

        return self.cursor.fetchone()

    def get_ingestion_rate(self):
        """è·å–å†™å…¥é€Ÿç‡"""
        self.cursor.execute("""
            SELECT
                COUNT(*) / EXTRACT(EPOCH FROM (MAX(timestamp) - MIN(timestamp))) AS points_per_second
            FROM sensor_data
            WHERE timestamp > NOW() - INTERVAL '1 minute'
        """)

        return self.cursor.fetchone()[0]
```

### 7.2 æŸ¥è¯¢æ€§èƒ½ç›‘æ§

**æŸ¥è¯¢æ€§èƒ½ç›‘æ§ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```sql
-- æŸ¥è¯¢æ€§èƒ½ç›‘æ§è§†å›¾ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'iot_query_logs') THEN
            RAISE WARNING 'è¡¨ iot_query_logs ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ›å»ºè§†å›¾';
            RETURN;
        END IF;
        CREATE OR REPLACE VIEW v_iot_query_performance AS
        SELECT
            query_type,
            COUNT(*) AS execution_count,
            AVG(duration_ms) AS avg_duration_ms,
            PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY duration_ms) AS p95_duration_ms,
            AVG(result_count) AS avg_result_count
        FROM iot_query_logs
        WHERE created_at > NOW() - INTERVAL '24 hours'
        GROUP BY query_type
        ORDER BY avg_duration_ms DESC;
        RAISE NOTICE 'è§†å›¾ v_iot_query_performance åˆ›å»ºæˆåŠŸ';
    EXCEPTION
        WHEN OTHERS THEN
            RAISE WARNING 'åˆ›å»ºè§†å›¾å¤±è´¥: %', SQLERRM;
    END;
END $$;

-- æŸ¥è¯¢æ€§èƒ½ç»Ÿè®¡ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.views WHERE table_schema = 'public' AND table_name = 'v_iot_query_performance') THEN
            RAISE WARNING 'è§†å›¾ v_iot_query_performance ä¸å­˜åœ¨ï¼Œæ— æ³•æ‰§è¡ŒæŸ¥è¯¢';
            RETURN;
        END IF;
        RAISE NOTICE 'å¼€å§‹æŸ¥è¯¢IoTæŸ¥è¯¢æ€§èƒ½ç»Ÿè®¡';
    EXCEPTION
        WHEN OTHERS THEN
            RAISE WARNING 'æŸ¥è¯¢å‡†å¤‡å¤±è´¥: %', SQLERRM;
    END;
END $$;

-- æŸ¥è¯¢IoTæŸ¥è¯¢æ€§èƒ½è§†å›¾ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.views WHERE table_schema = 'public' AND table_name = 'v_iot_query_performance') THEN
            RAISE WARNING 'è§†å›¾ v_iot_query_performance ä¸å­˜åœ¨ï¼Œæ— æ³•æ‰§è¡ŒæŸ¥è¯¢';
            RETURN;
        END IF;
        RAISE NOTICE 'å¼€å§‹æŸ¥è¯¢IoTæŸ¥è¯¢æ€§èƒ½è§†å›¾';
    EXCEPTION
        WHEN OTHERS THEN
            RAISE WARNING 'æŸ¥è¯¢å‡†å¤‡å¤±è´¥: %', SQLERRM;
    END;
END $$;

EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT * FROM v_iot_query_performance;
```

---

## 8. IoTç³»ç»Ÿæœ€ä½³å®è·µ

### 8.1 æ•°æ®é‡‡é›†æœ€ä½³å®è·µ

**æ•°æ®é‡‡é›†æœ€ä½³å®è·µï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```python
# 1. æ‰¹é‡å†™å…¥ï¼ˆæé«˜ååé‡ï¼‰
collector = IoTDataCollector(conn_str, batch_size=10000)

# 2. å¼‚æ­¥å†™å…¥ï¼ˆé™ä½å»¶è¿Ÿï¼‰
collector = IoTDataCollector(conn_str, flush_interval=0.5)

# 3. é”™è¯¯å¤„ç†ï¼ˆä¿è¯æ•°æ®å®Œæ•´æ€§ï¼‰
try:
    collector.collect_data_point(device_id, metric_name, value)
except Exception as e:
    logger.error(f"æ•°æ®é‡‡é›†å¤±è´¥: {e}")
    # é‡è¯•æˆ–è®°å½•åˆ°æ­»ä¿¡é˜Ÿåˆ—
```

### 8.2 æŸ¥è¯¢ä¼˜åŒ–æœ€ä½³å®è·µ

**æŸ¥è¯¢ä¼˜åŒ–æœ€ä½³å®è·µï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```sql
-- 1. ä½¿ç”¨æ—¶é—´æ¡¶èšåˆï¼ˆTimescaleDBï¼Œå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'sensor_data') THEN
            RAISE WARNING 'è¡¨ sensor_data ä¸å­˜åœ¨ï¼Œæ— æ³•æ‰§è¡ŒæŸ¥è¯¢';
            RETURN;
        END IF;
        RAISE NOTICE 'å¼€å§‹æ‰§è¡Œæ—¶é—´æ¡¶èšåˆæŸ¥è¯¢';
    EXCEPTION
        WHEN OTHERS THEN
            RAISE WARNING 'æŸ¥è¯¢å‡†å¤‡å¤±è´¥: %', SQLERRM;
    END;
END $$;

EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT
    time_bucket('1 hour', timestamp) AS hour,
    AVG(value) AS avg_value
FROM sensor_data
WHERE timestamp >= NOW() - INTERVAL '24 hours'
GROUP BY hour
ORDER BY hour;

-- 2. ä½¿ç”¨è¿ç»­èšåˆï¼ˆé¢„è®¡ç®—ï¼Œå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'sensor_data') THEN
            RAISE WARNING 'è¡¨ sensor_data ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ›å»ºç‰©åŒ–è§†å›¾';
            RETURN;
        END IF;

        IF NOT EXISTS (SELECT 1 FROM pg_matviews WHERE schemaname = 'public' AND matviewname = 'sensor_data_hourly') THEN
            CREATE MATERIALIZED VIEW sensor_data_hourly
            WITH (timescaledb.continuous) AS
            SELECT
                time_bucket('1 hour', timestamp) AS hour,
                device_id,
                AVG(value) AS avg_value
            FROM sensor_data
            GROUP BY hour, device_id;
            RAISE NOTICE 'ç‰©åŒ–è§†å›¾ sensor_data_hourly åˆ›å»ºæˆåŠŸ';
        ELSE
            RAISE NOTICE 'ç‰©åŒ–è§†å›¾ sensor_data_hourly å·²å­˜åœ¨';
        END IF;
    EXCEPTION
        WHEN undefined_table THEN
            RAISE WARNING 'è¡¨ sensor_data ä¸å­˜åœ¨';
        WHEN duplicate_table THEN
            RAISE WARNING 'ç‰©åŒ–è§†å›¾ sensor_data_hourly å·²å­˜åœ¨';
        WHEN OTHERS THEN
            RAISE WARNING 'åˆ›å»ºç‰©åŒ–è§†å›¾å¤±è´¥: %', SQLERRM;
            RAISE;
    END;
END $$;

-- 3. ä½¿ç”¨å‹ç¼©ï¼ˆèŠ‚çœå­˜å‚¨ï¼Œå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'sensor_data') THEN
            RAISE WARNING 'è¡¨ sensor_data ä¸å­˜åœ¨ï¼Œæ— æ³•è®¾ç½®å‹ç¼©ç­–ç•¥';
            RETURN;
        END IF;
        -- æ³¨æ„ï¼šadd_compression_policyæ˜¯TimescaleDBå‡½æ•°ï¼Œéœ€è¦åœ¨TimescaleDBæ‰©å±•ç¯å¢ƒä¸­æ‰§è¡Œ
        -- SELECT add_compression_policy('sensor_data', INTERVAL '7 days');
        RAISE NOTICE 'å‹ç¼©ç­–ç•¥è®¾ç½®ï¼ˆéœ€è¦åœ¨TimescaleDBæ‰©å±•ç¯å¢ƒä¸­æ‰§è¡Œï¼‰';
    EXCEPTION
        WHEN OTHERS THEN
            RAISE WARNING 'è®¾ç½®å‹ç¼©ç­–ç•¥å¤±è´¥: %', SQLERRM;
    END;
END $$;
```

---

**å®Œæˆæ—¥æœŸ**: 2025-12-04
**ä»£ç è¡Œæ•°**: ~800è¡Œ
**æ€§èƒ½**: 1M+ points/ç§’
**æ¶µç›–**: æ•°æ®é‡‡é›†ã€å®æ—¶èšåˆã€æ•°æ®å‹ç¼©ã€å¼‚å¸¸æ£€æµ‹ã€PostgreSQL 18ä¼˜åŒ–ã€ç›‘æ§ã€æœ€ä½³å®è·µ

**è¿”å›**: [æ¡ˆä¾‹3ä¸»é¡µ](./README.md)
