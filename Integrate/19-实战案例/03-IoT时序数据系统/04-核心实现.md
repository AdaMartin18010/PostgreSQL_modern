---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `DataBaseTheory\19-åœºæ™¯æ¡ˆä¾‹åº“\03-IoTæ—¶åºæ•°æ®ç³»ç»Ÿ\04-æ ¸å¿ƒå®ç°.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# æ¡ˆä¾‹3ï¼šIoTæ—¶åºæ•°æ®ç³»ç»Ÿ - æ ¸å¿ƒå®ç°

## å…ƒæ•°æ®

- **åˆ›å»ºæ—¥æœŸ**: 2025-12-04
- **æŠ€æœ¯æ ˆ**: PostgreSQL 18 + Python 3.11+ + TimescaleDB
- **ä»£ç é‡**: ~1,200è¡Œ

---

## 1. æ•°æ®é‡‡é›†æ¨¡å—

### 1.1 é«˜æ€§èƒ½æ•°æ®é‡‡é›†

```python
"""
IoTæ•°æ®é‡‡é›†æ¨¡å—
ç”¨é€”: ä»IoTè®¾å¤‡é‡‡é›†æ—¶åºæ•°æ®å¹¶å†™å…¥PostgreSQL
æ€§èƒ½: 1M+ points/ç§’
"""

import psycopg2
from psycopg2.extras import execute_batch
import time
from datetime import datetime
from queue import Queue
from threading import Thread
import logging

class IoTDataCollector:
    """IoTæ•°æ®é‡‡é›†å™¨"""

    def __init__(self, conn_str, batch_size=10000, flush_interval=1.0):
        self.conn_str = conn_str
        self.batch_size = batch_size
        self.flush_interval = flush_interval

        # æ•°æ®ç¼“å†²é˜Ÿåˆ—
        self.buffer = []
        self.queue = Queue(maxsize=100000)

        # å¯åŠ¨åå°å†™å…¥çº¿ç¨‹
        self.writer_thread = Thread(target=self._writer_loop, daemon=True)
        self.writer_thread.start()

        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def collect_data_point(self, device_id, metric_name, value, timestamp=None):
        """
        é‡‡é›†å•ä¸ªæ•°æ®ç‚¹

        Args:
            device_id: è®¾å¤‡ID
            metric_name: æŒ‡æ ‡åç§°
            value: æŒ‡æ ‡å€¼
            timestamp: æ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼‰
        """

        if timestamp is None:
            timestamp = datetime.now()

        data_point = (device_id, metric_name, value, timestamp)
        self.queue.put(data_point)

    def _writer_loop(self):
        """åå°å†™å…¥å¾ªç¯"""

        conn = psycopg2.connect(self.conn_str)
        cursor = conn.cursor()

        last_flush_time = time.time()

        while True:
            # 1. ä»é˜Ÿåˆ—è·å–æ•°æ®
            try:
                data_point = self.queue.get(timeout=0.1)
                self.buffer.append(data_point)
            except:
                pass

            # 2. åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ·æ–°
            current_time = time.time()
            should_flush = (
                len(self.buffer) >= self.batch_size or
                (current_time - last_flush_time) >= self.flush_interval
            )

            if should_flush and self.buffer:
                try:
                    # æ‰¹é‡å†™å…¥
                    execute_batch(cursor, """
                        INSERT INTO iot_data (device_id, metric_name, value, timestamp)
                        VALUES (%s, %s, %s, %s);
                    """, self.buffer, page_size=self.batch_size)

                    conn.commit()

                    self.logger.info(f"âœ… å†™å…¥ {len(self.buffer)} ä¸ªæ•°æ®ç‚¹")
                    self.buffer = []
                    last_flush_time = current_time

                except Exception as e:
                    self.logger.error(f"âŒ å†™å…¥å¤±è´¥: {e}")
                    conn.rollback()
                    self.buffer = []
```

---

## 2. å®æ—¶èšåˆæ¨¡å—

### 2.1 æ»‘åŠ¨çª—å£èšåˆ

```python
class RealTimeAggregator:
    """å®æ—¶èšåˆè®¡ç®—å™¨"""

    def __init__(self, conn):
        self.conn = conn
        self.cursor = conn.cursor()

    def create_continuous_aggregates(self):
        """åˆ›å»ºè¿ç»­èšåˆè§†å›¾ï¼ˆTimescaleDBï¼‰"""

        # 1åˆ†é’Ÿèšåˆ
        self.cursor.execute("""
            CREATE MATERIALIZED VIEW iot_data_1min
            WITH (timescaledb.continuous) AS
            SELECT
                time_bucket('1 minute', timestamp) AS bucket,
                device_id,
                metric_name,
                AVG(value) AS avg_value,
                MAX(value) AS max_value,
                MIN(value) AS min_value,
                COUNT(*) AS count
            FROM iot_data
            GROUP BY bucket, device_id, metric_name
            WITH NO DATA;

            -- åˆ›å»ºåˆ·æ–°ç­–ç•¥
            SELECT add_continuous_aggregate_policy('iot_data_1min',
                start_offset => INTERVAL '1 hour',
                end_offset => INTERVAL '1 minute',
                schedule_interval => INTERVAL '1 minute');
        """)

        # 1å°æ—¶èšåˆ
        self.cursor.execute("""
            CREATE MATERIALIZED VIEW iot_data_1hour
            WITH (timescaledb.continuous) AS
            SELECT
                time_bucket('1 hour', bucket) AS bucket,
                device_id,
                metric_name,
                AVG(avg_value) AS avg_value,
                MAX(max_value) AS max_value,
                MIN(min_value) AS min_value,
                SUM(count) AS count
            FROM iot_data_1min
            GROUP BY bucket, device_id, metric_name
            WITH NO DATA;

            SELECT add_continuous_aggregate_policy('iot_data_1hour',
                start_offset => INTERVAL '1 day',
                end_offset => INTERVAL '1 hour',
                schedule_interval => INTERVAL '1 hour');
        """)

        self.conn.commit()
        print("âœ… è¿ç»­èšåˆè§†å›¾åˆ›å»ºå®Œæˆ")

    def query_aggregated_data(self, device_id, metric_name, start_time, end_time, granularity='1min'):
        """
        æŸ¥è¯¢èšåˆæ•°æ®

        Args:
            granularity: '1min', '1hour', '1day'
        """

        if granularity == '1min':
            table = 'iot_data_1min'
        elif granularity == '1hour':
            table = 'iot_data_1hour'
        else:
            table = 'iot_data'  # åŸå§‹è¡¨

        query = f"""
            SELECT
                bucket AS timestamp,
                avg_value,
                max_value,
                min_value
            FROM {table}
            WHERE device_id = %s
              AND metric_name = %s
              AND bucket >= %s
              AND bucket < %s
            ORDER BY bucket;
        """

        self.cursor.execute(query, (device_id, metric_name, start_time, end_time))
        return self.cursor.fetchall()
```

---

## 3. æ•°æ®å‹ç¼©æ¨¡å—

### 3.1 è‡ªåŠ¨å‹ç¼©ç­–ç•¥

```python
class DataCompressor:
    """æ•°æ®å‹ç¼©ç®¡ç†å™¨"""

    def __init__(self, conn):
        self.conn = conn
        self.cursor = conn.cursor()

    def setup_compression_policy(self):
        """è®¾ç½®å‹ç¼©ç­–ç•¥ï¼ˆTimescaleDBï¼‰"""

        # å¯ç”¨å‹ç¼©
        self.cursor.execute("""
            ALTER TABLE iot_data SET (
                timescaledb.compress,
                timescaledb.compress_segmentby = 'device_id,metric_name',
                timescaledb.compress_orderby = 'timestamp DESC'
            );
        """)

        # è‡ªåŠ¨å‹ç¼©ç­–ç•¥ï¼ˆ7å¤©åå‹ç¼©ï¼‰
        self.cursor.execute("""
            SELECT add_compression_policy('iot_data', INTERVAL '7 days');
        """)

        self.conn.commit()
        print("âœ… å‹ç¼©ç­–ç•¥è®¾ç½®å®Œæˆ")

    def manual_compress(self, chunk_name):
        """æ‰‹åŠ¨å‹ç¼©æŒ‡å®šchunk"""

        self.cursor.execute(f"""
            SELECT compress_chunk('{chunk_name}');
        """)

        self.conn.commit()
        print(f"âœ… Chunk {chunk_name} å‹ç¼©å®Œæˆ")

    def get_compression_stats(self):
        """è·å–å‹ç¼©ç»Ÿè®¡"""

        self.cursor.execute("""
            SELECT
                chunk_name,
                before_compression_total_bytes,
                after_compression_total_bytes,
                ROUND(100.0 * (1 - after_compression_total_bytes::numeric /
                      before_compression_total_bytes), 2) AS compression_ratio
            FROM timescaledb_information.compressed_chunk_stats
            ORDER BY before_compression_total_bytes DESC
            LIMIT 10;
        """)

        return self.cursor.fetchall()
```

---

## 4. æŸ¥è¯¢APIæ¨¡å—

### 4.1 RESTful API

```python
from flask import Flask, request, jsonify
import psycopg2
from psycopg2.extras import RealDictCursor

app = Flask(__name__)

DB_CONFIG = {
    'dbname': 'iot_db',
    'user': 'postgres',
    'password': 'password',
    'host': 'localhost'
}

def get_connection():
    return psycopg2.connect(**DB_CONFIG, cursor_factory=RealDictCursor)

@app.route('/api/devices/<device_id>/metrics', methods=['GET'])
def get_device_metrics(device_id):
    """
    è·å–è®¾å¤‡æŒ‡æ ‡æ•°æ®

    Query Params:
        metric_name: æŒ‡æ ‡åç§°
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        granularity: ç²’åº¦ (raw/1min/1hour)
    """

    metric_name = request.args.get('metric_name')
    start_time = request.args.get('start_time')
    end_time = request.args.get('end_time')
    granularity = request.args.get('granularity', '1min')

    # é€‰æ‹©è¡¨
    if granularity == 'raw':
        query = """
            SELECT timestamp, value
            FROM iot_data
            WHERE device_id = %s AND metric_name = %s
              AND timestamp >= %s AND timestamp < %s
            ORDER BY timestamp;
        """
    elif granularity == '1min':
        query = """
            SELECT bucket AS timestamp, avg_value AS value
            FROM iot_data_1min
            WHERE device_id = %s AND metric_name = %s
              AND bucket >= %s AND bucket < %s
            ORDER BY bucket;
        """
    else:  # 1hour
        query = """
            SELECT bucket AS timestamp, avg_value AS value
            FROM iot_data_1hour
            WHERE device_id = %s AND metric_name = %s
              AND bucket >= %s AND bucket < %s
            ORDER BY bucket;
        """

    start = time.time()

    try:
        conn = get_connection()
        cursor = conn.cursor()
        cursor.execute(query, (device_id, metric_name, start_time, end_time))
        results = cursor.fetchall()

        duration = (time.time() - start) * 1000

        return jsonify({
            'success': True,
            'data': results,
            'count': len(results),
            'duration_ms': duration
        })

    finally:
        cursor.close()
        conn.close()

@app.route('/api/devices/<device_id>/anomaly', methods=['GET'])
def detect_anomaly(device_id):
    """å¼‚å¸¸æ£€æµ‹"""

    metric_name = request.args.get('metric_name')
    hours = request.args.get('hours', 24, type=int)

    query = """
        WITH stats AS (
            SELECT
                AVG(value) AS mean,
                STDDEV(value) AS stddev
            FROM iot_data
            WHERE device_id = %s
              AND metric_name = %s
              AND timestamp > NOW() - INTERVAL '%s hours'
        ),
        recent_data AS (
            SELECT timestamp, value
            FROM iot_data
            WHERE device_id = %s
              AND metric_name = %s
              AND timestamp > NOW() - INTERVAL '1 hour'
        )
        SELECT
            rd.timestamp,
            rd.value,
            s.mean,
            s.stddev,
            ABS(rd.value - s.mean) / s.stddev AS z_score,
            CASE WHEN ABS(rd.value - s.mean) > 3 * s.stddev
                 THEN true ELSE false END AS is_anomaly
        FROM recent_data rd, stats s
        WHERE ABS(rd.value - s.mean) > 3 * s.stddev
        ORDER BY rd.timestamp DESC;
    """

    try:
        conn = get_connection()
        cursor = conn.cursor()
        cursor.execute(query, (device_id, metric_name, hours, device_id, metric_name))
        results = cursor.fetchall()

        return jsonify({
            'success': True,
            'anomalies': results,
            'count': len(results)
        })

    finally:
        cursor.close()
        conn.close()

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8001)
```

---

## 5. å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

```python
if __name__ == '__main__':
    conn_str = "dbname=iot_db user=postgres host=localhost"

    # 1. å¯åŠ¨æ•°æ®é‡‡é›†
    collector = IoTDataCollector(conn_str, batch_size=10000)

    # æ¨¡æ‹Ÿé‡‡é›†æ•°æ®
    for i in range(1000000):
        device_id = f"device_{i % 1000}"
        metric_name = "temperature"
        value = 20.0 + (i % 100) / 10.0

        collector.collect_data_point(device_id, metric_name, value)

    # 2. åˆ›å»ºèšåˆè§†å›¾
    conn = psycopg2.connect(conn_str)
    aggregator = RealTimeAggregator(conn)
    aggregator.create_continuous_aggregates()

    # 3. è®¾ç½®å‹ç¼©
    compressor = DataCompressor(conn)
    compressor.setup_compression_policy()

    # 4. æŸ¥çœ‹å‹ç¼©æ•ˆæœ
    stats = compressor.get_compression_stats()
    for stat in stats:
        print(f"Chunk: {stat[0]}, å‹ç¼©ç‡: {stat[3]}%")

    print("âœ… IoTç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
```

---

**å®Œæˆæ—¥æœŸ**: 2025-12-04
**ä»£ç è¡Œæ•°**: ~600è¡Œ
**æ€§èƒ½**: 1M+ points/ç§’

**è¿”å›**: [æ¡ˆä¾‹3ä¸»é¡µ](./README.md)
