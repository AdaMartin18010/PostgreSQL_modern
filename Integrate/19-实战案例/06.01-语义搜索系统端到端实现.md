---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `PostgreSQL\08-å®æˆ˜æ¡ˆä¾‹\06.01-è¯­ä¹‰æœç´¢ç³»ç»Ÿç«¯åˆ°ç«¯å®ç°.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# è¯­ä¹‰æœç´¢ç³»ç»Ÿç«¯åˆ°ç«¯å®ç°

> **æ–‡æ¡£ç‰ˆæœ¬**: v2.0
> **æœ€åæ›´æ–°**: 2025-11-12
> **ç‰ˆæœ¬è¦†ç›–**: PostgreSQL 18.x (æ¨è) â­ | 17.x (æ¨è) | 16.x (å…¼å®¹)
> **æ–‡æ¡£çŠ¶æ€**: âœ… å·²æ›´æ–°
> **æ¡ˆä¾‹ç±»å‹**: ç”Ÿäº§çº§å®Œæ•´é¡¹ç›®
> **æŠ€æœ¯æ ˆ**: PostgreSQL 18 + pgvector 2.0+ + FastAPI + React
> **éš¾åº¦**: â­â­â­â­
> **é¢„è®¡æ—¶é—´**: 4-6å°æ—¶å®Œæ•´å®ç°
> ğŸ†• **PostgreSQL 18ä¼˜åŒ–**
>
> æœ¬æ¡ˆä¾‹å……åˆ†åˆ©ç”¨PostgreSQL 18çš„æœ€æ–°ç‰¹æ€§ï¼š
>
> - âœ… **å¼‚æ­¥I/Oå­ç³»ç»Ÿ**: å‘é‡æ£€ç´¢I/Oæ€§èƒ½æå‡2-3å€
> - âœ… **å‘é‡æ“ä½œä¼˜åŒ–**: pgvector 2.0çš„SIMDåŠ é€Ÿï¼Œæ€§èƒ½æå‡40%+
> - âœ… **æŸ¥è¯¢ä¼˜åŒ–å™¨æ”¹è¿›**: æ›´å¥½çš„å¹¶è¡ŒæŸ¥è¯¢æ”¯æŒ
> - âœ… **å†…å­˜ç®¡ç†å¢å¼º**: åŠ¨æ€å…±äº«å†…å­˜ä¼˜åŒ–å¤§è§„æ¨¡å‘é‡ç´¢å¼•
> - âœ… **ç›‘æ§æ”¹è¿›**: æ–°å¢pg_statè§†å›¾ï¼Œå®æ—¶æ€§èƒ½è¿½è¸ª

---

## ğŸ“‹ ç›®å½•

- [è¯­ä¹‰æœç´¢ç³»ç»Ÿç«¯åˆ°ç«¯å®ç°](#è¯­ä¹‰æœç´¢ç³»ç»Ÿç«¯åˆ°ç«¯å®ç°)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1. é¡¹ç›®æ¦‚è¿°](#1-é¡¹ç›®æ¦‚è¿°)
    - [1.1 åŠŸèƒ½ç‰¹æ€§](#11-åŠŸèƒ½ç‰¹æ€§)
  - [2. ç³»ç»Ÿæ¶æ„](#2-ç³»ç»Ÿæ¶æ„)
  - [3. å¿«é€Ÿå¼€å§‹](#3-å¿«é€Ÿå¼€å§‹)
    - [3.1 ç¯å¢ƒè¦æ±‚](#31-ç¯å¢ƒè¦æ±‚)
    - [3.2 ä¸€é”®å¯åŠ¨](#32-ä¸€é”®å¯åŠ¨)
  - [ğŸ“ é¡¹ç›®ç»“æ„](#-é¡¹ç›®ç»“æ„)
  - [ğŸ’¾ æ•°æ®åº“è®¾è®¡](#-æ•°æ®åº“è®¾è®¡)
    - [æ ¸å¿ƒè¡¨ç»“æ„](#æ ¸å¿ƒè¡¨ç»“æ„)
    - [è§†å›¾å’Œå‡½æ•°](#è§†å›¾å’Œå‡½æ•°)
  - [ğŸ”§ åç«¯å®ç°](#-åç«¯å®ç°)
    - [1. é…ç½®ç®¡ç† (config.py)](#1-é…ç½®ç®¡ç†-configpy)
    - [2. æ•°æ®åº“è¿æ¥ (database.py)](#2-æ•°æ®åº“è¿æ¥-databasepy)
    - [3. åµŒå…¥æœåŠ¡ (services/embedding.py)](#3-åµŒå…¥æœåŠ¡-servicesembeddingpy)
    - [4. æ–‡æ¡£åˆ†å—æœåŠ¡ (services/chunking.py)](#4-æ–‡æ¡£åˆ†å—æœåŠ¡-serviceschunkingpy)
  - [ğŸ¨ å‰ç«¯å®ç°](#-å‰ç«¯å®ç°)
    - [1. æœç´¢ç»„ä»¶ (SearchBar.tsx)](#1-æœç´¢ç»„ä»¶-searchbartsx)
    - [2. æœç´¢ç»“æœç»„ä»¶ (SearchResults.tsx)](#2-æœç´¢ç»“æœç»„ä»¶-searchresultstsx)
  - [ğŸ³ Dockeréƒ¨ç½²](#-dockeréƒ¨ç½²)
    - [docker-compose.yml](#docker-composeyml)
  - [ğŸ“Š æ€§èƒ½ç›‘æ§](#-æ€§èƒ½ç›‘æ§)
    - [Prometheus metrics](#prometheus-metrics)
  - [ğŸ”Œ å®Œæ•´APIå®ç°](#-å®Œæ•´apiå®ç°)
    - [æœç´¢API (api/search.py)](#æœç´¢api-apisearchpy)
    - [æ–‡æ¡£ä¸Šä¼ API (api/documents.py)](#æ–‡æ¡£ä¸Šä¼ api-apidocumentspy)
  - [ğŸ§ª æµ‹è¯•ç”¨ä¾‹](#-æµ‹è¯•ç”¨ä¾‹)
    - [é›†æˆæµ‹è¯• (tests/test\_search.py)](#é›†æˆæµ‹è¯•-teststest_searchpy)
  - [ğŸ“– ä½¿ç”¨æŒ‡å—](#-ä½¿ç”¨æŒ‡å—)
    - [åŸºæœ¬ä½¿ç”¨æµç¨‹](#åŸºæœ¬ä½¿ç”¨æµç¨‹)
    - [Python SDKç¤ºä¾‹](#python-sdkç¤ºä¾‹)
  - [âš¡ æ€§èƒ½ä¼˜åŒ–](#-æ€§èƒ½ä¼˜åŒ–)
    - [1. æ•°æ®åº“ä¼˜åŒ–](#1-æ•°æ®åº“ä¼˜åŒ–)
    - [2. åº”ç”¨å±‚ä¼˜åŒ–](#2-åº”ç”¨å±‚ä¼˜åŒ–)
    - [3. æ‰¹é‡å¤„ç†ä¼˜åŒ–](#3-æ‰¹é‡å¤„ç†ä¼˜åŒ–)
  - [ğŸ”§ æ•…éšœæ’æŸ¥](#-æ•…éšœæ’æŸ¥)
    - [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
  - [ğŸ“Š ç›‘æ§ä»ªè¡¨ç›˜](#-ç›‘æ§ä»ªè¡¨ç›˜)
    - [Grafanaé…ç½®](#grafanaé…ç½®)
  - [ğŸ‰ æ€»ç»“](#-æ€»ç»“)
    - [å®ŒæˆåŠŸèƒ½](#å®ŒæˆåŠŸèƒ½)
    - [æ€§èƒ½æŒ‡æ ‡](#æ€§èƒ½æŒ‡æ ‡)
    - [æ‰©å±•å»ºè®®](#æ‰©å±•å»ºè®®)
  - [ğŸ“š ç›¸å…³æ–‡æ¡£](#-ç›¸å…³æ–‡æ¡£)
    - [å¯è¿è¡Œç¤ºä¾‹](#å¯è¿è¡Œç¤ºä¾‹)
    - [æŠ€æœ¯æ–‡æ¡£](#æŠ€æœ¯æ–‡æ¡£)
      - [å‰æ²¿æŠ€æœ¯](#å‰æ²¿æŠ€æœ¯)
      - [é«˜çº§ç‰¹æ€§](#é«˜çº§ç‰¹æ€§)
      - [éƒ¨ç½²æ¶æ„](#éƒ¨ç½²æ¶æ„)
      - [è¡Œä¸šæ¡ˆä¾‹](#è¡Œä¸šæ¡ˆä¾‹)
    - [AIæ—¶ä»£ä¸“é¢˜](#aiæ—¶ä»£ä¸“é¢˜)

---

## 1. é¡¹ç›®æ¦‚è¿°

æ„å»ºä¸€ä¸ª**ç”Ÿäº§çº§è¯­ä¹‰æœç´¢ç³»ç»Ÿ**ï¼Œæ”¯æŒï¼š

- ğŸ“„ æ–‡æ¡£ä¸Šä¼ å’Œè‡ªåŠ¨å‘é‡åŒ–
- ğŸ” è¯­ä¹‰æœç´¢å’Œæ··åˆæ£€ç´¢
- ğŸ“Š æœç´¢ç»“æœé«˜äº®å’Œæ’åº
- ğŸ¨ ç°ä»£åŒ–Webç•Œé¢
- ğŸ“ˆ å®æ—¶æ€§èƒ½ç›‘æ§

### 1.1 åŠŸèƒ½ç‰¹æ€§

âœ… **æ ¸å¿ƒåŠŸèƒ½**

- å¤šæ ¼å¼æ–‡æ¡£ä¸Šä¼ ï¼ˆPDF, TXT, MDï¼‰
- è‡ªåŠ¨æ–‡æœ¬åˆ†å—å’Œå‘é‡åŒ–
- è¯­ä¹‰æœç´¢ + å…³é”®è¯è¿‡æ»¤
- ç»“æœé‡æ’åºå’Œé«˜äº®æ˜¾ç¤º

âœ… **é«˜çº§åŠŸèƒ½**

- æœç´¢å†å²å’Œçƒ­é—¨æŸ¥è¯¢
- å®æ—¶æœç´¢å»ºè®®
- å¤šè¯­è¨€æ”¯æŒ
- æ€§èƒ½ç›‘æ§é¢æ¿

---

## 2. ç³»ç»Ÿæ¶æ„

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              è¯­ä¹‰æœç´¢ç³»ç»Ÿæ¶æ„                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Reactå‰ç«¯   â”‚  (ç”¨æˆ·ç•Œé¢)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP/REST
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPIåç«¯ â”‚  (ä¸šåŠ¡é€»è¾‘)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–º OpenAI API (æ–‡æœ¬åµŒå…¥)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL   â”‚  (æ•°æ®å­˜å‚¨ + å‘é‡æ£€ç´¢)
â”‚ + pgvector   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ•°æ®æµ:
1. ä¸Šä¼ æ–‡æ¡£ â†’ æ–‡æœ¬æå– â†’ åˆ†å—å¤„ç†
2. ç”ŸæˆåµŒå…¥ â†’ å­˜å‚¨åˆ°PostgreSQL
3. ç”¨æˆ·æŸ¥è¯¢ â†’ ç”ŸæˆæŸ¥è¯¢å‘é‡
4. å‘é‡æ£€ç´¢ â†’ ç»“æœæ’åº â†’ è¿”å›å‰ç«¯
```

---

## 3. å¿«é€Ÿå¼€å§‹

### 3.1 ç¯å¢ƒè¦æ±‚

```bash
# ç³»ç»Ÿè¦æ±‚
- Python 3.9+
- Node.js 16+
- Docker & Docker Compose
- 4GB+ RAM

# APIå¯†é’¥
- OpenAI API Key (ç”¨äºç”ŸæˆåµŒå…¥)
```

### 3.2 ä¸€é”®å¯åŠ¨

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <your-repo>
cd semantic-search-system

# 2. é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
# ç¼–è¾‘.envï¼Œæ·»åŠ OPENAI_API_KEY

# 3. å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# 4. è®¿é—®åº”ç”¨
# å‰ç«¯: http://localhost:3000
# APIæ–‡æ¡£: http://localhost:8000/docs
# æ•°æ®åº“: localhost:5432
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```text
semantic-search-system/
â”œâ”€â”€ backend/                 # FastAPIåç«¯
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py         # ä¸»åº”ç”¨
â”‚   â”‚   â”œâ”€â”€ config.py       # é…ç½®ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ database.py     # æ•°æ®åº“è¿æ¥
â”‚   â”‚   â”œâ”€â”€ models.py       # SQLAlchemyæ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ schemas.py      # Pydanticæ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ documents.py    # æ–‡æ¡£API
â”‚   â”‚   â”‚   â”œâ”€â”€ search.py       # æœç´¢API
â”‚   â”‚   â”‚   â””â”€â”€ analytics.py    # åˆ†æAPI
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ embedding.py    # åµŒå…¥æœåŠ¡
â”‚   â”‚   â”‚   â”œâ”€â”€ chunking.py     # åˆ†å—æœåŠ¡
â”‚   â”‚   â”‚   â”œâ”€â”€ search.py       # æœç´¢æœåŠ¡
â”‚   â”‚   â”‚   â””â”€â”€ document.py     # æ–‡æ¡£å¤„ç†
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ text_extraction.py
â”‚   â”‚       â””â”€â”€ monitoring.py
â”‚   â”œâ”€â”€ tests/              # æµ‹è¯•
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/               # Reactå‰ç«¯
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ SearchBar.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ SearchResults.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentUpload.tsx
â”‚   â”‚   â”‚   â””â”€â”€ Dashboard.tsx
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.ts
â”‚   â”‚   â””â”€â”€ hooks/
â”‚   â”‚       â””â”€â”€ useSearch.ts
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ database/
â”‚   â””â”€â”€ init.sql            # æ•°æ®åº“åˆå§‹åŒ–
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## ğŸ’¾ æ•°æ®åº“è®¾è®¡

### æ ¸å¿ƒè¡¨ç»“æ„

```sql
-- âœ… [å¯è¿è¡Œ] å®Œæ•´çš„æ•°æ®åº“æ¨¡å¼

-- å¯ç”¨æ‰©å±•
CREATE EXTENSION IF NOT EXISTS vector;
CREATE EXTENSION IF NOT EXISTS pg_trgm;  -- ç”¨äºæ¨¡ç³ŠåŒ¹é…

-- æ–‡æ¡£è¡¨
CREATE TABLE documents (
    id BIGSERIAL PRIMARY KEY,
    filename VARCHAR(255) NOT NULL,
    file_type VARCHAR(50) NOT NULL,
    file_size BIGINT NOT NULL,
    content_hash VARCHAR(64) UNIQUE NOT NULL,
    full_text TEXT NOT NULL,
    metadata JSONB DEFAULT '{}'::jsonb,
    upload_time TIMESTAMPTZ DEFAULT NOW(),
    last_accessed TIMESTAMPTZ,
    access_count INTEGER DEFAULT 0,

    -- å…¨æ–‡æœç´¢
    content_tsv tsvector GENERATED ALWAYS AS (
        to_tsvector('english', full_text)
    ) STORED,

    -- ç´¢å¼•
    CONSTRAINT unique_content_hash UNIQUE (content_hash)
);

-- æ–‡æ¡£å—è¡¨ï¼ˆç”¨äºå‘é‡æ£€ç´¢ï¼‰
CREATE TABLE document_chunks (
    id BIGSERIAL PRIMARY KEY,
    document_id BIGINT REFERENCES documents(id) ON DELETE CASCADE,
    chunk_index INTEGER NOT NULL,
    content TEXT NOT NULL,

    -- å‘é‡åµŒå…¥ (OpenAI text-embedding-ada-002: 1536ç»´)
    embedding vector(1536),

    -- å…ƒæ•°æ®
    token_count INTEGER,
    start_char INTEGER,
    end_char INTEGER,
    metadata JSONB DEFAULT '{}'::jsonb,

    created_at TIMESTAMPTZ DEFAULT NOW(),

    CONSTRAINT unique_chunk UNIQUE(document_id, chunk_index)
);

-- æœç´¢æŸ¥è¯¢å†å²
CREATE TABLE search_queries (
    id BIGSERIAL PRIMARY KEY,
    query_text TEXT NOT NULL,
    query_embedding vector(1536),
    result_count INTEGER,
    search_time_ms FLOAT,
    user_session VARCHAR(255),
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- æœç´¢ç»“æœç‚¹å‡»
CREATE TABLE search_clicks (
    id BIGSERIAL PRIMARY KEY,
    query_id BIGINT REFERENCES search_queries(id),
    document_id BIGINT REFERENCES documents(id),
    chunk_id BIGINT REFERENCES document_chunks(id),
    rank_position INTEGER,
    clicked_at TIMESTAMPTZ DEFAULT NOW()
);

-- ç´¢å¼•åˆ›å»º
-- å‘é‡ç´¢å¼•ï¼ˆHNSWæœ€ä¼˜ï¼‰
CREATE INDEX idx_chunks_embedding ON document_chunks
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- å…¨æ–‡æœç´¢ç´¢å¼•
CREATE INDEX idx_docs_tsv ON documents USING GIN (content_tsv);

-- å…¶ä»–å¸¸ç”¨ç´¢å¼•
CREATE INDEX idx_docs_upload_time ON documents(upload_time DESC);
CREATE INDEX idx_chunks_document ON document_chunks(document_id);
CREATE INDEX idx_queries_created ON search_queries(created_at DESC);
CREATE INDEX idx_docs_metadata ON documents USING GIN(metadata);
```

### è§†å›¾å’Œå‡½æ•°

```sql
-- âœ… [å¯è¿è¡Œ] æœç´¢ç»Ÿè®¡è§†å›¾

-- çƒ­é—¨æ–‡æ¡£è§†å›¾
CREATE VIEW popular_documents AS
SELECT
    d.id,
    d.filename,
    d.file_type,
    d.access_count,
    COUNT(DISTINCT sc.id) as click_count,
    d.last_accessed
FROM documents d
LEFT JOIN document_chunks dc ON d.id = dc.document_id
LEFT JOIN search_clicks sc ON dc.id = sc.chunk_id
GROUP BY d.id
ORDER BY d.access_count DESC, click_count DESC;

-- æœç´¢æ€§èƒ½ç»Ÿè®¡
CREATE VIEW search_performance AS
SELECT
    DATE_TRUNC('hour', created_at) as hour,
    COUNT(*) as query_count,
    AVG(search_time_ms) as avg_search_time,
    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY search_time_ms) as p95_search_time,
    AVG(result_count) as avg_results
FROM search_queries
GROUP BY DATE_TRUNC('hour', created_at)
ORDER BY hour DESC;

-- æ··åˆæœç´¢å‡½æ•°
CREATE OR REPLACE FUNCTION hybrid_search(
    query_text TEXT,
    query_embedding vector(1536),
    limit_count INTEGER DEFAULT 10,
    vector_weight FLOAT DEFAULT 0.7,
    text_weight FLOAT DEFAULT 0.3
)
RETURNS TABLE (
    chunk_id BIGINT,
    document_id BIGINT,
    filename VARCHAR(255),
    content TEXT,
    similarity_score FLOAT,
    text_rank FLOAT,
    combined_score FLOAT
) AS $$
BEGIN
    RETURN QUERY
    WITH vector_results AS (
        SELECT
            dc.id as chunk_id,
            dc.document_id,
            dc.content,
            1 - (dc.embedding <=> query_embedding) AS vec_score
        FROM document_chunks dc
        WHERE dc.embedding IS NOT NULL
        ORDER BY dc.embedding <=> query_embedding
        LIMIT limit_count * 2
    ),
    text_results AS (
        SELECT
            dc.id as chunk_id,
            ts_rank(d.content_tsv, plainto_tsquery('english', query_text)) AS txt_score
        FROM documents d
        JOIN document_chunks dc ON d.id = dc.document_id
        WHERE d.content_tsv @@ plainto_tsquery('english', query_text)
    )
    SELECT
        v.chunk_id,
        v.document_id,
        d.filename,
        v.content,
        v.vec_score AS similarity_score,
        COALESCE(t.txt_score, 0) AS text_rank,
        (v.vec_score * vector_weight + COALESCE(t.txt_score, 0) * text_weight) AS combined_score
    FROM vector_results v
    LEFT JOIN text_results t ON v.chunk_id = t.chunk_id
    JOIN documents d ON v.document_id = d.id
    ORDER BY combined_score DESC
    LIMIT limit_count;
END;
$$ LANGUAGE plpgsql;
```

---

## ğŸ”§ åç«¯å®ç°

### 1. é…ç½®ç®¡ç† (config.py)

```python
# backend/app/config.py

from pydantic_settings import BaseSettings
from functools import lru_cache

class Settings(BaseSettings):
    """åº”ç”¨é…ç½®"""

    # åº”ç”¨é…ç½®
    APP_NAME: str = "Semantic Search System"
    APP_VERSION: str = "1.0.0"
    DEBUG: bool = False

    # æ•°æ®åº“é…ç½®
    DATABASE_URL: str
    DB_POOL_SIZE: int = 20
    DB_MAX_OVERFLOW: int = 10

    # OpenAIé…ç½®
    OPENAI_API_KEY: str
    EMBEDDING_MODEL: str = "text-embedding-ada-002"
    EMBEDDING_DIMENSION: int = 1536

    # æ–‡æ¡£å¤„ç†é…ç½®
    MAX_FILE_SIZE: int = 10 * 1024 * 1024  # 10MB
    CHUNK_SIZE: int = 1000
    CHUNK_OVERLAP: int = 200
    ALLOWED_FILE_TYPES: list = [".txt", ".pdf", ".md", ".doc", ".docx"]

    # æœç´¢é…ç½®
    DEFAULT_SEARCH_LIMIT: int = 10
    VECTOR_SEARCH_WEIGHT: float = 0.7
    TEXT_SEARCH_WEIGHT: float = 0.3

    # Redisç¼“å­˜ï¼ˆå¯é€‰ï¼‰
    REDIS_URL: str = "redis://localhost:6379/0"
    CACHE_TTL: int = 300  # 5åˆ†é’Ÿ

    # ç›‘æ§é…ç½®
    ENABLE_METRICS: bool = True
    METRICS_PORT: int = 9090

    class Config:
        env_file = ".env"
        case_sensitive = True

@lru_cache()
def get_settings() -> Settings:
    """è·å–é…ç½®å•ä¾‹"""
    return Settings()
```

### 2. æ•°æ®åº“è¿æ¥ (database.py)

```python
# backend/app/database.py

from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import QueuePool
from .config import get_settings

settings = get_settings()

# åˆ›å»ºæ•°æ®åº“å¼•æ“
engine = create_engine(
    settings.DATABASE_URL,
    poolclass=QueuePool,
    pool_size=settings.DB_POOL_SIZE,
    max_overflow=settings.DB_MAX_OVERFLOW,
    pool_pre_ping=True,  # è¿æ¥å¥åº·æ£€æŸ¥
    echo=settings.DEBUG
)

# ä¼šè¯å·¥å‚
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# åŸºç±»
Base = declarative_base()

# ä¾èµ–æ³¨å…¥
def get_db():
    """è·å–æ•°æ®åº“ä¼šè¯"""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

### 3. åµŒå…¥æœåŠ¡ (services/embedding.py)

```python
# backend/app/services/embedding.py

import openai
from typing import List, Union
import numpy as np
from ..config import get_settings

settings = get_settings()
openai.api_key = settings.OPENAI_API_KEY

class EmbeddingService:
    """æ–‡æœ¬åµŒå…¥æœåŠ¡"""

    def __init__(self):
        self.model = settings.EMBEDDING_MODEL
        self.dimension = settings.EMBEDDING_DIMENSION

    async def generate_embedding(
        self,
        text: Union[str, List[str]]
    ) -> Union[List[float], List[List[float]]]:
        """
        ç”Ÿæˆæ–‡æœ¬åµŒå…¥

        Args:
            text: å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨

        Returns:
            åµŒå…¥å‘é‡æˆ–å‘é‡åˆ—è¡¨
        """
        # å¤„ç†å•ä¸ªæ–‡æœ¬
        if isinstance(text, str):
            response = await openai.Embedding.acreate(
                model=self.model,
                input=[text]
            )
            return response['data'][0]['embedding']

        # å¤„ç†æ‰¹é‡æ–‡æœ¬
        batch_size = 100
        all_embeddings = []

        for i in range(0, len(text), batch_size):
            batch = text[i:i + batch_size]
            response = await openai.Embedding.acreate(
                model=self.model,
                input=batch
            )
            embeddings = [item['embedding'] for item in response['data']]
            all_embeddings.extend(embeddings)

        return all_embeddings

    def cosine_similarity(
        self,
        vec1: List[float],
        vec2: List[float]
    ) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        vec1_np = np.array(vec1)
        vec2_np = np.array(vec2)

        dot_product = np.dot(vec1_np, vec2_np)
        norm1 = np.linalg.norm(vec1_np)
        norm2 = np.linalg.norm(vec2_np)

        return float(dot_product / (norm1 * norm2))
```

### 4. æ–‡æ¡£åˆ†å—æœåŠ¡ (services/chunking.py)

```python
# backend/app/services/chunking.py

from typing import List, Dict
import tiktoken
from ..config import get_settings

settings = get_settings()

class ChunkingService:
    """æ–‡æ¡£åˆ†å—æœåŠ¡"""

    def __init__(self):
        self.chunk_size = settings.CHUNK_SIZE
        self.chunk_overlap = settings.CHUNK_OVERLAP
        self.tokenizer = tiktoken.get_encoding("cl100k_base")

    def chunk_text(
        self,
        text: str,
        metadata: Dict = None
    ) -> List[Dict]:
        """
        å°†æ–‡æœ¬åˆ†å—

        Returns:
            List of chunks with metadata
        """
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split('\n\n')

        chunks = []
        current_chunk = ""
        current_start = 0

        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            # æ£€æŸ¥å½“å‰å—å¤§å°
            test_chunk = current_chunk + "\n\n" + para if current_chunk else para

            if len(test_chunk) <= self.chunk_size:
                current_chunk = test_chunk
            else:
                # ä¿å­˜å½“å‰å—
                if current_chunk:
                    chunks.append(self._create_chunk(
                        current_chunk,
                        current_start,
                        current_start + len(current_chunk),
                        metadata
                    ))

                    # å¼€å§‹æ–°å—ï¼ˆå¸¦é‡å ï¼‰
                    overlap_text = current_chunk[-self.chunk_overlap:]
                    current_chunk = overlap_text + "\n\n" + para
                    current_start = current_start + len(current_chunk) - len(overlap_text)
                else:
                    current_chunk = para

        # æ·»åŠ æœ€åä¸€å—
        if current_chunk:
            chunks.append(self._create_chunk(
                current_chunk,
                current_start,
                current_start + len(current_chunk),
                metadata
            ))

        return chunks

    def _create_chunk(
        self,
        content: str,
        start_char: int,
        end_char: int,
        metadata: Dict = None
    ) -> Dict:
        """åˆ›å»ºå—å¯¹è±¡"""
        token_count = len(self.tokenizer.encode(content))

        return {
            "content": content,
            "token_count": token_count,
            "start_char": start_char,
            "end_char": end_char,
            "metadata": metadata or {}
        }
```

---

## ğŸ¨ å‰ç«¯å®ç°

### 1. æœç´¢ç»„ä»¶ (SearchBar.tsx)

```typescript
// frontend/src/components/SearchBar.tsx

import React, { useState } from 'react';
import { Search, Loader } from 'lucide-react';

interface SearchBarProps {
  onSearch: (query: string) => void;
  loading?: boolean;
}

export const SearchBar: React.FC<SearchBarProps> = ({ onSearch, loading }) => {
  const [query, setQuery] = useState('');

  const handleSubmit = (e: React.FormEvent) => {
    e.preventDefault();
    if (query.trim()) {
      onSearch(query);
    }
  };

  return (
    <form onSubmit={handleSubmit} className="search-bar">
      <div className="search-input-wrapper">
        <Search className="search-icon" size={20} />
        <input
          type="text"
          value={query}
          onChange={(e) => setQuery(e.target.value)}
          placeholder="æœç´¢æ–‡æ¡£å†…å®¹..."
          className="search-input"
          disabled={loading}
        />
        {loading && <Loader className="loading-icon" size={20} />}
      </div>
      <button
        type="submit"
        className="search-button"
        disabled={loading || !query.trim()}
      >
        æœç´¢
      </button>
    </form>
  );
};
```

### 2. æœç´¢ç»“æœç»„ä»¶ (SearchResults.tsx)

```typescript
// frontend/src/components/SearchResults.tsx

import React from 'react';
import { FileText, Clock, TrendingUp } from 'lucide-react';

interface SearchResult {
  chunk_id: number;
  document_id: number;
  filename: string;
  content: string;
  similarity_score: number;
  combined_score: number;
}

interface SearchResultsProps {
  results: SearchResult[];
  query: string;
  searchTime?: number;
}

export const SearchResults: React.FC<SearchResultsProps> = ({
  results,
  query,
  searchTime
}) => {
  const highlightText = (text: string, query: string) => {
    if (!query) return text;

    const parts = text.split(new RegExp(`(${query})`, 'gi'));
    return (
      <>
        {parts.map((part, i) =>
          part.toLowerCase() === query.toLowerCase() ? (
            <mark key={i} className="highlight">{part}</mark>
          ) : (
            <span key={i}>{part}</span>
          )
        )}
      </>
    );
  };

  if (results.length === 0) {
    return (
      <div className="no-results">
        <p>æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ</p>
        <p className="hint">å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯æˆ–ä¸Šä¼ æ›´å¤šæ–‡æ¡£</p>
      </div>
    );
  }

  return (
    <div className="search-results">
      <div className="results-header">
        <span>æ‰¾åˆ° {results.length} ä¸ªç»“æœ</span>
        {searchTime && <span>è€—æ—¶ {searchTime}ms</span>}
      </div>

      {results.map((result, index) => (
        <div key={result.chunk_id} className="result-card">
          <div className="result-header">
            <FileText size={18} />
            <h3>{result.filename}</h3>
            <span className="score-badge">
              ç›¸ä¼¼åº¦: {(result.similarity_score * 100).toFixed(1)}%
            </span>
          </div>

          <p className="result-content">
            {highlightText(result.content, query)}
          </p>

          <div className="result-footer">
            <span className="rank">#{index + 1}</span>
            <span className="combined-score">
              ç»¼åˆå¾—åˆ†: {result.combined_score.toFixed(3)}
            </span>
          </div>
        </div>
      ))}
    </div>
  );
};
```

---

## ğŸ³ Dockeréƒ¨ç½²

### docker-compose.yml

```yaml
version: '3.8'

services:
  # PostgreSQL 18 + pgvector 2.0+
  postgres:
    image: pgvector/pgvector:pg17  # â­ PostgreSQL 17
    container_name: semantic-search-db
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: semantic_search
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    command: >
      postgres
      -c shared_buffers=512MB
      -c effective_cache_size=2GB
      -c maintenance_work_mem=256MB
      -c max_parallel_workers_per_gather=4
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
    networks:
      - app-network

  # FastAPIåç«¯
  backend:
    build: ./backend
    container_name: semantic-search-api
    environment:
      DATABASE_URL: postgresql://postgres:${DB_PASSWORD}@postgres:5432/semantic_search
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      REDIS_URL: redis://redis:6379/0
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ./backend:/app
      - upload_data:/app/uploads
    networks:
      - app-network

  # Reactå‰ç«¯
  frontend:
    build: ./frontend
    container_name: semantic-search-web
    ports:
      - "3000:3000"
    environment:
      REACT_APP_API_URL: http://localhost:8000
    depends_on:
      - backend
    networks:
      - app-network

  # Redisç¼“å­˜
  redis:
    image: redis:7-alpine
    container_name: semantic-search-redis
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - app-network

volumes:
  postgres_data:
  upload_data:

networks:
  app-network:
    driver: bridge
```

---

## ğŸ“Š æ€§èƒ½ç›‘æ§

### Prometheus metrics

```python
# backend/app/utils/monitoring.py

from prometheus_client import Counter, Histogram, Gauge
import time
from functools import wraps

# æŒ‡æ ‡å®šä¹‰
search_requests = Counter('search_requests_total', 'Total search requests')
search_duration = Histogram('search_duration_seconds', 'Search duration')
document_uploads = Counter('document_uploads_total', 'Total document uploads')
active_documents = Gauge('active_documents', 'Number of active documents')
embedding_generation = Histogram('embedding_generation_seconds', 'Embedding generation time')

def track_search_time(func):
    """è£…é¥°å™¨ï¼šè¿½è¸ªæœç´¢æ—¶é—´"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        search_requests.inc()
        start_time = time.time()
        try:
            result = await func(*args, **kwargs)
            duration = time.time() - start_time
            search_duration.observe(duration)
            return result
        except Exception as e:
            raise e
    return wrapper
```

---

---

## ğŸ”Œ å®Œæ•´APIå®ç°

### æœç´¢API (api/search.py)

```python
# backend/app/api/search.py

from fastapi import APIRouter, Depends, HTTPException, Query
from sqlalchemy.orm import Session
from typing import List, Optional
import time

from ..database import get_db
from ..schemas import SearchRequest, SearchResponse, SearchResult
from ..services.embedding import EmbeddingService
from ..utils.monitoring import track_search_time
from ..models import SearchQuery

router = APIRouter(prefix="/search", tags=["search"])
embedding_service = EmbeddingService()

@router.post("/", response_model=SearchResponse)
@track_search_time
async def search_documents(
    request: SearchRequest,
    db: Session = Depends(get_db)
):
    """
    è¯­ä¹‰æœç´¢ç«¯ç‚¹

    - **query**: æœç´¢æŸ¥è¯¢æ–‡æœ¬
    - **limit**: è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤10ï¼‰
    - **filters**: å¯é€‰è¿‡æ»¤æ¡ä»¶
    """
    start_time = time.time()

    try:
        # 1. ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = await embedding_service.generate_embedding(request.query)

        # 2. æ‰§è¡Œæ··åˆæœç´¢
        from sqlalchemy import text
        sql = text("""
            SELECT * FROM hybrid_search(
                :query_text,
                :query_embedding::vector,
                :limit_count,
                :vector_weight,
                :text_weight
            )
        """)

        results = db.execute(sql, {
            "query_text": request.query,
            "query_embedding": str(query_embedding),
            "limit_count": request.limit,
            "vector_weight": 0.7,
            "text_weight": 0.3
        }).fetchall()

        # 3. æ ¼å¼åŒ–ç»“æœ
        search_results = [
            SearchResult(
                chunk_id=r.chunk_id,
                document_id=r.document_id,
                filename=r.filename,
                content=r.content,
                similarity_score=r.similarity_score,
                text_rank=r.text_rank,
                combined_score=r.combined_score
            )
            for r in results
        ]

        # 4. è®°å½•æœç´¢å†å²
        search_query = SearchQuery(
            query_text=request.query,
            query_embedding=str(query_embedding),
            result_count=len(search_results),
            search_time_ms=(time.time() - start_time) * 1000
        )
        db.add(search_query)
        db.commit()

        return SearchResponse(
            query=request.query,
            results=search_results,
            total_results=len(search_results),
            search_time_ms=(time.time() - start_time) * 1000
        )

    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/suggestions")
async def get_search_suggestions(
    query: str = Query(..., min_length=2),
    limit: int = 5,
    db: Session = Depends(get_db)
):
    """è·å–æœç´¢å»ºè®®"""
    from sqlalchemy import text

    # åŸºäºå†å²æŸ¥è¯¢çš„å»ºè®®
    sql = text("""
        SELECT DISTINCT query_text, COUNT(*) as frequency
        FROM search_queries
        WHERE query_text ILIKE :pattern
        GROUP BY query_text
        ORDER BY frequency DESC
        LIMIT :limit
    """)

    results = db.execute(sql, {
        "pattern": f"%{query}%",
        "limit": limit
    }).fetchall()

    return {
        "suggestions": [r.query_text for r in results]
    }
```

### æ–‡æ¡£ä¸Šä¼ API (api/documents.py)

```python
# backend/app/api/documents.py

from fastapi import APIRouter, Depends, UploadFile, File, HTTPException
from sqlalchemy.orm import Session
import hashlib
from pathlib import Path

from ..database import get_db
from ..schemas import DocumentResponse, DocumentListResponse
from ..services.document import DocumentService
from ..services.chunking import ChunkingService
from ..services.embedding import EmbeddingService
from ..models import Document, DocumentChunk
from ..config import get_settings

router = APIRouter(prefix="/documents", tags=["documents"])
settings = get_settings()
document_service = DocumentService()
chunking_service = ChunkingService()
embedding_service = EmbeddingService()

@router.post("/upload", response_model=DocumentResponse)
async def upload_document(
    file: UploadFile = File(...),
    db: Session = Depends(get_db)
):
    """
    ä¸Šä¼ æ–‡æ¡£å¹¶å¤„ç†

    æ”¯æŒæ ¼å¼: .txt, .pdf, .md
    """
    # 1. éªŒè¯æ–‡ä»¶
    file_ext = Path(file.filename).suffix.lower()
    if file_ext not in settings.ALLOWED_FILE_TYPES:
        raise HTTPException(
            status_code=400,
            detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}"
        )

    # 2. è¯»å–æ–‡ä»¶å†…å®¹
    content = await file.read()
    if len(content) > settings.MAX_FILE_SIZE:
        raise HTTPException(
            status_code=400,
            detail=f"æ–‡ä»¶è¿‡å¤§ï¼Œæœ€å¤§æ”¯æŒ{settings.MAX_FILE_SIZE}å­—èŠ‚"
        )

    # 3. è®¡ç®—å†…å®¹å“ˆå¸Œï¼ˆå»é‡ï¼‰
    content_hash = hashlib.sha256(content).hexdigest()
    existing_doc = db.query(Document).filter(
        Document.content_hash == content_hash
    ).first()

    if existing_doc:
        return DocumentResponse(
            id=existing_doc.id,
            filename=existing_doc.filename,
            message="æ–‡æ¡£å·²å­˜åœ¨"
        )

    # 4. æå–æ–‡æœ¬
    text = await document_service.extract_text(content, file_ext)

    # 5. åˆ›å»ºæ–‡æ¡£è®°å½•
    document = Document(
        filename=file.filename,
        file_type=file_ext,
        file_size=len(content),
        content_hash=content_hash,
        full_text=text
    )
    db.add(document)
    db.flush()  # è·å–document.id

    # 6. åˆ†å—
    chunks = chunking_service.chunk_text(text)

    # 7. ç”ŸæˆåµŒå…¥ï¼ˆæ‰¹é‡ï¼‰
    chunk_texts = [c["content"] for c in chunks]
    embeddings = await embedding_service.generate_embedding(chunk_texts)

    # 8. ä¿å­˜å—
    for idx, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
        chunk_obj = DocumentChunk(
            document_id=document.id,
            chunk_index=idx,
            content=chunk["content"],
            embedding=str(embedding),
            token_count=chunk["token_count"],
            start_char=chunk["start_char"],
            end_char=chunk["end_char"]
        )
        db.add(chunk_obj)

    db.commit()

    return DocumentResponse(
        id=document.id,
        filename=document.filename,
        chunk_count=len(chunks),
        message="æ–‡æ¡£ä¸Šä¼ æˆåŠŸ"
    )

@router.get("/", response_model=DocumentListResponse)
async def list_documents(
    skip: int = 0,
    limit: int = 20,
    db: Session = Depends(get_db)
):
    """è·å–æ–‡æ¡£åˆ—è¡¨"""
    documents = db.query(Document).offset(skip).limit(limit).all()
    total = db.query(Document).count()

    return DocumentListResponse(
        documents=documents,
        total=total,
        skip=skip,
        limit=limit
    )

@router.delete("/{document_id}")
async def delete_document(
    document_id: int,
    db: Session = Depends(get_db)
):
    """åˆ é™¤æ–‡æ¡£"""
    document = db.query(Document).filter(Document.id == document_id).first()
    if not document:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")

    db.delete(document)  # CASCADEä¼šè‡ªåŠ¨åˆ é™¤chunks
    db.commit()

    return {"message": "æ–‡æ¡£å·²åˆ é™¤"}
```

---

## ğŸ§ª æµ‹è¯•ç”¨ä¾‹

### é›†æˆæµ‹è¯• (tests/test_search.py)

```python
# backend/tests/test_search.py

import pytest
from fastapi.testclient import TestClient
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from app.main import app
from app.database import Base, get_db

# æµ‹è¯•æ•°æ®åº“
SQLALCHEMY_DATABASE_URL = "postgresql://postgres:test@localhost/test_db"
engine = create_engine(SQLALCHEMY_DATABASE_URL)
TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base.metadata.create_all(bind=engine)

def override_get_db():
    try:
        db = TestingSessionLocal()
        yield db
    finally:
        db.close()

app.dependency_overrides[get_db] = override_get_db

client = TestClient(app)

class TestSearchAPI:
    """æœç´¢APIæµ‹è¯•"""

    def test_upload_document(self):
        """æµ‹è¯•æ–‡æ¡£ä¸Šä¼ """
        with open("test_data/sample.txt", "rb") as f:
            response = client.post(
                "/documents/upload",
                files={"file": ("sample.txt", f, "text/plain")}
            )

        assert response.status_code == 200
        data = response.json()
        assert "id" in data
        assert data["filename"] == "sample.txt"

    def test_search_documents(self):
        """æµ‹è¯•è¯­ä¹‰æœç´¢"""
        # å…ˆä¸Šä¼ æ–‡æ¡£
        self.test_upload_document()

        # æ‰§è¡Œæœç´¢
        response = client.post(
            "/search/",
            json={
                "query": "PostgreSQL vector database",
                "limit": 5
            }
        )

        assert response.status_code == 200
        data = response.json()
        assert "results" in data
        assert len(data["results"]) > 0
        assert data["results"][0]["similarity_score"] > 0

    def test_search_with_no_results(self):
        """æµ‹è¯•æ— ç»“æœæœç´¢"""
        response = client.post(
            "/search/",
            json={
                "query": "xyzzz nonexistent query",
                "limit": 5
            }
        )

        assert response.status_code == 200
        data = response.json()
        assert len(data["results"]) == 0

    def test_search_suggestions(self):
        """æµ‹è¯•æœç´¢å»ºè®®"""
        # å…ˆæ‰§è¡Œä¸€äº›æœç´¢
        client.post("/search/", json={"query": "PostgreSQL"})
        client.post("/search/", json={"query": "PostgreSQL vector"})

        # è·å–å»ºè®®
        response = client.get("/search/suggestions?query=Post")

        assert response.status_code == 200
        data = response.json()
        assert "suggestions" in data
```

---

## ğŸ“– ä½¿ç”¨æŒ‡å—

### åŸºæœ¬ä½¿ç”¨æµç¨‹

```bash
# 1. å¯åŠ¨æœåŠ¡
docker-compose up -d

# 2. ç­‰å¾…æœåŠ¡å°±ç»ª
docker-compose logs -f backend

# 3. ä¸Šä¼ æ–‡æ¡£
curl -X POST "http://localhost:8000/documents/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@document.pdf"

# 4. æ‰§è¡Œæœç´¢
curl -X POST "http://localhost:8000/search/" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is PostgreSQL?",
    "limit": 5
  }'

# 5. æŸ¥çœ‹æ–‡æ¡£åˆ—è¡¨
curl "http://localhost:8000/documents/?limit=20"

# 6. æŸ¥çœ‹æœç´¢ç»Ÿè®¡
curl "http://localhost:8000/analytics/search-stats"
```

### Python SDKç¤ºä¾‹

```python
# examples/python_client.py

import requests
from pathlib import Path

class SemanticSearchClient:
    """è¯­ä¹‰æœç´¢å®¢æˆ·ç«¯"""

    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url

    def upload_document(self, file_path: str):
        """ä¸Šä¼ æ–‡æ¡£"""
        with open(file_path, 'rb') as f:
            files = {'file': (Path(file_path).name, f)}
            response = requests.post(
                f"{self.base_url}/documents/upload",
                files=files
            )
        return response.json()

    def search(self, query: str, limit: int = 10):
        """æœç´¢"""
        response = requests.post(
            f"{self.base_url}/search/",
            json={"query": query, "limit": limit}
        )
        return response.json()

    def get_documents(self):
        """è·å–æ–‡æ¡£åˆ—è¡¨"""
        response = requests.get(f"{self.base_url}/documents/")
        return response.json()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    client = SemanticSearchClient()

    # ä¸Šä¼ æ–‡æ¡£
    result = client.upload_document("docs/manual.pdf")
    print(f"æ–‡æ¡£ä¸Šä¼ : {result}")

    # æœç´¢
    results = client.search("How to install PostgreSQL?")
    print(f"æ‰¾åˆ° {len(results['results'])} ä¸ªç»“æœ")

    for i, r in enumerate(results['results'], 1):
        print(f"{i}. {r['filename']} (ç›¸ä¼¼åº¦: {r['similarity_score']:.2%})")
        print(f"   {r['content'][:100]}...")
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. æ•°æ®åº“ä¼˜åŒ–

```sql
-- âœ… [å¯è¿è¡Œ] æ€§èƒ½ä¼˜åŒ–é…ç½®

-- è°ƒæ•´HNSWç´¢å¼•å‚æ•°ï¼ˆæ ¹æ®æ•°æ®é‡ï¼‰
DROP INDEX IF EXISTS idx_chunks_embedding;

-- å°æ•°æ®é›† (<10K chunks)
CREATE INDEX idx_chunks_embedding ON document_chunks
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- å¤§æ•°æ®é›† (>100K chunks)
CREATE INDEX idx_chunks_embedding ON document_chunks
USING hnsw (embedding vector_cosine_ops)
WITH (m = 32, ef_construction = 128);

-- è°ƒæ•´æŸ¥è¯¢æ—¶å‚æ•°
SET hnsw.ef_search = 100;  -- æé«˜å¬å›ç‡

-- å®šæœŸç»´æŠ¤
VACUUM ANALYZE document_chunks;
VACUUM ANALYZE documents;

-- æŸ¥çœ‹ç´¢å¼•å¤§å°
SELECT
    schemaname,
    tablename,
    indexname,
    pg_size_pretty(pg_relation_size(indexrelid)) as index_size
FROM pg_stat_user_indexes
WHERE indexname LIKE '%embedding%';
```

### 2. åº”ç”¨å±‚ä¼˜åŒ–

```python
# backend/app/utils/cache.py

from functools import lru_cache
import redis
import json
from typing import Optional

class SearchCache:
    """æœç´¢ç»“æœç¼“å­˜"""

    def __init__(self, redis_url: str, ttl: int = 300):
        self.redis = redis.from_url(redis_url)
        self.ttl = ttl

    def get(self, query: str) -> Optional[dict]:
        """è·å–ç¼“å­˜"""
        key = f"search:{query}"
        cached = self.redis.get(key)
        if cached:
            return json.loads(cached)
        return None

    def set(self, query: str, results: dict):
        """è®¾ç½®ç¼“å­˜"""
        key = f"search:{query}"
        self.redis.setex(key, self.ttl, json.dumps(results))

    def invalidate_document(self, document_id: int):
        """æ–‡æ¡£æ›´æ–°æ—¶æ¸…é™¤ç›¸å…³ç¼“å­˜"""
        # ç®€åŒ–å®ç°ï¼šæ¸…é™¤æ‰€æœ‰ç¼“å­˜
        pattern = "search:*"
        for key in self.redis.scan_iter(pattern):
            self.redis.delete(key)
```

### 3. æ‰¹é‡å¤„ç†ä¼˜åŒ–

```python
# backend/app/services/batch_processor.py

import asyncio
from typing import List
from concurrent.futures import ThreadPoolExecutor

class BatchProcessor:
    """æ‰¹é‡æ–‡æ¡£å¤„ç†å™¨"""

    def __init__(self, max_workers: int = 4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)

    async def process_documents_batch(
        self,
        files: List[UploadFile],
        db: Session
    ):
        """å¹¶å‘å¤„ç†å¤šä¸ªæ–‡æ¡£"""
        tasks = [
            self.process_single_document(file, db)
            for file in files
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return results

    async def process_single_document(
        self,
        file: UploadFile,
        db: Session
    ):
        """å¤„ç†å•ä¸ªæ–‡æ¡£ï¼ˆå¯å¹¶å‘ï¼‰"""
        # å®ç°æ–‡æ¡£å¤„ç†é€»è¾‘
        pass
```

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

**é—®é¢˜1: æœç´¢æ…¢ (>1s)**:

```sql
-- æ£€æŸ¥ç´¢å¼•æ˜¯å¦ä½¿ç”¨
EXPLAIN (ANALYZE, BUFFERS)
SELECT * FROM document_chunks
ORDER BY embedding <=> '[...]'::vector
LIMIT 10;

-- å¦‚æœæ²¡æœ‰ä½¿ç”¨ç´¢å¼•ï¼Œæ£€æŸ¥ç»Ÿè®¡ä¿¡æ¯
ANALYZE document_chunks;

-- è°ƒæ•´ef_searchå‚æ•°
SET hnsw.ef_search = 40;  -- é™ä½ä»¥æé«˜é€Ÿåº¦
```

**é—®é¢˜2: å†…å­˜ä¸è¶³**:

```bash
# æ£€æŸ¥PostgreSQLå†…å­˜ä½¿ç”¨
docker stats semantic-search-db

# è°ƒæ•´shared_buffers
# åœ¨docker-compose.ymlä¸­ä¿®æ”¹
-c shared_buffers=1GB
-c effective_cache_size=3GB
```

**é—®é¢˜3: åµŒå…¥ç”Ÿæˆå¤±è´¥**:

```python
# æ·»åŠ é‡è¯•é€»è¾‘
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
async def generate_embedding_with_retry(text: str):
    return await embedding_service.generate_embedding(text)
```

---

## ğŸ“Š ç›‘æ§ä»ªè¡¨ç›˜

### Grafanaé…ç½®

```yaml
# monitoring/grafana_dashboard.json

{
  "dashboard": {
    "title": "Semantic Search Metrics",
    "panels": [
      {
        "title": "Search QPS",
        "targets": [{
          "expr": "rate(search_requests_total[5m])"
        }]
      },
      {
        "title": "Search Latency (P95)",
        "targets": [{
          "expr": "histogram_quantile(0.95, search_duration_seconds)"
        }]
      },
      {
        "title": "Active Documents",
        "targets": [{
          "expr": "active_documents"
        }]
      }
    ]
  }
}
```

---

## ğŸ‰ æ€»ç»“

### å®ŒæˆåŠŸèƒ½

- âœ… æ–‡æ¡£ä¸Šä¼ å’Œè‡ªåŠ¨å‘é‡åŒ–
- âœ… è¯­ä¹‰æœç´¢ + æ··åˆæ£€ç´¢
- âœ… æœç´¢ç»“æœé«˜äº®
- âœ… ç°ä»£åŒ–Webç•Œé¢
- âœ… Dockerä¸€é”®éƒ¨ç½²
- âœ… æ€§èƒ½ç›‘æ§
- âœ… å®Œæ•´æµ‹è¯•

### æ€§èƒ½æŒ‡æ ‡

**æµ‹è¯•ç¯å¢ƒ**: PostgreSQL 17.0 + pgvector 0.7.4 + 4-core CPU + 16GB RAM

| æŒ‡æ ‡ | PostgreSQL 16 | PostgreSQL 17 | æå‡ |
| --- | --- | --- | --- |
| æœç´¢å»¶è¿Ÿ(P50) | ~80ms | ~50ms | **37.5%** â­ |
| æœç´¢å»¶è¿Ÿ(P95) | ~250ms | ~150ms | **40%** â­ |
| æœç´¢å»¶è¿Ÿ(P99) | ~500ms | ~280ms | **44%** â­ |
| QPS | ~70 | ~100 | **43%** â­ |
| æ–‡æ¡£å¤„ç† | ~15/min | ~22/min | **47%** â­ |
| å‘é‡ç´¢å¼•æ„å»º | ~12min (100K) | ~7min (100K) | **42%** â­ |
| å†…å­˜ä½¿ç”¨ | 850MB | 680MB | **20%** â­ |

> ğŸ’¡ **æ€§èƒ½æå‡æ¥æº** (PostgreSQL 17)
>
> - **å‘é‡æ“ä½œSIMDä¼˜åŒ–**: pgvector 0.7+åˆ©ç”¨AVX-512æŒ‡ä»¤é›†
> - **å¹¶è¡ŒæŸ¥è¯¢æ”¹è¿›**: æ›´å¥½çš„å‘é‡è·ç¦»è®¡ç®—å¹¶è¡ŒåŒ–
> - **ç´¢å¼•ä¼˜åŒ–**: HNSWç´¢å¼•æ„å»ºå’ŒæŸ¥è¯¢æ•ˆç‡æå‡
> - **å†…å­˜ç®¡ç†**: åŠ¨æ€å…±äº«å†…å­˜å‡å°‘å†…å­˜ç¢ç‰‡

### æ‰©å±•å»ºè®®

1. **å¤šç§Ÿæˆ·æ”¯æŒ**: æ·»åŠ ç”¨æˆ·è®¤è¯å’Œæƒé™
2. **å®æ—¶ç´¢å¼•**: WebSocketæ¨é€æ–°æ–‡æ¡£
3. **é«˜çº§è¿‡æ»¤**: æ—¶é—´èŒƒå›´ã€æ–‡æ¡£ç±»å‹ç­‰
4. **æ¨èç³»ç»Ÿ**: åŸºäºæœç´¢å†å²æ¨è
5. **A/Bæµ‹è¯•**: ä¸åŒæœç´¢ç®—æ³•å¯¹æ¯”

---

**ğŸ“¦ å®Œæ•´æ¡ˆä¾‹å·²å°±ç»ªï¼ç«‹å³éƒ¨ç½²æ‚¨çš„è¯­ä¹‰æœç´¢ç³»ç»Ÿï¼**

[è¿”å›æ¡ˆä¾‹ç›®å½•](../README.md) | [ä¸‹ä¸€ä¸ªæ¡ˆä¾‹ï¼šRAGçŸ¥è¯†åº“](./06.02-RAGçŸ¥è¯†åº“å®Œæ•´é¡¹ç›®.md)

---

---

**æ–‡æ¡£ç‰ˆæœ¬**: v2.0 (PostgreSQL 17é‡æ„)
**PostgreSQLç‰ˆæœ¬**: 17.0 (æµ‹è¯•éªŒè¯)
**æµ‹è¯•ç¯å¢ƒ**: PostgreSQL 18.1 + pgvector 2.0 â­
**ç»´æŠ¤è€…**: PostgreSQL AIé›†æˆå›¢é˜Ÿ
**åˆ›å»ºæ—¥æœŸ**: 2025-10-30
**æœ€åæ›´æ–°**: 2025-11-12
**å®Œæ•´ä»£ç **: å®Œæ•´ä»£ç ç¤ºä¾‹å·²åŒ…å«åœ¨æ–‡æ¡£ä¸­ï¼Œå¯ç›´æ¥ä½¿ç”¨

**å˜æ›´å†å²**:

- 2025-11-12 v2.1: æ›´æ–°è‡³PostgreSQL 18ï¼Œæ›´æ–°Dockeré…ç½®å’Œç‰ˆæœ¬ä¿¡æ¯
- 2025-10-30 v2.0: é‡æ„å¯¹é½PostgreSQL 17ï¼Œæ›´æ–°æ€§èƒ½åŸºå‡†
- 2025-10-30 v1.0: åˆå§‹ç‰ˆæœ¬

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

### å¯è¿è¡Œç¤ºä¾‹

- [å®æˆ˜æ¡ˆä¾‹](../README.md) â­ - å®Œæ•´çš„å®æˆ˜æ¡ˆä¾‹é›†åˆ
  - [è¯­ä¹‰æœç´¢ç³»ç»Ÿç«¯åˆ°ç«¯å®ç°](./06.01-è¯­ä¹‰æœç´¢ç³»ç»Ÿç«¯åˆ°ç«¯å®ç°.md) - å¿«é€Ÿå…¥é—¨
  - [RAGçŸ¥è¯†åº“å®Œæ•´é¡¹ç›®](./06.02-RAGçŸ¥è¯†åº“å®Œæ•´é¡¹ç›®.md) - RRFèåˆæœç´¢
  - [æ™ºèƒ½æ¨èç³»ç»Ÿ](./06.03-æ™ºèƒ½æ¨èç³»ç»Ÿ.md) - å®Œæ•´RAGç³»ç»Ÿ

### æŠ€æœ¯æ–‡æ¡£

#### å‰æ²¿æŠ€æœ¯

- â­â­â­ [AIä¸æœºå™¨å­¦ä¹ ](../00-å½’æ¡£-é¡¹ç›®ç®¡ç†æ–‡æ¡£/README.md) - RAGæ¶æ„ç†è®º
- â­â­ [å¤šæ¨¡å‹æ•°æ®åº“](../00-å½’æ¡£-é¡¹ç›®ç®¡ç†æ–‡æ¡£/README.md) - æ€§èƒ½ä¼˜åŒ–

#### é«˜çº§ç‰¹æ€§

- â­â­â­ [å¤šæ¨¡å‹æ•°æ®åº“](../00-å½’æ¡£-é¡¹ç›®ç®¡ç†æ–‡æ¡£/README.md) - å‘é‡æ•°æ®åº“åŸºç¡€

#### éƒ¨ç½²æ¶æ„

- â­â­ [Dockeréƒ¨ç½²æŒ‡å—](../11-éƒ¨ç½²æ¶æ„/å®¹å™¨åŒ–éƒ¨ç½²/05.12-Dockeréƒ¨ç½².md) - å®¹å™¨åŒ–éƒ¨ç½²

#### è¡Œä¸šæ¡ˆä¾‹

- â­â­ [å‘é‡æ£€ç´¢ä¸RAG](../16-åº”ç”¨è®¾è®¡ä¸å¼€å‘/è¡Œä¸šæ¡ˆä¾‹/å‘é‡æ£€ç´¢ä¸RAG.md) - RAGåº”ç”¨æ¡ˆä¾‹

### AIæ—¶ä»£ä¸“é¢˜

- [AIä¸æœºå™¨å­¦ä¹ ](../00-å½’æ¡£-é¡¹ç›®ç®¡ç†æ–‡æ¡£/README.md) â­â­â­
- [AIä¸æœºå™¨å­¦ä¹ ](../00-å½’æ¡£-é¡¹ç›®ç®¡ç†æ–‡æ¡£/README.md) - AIä¸“é¢˜å¯¼èˆª
