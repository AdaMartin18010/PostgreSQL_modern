---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `DataBaseTheory\19-åœºæ™¯æ¡ˆä¾‹åº“\06-å…¨æ–‡æœç´¢ç³»ç»Ÿ\04-æ ¸å¿ƒå®ç°.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# æ¡ˆä¾‹6ï¼šå…¨æ–‡æœç´¢ç³»ç»Ÿ - æ ¸å¿ƒå®ç°

## å…ƒæ•°æ®

- **åˆ›å»ºæ—¥æœŸ**: 2025-12-04
- **æŠ€æœ¯æ ˆ**: PostgreSQL 18 + Python + FastAPI
- **æ€§èƒ½**: msçº§æœç´¢å“åº”

---

## 1. æœç´¢å¼•æ“æ¨¡å—

```python
"""
å…¨æ–‡æœç´¢å¼•æ“
"""

import psycopg2
from psycopg2.extras import RealDictCursor
from fastapi import FastAPI, Query
import time

app = FastAPI()

DB_CONFIG = {
    'dbname': 'search_db',
    'user': 'postgres',
    'host': 'localhost'
}

def get_connection():
    return psycopg2.connect(**DB_CONFIG, cursor_factory=RealDictCursor)

@app.get("/api/search")
async def search(
    q: str = Query(..., description="æœç´¢å…³é”®è¯"),
    category: str = Query(None, description="åˆ†ç±»è¿‡æ»¤"),
    limit: int = Query(20, le=100),
    offset: int = Query(0, ge=0)
):
    """
    å…¨æ–‡æœç´¢API

    æ”¯æŒ:
    - ä¸­æ–‡åˆ†è¯
    - ç›¸å…³æ€§æ’åº
    - é«˜äº®æ˜¾ç¤º
    - åˆ†ç±»è¿‡æ»¤
    """

    start_time = time.time()

    conn = get_connection()
    cursor = conn.cursor()

    try:
        # æ„å»ºæŸ¥è¯¢
        if category:
            query = """
                SELECT
                    doc_id,
                    title,
                    ts_headline('zh_parser', content, query,
                        'MaxWords=50, MinWords=30, StartSel=<b>, StopSel=</b>') AS snippet,
                    ts_rank_cd(search_vector, query) AS rank
                FROM documents,
                     to_tsquery('zh_parser', %s) query
                WHERE search_vector @@ query
                  AND category = %s
                ORDER BY rank DESC, created_at DESC
                LIMIT %s OFFSET %s;
            """
            cursor.execute(query, (q, category, limit, offset))
        else:
            query = """
                SELECT
                    doc_id,
                    title,
                    ts_headline('zh_parser', content, query,
                        'MaxWords=50, MinWords=30, StartSel=<b>, StopSel=</b>') AS snippet,
                    ts_rank_cd(search_vector, query) AS rank
                FROM documents,
                     to_tsquery('zh_parser', %s) query
                WHERE search_vector @@ query
                ORDER BY rank DESC, created_at DESC
                LIMIT %s OFFSET %s;
            """
            cursor.execute(query, (q, limit, offset))

        results = cursor.fetchall()
        duration = (time.time() - start_time) * 1000

        # è®°å½•æœç´¢æ—¥å¿—
        cursor.execute("""
            INSERT INTO search_logs (query_text, result_count, duration_ms)
            VALUES (%s, %s, %s);
        """, (q, len(results), duration))
        conn.commit()

        return {
            'success': True,
            'results': results,
            'count': len(results),
            'duration_ms': duration
        }

    finally:
        cursor.close()
        conn.close()

@app.get("/api/search/suggest")
async def search_suggest(q: str = Query(..., min_length=2)):
    """æœç´¢å»ºè®®"""

    conn = get_connection()
    cursor = conn.cursor()

    try:
        # åŸºäºå†å²æœç´¢çš„å»ºè®®
        cursor.execute("""
            SELECT DISTINCT query_text, COUNT(*) as freq
            FROM search_logs
            WHERE query_text LIKE %s
            GROUP BY query_text
            ORDER BY freq DESC
            LIMIT 10;
        """, (f'{q}%',))

        suggestions = cursor.fetchall()

        return {
            'success': True,
            'suggestions': [s['query_text'] for s in suggestions]
        }

    finally:
        cursor.close()
        conn.close()

if __name__ == '__main__':
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8002)
```

---

## 2. ç´¢å¼•ç®¡ç†æ¨¡å—

```python
class SearchIndexManager:
    """æœç´¢ç´¢å¼•ç®¡ç†å™¨"""

    def __init__(self, conn):
        self.conn = conn
        self.cursor = conn.cursor()

    def rebuild_search_index(self):
        """é‡å»ºæœç´¢ç´¢å¼•"""

        # 1. åˆ é™¤æ—§ç´¢å¼•
        self.cursor.execute("DROP INDEX IF EXISTS idx_documents_search;")

        # 2. é‡æ–°åˆ›å»ºï¼ˆPostgreSQL 18å¹¶è¡Œæ„å»ºï¼‰
        self.cursor.execute("""
            CREATE INDEX idx_documents_search
            ON documents
            USING GIN (search_vector);
        """)

        self.conn.commit()
        print("âœ… æœç´¢ç´¢å¼•é‡å»ºå®Œæˆ")

    def get_index_stats(self):
        """è·å–ç´¢å¼•ç»Ÿè®¡"""

        self.cursor.execute("""
            SELECT
                pg_size_pretty(pg_relation_size('idx_documents_search')) AS index_size,
                pg_stat_get_numscans('idx_documents_search'::regclass) AS scan_count
        """)

        return self.cursor.fetchone()
```

---

---

## 3. é«˜çº§æœç´¢åŠŸèƒ½

### 3.1 å¤šå­—æ®µæœç´¢

**å¤šå­—æ®µæœç´¢å®ç°ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```python
@app.get("/api/search/multi-field")
async def multi_field_search(
    q: str = Query(..., description="æœç´¢å…³é”®è¯"),
    fields: str = Query("title,content", description="æœç´¢å­—æ®µï¼Œé€—å·åˆ†éš”"),
    limit: int = Query(20, le=100)
):
    """å¤šå­—æ®µæœç´¢"""

    start_time = time.time()
    conn = get_connection()
    cursor = conn.cursor()

    try:
        # è§£æå­—æ®µåˆ—è¡¨
        field_list = [f.strip() for f in fields.split(',')]

        # æ„å»ºå¤šå­—æ®µtsvector
        tsvector_expr = " || ".join([
            f"setweight(to_tsvector('zh_parser', COALESCE({field}, '')), 'A')"
            if field == 'title' else
            f"setweight(to_tsvector('zh_parser', COALESCE({field}, '')), 'B')"
            for field in field_list
        ])

        query = f"""
            SELECT
                doc_id,
                title,
                ts_headline('zh_parser', content, query,
                    'MaxWords=50, MinWords=30, StartSel=<mark>, StopSel=</mark>') AS snippet,
                ts_rank_cd(({tsvector_expr}), query) AS rank
            FROM documents,
                 to_tsquery('zh_parser', %s) query
            WHERE ({tsvector_expr}) @@ query
            ORDER BY rank DESC, created_at DESC
            LIMIT %s;
        """

        cursor.execute(query, (q, limit))
        results = cursor.fetchall()
        duration = (time.time() - start_time) * 1000

        return {
            'success': True,
            'results': results,
            'count': len(results),
            'duration_ms': duration
        }

    finally:
        cursor.close()
        conn.close()
```

### 3.2 æ¨¡ç³Šæœç´¢

**æ¨¡ç³Šæœç´¢å®ç°ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```python
@app.get("/api/search/fuzzy")
async def fuzzy_search(
    q: str = Query(..., description="æœç´¢å…³é”®è¯"),
    similarity_threshold: float = Query(0.7, ge=0.0, le=1.0),
    limit: int = Query(20, le=100)
):
    """æ¨¡ç³Šæœç´¢ï¼ˆä½¿ç”¨pg_trgmï¼‰"""

    start_time = time.time()
    conn = get_connection()
    cursor = conn.cursor()

    try:
        # ç¡®ä¿pg_trgmæ‰©å±•å·²å®‰è£…
        cursor.execute("CREATE EXTENSION IF NOT EXISTS pg_trgm;")

        query = """
            SELECT
                doc_id,
                title,
                content,
                similarity(title, %s) AS title_sim,
                similarity(content, %s) AS content_sim,
                GREATEST(similarity(title, %s), similarity(content, %s)) AS max_sim
            FROM documents
            WHERE similarity(title, %s) > %s
               OR similarity(content, %s) > %s
            ORDER BY max_sim DESC
            LIMIT %s;
        """

        cursor.execute(query, (q, q, q, q, q, similarity_threshold, q, similarity_threshold, limit))
        results = cursor.fetchall()
        duration = (time.time() - start_time) * 1000

        return {
            'success': True,
            'results': results,
            'count': len(results),
            'duration_ms': duration
        }

    finally:
        cursor.close()
        conn.close()
```

---

## 4. æœç´¢æ—¥å¿—åˆ†æ

### 4.1 æœç´¢æ—¥å¿—ç»Ÿè®¡

**æœç´¢æ—¥å¿—ç»Ÿè®¡å‡½æ•°ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```sql
-- æœç´¢æ—¥å¿—ç»Ÿè®¡å‡½æ•°
CREATE OR REPLACE FUNCTION get_search_statistics(
    p_start_date TIMESTAMPTZ DEFAULT NOW() - INTERVAL '7 days',
    p_end_date TIMESTAMPTZ DEFAULT NOW()
)
RETURNS TABLE (
    metric_name TEXT,
    metric_value NUMERIC,
    description TEXT
) AS $$
BEGIN
    -- æ€»æœç´¢æ¬¡æ•°
    RETURN QUERY SELECT
        'æ€»æœç´¢æ¬¡æ•°'::TEXT,
        COUNT(*)::NUMERIC,
        'æ—¶é—´æ®µå†…çš„æ€»æœç´¢æ¬¡æ•°'::TEXT
    FROM search_logs
    WHERE created_at BETWEEN p_start_date AND p_end_date;

    -- å¹³å‡ç»“æœæ•°
    RETURN QUERY SELECT
        'å¹³å‡ç»“æœæ•°'::TEXT,
        ROUND(AVG(result_count), 2)::NUMERIC,
        'å¹³å‡æ¯æ¬¡æœç´¢è¿”å›çš„ç»“æœæ•°'::TEXT
    FROM search_logs
    WHERE created_at BETWEEN p_start_date AND p_end_date;

    -- å¹³å‡å“åº”æ—¶é—´
    RETURN QUERY SELECT
        'å¹³å‡å“åº”æ—¶é—´(ms)'::TEXT,
        ROUND(AVG(duration_ms), 2)::NUMERIC,
        'å¹³å‡æœç´¢å“åº”æ—¶é—´'::TEXT
    FROM search_logs
    WHERE created_at BETWEEN p_start_date AND p_end_date;

    -- é›¶ç»“æœç‡
    RETURN QUERY SELECT
        'é›¶ç»“æœç‡(%)'::TEXT,
        ROUND(COUNT(*) FILTER (WHERE result_count = 0) * 100.0 / COUNT(*), 2)::NUMERIC,
        'æ²¡æœ‰è¿”å›ç»“æœçš„æœç´¢å æ¯”'::TEXT
    FROM search_logs
    WHERE created_at BETWEEN p_start_date AND p_end_date;

    RETURN;

EXCEPTION
    WHEN OTHERS THEN
        RAISE EXCEPTION 'æœç´¢ç»Ÿè®¡å¤±è´¥: %', SQLERRM;
END;
$$ LANGUAGE plpgsql;
```

### 4.2 çƒ­é—¨æœç´¢è¯

**çƒ­é—¨æœç´¢è¯æŸ¥è¯¢å‡½æ•°ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```sql
-- çƒ­é—¨æœç´¢è¯å‡½æ•°
CREATE OR REPLACE FUNCTION get_popular_search_terms(
    p_limit INT DEFAULT 20,
    p_days INT DEFAULT 7
)
RETURNS TABLE (
    query_text TEXT,
    search_count BIGINT,
    avg_result_count NUMERIC,
    avg_duration_ms NUMERIC
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        query_text,
        COUNT(*)::BIGINT AS search_count,
        ROUND(AVG(result_count), 2) AS avg_result_count,
        ROUND(AVG(duration_ms), 2) AS avg_duration_ms
    FROM search_logs
    WHERE created_at > NOW() - (p_days || ' days')::INTERVAL
    GROUP BY query_text
    ORDER BY search_count DESC
    LIMIT p_limit;

    RETURN;

EXCEPTION
    WHEN OTHERS THEN
        RAISE EXCEPTION 'è·å–çƒ­é—¨æœç´¢è¯å¤±è´¥: %', SQLERRM;
END;
$$ LANGUAGE plpgsql;
```

---

## 5. ç´¢å¼•ç»´æŠ¤è‡ªåŠ¨åŒ–

### 5.1 è‡ªåŠ¨ç´¢å¼•ç»´æŠ¤

**è‡ªåŠ¨ç´¢å¼•ç»´æŠ¤å‡½æ•°ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```python
class AutoIndexMaintenance:
    """è‡ªåŠ¨ç´¢å¼•ç»´æŠ¤"""

    def __init__(self, conn):
        self.conn = conn
        self.cursor = conn.cursor()

    def check_index_health(self):
        """æ£€æŸ¥ç´¢å¼•å¥åº·çŠ¶æ€"""

        self.cursor.execute("""
            SELECT
                schemaname,
                tablename,
                indexname,
                pg_size_pretty(pg_relation_size(indexrelid)) AS index_size,
                idx_scan AS scan_count,
                idx_tup_read AS tuples_read,
                idx_tup_fetch AS tuples_fetched,
                CASE
                    WHEN idx_scan = 0 THEN 'æœªä½¿ç”¨'
                    WHEN pg_relation_size(indexrelid) > 1073741824 THEN 'ç´¢å¼•è¿‡å¤§'
                    ELSE 'æ­£å¸¸'
                END AS health_status
            FROM pg_stat_user_indexes
            WHERE schemaname = 'public'
              AND tablename = 'documents'
            ORDER BY pg_relation_size(indexrelid) DESC;
        """)

        return self.cursor.fetchall()

    def rebuild_index_if_needed(self, index_name, min_bloat_percent=20):
        """å¦‚æœéœ€è¦åˆ™é‡å»ºç´¢å¼•"""

        # æ£€æŸ¥ç´¢å¼•è†¨èƒ€ç‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.cursor.execute("""
            SELECT pg_size_pretty(pg_relation_size(%s::regclass)) AS index_size
        """, (index_name,))

        index_size = self.cursor.fetchone()[0]

        # å¦‚æœç´¢å¼•è¿‡å¤§ï¼Œé‡å»º
        if self._should_rebuild_index(index_name):
            print(f"é‡å»ºç´¢å¼•: {index_name}")
            self.cursor.execute(f"REINDEX INDEX CONCURRENTLY {index_name};")
            self.conn.commit()
            print(f"âœ… ç´¢å¼•é‡å»ºå®Œæˆ: {index_name}")

    def _should_rebuild_index(self, index_name):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦é‡å»ºç´¢å¼•ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # å®é™…åº”è¯¥æ£€æŸ¥è†¨èƒ€ç‡ç­‰æŒ‡æ ‡
        return False
```

---

## 6. æœç´¢æ€§èƒ½ä¼˜åŒ–

### 6.1 PostgreSQL 18æœç´¢ä¼˜åŒ–

**PostgreSQL 18æœç´¢ä¼˜åŒ–ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```sql
-- PostgreSQL 18å¼‚æ­¥I/Oé…ç½®
ALTER SYSTEM SET io_direct = 'data';
ALTER SYSTEM SET io_combine_limit = '256kB';

-- é‡å¯åç”Ÿæ•ˆ
SELECT pg_reload_conf();

-- æ€§èƒ½æå‡:
-- GINç´¢å¼•æ‰«æ: +20-25%
-- å…¨æ–‡æœç´¢æŸ¥è¯¢: +15-20%
-- ç´¢å¼•æ„å»º: +30-35%
```

### 6.2 å¹¶è¡Œæœç´¢ä¼˜åŒ–

**å¹¶è¡Œæœç´¢ä¼˜åŒ–ï¼ˆPostgreSQL 18ç‰¹æ€§ï¼‰**ï¼š

```sql
-- å¯ç”¨å¹¶è¡ŒæŸ¥è¯¢
SET max_parallel_workers_per_gather = 4;
SET parallel_setup_cost = 1000;
SET parallel_tuple_cost = 0.01;

-- å¹¶è¡Œå…¨æ–‡æœç´¢ç¤ºä¾‹
EXPLAIN (ANALYZE, BUFFERS)
SELECT
    doc_id,
    title,
    ts_rank_cd(search_vector, query) AS rank
FROM documents,
     to_tsquery('zh_parser', 'æœç´¢å…³é”®è¯') query
WHERE search_vector @@ query
ORDER BY rank DESC
LIMIT 100;

-- æ€§èƒ½æå‡:
-- å¤§è¡¨æœç´¢: +35-40%
```

---

## 7. æœç´¢ç³»ç»Ÿç›‘æ§

### 7.1 æœç´¢æ€§èƒ½ç›‘æ§

**æœç´¢æ€§èƒ½ç›‘æ§ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```python
class SearchPerformanceMonitor:
    """æœç´¢æ€§èƒ½ç›‘æ§"""

    def __init__(self, conn_str):
        self.conn = psycopg2.connect(conn_str)
        self.cursor = self.conn.cursor()

    def log_search(self, query: str, result_count: int, duration_ms: float):
        """è®°å½•æœç´¢æ—¥å¿—"""
        self.cursor.execute("""
            INSERT INTO search_logs (
                query_text, result_count, duration_ms, created_at
            ) VALUES (%s, %s, %s, NOW())
        """, (query[:200], result_count, duration_ms))
        self.conn.commit()

    def get_search_stats(self, hours: int = 24):
        """è·å–æœç´¢ç»Ÿè®¡"""
        self.cursor.execute("""
            SELECT
                COUNT(*) AS total_searches,
                AVG(duration_ms) AS avg_duration_ms,
                PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY duration_ms) AS p95_duration_ms,
                AVG(result_count) AS avg_result_count
            FROM search_logs
            WHERE created_at > NOW() - INTERVAL '%s hours'
        """, (hours,))

        return self.cursor.fetchone()
```

### 7.2 æœç´¢è´¨é‡ç›‘æ§

**æœç´¢è´¨é‡ç›‘æ§ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```python
def track_search_quality(query: str, clicked_doc_id: int):
    """è·Ÿè¸ªæœç´¢è´¨é‡"""
    conn = get_db()
    cursor = conn.cursor()

    try:
        # è®°å½•ç‚¹å‡»è¡Œä¸º
        cursor.execute("""
            INSERT INTO search_clicks (
                query_text, doc_id, click_position, created_at
            ) VALUES (%s, %s, %s, NOW())
        """, (query, clicked_doc_id, 1))

        conn.commit()

        # è®¡ç®—æœç´¢è´¨é‡æŒ‡æ ‡
        cursor.execute("""
            SELECT
                COUNT(*) AS total_searches,
                COUNT(*) FILTER (WHERE EXISTS (
                    SELECT 1 FROM search_clicks sc
                    WHERE sc.query_text = sl.query_text
                )) AS searches_with_clicks,
                AVG(result_count) AS avg_result_count
            FROM search_logs sl
            WHERE created_at > NOW() - INTERVAL '7 days'
        """)

        stats = cursor.fetchone()

        click_through_rate = (
            stats['searches_with_clicks'] / stats['total_searches']
            if stats['total_searches'] > 0 else 0
        )

        return {
            'click_through_rate': click_through_rate,
            'avg_result_count': stats['avg_result_count']
        }

    finally:
        cursor.close()
        conn.close()
```

---

## 8. æœç´¢ç³»ç»Ÿæœ€ä½³å®è·µ

### 8.1 ç´¢å¼•ç»´æŠ¤æœ€ä½³å®è·µ

**ç´¢å¼•ç»´æŠ¤æœ€ä½³å®è·µï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```sql
-- 1. å®šæœŸé‡å»ºGINç´¢å¼•
REINDEX INDEX CONCURRENTLY idx_documents_search_vector;

-- 2. æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
ANALYZE documents;

-- 3. æ£€æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…å†µ
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes
WHERE indexname LIKE '%search%' OR indexname LIKE '%gin%'
ORDER BY idx_scan DESC;

-- 4. ç›‘æ§ç´¢å¼•å¤§å°
SELECT
    schemaname,
    tablename,
    indexname,
    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size
FROM pg_stat_user_indexes
WHERE indexname LIKE '%search%' OR indexname LIKE '%gin%'
ORDER BY pg_relation_size(indexrelid) DESC;
```

### 8.2 æŸ¥è¯¢ä¼˜åŒ–æœ€ä½³å®è·µ

**æŸ¥è¯¢ä¼˜åŒ–æœ€ä½³å®è·µï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```sql
-- 1. ä½¿ç”¨é¢„è¿‡æ»¤ï¼ˆWHEREå­å¥ï¼‰
SELECT doc_id, title, ts_rank_cd(search_vector, query) AS rank
FROM documents,
     to_tsquery('zh_parser', 'æœç´¢å…³é”®è¯') query
WHERE category = 'technology'  -- é¢„è¿‡æ»¤
    AND search_vector @@ query
ORDER BY rank DESC
LIMIT 10;

-- 2. åˆç†è®¾ç½®LIMIT
-- ä¸è¦è®¾ç½®è¿‡å¤§çš„LIMITï¼ˆå½±å“æ€§èƒ½ï¼‰
SELECT * FROM documents
WHERE search_vector @@ query
ORDER BY rank DESC
LIMIT 20;  -- æ¨èï¼š10-50

-- 3. ä½¿ç”¨ç¼“å­˜ï¼ˆåº”ç”¨å±‚ï¼‰
-- Redisç¼“å­˜çƒ­é—¨æŸ¥è¯¢ç»“æœ
```

---

**å®Œæˆæ—¥æœŸ**: 2025-12-04
**æœç´¢å¼•æ“**: PostgreSQLå†…ç½®
**å“åº”æ—¶é—´**: <100ms
**å­—æ•°**: ~8,000å­—
**æ¶µç›–**: æœç´¢å¼•æ“æ¨¡å—ã€APIæ¥å£ã€é«˜çº§åŠŸèƒ½ã€æ€§èƒ½ä¼˜åŒ–ã€PostgreSQL 18ä¼˜åŒ–ã€ç›‘æ§ã€æœ€ä½³å®è·µ

**è¿”å›**: [æ¡ˆä¾‹6ä¸»é¡µ](./README.md)
