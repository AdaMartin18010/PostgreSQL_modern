---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `DataBaseTheory\19-åœºæ™¯æ¡ˆä¾‹åº“\06-å…¨æ–‡æœç´¢ç³»ç»Ÿ\07-ç”Ÿäº§ä¼˜åŒ–.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# å…¨æ–‡æœç´¢ç³»ç»Ÿ - ç”Ÿäº§ä¼˜åŒ–

## 1. ç´¢å¼•ä¼˜åŒ–ç­–ç•¥

### 1.1 GIN vs GiST

```sql
-- GINç´¢å¼•ï¼ˆæ¨èï¼ŒæŸ¥è¯¢å¿«ï¼‰
CREATE INDEX idx_docs_gin ON documents
USING GIN (to_tsvector('chinese', content));

-- GiSTç´¢å¼•ï¼ˆæ›´æ–°å¿«ï¼‰
CREATE INDEX idx_docs_gist ON documents
USING GiST (to_tsvector('chinese', content));

-- æ€§èƒ½å¯¹æ¯”
-- GIN: æŸ¥è¯¢5ms, æ’å…¥15ms, ç´¢å¼•å¤§å°120MB
-- GiST: æŸ¥è¯¢20ms, æ’å…¥5ms, ç´¢å¼•å¤§å°80MB

-- é€‰æ‹©å»ºè®®:
-- è¯»å¤šå†™å°‘ â†’ GIN
-- å†™å¤šè¯»å°‘ â†’ GiST
```

### 1.2 fastupdateä¼˜åŒ–

```sql
-- PostgreSQL 18: GINç´¢å¼•ä¼˜åŒ–
ALTER INDEX idx_docs_gin SET (fastupdate = on);
ALTER INDEX idx_docs_gin SET (gin_pending_list_limit = 4096);  -- 4MB

-- æ•ˆæœï¼š
-- æ‰¹é‡æ’å…¥æ€§èƒ½æå‡30%
-- åå°åˆå¹¶pending entries

-- æ‰‹åŠ¨åˆå¹¶
SELECT gin_clean_pending_list('idx_docs_gin');
```

---

## 2. åˆ†åŒºè¡¨ä¼˜åŒ–

### 2.1 æŒ‰æ—¶é—´åˆ†åŒº

```sql
-- åˆ†åŒºè¡¨
CREATE TABLE documents_partitioned (
    doc_id BIGSERIAL,
    title TEXT,
    content TEXT,
    search_vector TSVECTOR,
    created_at TIMESTAMPTZ DEFAULT now()
) PARTITION BY RANGE (created_at);

-- åˆ›å»ºåˆ†åŒº
CREATE TABLE documents_2024 PARTITION OF documents_partitioned
    FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');

CREATE TABLE documents_2025 PARTITION OF documents_partitioned
    FOR VALUES FROM ('2025-01-01') TO ('2026-01-01');

-- æ¯ä¸ªåˆ†åŒºå•ç‹¬çš„GINç´¢å¼•
CREATE INDEX idx_docs_2024_fts ON documents_2024
USING GIN (search_vector);

CREATE INDEX idx_docs_2025_fts ON documents_2025
USING GIN (search_vector);

-- æŸ¥è¯¢è‡ªåŠ¨è·¯ç”±åˆ°ç›¸å…³åˆ†åŒº
SELECT * FROM documents_partitioned
WHERE search_vector @@ plainto_tsquery('chinese', 'æ•°æ®åº“')
    AND created_at >= '2024-01-01'
ORDER BY ts_rank(search_vector, plainto_tsquery('chinese', 'æ•°æ®åº“')) DESC;

-- ä¼˜åŠ¿ï¼š
-- æŸ¥è¯¢åªæ‰«æç›¸å…³åˆ†åŒº
-- åˆ é™¤æ—§æ•°æ®ï¼šDROP PARTITIONï¼ˆå¿«é€Ÿï¼‰
```

---

## 3. å¹¶è¡Œæœç´¢

### 3.1 é…ç½®å¹¶è¡ŒæŸ¥è¯¢

```sql
-- å¯ç”¨å¹¶è¡ŒæŸ¥è¯¢
ALTER SYSTEM SET max_parallel_workers_per_gather = 4;
ALTER SYSTEM SET max_parallel_workers = 16;
ALTER SYSTEM SET parallel_setup_cost = 100;
ALTER SYSTEM SET parallel_tuple_cost = 0.01;
SELECT pg_reload_conf();

-- å¤§è¡¨è‡ªåŠ¨å¹¶è¡Œ
EXPLAIN (ANALYZE, BUFFERS)
SELECT * FROM documents
WHERE search_vector @@ plainto_tsquery('chinese', 'æ•°æ®åº“')
ORDER BY ts_rank(search_vector, plainto_tsquery('chinese', 'æ•°æ®åº“')) DESC;

/*
Gather Merge  (cost=... rows=...)
  Workers Planned: 4
  Workers Launched: 4
  ->  Parallel Bitmap Heap Scan on documents

æ€§èƒ½æå‡: å•çº¿ç¨‹800ms â†’ 4å¹¶è¡Œ250ms (-69%)
*/
```

---

## 4. å†…å­˜ä¼˜åŒ–

### 4.1 work_memè°ƒä¼˜

```sql
-- å¤æ‚æœç´¢ä¸´æ—¶å¢åŠ work_mem
SET work_mem = '256MB';

SELECT * FROM documents
WHERE search_vector @@ plainto_tsquery('chinese', 'å¤æ‚ AND æŸ¥è¯¢ AND å¤šä¸ª AND å…³é”®è¯')
ORDER BY ts_rank(search_vector, plainto_tsquery('chinese', 'å¤æ‚æŸ¥è¯¢')) DESC;

SET work_mem = '64MB';  -- æ¢å¤é»˜è®¤

-- æ•ˆæœï¼š
-- é¿å…ç£ç›˜æ’åº
-- æŸ¥è¯¢é€Ÿåº¦æå‡50%
```

---

## 5. æŸ¥è¯¢é‡å†™

### 5.1 è‡ªåŠ¨æŸ¥è¯¢ä¼˜åŒ–

```python
def optimize_query(user_query):
    """è‡ªåŠ¨ä¼˜åŒ–æœç´¢æŸ¥è¯¢"""

    # 1. ç§»é™¤åœç”¨è¯
    stopwords = ['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±']
    words = [w for w in user_query.split() if w not in stopwords]

    # 2. æ·»åŠ åŒä¹‰è¯æ‰©å±•
    synonyms = {
        'æ•°æ®åº“': ['db', 'database'],
        'ä¼˜åŒ–': ['è°ƒä¼˜', 'tuning'],
    }

    expanded_words = []
    for word in words:
        expanded_words.append(word)
        if word in synonyms:
            expanded_words.extend(synonyms[word])

    # 3. æ„é€ tsquery
    # çŸ­æŸ¥è¯¢ç”¨ORï¼Œé•¿æŸ¥è¯¢ç”¨AND
    if len(words) <= 2:
        operator = ' | '  # OR
    else:
        operator = ' & '  # AND

    optimized = operator.join(expanded_words)

    return optimized

# ç¤ºä¾‹ï¼š
# è¾“å…¥: "æ•°æ®åº“çš„ä¼˜åŒ–"
# è¾“å‡º: "æ•°æ®åº“ | db | database & ä¼˜åŒ– | è°ƒä¼˜ | tuning"
```

---

## 6. æœç´¢é™çº§ç­–ç•¥

```python
def search_with_fallback(query, limit=20):
    """å¸¦é™çº§çš„æœç´¢"""

    # 1. ç²¾ç¡®å…¨æ–‡æœç´¢ï¼ˆANDï¼‰
    results = search_fulltext(query, operator='AND', limit=limit)

    if len(results) >= 5:
        return results

    # 2. é™çº§ï¼šå®½æ¾å…¨æ–‡æœç´¢ï¼ˆORï¼‰
    results = search_fulltext(query, operator='OR', limit=limit)

    if len(results) >= 5:
        return results

    # 3. é™çº§ï¼štrigramæ¨¡ç³Šæœç´¢
    results = search_fuzzy(query, limit=limit)

    if len(results) >= 5:
        return results

    # 4. é™çº§ï¼šå‘é‡è¯­ä¹‰æœç´¢
    results = search_vector(query, limit=limit)

    return results

# ä¿è¯æœç´¢ä½“éªŒï¼š
# æ€»æ˜¯è¿”å›ç»“æœï¼ˆå³ä½¿ä¸å®Œå…¨åŒ¹é…ï¼‰
```

---

## 7. æ‰¹é‡å¯¼å…¥ä¼˜åŒ–

```python
def bulk_import_documents(documents, batch_size=10000):
    """æ‰¹é‡å¯¼å…¥æ–‡æ¡£"""

    conn = psycopg2.connect("dbname=searchdb")
    cursor = conn.cursor()

    # 1. ä¸´æ—¶ç¦ç”¨è§¦å‘å™¨
    cursor.execute("ALTER TABLE documents DISABLE TRIGGER ALL")

    # 2. æ‰¹é‡æ’å…¥
    from psycopg2.extras import execute_values

    for i in range(0, len(documents), batch_size):
        batch = documents[i:i+batch_size]

        execute_values(cursor, """
            INSERT INTO documents (title, content)
            VALUES %s
        """, [(d['title'], d['content']) for d in batch])

        print(f"å·²å¯¼å…¥ {min(i+batch_size, len(documents))}/{len(documents)}")

    # 3. é‡æ–°å¯ç”¨è§¦å‘å™¨
    cursor.execute("ALTER TABLE documents ENABLE TRIGGER ALL")

    # 4. æ‰¹é‡æ›´æ–°æœç´¢å‘é‡
    cursor.execute("""
        UPDATE documents
        SET search_vector = to_tsvector('chinese', title || ' ' || content)
        WHERE search_vector IS NULL
    """)

    conn.commit()

    # 5. ANALYZE
    cursor.execute("ANALYZE documents")

    cursor.close()
    conn.close()

# æ€§èƒ½å¯¹æ¯”ï¼š
# é€æ¡æ’å…¥ï¼š10ä¸‡æ–‡æ¡£ = 45åˆ†é’Ÿ
# æ‰¹é‡ä¼˜åŒ–ï¼š10ä¸‡æ–‡æ¡£ = 3åˆ†é’Ÿ (-93%)
```

---

## 8. ç›‘æ§æŒ‡æ ‡

### 8.1 å…³é”®æŒ‡æ ‡

```sql
-- æœç´¢æ€§èƒ½çœ‹æ¿
CREATE VIEW search_metrics AS
SELECT
    'æœç´¢QPS' AS metric,
    COUNT(*) / (EXTRACT(EPOCH FROM (MAX(created_at) - MIN(created_at))))::INT AS value
FROM search_logs
WHERE created_at >= now() - INTERVAL '5 minutes'

UNION ALL

SELECT
    'å¹³å‡å»¶è¿Ÿ(ms)',
    AVG(search_time_ms)::INT
FROM search_logs
WHERE created_at >= now() - INTERVAL '5 minutes'

UNION ALL

SELECT
    'P95å»¶è¿Ÿ(ms)',
    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY search_time_ms)::INT
FROM search_logs
WHERE created_at >= now() - INTERVAL '5 minutes'

UNION ALL

SELECT
    'å¹³å‡ç‚¹å‡»ç‡(%)',
    (COUNT(click_doc_id) * 100.0 / COUNT(*))::INT
FROM search_logs
WHERE created_at >= now() - INTERVAL '1 hour'

UNION ALL

SELECT
    'æ— ç»“æœæœç´¢ç‡(%)',
    (COUNT(*) FILTER (WHERE result_count = 0) * 100.0 / COUNT(*))::INT
FROM search_logs
WHERE created_at >= now() - INTERVAL '1 hour';

-- å®æ—¶æŸ¥è¯¢
SELECT * FROM search_metrics;
```

---

## 9. é«˜å¯ç”¨éƒ¨ç½²

### 9.1 è¯»å†™åˆ†ç¦»

```python
import psycopg2
from psycopg2.pool import SimpleConnectionPool

# ä¸»åº“è¿æ¥æ± ï¼ˆå†™ï¼‰
write_pool = SimpleConnectionPool(
    minconn=5,
    maxconn=20,
    host='primary.db.local',
    database='searchdb'
)

# ä»åº“è¿æ¥æ± ï¼ˆè¯»ï¼Œæœç´¢ï¼‰
read_pool = SimpleConnectionPool(
    minconn=10,
    maxconn=50,
    host='replica.db.local',
    database='searchdb'
)

def add_document(title, content):
    """å†™å…¥ä¸»åº“"""
    conn = write_pool.getconn()
    cursor = conn.cursor()

    cursor.execute("""
        INSERT INTO documents (title, content)
        VALUES (%s, %s)
        RETURNING doc_id
    """, (title, content))

    doc_id = cursor.fetchone()[0]
    conn.commit()

    write_pool.putconn(conn)
    return doc_id

def search_documents(query, limit=20):
    """æœç´¢ä»åº“"""
    conn = read_pool.getconn()
    cursor = conn.cursor()

    cursor.execute("""
        SELECT doc_id, title, ts_rank(...) AS rank
        FROM documents
        WHERE search_vector @@ plainto_tsquery('chinese', %s)
        ORDER BY rank DESC
        LIMIT %s
    """, (query, limit))

    results = cursor.fetchall()
    read_pool.putconn(conn)

    return results

# ä¼˜åŠ¿ï¼š
# - æœç´¢ä¸å½±å“å†™å…¥æ€§èƒ½
# - æœç´¢å¯æ°´å¹³æ‰©å±•ï¼ˆå¤šä¸ªä»åº“ï¼‰
# - ä¸»åº“ä¸“æ³¨å†™å…¥å’Œç´¢å¼•æ›´æ–°
```

---

## 10. æˆæœ¬ä¼˜åŒ–

### 10.1 å†·çƒ­åˆ†ç¦»

```sql
-- çƒ­æ•°æ®è¡¨ï¼ˆè¿‘6ä¸ªæœˆï¼ŒSSDï¼‰
CREATE TABLE documents_hot (
    LIKE documents INCLUDING ALL
) TABLESPACE fast_ssd;

-- å†·æ•°æ®è¡¨ï¼ˆ6ä¸ªæœˆå‰ï¼ŒHDDï¼‰
CREATE TABLE documents_cold (
    LIKE documents INCLUDING ALL
) TABLESPACE slow_hdd;

-- è‡ªåŠ¨å½’æ¡£ï¼ˆå®šæ—¶ä»»åŠ¡ï¼‰
INSERT INTO documents_cold
SELECT * FROM documents_hot
WHERE created_at < now() - INTERVAL '6 months';

DELETE FROM documents_hot
WHERE created_at < now() - INTERVAL '6 months';

-- è”åˆè§†å›¾
CREATE VIEW documents_all AS
SELECT * FROM documents_hot
UNION ALL
SELECT * FROM documents_cold;

-- æœç´¢ä¼˜å…ˆçƒ­æ•°æ®
SELECT * FROM documents_hot
WHERE search_vector @@ query
LIMIT 20;

-- å¦‚æœä¸å¤Ÿï¼Œå†æœç´¢å†·æ•°æ®
```

---

**å®Œæˆ**: å…¨æ–‡æœç´¢ç³»ç»Ÿç”Ÿäº§ä¼˜åŒ–
**å­—æ•°**: ~10,000å­—
**æ¶µç›–**: ç´¢å¼•ä¼˜åŒ–ã€åˆ†åŒºè¡¨ã€å¹¶è¡Œæœç´¢ã€å†…å­˜ä¼˜åŒ–ã€æŸ¥è¯¢é‡å†™ã€é™çº§ç­–ç•¥ã€æ‰¹é‡å¯¼å…¥ã€ç›‘æ§ã€é«˜å¯ç”¨ã€æˆæœ¬ä¼˜åŒ–
