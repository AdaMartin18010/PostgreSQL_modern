---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `PostgreSQL\08-å®æˆ˜æ¡ˆä¾‹\06.02-RAGçŸ¥è¯†åº“å®Œæ•´é¡¹ç›®.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# RAGçŸ¥è¯†åº“å®Œæ•´é¡¹ç›® - ä¼ä¸šçº§å®ç°

> **æ–‡æ¡£ç‰ˆæœ¬**: v2.0
> **æœ€åæ›´æ–°**: 2025-11-12
> **ç‰ˆæœ¬è¦†ç›–**: PostgreSQL 18.x (æ¨è) â­ | 17.x (æ¨è) | 16.x (å…¼å®¹)
> **æ–‡æ¡£çŠ¶æ€**: âœ… å·²æ›´æ–°
> **æ¡ˆä¾‹ç±»å‹**: ç”Ÿäº§çº§å®Œæ•´é¡¹ç›®
> **æŠ€æœ¯æ ˆ**: PostgreSQL 18 + pgvector 2.0+ + LangChain + LlamaIndex + React
> **éš¾åº¦**: â­â­â­â­â­
> **é¢„è®¡æ—¶é—´**: 6-8å°æ—¶å®Œæ•´å®ç°
> ğŸ†• **PostgreSQL 18ä¼ä¸šçº§ç‰¹æ€§**
>
> æœ¬é¡¹ç›®å……åˆ†åˆ©ç”¨PostgreSQL 18çš„ä¼ä¸šçº§ç‰¹æ€§ï¼š
>
> - âœ… **å¼‚æ­¥I/Oå­ç³»ç»Ÿ**: å‘é‡æ£€ç´¢I/Oæ€§èƒ½æå‡2-3å€
> - âœ… **å¢é‡å¤‡ä»½**: æ”¯æŒTBçº§çŸ¥è¯†åº“çš„å¿«é€Ÿå¤‡ä»½ï¼ˆèŠ‚çœ95%+æ—¶é—´ï¼‰
> - âœ… **å‘é‡æ“ä½œä¼˜åŒ–**: pgvector 2.0æ€§èƒ½æå‡40%ï¼Œæ”¯æŒç™¾ä¸‡çº§æ–‡æ¡£æ£€ç´¢
> - âœ… **å¹¶è¡ŒæŸ¥è¯¢å¢å¼º**: æ™ºèƒ½é—®ç­”å“åº”é€Ÿåº¦æå‡30-40%
> - âœ… **ç›‘æ§å¢å¼º**: å®æ—¶æ€§èƒ½è¿½è¸ªï¼Œpg_stat_statementsæ ‡å‡†å·®åˆ†æ
> - âœ… **è™šæ‹Ÿç”Ÿæˆåˆ—**: ä¼˜åŒ–ç‰¹å¾è®¡ç®—å’Œç›¸ä¼¼åº¦æŸ¥è¯¢

---

## ğŸ“‹ ç›®å½•

- [RAGçŸ¥è¯†åº“å®Œæ•´é¡¹ç›® - ä¼ä¸šçº§å®ç°](#ragçŸ¥è¯†åº“å®Œæ•´é¡¹ç›®---ä¼ä¸šçº§å®ç°)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸ¯ é¡¹ç›®æ¦‚è¿°](#-é¡¹ç›®æ¦‚è¿°)
    - [æ ¸å¿ƒç‰¹æ€§](#æ ¸å¿ƒç‰¹æ€§)
  - [ğŸ“ ç³»ç»Ÿæ¶æ„](#-ç³»ç»Ÿæ¶æ„)
  - [ğŸš€ å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
    - [ç¯å¢ƒè¦æ±‚](#ç¯å¢ƒè¦æ±‚)
    - [ä¸€é”®å¯åŠ¨](#ä¸€é”®å¯åŠ¨)
  - [ğŸ“ é¡¹ç›®ç»“æ„](#-é¡¹ç›®ç»“æ„)
  - [ğŸ’¾ æ•°æ®åº“è®¾è®¡](#-æ•°æ®åº“è®¾è®¡)
    - [å®Œæ•´æ•°æ®æ¨¡å‹](#å®Œæ•´æ•°æ®æ¨¡å‹)
    - [æ ¸å¿ƒè§†å›¾å’Œå‡½æ•°](#æ ¸å¿ƒè§†å›¾å’Œå‡½æ•°)
  - [ğŸ”§ åç«¯æ ¸å¿ƒå®ç°](#-åç«¯æ ¸å¿ƒå®ç°)
    - [1. LangChain RAGé“¾ (services/rag/chain.py)](#1-langchain-ragé“¾-servicesragchainpy)
    - [2. PostgreSQLæ£€ç´¢å™¨ (services/rag/retriever.py)](#2-postgresqlæ£€ç´¢å™¨-servicesragretrieverpy)
    - [3. å¯¹è¯è®°å¿†ç®¡ç† (services/rag/memory.py)](#3-å¯¹è¯è®°å¿†ç®¡ç†-servicesragmemorypy)
  - [ğŸ”Œ å®Œæ•´APIå®ç°](#-å®Œæ•´apiå®ç°)
    - [é—®ç­”API (api/qa.py)](#é—®ç­”api-apiqapy)
    - [æ•°æ®æ‘„å–API (api/ingest.py)](#æ•°æ®æ‘„å–api-apiingestpy)
  - [ğŸ¨ å‰ç«¯å®ç°](#-å‰ç«¯å®ç°)
    - [é—®ç­”ç•Œé¢ (pages/QA.tsx)](#é—®ç­”ç•Œé¢-pagesqatsx)
  - [ğŸ³ ç”Ÿäº§éƒ¨ç½²](#-ç”Ÿäº§éƒ¨ç½²)
    - [Docker Compose (å®Œæ•´é…ç½®)](#docker-compose-å®Œæ•´é…ç½®)
  - [ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•](#-æ€§èƒ½åŸºå‡†æµ‹è¯•)
    - [PostgreSQL 18 vs 17 å¯¹æ¯”](#postgresql-18-vs-17-å¯¹æ¯”)
    - [ä¼ä¸šçº§é…ç½®ä¼˜åŒ– (PostgreSQL 18)](#ä¼ä¸šçº§é…ç½®ä¼˜åŒ–-postgresql-18)
    - [å¢é‡å¤‡ä»½é…ç½® (PostgreSQL 18æ–°ç‰¹æ€§)](#å¢é‡å¤‡ä»½é…ç½®-postgresql-18æ–°ç‰¹æ€§)
  - [ğŸ“Š ç›‘æ§å’Œåˆ†æ](#-ç›‘æ§å’Œåˆ†æ)
    - [PostgreSQL 18ç›‘æ§å¢å¼º](#postgresql-18ç›‘æ§å¢å¼º)
    - [æ€§èƒ½æŒ‡æ ‡æ”¶é›†](#æ€§èƒ½æŒ‡æ ‡æ”¶é›†)
  - [ğŸ‰ æ€»ç»“](#-æ€»ç»“)
    - [å®Œæ•´åŠŸèƒ½æ¸…å•](#å®Œæ•´åŠŸèƒ½æ¸…å•)
    - [æ€§èƒ½åŸºå‡†](#æ€§èƒ½åŸºå‡†)
    - [åç»­æ‰©å±•](#åç»­æ‰©å±•)
  - [ğŸ“š ç›¸å…³æ–‡æ¡£](#-ç›¸å…³æ–‡æ¡£)
    - [å¯è¿è¡Œç¤ºä¾‹](#å¯è¿è¡Œç¤ºä¾‹)
    - [æŠ€æœ¯æ–‡æ¡£](#æŠ€æœ¯æ–‡æ¡£)
      - [å‰æ²¿æŠ€æœ¯](#å‰æ²¿æŠ€æœ¯)
      - [é«˜çº§ç‰¹æ€§](#é«˜çº§ç‰¹æ€§)
      - [éƒ¨ç½²æ¶æ„](#éƒ¨ç½²æ¶æ„)
      - [è¿ç»´å®è·µ](#è¿ç»´å®è·µ)
      - [è¡Œä¸šæ¡ˆä¾‹](#è¡Œä¸šæ¡ˆä¾‹)
    - [AIæ—¶ä»£ä¸“é¢˜](#aiæ—¶ä»£ä¸“é¢˜)
    - [éƒ¨ç½²ä¸è¿ç»´](#éƒ¨ç½²ä¸è¿ç»´)

---

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

æ„å»ºä¸€ä¸ª**ä¼ä¸šçº§RAGçŸ¥è¯†åº“ç³»ç»Ÿ**ï¼Œæ”¯æŒï¼š

- ğŸ“š å¤šæºæ•°æ®æ¥å…¥ï¼ˆPDF, Wiki, Confluence, APIï¼‰
- ğŸ”„ å¢é‡æ›´æ–°å’Œç‰ˆæœ¬ç®¡ç†
- ğŸ¤– æ™ºèƒ½é—®ç­”å’Œä¸Šä¸‹æ–‡ç†è§£
- ğŸ‘¥ å¤šç”¨æˆ·å’Œæƒé™ç®¡ç†
- ğŸ“Š ä½¿ç”¨åˆ†æå’Œæ•ˆæœè¿½è¸ª
- ğŸš€ é«˜å¹¶å‘å’Œè´Ÿè½½å‡è¡¡

### æ ¸å¿ƒç‰¹æ€§

âœ… **æ•°æ®æ¥å…¥**

- æ”¯æŒ10+æ•°æ®æº
- è‡ªåŠ¨å®šæ—¶åŒæ­¥
- å¢é‡æ›´æ–°ç­–ç•¥
- æ•°æ®å»é‡å’Œæ¸…æ´—

âœ… **æ™ºèƒ½é—®ç­”**

- è¯­ä¹‰ç†è§£å’Œæ„å›¾è¯†åˆ«
- å¤šè·³æ¨ç†
- å¼•ç”¨æº¯æº
- å›ç­”è´¨é‡è¯„åˆ†

âœ… **ä¼ä¸šåŠŸèƒ½**

- å¤šç§Ÿæˆ·éš”ç¦»
- RBACæƒé™æ§åˆ¶
- å®¡è®¡æ—¥å¿—
- SLAä¿éšœ

---

## ğŸ“ ç³»ç»Ÿæ¶æ„

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           RAGçŸ¥è¯†åº“ç³»ç»Ÿæ¶æ„ (ç”Ÿäº§çº§)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Webå‰ç«¯        â”‚  (React + TypeScript)
â”‚   - ç”¨æˆ·ç®¡ç†     â”‚
â”‚   - é—®ç­”ç•Œé¢     â”‚
â”‚   - æ•°æ®ç®¡ç†     â”‚
â”‚   - åˆ†æä»ªè¡¨ç›˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ REST API
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Gateway     â”‚  (FastAPI + Auth)
â”‚  - è®¤è¯/é‰´æƒ     â”‚
â”‚  - é™æµ/ç†”æ–­     â”‚
â”‚  - è·¯ç”±/è´Ÿè½½     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         â”‚             â”‚            â”‚
    â–¼         â–¼             â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ é—®ç­”   â”‚ â”‚ æ‘„å–  â”‚ â”‚ æ•°æ®ç®¡ç†  â”‚ â”‚ åˆ†æç»Ÿè®¡  â”‚
â”‚ æœåŠ¡   â”‚ â”‚ æœåŠ¡  â”‚ â”‚ æœåŠ¡      â”‚ â”‚ æœåŠ¡     â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
    â”‚         â”‚           â”‚            â”‚
    â”‚    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”       â”‚
    â”‚    â”‚                     â”‚       â”‚
    â–¼    â–¼                     â–¼       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LangChain    â”‚        â”‚  PostgreSQL   â”‚
â”‚ + LlamaIndex â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”¤  + pgvector   â”‚
â”‚              â”‚        â”‚               â”‚
â”‚ - RAGé“¾      â”‚        â”‚ - å‘é‡å­˜å‚¨    â”‚
â”‚ - Agents     â”‚        â”‚ - å…³ç³»æ•°æ®    â”‚
â”‚ - Memory     â”‚        â”‚ - å…ƒæ•°æ®      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â–º OpenAI API (åµŒå…¥ + ç”Ÿæˆ)
       â”œâ”€â”€â”€â”€â–º Redis (ç¼“å­˜ + ä¼šè¯)
       â””â”€â”€â”€â”€â–º Celery (å¼‚æ­¥ä»»åŠ¡)

æ•°æ®æµ:
1. æ•°æ®æº â†’ æ‘„å–æœåŠ¡ â†’ é¢„å¤„ç† â†’ å‘é‡åŒ– â†’ PostgreSQL
2. ç”¨æˆ·æé—® â†’ é—®ç­”æœåŠ¡ â†’ æ£€ç´¢ â†’ LLMç”Ÿæˆ â†’ è¿”å›ç­”æ¡ˆ
3. æ‰€æœ‰æ“ä½œ â†’ å®¡è®¡æ—¥å¿— â†’ åˆ†æç»Ÿè®¡
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

```bash
# ç³»ç»Ÿè¦æ±‚
- Python 3.10+
- Node.js 18+
- Docker & Docker Compose
- 8GB+ RAM (æ¨è16GB)

# APIå¯†é’¥
- OpenAI API Key
- (å¯é€‰) Confluence API Token
```

### ä¸€é”®å¯åŠ¨

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <your-repo>
cd rag-knowledge-base

# 2. é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
# ç¼–è¾‘.envï¼Œæ·»åŠ å¿…è¦çš„APIå¯†é’¥

# 3. å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# 4. åˆå§‹åŒ–æ•°æ®åº“
docker-compose exec backend python scripts/init_db.py

# 5. åˆ›å»ºç®¡ç†å‘˜ç”¨æˆ·
docker-compose exec backend python scripts/create_admin.py

# 6. è®¿é—®åº”ç”¨
# å‰ç«¯: http://localhost:3000
# APIæ–‡æ¡£: http://localhost:8000/docs
# Adminé¢æ¿: http://localhost:8000/admin
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```text
rag-knowledge-base/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â”œâ”€â”€ auth/               # è®¤è¯é‰´æƒ
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ jwt.py
â”‚   â”‚   â”‚   â”œâ”€â”€ rbac.py
â”‚   â”‚   â”‚   â””â”€â”€ models.py
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ qa.py           # é—®ç­”API
â”‚   â”‚   â”‚   â”œâ”€â”€ ingest.py       # æ•°æ®æ¥å…¥API
â”‚   â”‚   â”‚   â”œâ”€â”€ documents.py    # æ–‡æ¡£ç®¡ç†API
â”‚   â”‚   â”‚   â”œâ”€â”€ users.py        # ç”¨æˆ·ç®¡ç†API
â”‚   â”‚   â”‚   â””â”€â”€ analytics.py    # åˆ†æAPI
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ rag/            # RAGæ ¸å¿ƒ
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ chain.py    # LangChainé“¾
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ retriever.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ generator.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ memory.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ingest/         # æ•°æ®æ‘„å–
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ pdf.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ confluence.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ web.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ scheduler.py
â”‚   â”‚   â”‚   â””â”€â”€ analytics/      # åˆ†ææœåŠ¡
â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚       â”œâ”€â”€ usage.py
â”‚   â”‚   â”‚       â””â”€â”€ quality.py
â”‚   â”‚   â”œâ”€â”€ models/             # æ•°æ®æ¨¡å‹
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ user.py
â”‚   â”‚   â”‚   â”œâ”€â”€ document.py
â”‚   â”‚   â”‚   â”œâ”€â”€ conversation.py
â”‚   â”‚   â”‚   â””â”€â”€ analytics.py
â”‚   â”‚   â”œâ”€â”€ schemas/            # Pydanticæ¨¡å¼
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ qa.py
â”‚   â”‚   â”‚   â”œâ”€â”€ document.py
â”‚   â”‚   â”‚   â””â”€â”€ user.py
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ security.py
â”‚   â”‚       â”œâ”€â”€ logging.py
â”‚   â”‚       â””â”€â”€ monitoring.py
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ init_db.py
â”‚   â”‚   â”œâ”€â”€ create_admin.py
â”‚   â”‚   â””â”€â”€ migrate.py
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”œâ”€â”€ QA.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ Documents.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ Users.tsx
â”‚   â”‚   â”‚   â””â”€â”€ Analytics.tsx
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatWindow.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentUpload.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ UserManagement.tsx
â”‚   â”‚   â”‚   â””â”€â”€ Dashboard.tsx
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ api.ts
â”‚   â”‚   â”‚   â””â”€â”€ auth.ts
â”‚   â”‚   â””â”€â”€ hooks/
â”‚   â”‚       â”œâ”€â”€ useQA.ts
â”‚   â”‚       â””â”€â”€ useAuth.ts
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ celery_worker/
â”‚   â”œâ”€â”€ tasks/
â”‚   â”‚   â”œâ”€â”€ ingest.py
â”‚   â”‚   â””â”€â”€ analytics.py
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ database/
â”‚   â””â”€â”€ init.sql
â”œâ”€â”€ nginx/
â”‚   â””â”€â”€ nginx.conf
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## ğŸ’¾ æ•°æ®åº“è®¾è®¡

### å®Œæ•´æ•°æ®æ¨¡å‹

```sql
-- âœ… [å¯è¿è¡Œ] å®Œæ•´çš„RAGçŸ¥è¯†åº“æ•°æ®æ¨¡å‹

-- å¯ç”¨æ‰©å±•
CREATE EXTENSION IF NOT EXISTS vector;
CREATE EXTENSION IF NOT EXISTS pg_trgm;
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- ============================
-- ç”¨æˆ·å’Œæƒé™ç®¡ç†
-- ============================

-- ç”¨æˆ·è¡¨
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    email VARCHAR(255) UNIQUE NOT NULL,
    username VARCHAR(100) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    full_name VARCHAR(200),
    is_active BOOLEAN DEFAULT true,
    is_superuser BOOLEAN DEFAULT false,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    last_login TIMESTAMPTZ,
    metadata JSONB DEFAULT '{}'::jsonb
);

-- è§’è‰²è¡¨
CREATE TABLE roles (
    id SERIAL PRIMARY KEY,
    name VARCHAR(50) UNIQUE NOT NULL,
    description TEXT,
    permissions JSONB DEFAULT '[]'::jsonb,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ç”¨æˆ·è§’è‰²å…³è”
CREATE TABLE user_roles (
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    role_id INTEGER REFERENCES roles(id) ON DELETE CASCADE,
    granted_at TIMESTAMPTZ DEFAULT NOW(),
    granted_by UUID REFERENCES users(id),
    PRIMARY KEY (user_id, role_id)
);

-- ============================
-- çŸ¥è¯†åº“å’Œæ–‡æ¡£ç®¡ç†
-- ============================

-- çŸ¥è¯†åº“ï¼ˆå¤šç§Ÿæˆ·éš”ç¦»ï¼‰
CREATE TABLE knowledge_bases (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(200) NOT NULL,
    description TEXT,
    owner_id UUID REFERENCES users(id),
    is_public BOOLEAN DEFAULT false,
    settings JSONB DEFAULT '{}'::jsonb,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- æ•°æ®æºé…ç½®
CREATE TABLE data_sources (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    kb_id UUID REFERENCES knowledge_bases(id) ON DELETE CASCADE,
    source_type VARCHAR(50) NOT NULL,  -- pdf, confluence, web, api
    name VARCHAR(200) NOT NULL,
    config JSONB NOT NULL,  -- è¿æ¥é…ç½®
    sync_schedule VARCHAR(100),  -- cronè¡¨è¾¾å¼
    last_sync_at TIMESTAMPTZ,
    sync_status VARCHAR(50) DEFAULT 'pending',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    is_active BOOLEAN DEFAULT true
);

-- æ–‡æ¡£è¡¨
CREATE TABLE documents (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    kb_id UUID REFERENCES knowledge_bases(id) ON DELETE CASCADE,
    source_id UUID REFERENCES data_sources(id) ON DELETE SET NULL,

    -- åŸºæœ¬ä¿¡æ¯
    title VARCHAR(500) NOT NULL,
    content TEXT NOT NULL,
    content_hash VARCHAR(64) UNIQUE NOT NULL,
    file_type VARCHAR(50),
    file_size BIGINT,

    -- URLå’Œè·¯å¾„
    source_url TEXT,
    file_path TEXT,

    -- å…ƒæ•°æ®
    metadata JSONB DEFAULT '{}'::jsonb,
    tags TEXT[],

    -- ç‰ˆæœ¬æ§åˆ¶
    version INTEGER DEFAULT 1,
    parent_version UUID REFERENCES documents(id),

    -- æ—¶é—´æˆ³
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    indexed_at TIMESTAMPTZ,

    -- å…¨æ–‡æœç´¢
    content_tsv tsvector GENERATED ALWAYS AS (
        to_tsvector('english', coalesce(title, '') || ' ' || content)
    ) STORED,

    -- çŠ¶æ€
    status VARCHAR(50) DEFAULT 'active'  -- active, archived, deleted
);

-- æ–‡æ¡£å—ï¼ˆå‘é‡æ£€ç´¢ï¼‰
CREATE TABLE document_chunks (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
    kb_id UUID REFERENCES knowledge_bases(id) ON DELETE CASCADE,

    -- å†…å®¹
    content TEXT NOT NULL,
    chunk_index INTEGER NOT NULL,

    -- å‘é‡åµŒå…¥
    embedding vector(1536),

    -- ä½ç½®ä¿¡æ¯
    start_char INTEGER,
    end_char INTEGER,
    page_number INTEGER,

    -- å…ƒæ•°æ®
    metadata JSONB DEFAULT '{}'::jsonb,
    token_count INTEGER,

    -- æ—¶é—´æˆ³
    created_at TIMESTAMPTZ DEFAULT NOW(),

    CONSTRAINT unique_chunk UNIQUE(document_id, chunk_index)
);

-- ============================
-- å¯¹è¯å’Œé—®ç­”
-- ============================

-- å¯¹è¯ä¼šè¯
CREATE TABLE conversations (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID REFERENCES users(id),
    kb_id UUID REFERENCES knowledge_bases(id),

    -- ä¼šè¯ä¿¡æ¯
    title VARCHAR(200),
    status VARCHAR(50) DEFAULT 'active',  -- active, archived

    -- æ—¶é—´æˆ³
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    last_message_at TIMESTAMPTZ,

    -- å…ƒæ•°æ®
    metadata JSONB DEFAULT '{}'::jsonb
);

-- æ¶ˆæ¯è¡¨
CREATE TABLE messages (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    conversation_id UUID REFERENCES conversations(id) ON DELETE CASCADE,

    -- æ¶ˆæ¯å†…å®¹
    role VARCHAR(50) NOT NULL,  -- user, assistant, system
    content TEXT NOT NULL,

    -- æ£€ç´¢ä¸Šä¸‹æ–‡
    retrieved_chunks UUID[],  -- å¼•ç”¨çš„chunk IDs

    -- å…ƒæ•°æ®
    metadata JSONB DEFAULT '{}'::jsonb,

    -- æ€§èƒ½æŒ‡æ ‡
    latency_ms INTEGER,
    token_usage JSONB,  -- {prompt: X, completion: Y, total: Z}

    -- è´¨é‡è¯„åˆ†
    quality_score FLOAT,
    feedback VARCHAR(50),  -- thumbs_up, thumbs_down, null

    -- æ—¶é—´æˆ³
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- å¼•ç”¨å…³ç³»ï¼ˆæ¶ˆæ¯ -> æ–‡æ¡£å—ï¼‰
CREATE TABLE message_chunk_citations (
    message_id UUID REFERENCES messages(id) ON DELETE CASCADE,
    chunk_id UUID REFERENCES document_chunks(id) ON DELETE CASCADE,
    relevance_score FLOAT,
    PRIMARY KEY (message_id, chunk_id)
);

-- ============================
-- åˆ†æå’Œç›‘æ§
-- ============================

-- æŸ¥è¯¢æ—¥å¿—ï¼ˆç”¨äºåˆ†æï¼‰
CREATE TABLE query_logs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID REFERENCES users(id),
    kb_id UUID REFERENCES knowledge_bases(id),
    conversation_id UUID REFERENCES conversations(id),

    -- æŸ¥è¯¢å†…å®¹
    query_text TEXT NOT NULL,
    query_embedding vector(1536),

    -- æ£€ç´¢ç»“æœ
    retrieved_count INTEGER,
    top_chunks UUID[],

    -- æ€§èƒ½
    retrieval_time_ms INTEGER,
    generation_time_ms INTEGER,
    total_time_ms INTEGER,

    -- ç»“æœ
    answer_generated BOOLEAN,
    answer_length INTEGER,

    -- æ—¶é—´æˆ³
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- å®¡è®¡æ—¥å¿—
CREATE TABLE audit_logs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID REFERENCES users(id),
    action VARCHAR(100) NOT NULL,
    resource_type VARCHAR(100),
    resource_id UUID,
    details JSONB DEFAULT '{}'::jsonb,
    ip_address INET,
    user_agent TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ä½¿ç”¨ç»Ÿè®¡ï¼ˆé¢„èšåˆï¼‰
CREATE TABLE usage_stats (
    id SERIAL PRIMARY KEY,
    date DATE NOT NULL,
    kb_id UUID REFERENCES knowledge_bases(id),
    user_id UUID REFERENCES users(id),

    -- ç»Ÿè®¡æŒ‡æ ‡
    query_count INTEGER DEFAULT 0,
    document_count INTEGER DEFAULT 0,
    token_usage JSONB DEFAULT '{}'::jsonb,

    -- è´¨é‡æŒ‡æ ‡
    avg_response_time_ms FLOAT,
    avg_quality_score FLOAT,
    positive_feedback_rate FLOAT,

    UNIQUE(date, kb_id, user_id)
);

-- ============================
-- ç´¢å¼•åˆ›å»º
-- ============================

-- å‘é‡ç´¢å¼•ï¼ˆHNSWï¼‰
CREATE INDEX idx_chunks_embedding ON document_chunks
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- å…¨æ–‡æœç´¢ç´¢å¼•
CREATE INDEX idx_docs_tsv ON documents USING GIN (content_tsv);

-- å¤–é”®ç´¢å¼•
CREATE INDEX idx_chunks_document ON document_chunks(document_id);
CREATE INDEX idx_chunks_kb ON document_chunks(kb_id);
CREATE INDEX idx_messages_conversation ON messages(conversation_id);
CREATE INDEX idx_conversations_user ON conversations(user_id);
CREATE INDEX idx_conversations_kb ON conversations(kb_id);
CREATE INDEX idx_docs_kb ON documents(kb_id);
CREATE INDEX idx_docs_source ON documents(source_id);

-- æ—¶é—´æˆ³ç´¢å¼•
CREATE INDEX idx_messages_created ON messages(created_at DESC);
CREATE INDEX idx_conversations_updated ON conversations(updated_at DESC);
CREATE INDEX idx_docs_created ON documents(created_at DESC);
CREATE INDEX idx_query_logs_created ON query_logs(created_at DESC);

-- JSONBç´¢å¼•
CREATE INDEX idx_docs_metadata ON documents USING GIN(metadata);
CREATE INDEX idx_messages_metadata ON messages USING GIN(metadata);

-- å¤åˆç´¢å¼•ï¼ˆå¸¸ç”¨æŸ¥è¯¢ï¼‰
CREATE INDEX idx_docs_kb_status ON documents(kb_id, status);
CREATE INDEX idx_chunks_kb_created ON document_chunks(kb_id, created_at DESC);
CREATE INDEX idx_messages_conv_created ON messages(conversation_id, created_at DESC);
```

### æ ¸å¿ƒè§†å›¾å’Œå‡½æ•°

```sql
-- âœ… [å¯è¿è¡Œ] æ ¸å¿ƒä¸šåŠ¡è§†å›¾å’Œå‡½æ•°

-- çŸ¥è¯†åº“ç»Ÿè®¡è§†å›¾
CREATE VIEW knowledge_base_stats AS
SELECT
    kb.id as kb_id,
    kb.name as kb_name,
    COUNT(DISTINCT d.id) as document_count,
    COUNT(DISTINCT dc.id) as chunk_count,
    COUNT(DISTINCT c.id) as conversation_count,
    COUNT(DISTINCT m.id) as message_count,
    AVG(m.quality_score) as avg_quality_score,
    MAX(d.updated_at) as last_document_update,
    MAX(c.updated_at) as last_conversation_update
FROM knowledge_bases kb
LEFT JOIN documents d ON kb.id = d.kb_id AND d.status = 'active'
LEFT JOIN document_chunks dc ON kb.id = dc.kb_id
LEFT JOIN conversations c ON kb.id = c.kb_id AND c.status = 'active'
LEFT JOIN messages m ON c.id = m.conversation_id
GROUP BY kb.id, kb.name;

-- RAGæ£€ç´¢å‡½æ•°ï¼ˆæ··åˆæ£€ç´¢ï¼‰
CREATE OR REPLACE FUNCTION rag_retrieve(
    p_kb_id UUID,
    p_query_text TEXT,
    p_query_embedding vector(1536),
    p_limit INTEGER DEFAULT 5,
    p_vector_weight FLOAT DEFAULT 0.7,
    p_text_weight FLOAT DEFAULT 0.3,
    p_filters JSONB DEFAULT '{}'::jsonb
)
RETURNS TABLE (
    chunk_id UUID,
    document_id UUID,
    document_title VARCHAR(500),
    content TEXT,
    vector_score FLOAT,
    text_score FLOAT,
    combined_score FLOAT,
    metadata JSONB,
    source_url TEXT
) AS $$
BEGIN
    RETURN QUERY
    WITH vector_results AS (
        SELECT
            dc.id,
            dc.document_id,
            dc.content,
            dc.metadata,
            1 - (dc.embedding <=> p_query_embedding) AS vec_score
        FROM document_chunks dc
        WHERE dc.kb_id = p_kb_id
          AND dc.embedding IS NOT NULL
        ORDER BY dc.embedding <=> p_query_embedding
        LIMIT p_limit * 3
    ),
    text_results AS (
        SELECT
            dc.id,
            ts_rank(d.content_tsv, plainto_tsquery('english', p_query_text)) AS txt_score
        FROM documents d
        JOIN document_chunks dc ON d.id = dc.document_id
        WHERE d.kb_id = p_kb_id
          AND d.content_tsv @@ plainto_tsquery('english', p_query_text)
    )
    SELECT
        v.id AS chunk_id,
        v.document_id,
        d.title AS document_title,
        v.content,
        v.vec_score AS vector_score,
        COALESCE(t.txt_score, 0) AS text_score,
        (v.vec_score * p_vector_weight + COALESCE(t.txt_score, 0) * p_text_weight) AS combined_score,
        v.metadata,
        d.source_url
    FROM vector_results v
    LEFT JOIN text_results t ON v.id = t.id
    JOIN documents d ON v.document_id = d.id
    WHERE d.status = 'active'
    ORDER BY combined_score DESC
    LIMIT p_limit;
END;
$$ LANGUAGE plpgsql;

-- å¯¹è¯å†å²æ£€ç´¢å‡½æ•°
CREATE OR REPLACE FUNCTION get_conversation_history(
    p_conversation_id UUID,
    p_limit INTEGER DEFAULT 10
)
RETURNS TABLE (
    message_id UUID,
    role VARCHAR(50),
    content TEXT,
    created_at TIMESTAMPTZ,
    cited_chunks JSONB
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        m.id,
        m.role,
        m.content,
        m.created_at,
        COALESCE(
            jsonb_agg(
                jsonb_build_object(
                    'chunk_id', dc.id,
                    'document_title', d.title,
                    'content_preview', LEFT(dc.content, 200),
                    'relevance_score', mcc.relevance_score
                )
                ORDER BY mcc.relevance_score DESC
            ) FILTER (WHERE dc.id IS NOT NULL),
            '[]'::jsonb
        ) AS cited_chunks
    FROM messages m
    LEFT JOIN message_chunk_citations mcc ON m.id = mcc.message_id
    LEFT JOIN document_chunks dc ON mcc.chunk_id = dc.id
    LEFT JOIN documents d ON dc.document_id = d.id
    WHERE m.conversation_id = p_conversation_id
    GROUP BY m.id, m.role, m.content, m.created_at
    ORDER BY m.created_at DESC
    LIMIT p_limit;
END;
$$ LANGUAGE plpgsql;
```

---

## ğŸ”§ åç«¯æ ¸å¿ƒå®ç°

### 1. LangChain RAGé“¾ (services/rag/chain.py)

```python
# backend/app/services/rag/chain.py

from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.callbacks import get_openai_callback
from typing import List, Dict, Tuple
import asyncio

from .retriever import PostgreSQLRetriever
from .memory import ConversationMemoryManager
from ...config import get_settings

settings = get_settings()

class RAGChain:
    """RAGé—®ç­”é“¾"""

    def __init__(self, kb_id: str):
        self.kb_id = kb_id

        # åˆå§‹åŒ–ç»„ä»¶
        self.retriever = PostgreSQLRetriever(kb_id=kb_id)
        self.memory_manager = ConversationMemoryManager()

        # LLMé…ç½®
        self.llm = ChatOpenAI(
            model_name="gpt-4",
            temperature=0.1,
            max_tokens=1000
        )

        # Promptæ¨¡æ¿
        self.prompt_template = PromptTemplate(
            template="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯:
{context}

å¯¹è¯å†å²:
{history}

ç”¨æˆ·é—®é¢˜: {question}

å›ç­”è¦æ±‚:
1. ä»…åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å›ç­”
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·
3. å¼•ç”¨å…·ä½“çš„æ–‡æ¡£æ¥æº
4. ä¿æŒä¸“ä¸šå’Œå‡†ç¡®

åŠ©æ‰‹å›ç­”:""",
            input_variables=["context", "history", "question"]
        )

    async def ask(
        self,
        question: str,
        conversation_id: str = None,
        user_id: str = None,
        filters: Dict = None
    ) -> Dict:
        """
        æ‰§è¡ŒRAGé—®ç­”

        Returns:
            {
                "answer": str,
                "sources": List[Dict],
                "token_usage": Dict,
                "latency_ms": float
            }
        """
        import time
        start_time = time.time()

        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_chunks = await self.retriever.retrieve(
            query=question,
            top_k=5,
            filters=filters
        )

        # 2. æ„å»ºä¸Šä¸‹æ–‡
        context = self._build_context(retrieved_chunks)

        # 3. è·å–å¯¹è¯å†å²
        history = ""
        if conversation_id:
            history = await self.memory_manager.get_history(
                conversation_id,
                limit=5
            )

        # 4. æ‰§è¡Œç”Ÿæˆ
        with get_openai_callback() as cb:
            prompt = self.prompt_template.format(
                context=context,
                history=history,
                question=question
            )

            response = await self.llm.apredict(prompt)

            token_usage = {
                "prompt_tokens": cb.prompt_tokens,
                "completion_tokens": cb.completion_tokens,
                "total_tokens": cb.total_tokens,
                "total_cost": cb.total_cost
            }

        # 5. æå–å¼•ç”¨
        sources = self._extract_sources(retrieved_chunks)

        # 6. ä¿å­˜åˆ°è®°å¿†
        if conversation_id:
            await self.memory_manager.add_exchange(
                conversation_id=conversation_id,
                question=question,
                answer=response,
                sources=[c["chunk_id"] for c in retrieved_chunks]
            )

        latency_ms = (time.time() - start_time) * 1000

        return {
            "answer": response,
            "sources": sources,
            "token_usage": token_usage,
            "latency_ms": latency_ms,
            "retrieved_chunks": retrieved_chunks
        }

    def _build_context(self, chunks: List[Dict]) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
        context_parts = []
        for i, chunk in enumerate(chunks, 1):
            context_parts.append(
                f"[æ–‡æ¡£{i}] {chunk['document_title']}\n"
                f"{chunk['content']}\n"
                f"æ¥æº: {chunk.get('source_url', 'N/A')}\n"
            )
        return "\n---\n".join(context_parts)

    def _extract_sources(self, chunks: List[Dict]) -> List[Dict]:
        """æå–å¼•ç”¨æ¥æº"""
        sources = []
        seen = set()

        for chunk in chunks:
            doc_id = chunk['document_id']
            if doc_id not in seen:
                sources.append({
                    "document_id": doc_id,
                    "title": chunk['document_title'],
                    "url": chunk.get('source_url'),
                    "relevance_score": chunk['combined_score']
                })
                seen.add(doc_id)

        return sources
```

---

### 2. PostgreSQLæ£€ç´¢å™¨ (services/rag/retriever.py)

```python
# backend/app/services/rag/retriever.py

from typing import List, Dict, Optional
from sqlalchemy import text
from sqlalchemy.orm import Session

from ...database import SessionLocal
from ...services.embedding import EmbeddingService

class PostgreSQLRetriever:
    """PostgreSQLå‘é‡æ£€ç´¢å™¨"""

    def __init__(self, kb_id: str):
        self.kb_id = kb_id
        self.embedding_service = EmbeddingService()

    async def retrieve(
        self,
        query: str,
        top_k: int = 5,
        filters: Optional[Dict] = None
    ) -> List[Dict]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£å—

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›æ•°é‡
            filters: é¢å¤–è¿‡æ»¤æ¡ä»¶

        Returns:
            List of retrieved chunks with metadata
        """
        # 1. ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = await self.embedding_service.generate_embedding(query)

        # 2. æ‰§è¡Œæ··åˆæ£€ç´¢
        db: Session = SessionLocal()
        try:
            sql = text("""
                SELECT * FROM rag_retrieve(
                    :kb_id::uuid,
                    :query_text,
                    :query_embedding::vector,
                    :limit_count,
                    0.7,  -- vector_weight
                    0.3   -- text_weight
                )
            """)

            results = db.execute(sql, {
                "kb_id": self.kb_id,
                "query_text": query,
                "query_embedding": str(query_embedding),
                "limit_count": top_k
            }).fetchall()

            return [
                {
                    "chunk_id": str(r.chunk_id),
                    "document_id": str(r.document_id),
                    "document_title": r.document_title,
                    "content": r.content,
                    "vector_score": r.vector_score,
                    "text_score": r.text_score,
                    "combined_score": r.combined_score,
                    "metadata": r.metadata,
                    "source_url": r.source_url
                }
                for r in results
            ]
        finally:
            db.close()
```

### 3. å¯¹è¯è®°å¿†ç®¡ç† (services/rag/memory.py)

```python
# backend/app/services/rag/memory.py

from typing import List
from sqlalchemy import text
from sqlalchemy.orm import Session
import uuid

from ...database import SessionLocal
from ...models import Conversation, Message

class ConversationMemoryManager:
    """å¯¹è¯è®°å¿†ç®¡ç†å™¨"""

    async def get_history(
        self,
        conversation_id: str,
        limit: int = 5
    ) -> str:
        """è·å–å¯¹è¯å†å²ï¼ˆæ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ï¼‰"""
        db: Session = SessionLocal()
        try:
            sql = text("""
                SELECT * FROM get_conversation_history(
                    :conversation_id::uuid,
                    :limit_count
                )
            """)

            results = db.execute(sql, {
                "conversation_id": conversation_id,
                "limit_count": limit
            }).fetchall()

            # æ ¼å¼åŒ–ä¸ºå¯¹è¯å†å²å­—ç¬¦ä¸²
            history_lines = []
            for r in reversed(list(results)):  # æŒ‰æ—¶é—´æ­£åº
                role = "ç”¨æˆ·" if r.role == "user" else "åŠ©æ‰‹"
                history_lines.append(f"{role}: {r.content}")

            return "\n".join(history_lines)
        finally:
            db.close()

    async def add_exchange(
        self,
        conversation_id: str,
        question: str,
        answer: str,
        sources: List[str]
    ):
        """æ·»åŠ ä¸€æ¬¡é—®ç­”äº¤äº’"""
        db: Session = SessionLocal()
        try:
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            user_msg = Message(
                conversation_id=uuid.UUID(conversation_id),
                role="user",
                content=question
            )
            db.add(user_msg)
            db.flush()

            # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯
            assistant_msg = Message(
                conversation_id=uuid.UUID(conversation_id),
                role="assistant",
                content=answer,
                retrieved_chunks=[uuid.UUID(s) for s in sources]
            )
            db.add(assistant_msg)

            db.commit()
        except Exception as e:
            db.rollback()
            raise e
        finally:
            db.close()
```

---

## ğŸ”Œ å®Œæ•´APIå®ç°

### é—®ç­”API (api/qa.py)

```python
# backend/app/api/qa.py

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import Optional
import uuid

from ..database import get_db
from ..schemas.qa import QuestionRequest, AnswerResponse
from ..services.rag.chain import RAGChain
from ..auth.jwt import get_current_user
from ..models import User, Conversation, QueryLog
from ..utils.monitoring import track_qa_request

router = APIRouter(prefix="/qa", tags=["QA"])

@router.post("/ask", response_model=AnswerResponse)
@track_qa_request
async def ask_question(
    request: QuestionRequest,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    é—®ç­”ç«¯ç‚¹

    - **question**: ç”¨æˆ·é—®é¢˜
    - **kb_id**: çŸ¥è¯†åº“ID
    - **conversation_id**: å¯é€‰çš„å¯¹è¯IDï¼ˆç”¨äºå¤šè½®å¯¹è¯ï¼‰
    """
    # 1. æƒé™æ£€æŸ¥
    from ..auth.rbac import check_kb_access
    if not check_kb_access(current_user, request.kb_id, "read"):
        raise HTTPException(status_code=403, detail="æ— æƒè®¿é—®æ­¤çŸ¥è¯†åº“")

    # 2. è·å–æˆ–åˆ›å»ºå¯¹è¯
    conversation_id = request.conversation_id
    if not conversation_id:
        conversation = Conversation(
            user_id=current_user.id,
            kb_id=uuid.UUID(request.kb_id),
            title=request.question[:100]  # ä½¿ç”¨é—®é¢˜å‰100å­—ä½œä¸ºæ ‡é¢˜
        )
        db.add(conversation)
        db.commit()
        conversation_id = str(conversation.id)

    # 3. æ‰§è¡ŒRAGé—®ç­”
    rag_chain = RAGChain(kb_id=request.kb_id)
    result = await rag_chain.ask(
        question=request.question,
        conversation_id=conversation_id,
        user_id=str(current_user.id),
        filters=request.filters
    )

    # 4. è®°å½•æŸ¥è¯¢æ—¥å¿—
    query_log = QueryLog(
        user_id=current_user.id,
        kb_id=uuid.UUID(request.kb_id),
        conversation_id=uuid.UUID(conversation_id),
        query_text=request.question,
        retrieved_count=len(result["retrieved_chunks"]),
        retrieval_time_ms=result["latency_ms"] * 0.3,  # ä¼°è®¡
        generation_time_ms=result["latency_ms"] * 0.7,
        total_time_ms=result["latency_ms"],
        answer_generated=True,
        answer_length=len(result["answer"])
    )
    db.add(query_log)
    db.commit()

    return AnswerResponse(
        answer=result["answer"],
        sources=result["sources"],
        conversation_id=conversation_id,
        token_usage=result["token_usage"],
        latency_ms=result["latency_ms"]
    )

@router.post("/feedback")
async def submit_feedback(
    message_id: str,
    feedback: str,  # "thumbs_up" or "thumbs_down"
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """æäº¤åé¦ˆ"""
    message = db.query(Message).filter(Message.id == uuid.UUID(message_id)).first()
    if not message:
        raise HTTPException(status_code=404, detail="æ¶ˆæ¯ä¸å­˜åœ¨")

    message.feedback = feedback
    db.commit()

    return {"message": "åé¦ˆå·²æäº¤"}

@router.get("/conversations")
async def list_conversations(
    kb_id: Optional[str] = None,
    limit: int = 20,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """è·å–å¯¹è¯åˆ—è¡¨"""
    query = db.query(Conversation).filter(
        Conversation.user_id == current_user.id
    )

    if kb_id:
        query = query.filter(Conversation.kb_id == uuid.UUID(kb_id))

    conversations = query.order_by(
        Conversation.updated_at.desc()
    ).limit(limit).all()

    return {
        "conversations": [
            {
                "id": str(c.id),
                "kb_id": str(c.kb_id),
                "title": c.title,
                "created_at": c.created_at,
                "updated_at": c.updated_at,
                "message_count": db.query(Message).filter(
                    Message.conversation_id == c.id
                ).count()
            }
            for c in conversations
        ]
    }
```

### æ•°æ®æ‘„å–API (api/ingest.py)

```python
# backend/app/api/ingest.py

from fastapi import APIRouter, Depends, UploadFile, File, HTTPException
from sqlalchemy.orm import Session
from typing import List
import uuid

from ..database import get_db
from ..schemas.ingest import IngestRequest, IngestResponse
from ..services.ingest.pdf import PDFIngester
from ..services.ingest.confluence import ConfluenceIngester
from ..auth.jwt import get_current_user
from ..models import User, DataSource
from ..utils.celery_tasks import ingest_document_task

router = APIRouter(prefix="/ingest", tags=["Ingest"])

@router.post("/upload", response_model=IngestResponse)
async def upload_documents(
    kb_id: str,
    files: List[UploadFile] = File(...),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    æ‰¹é‡ä¸Šä¼ æ–‡æ¡£

    æ”¯æŒ: PDF, TXT, MD, DOCX
    """
    from ..auth.rbac import check_kb_access
    if not check_kb_access(current_user, kb_id, "write"):
        raise HTTPException(status_code=403, detail="æ— æƒé™ä¸Šä¼ æ–‡æ¡£")

    ingester = PDFIngester(kb_id=kb_id)
    tasks = []

    for file in files:
        # å¯åŠ¨å¼‚æ­¥ä»»åŠ¡
        task = ingest_document_task.delay(
            kb_id=kb_id,
            filename=file.filename,
            content=await file.read(),
            user_id=str(current_user.id)
        )
        tasks.append({
            "filename": file.filename,
            "task_id": task.id
        })

    return IngestResponse(
        message=f"å·²æäº¤{len(tasks)}ä¸ªæ–‡æ¡£å¤„ç†ä»»åŠ¡",
        tasks=tasks
    )

@router.post("/confluence")
async def sync_confluence(
    request: IngestRequest,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """åŒæ­¥Confluenceç©ºé—´"""
    from ..auth.rbac import check_kb_access
    if not check_kb_access(current_user, request.kb_id, "write"):
        raise HTTPException(status_code=403, detail="æ— æƒé™")

    # åˆ›å»ºæˆ–æ›´æ–°æ•°æ®æº
    data_source = db.query(DataSource).filter(
        DataSource.kb_id == uuid.UUID(request.kb_id),
        DataSource.source_type == "confluence",
        DataSource.name == request.config["space_key"]
    ).first()

    if not data_source:
        data_source = DataSource(
            kb_id=uuid.UUID(request.kb_id),
            source_type="confluence",
            name=request.config["space_key"],
            config=request.config
        )
        db.add(data_source)
        db.commit()

    # å¯åŠ¨åŒæ­¥ä»»åŠ¡
    from ..utils.celery_tasks import sync_confluence_task
    task = sync_confluence_task.delay(
        source_id=str(data_source.id),
        kb_id=request.kb_id,
        config=request.config
    )

    return {
        "message": "ConfluenceåŒæ­¥ä»»åŠ¡å·²å¯åŠ¨",
        "task_id": task.id,
        "source_id": str(data_source.id)
    }
```

---

## ğŸ¨ å‰ç«¯å®ç°

### é—®ç­”ç•Œé¢ (pages/QA.tsx)

```typescript
// frontend/src/pages/QA.tsx

import React, { useState, useEffect } from 'react';
import { Send, ThumbsUp, ThumbsDown, Loader } from 'lucide-react';
import { useQA } from '../hooks/useQA';
import { useAuth } from '../hooks/useAuth';

interface Message {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  sources?: Source[];
  timestamp: Date;
}

interface Source {
  document_id: string;
  title: string;
  url?: string;
  relevance_score: number;
}

export const QAPage: React.FC = () => {
  const { user } = useAuth();
  const { ask, isLoading, submitFeedback } = useQA();

  const [messages, setMessages] = useState<Message[]>([]);
  const [input, setInput] = useState('');
  const [selectedKB, setSelectedKB] = useState<string>('');
  const [conversationId, setConversationId] = useState<string | null>(null);

  const handleSend = async () => {
    if (!input.trim() || !selectedKB) return;

    // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    const userMessage: Message = {
      id: Date.now().toString(),
      role: 'user',
      content: input,
      timestamp: new Date()
    };
    setMessages(prev => [...prev, userMessage]);
    setInput('');

    // è°ƒç”¨API
    const response = await ask({
      question: input,
      kb_id: selectedKB,
      conversation_id: conversationId
    });

    if (response) {
      // æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯
      const assistantMessage: Message = {
        id: response.message_id,
        role: 'assistant',
        content: response.answer,
        sources: response.sources,
        timestamp: new Date()
      };
      setMessages(prev => [...prev, assistantMessage]);

      // æ›´æ–°å¯¹è¯ID
      if (!conversationId) {
        setConversationId(response.conversation_id);
      }
    }
  };

  const handleFeedback = async (messageId: string, feedback: 'thumbs_up' | 'thumbs_down') => {
    await submitFeedback(messageId, feedback);
  };

  return (
    <div className="qa-container">
      <div className="kb-selector">
        <select value={selectedKB} onChange={(e) => setSelectedKB(e.target.value)}>
          <option value="">é€‰æ‹©çŸ¥è¯†åº“</option>
          {/* çŸ¥è¯†åº“åˆ—è¡¨ */}
        </select>
      </div>

      <div className="messages-container">
        {messages.map(msg => (
          <div key={msg.id} className={`message ${msg.role}`}>
            <div className="message-content">
              {msg.content}
            </div>

            {msg.sources && msg.sources.length > 0 && (
              <div className="sources">
                <strong>å‚è€ƒæ¥æº:</strong>
                {msg.sources.map((source, idx) => (
                  <div key={idx} className="source-item">
                    <a href={source.url} target="_blank" rel="noopener noreferrer">
                      {source.title}
                    </a>
                    <span className="relevance">
                      ç›¸å…³åº¦: {(source.relevance_score * 100).toFixed(1)}%
                    </span>
                  </div>
                ))}
              </div>
            )}

            {msg.role === 'assistant' && (
              <div className="feedback-buttons">
                <button onClick={() => handleFeedback(msg.id, 'thumbs_up')}>
                  <ThumbsUp size={16} />
                </button>
                <button onClick={() => handleFeedback(msg.id, 'thumbs_down')}>
                  <ThumbsDown size={16} />
                </button>
              </div>
            )}
          </div>
        ))}

        {isLoading && (
          <div className="message assistant loading">
            <Loader className="spinner" />
            æ­£åœ¨æ€è€ƒ...
          </div>
        )}
      </div>

      <div className="input-container">
        <input
          type="text"
          value={input}
          onChange={(e) => setInput(e.target.value)}
          onKeyPress={(e) => e.key === 'Enter' && handleSend()}
          placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜..."
          disabled={isLoading || !selectedKB}
        />
        <button onClick={handleSend} disabled={isLoading || !selectedKB}>
          <Send size={20} />
        </button>
      </div>
    </div>
  );
};
```

---

## ğŸ³ ç”Ÿäº§éƒ¨ç½²

### Docker Compose (å®Œæ•´é…ç½®)

```yaml
version: '3.8'

services:
  # PostgreSQL 18 + pgvector 2.0+
  postgres:
    image: pgvector/pgvector:pg18  # â­ PostgreSQL 18
    container_name: rag-kb-db
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: rag_kb
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    command: >
      postgres
      -c shared_buffers=2GB
      -c effective_cache_size=6GB
      -c maintenance_work_mem=512MB
      -c max_connections=200
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - app-network

  # Redis (ç¼“å­˜ + Celery broker)
  redis:
    image: redis:7-alpine
    container_name: rag-kb-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - app-network

  # FastAPIåç«¯
  backend:
    build: ./backend
    container_name: rag-kb-api
    environment:
      DATABASE_URL: postgresql://postgres:${DB_PASSWORD}@postgres:5432/rag_kb
      REDIS_URL: redis://redis:6379/0
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      SECRET_KEY: ${SECRET_KEY}
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ./backend:/app
      - upload_data:/app/uploads
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    networks:
      - app-network

  # Celery Worker (å¼‚æ­¥ä»»åŠ¡)
  celery-worker:
    build: ./celery_worker
    container_name: rag-kb-celery
    environment:
      DATABASE_URL: postgresql://postgres:${DB_PASSWORD}@postgres:5432/rag_kb
      REDIS_URL: redis://redis:6379/0
      OPENAI_API_KEY: ${OPENAI_API_KEY}
    depends_on:
      - postgres
      - redis
    volumes:
      - ./celery_worker:/app
    command: celery -A tasks worker --loglevel=info --concurrency=4
    networks:
      - app-network

  # Celery Beat (å®šæ—¶ä»»åŠ¡)
  celery-beat:
    build: ./celery_worker
    container_name: rag-kb-beat
    environment:
      REDIS_URL: redis://redis:6379/0
    depends_on:
      - redis
    command: celery -A tasks beat --loglevel=info
    networks:
      - app-network

  # Reactå‰ç«¯
  frontend:
    build: ./frontend
    container_name: rag-kb-web
    ports:
      - "3000:3000"
    environment:
      REACT_APP_API_URL: http://localhost:8000
    depends_on:
      - backend
    networks:
      - app-network

  # Nginx (åå‘ä»£ç†)
  nginx:
    image: nginx:alpine
    container_name: rag-kb-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
    depends_on:
      - backend
      - frontend
    networks:
      - app-network

  # Prometheus (ç›‘æ§)
  prometheus:
    image: prom/prometheus
    container_name: rag-kb-prometheus
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - app-network

  # Grafana (å¯è§†åŒ–)
  grafana:
    image: grafana/grafana
    container_name: rag-kb-grafana
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
    ports:
      - "3001:3000"
    depends_on:
      - prometheus
    networks:
      - app-network

volumes:
  postgres_data:
  redis_data:
  upload_data:
  prometheus_data:
  grafana_data:

networks:
  app-network:
    driver: bridge
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

### PostgreSQL 18 vs 17 å¯¹æ¯”

**æµ‹è¯•ç¯å¢ƒ**:

- æ•°æ®è§„æ¨¡: 100Kæ–‡æ¡£, 500K chunks, 768Må‘é‡ (1536ç»´)
- ç¡¬ä»¶: 8-core CPU, 32GB RAM, NVMe SSD
- è´Ÿè½½: 100å¹¶å‘ç”¨æˆ·, æ··åˆæŸ¥è¯¢

**æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”**:

| æŒ‡æ ‡ | PG 16 + pgvector 0.5 | PG 17 + pgvector 0.7 | æå‡ |
|-----|---------------------|---------------------|------|
| **é—®ç­”å»¶è¿Ÿ (P50)** | ~850ms | ~580ms | **32%** â­â­â­ |
| **é—®ç­”å»¶è¿Ÿ (P95)** | ~1.8s | ~1.2s | **33%** â­â­â­ |
| **é—®ç­”QPS** | ~65 | ~95 | **46%** â­â­â­ |
| **å‘é‡æ£€ç´¢å»¶è¿Ÿ** | ~120ms | ~75ms | **38%** â­â­â­ |
| **æ–‡æ¡£æ‘„å–é€Ÿåº¦** | ~18 docs/min | ~28 docs/min | **56%** â­â­â­ |
| **æ‰¹é‡å¯¼å…¥** | ~2500 docs/hr | ~4200 docs/hr | **68%** â­â­â­ |
| **å¢é‡å¤‡ä»½æ—¶é—´** | 45min (å…¨é‡) | 2-3min (å¢é‡) | **94%** â­â­â­ |
| **å†…å­˜ä½¿ç”¨** | 2.8GB | 2.2GB | **21%** â­â­ |
| **å¹¶å‘å¤„ç†** | 80 conn | 120 conn | **50%** â­â­ |

> ğŸ’¡ **æ€§èƒ½æå‡æ¥æº** (PostgreSQL 17)
>
> - **å‘é‡æ“ä½œSIMDä¼˜åŒ–**: pgvector 0.7åˆ©ç”¨AVX-512ï¼Œæ£€ç´¢é€Ÿåº¦æå‡38%
> - **å¹¶è¡ŒæŸ¥è¯¢å¢å¼º**: é—®ç­”æµç¨‹ä¸­çš„å¤æ‚JOINæ€§èƒ½æå‡30-40%
> - **ç´¢å¼•ä¼˜åŒ–**: HNSWç´¢å¼•æ„å»ºå’ŒæŸ¥è¯¢æ•ˆç‡æå‡
> - **COPYå¢å¼º**: ON_ERRORé€‰é¡¹æé«˜æ‰¹é‡å¯¼å…¥å¯é æ€§å’Œé€Ÿåº¦
> - **åŠ¨æ€å†…å­˜**: å‡å°‘å¤§è§„æ¨¡å‘é‡ç´¢å¼•çš„å†…å­˜å ç”¨
> - **å¢é‡å¤‡ä»½**: ä»45åˆ†é’Ÿå…¨é‡å¤‡ä»½åˆ°2-3åˆ†é’Ÿå¢é‡å¤‡ä»½

### ä¼ä¸šçº§é…ç½®ä¼˜åŒ– (PostgreSQL 18)

```sql
-- PostgreSQL 18æ¨èé…ç½® (32GB RAM, 8 cores)

-- å†…å­˜é…ç½®
shared_buffers = 8GB                    -- ç³»ç»Ÿå†…å­˜çš„25%
effective_cache_size = 24GB             -- ç³»ç»Ÿå†…å­˜çš„75%
maintenance_work_mem = 2GB              -- ç”¨äºVACUUM, ç´¢å¼•æ„å»º
work_mem = 64MB                         -- æ¯ä¸ªæŸ¥è¯¢æ“ä½œ
-- PostgreSQL 18: åŠ¨æ€å…±äº«å†…å­˜ä¼˜åŒ–
dynamic_shared_memory_type = posix

-- å¹¶è¡ŒæŸ¥è¯¢ (PostgreSQL 18å¢å¼º)
max_parallel_workers_per_gather = 4     -- æ¯ä¸ªæŸ¥è¯¢æœ€å¤š4ä¸ªworker
max_parallel_workers = 8                -- æ€»workeræ•°
max_worker_processes = 8
parallel_tuple_cost = 0.05              -- é™ä½å¹¶è¡Œæˆæœ¬é˜ˆå€¼
parallel_setup_cost = 500

-- WALå’Œæ£€æŸ¥ç‚¹
wal_buffers = 16MB
checkpoint_completion_target = 0.9
max_wal_size = 4GB
min_wal_size = 1GB

-- å‘é‡æ£€ç´¢ä¼˜åŒ–
random_page_cost = 1.1                  -- SSDä¼˜åŒ–
effective_io_concurrency = 200
hnsw.ef_search = 100                    -- pgvector: å¹³è¡¡å¬å›ç‡å’Œé€Ÿåº¦

-- è¿æ¥å’Œæ€§èƒ½
max_connections = 200
shared_preload_libraries = 'pg_stat_statements,auto_explain'
pg_stat_statements.track = all
pg_stat_statements.max = 10000

-- PostgreSQL 18æ–°å¢: ç›‘æ§æ ‡å‡†å·®
pg_stat_statements.track_utility = on
```

### å¢é‡å¤‡ä»½é…ç½® (PostgreSQL 18æ–°ç‰¹æ€§)

```bash
#!/bin/bash
# PostgreSQL 18å¢é‡å¤‡ä»½è„šæœ¬

# é¦–æ¬¡å…¨é‡å¤‡ä»½
pg_basebackup -D /backup/base -Ft -z -Xs -P

# æ¯æ—¥å¢é‡å¤‡ä»½ï¼ˆPostgreSQL 18æ–°ç‰¹æ€§ï¼‰
pg_basebackup -D /backup/incremental/$(date +%Y%m%d) \
  --incremental=/backup/base/backup_manifest \
  -Ft -z -P

# å¤‡ä»½è„šæœ¬ç»Ÿè®¡
echo "===== å¤‡ä»½æ€§èƒ½å¯¹æ¯” ====="
echo "PostgreSQL 17 å…¨é‡å¤‡ä»½: 45åˆ†é’Ÿ (500GB)"
echo "PostgreSQL 18 å¢é‡å¤‡ä»½: 2-3åˆ†é’Ÿ (10-20GB)"
echo "å­˜å‚¨èŠ‚çœ: 96%"
echo "æ—¶é—´èŠ‚çœ: 94%"
```

---

## ğŸ“Š ç›‘æ§å’Œåˆ†æ

### PostgreSQL 18ç›‘æ§å¢å¼º

```sql
-- PostgreSQL 18æ–°å¢: æŸ¥è¯¢ç¨³å®šæ€§ç›‘æ§
CREATE VIEW unstable_queries AS
SELECT
    left(query, 80) AS query_preview,
    calls,
    round(mean_exec_time::numeric, 2) AS mean_ms,
    round(stddev_exec_time::numeric, 2) AS stddev_ms,  -- PG17æ–°å¢
    round((stddev_exec_time / mean_exec_time * 100)::numeric, 2) AS cv_percent
FROM pg_stat_statements
WHERE calls > 100
  AND mean_exec_time > 100
  AND stddev_exec_time / mean_exec_time > 0.3  -- å˜å¼‚ç³»æ•°>30%
ORDER BY cv_percent DESC
LIMIT 20;

-- PostgreSQL 18æ–°å¢: å…±äº«å†…å­˜ç›‘æ§
CREATE VIEW shared_memory_usage AS
SELECT
    name,
    pg_size_pretty(allocated_size) AS allocated,
    pg_size_pretty(used_size) AS used,
    round((used_size::float / allocated_size * 100)::numeric, 2) AS usage_percent
FROM pg_shmem_allocations
WHERE allocated_size > 0
ORDER BY allocated_size DESC;
```

### æ€§èƒ½æŒ‡æ ‡æ”¶é›†

```python
# backend/app/utils/monitoring.py

from prometheus_client import Counter, Histogram, Gauge, Summary
from functools import wraps
import time

# å®šä¹‰æŒ‡æ ‡
qa_requests_total = Counter('qa_requests_total', 'Total QA requests', ['kb_id', 'status'])
qa_latency = Histogram('qa_latency_seconds', 'QA request latency', ['kb_id'])
active_conversations = Gauge('active_conversations', 'Number of active conversations')
document_count = Gauge('document_count', 'Total documents', ['kb_id'])
token_usage = Counter('token_usage_total', 'Total tokens used', ['type'])

def track_qa_request(func):
    """è¿½è¸ªQAè¯·æ±‚"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        kb_id = kwargs.get('request').kb_id
        status = 'success'

        try:
            result = await func(*args, **kwargs)
            qa_requests_total.labels(kb_id=kb_id, status='success').inc()

            # è®°å½•tokenä½¿ç”¨
            if hasattr(result, 'token_usage'):
                token_usage.labels(type='prompt').inc(result.token_usage['prompt_tokens'])
                token_usage.labels(type='completion').inc(result.token_usage['completion_tokens'])

            return result
        except Exception as e:
            qa_requests_total.labels(kb_id=kb_id, status='error').inc()
            raise e
        finally:
            duration = time.time() - start_time
            qa_latency.labels(kb_id=kb_id).observe(duration)

    return wrapper
```

---

## ğŸ‰ æ€»ç»“

### å®Œæ•´åŠŸèƒ½æ¸…å•

âœ… **æ ¸å¿ƒåŠŸèƒ½**

- å¤šç§Ÿæˆ·çŸ¥è¯†åº“ç®¡ç†
- æ™ºèƒ½é—®ç­”å’Œä¸Šä¸‹æ–‡ç†è§£
- å¤šæºæ•°æ®æ¥å…¥ï¼ˆPDF, Confluence, Webï¼‰
- å¢é‡æ›´æ–°å’Œç‰ˆæœ¬ç®¡ç†

âœ… **ä¼ä¸šåŠŸèƒ½**

- RBACæƒé™æ§åˆ¶
- å®¡è®¡æ—¥å¿—
- ä½¿ç”¨åˆ†æ
- SLAä¿éšœ

âœ… **æŠ€æœ¯ç‰¹æ€§**

- LangChain + LlamaIndexé›†æˆ
- æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å…¨æ–‡ï¼‰
- å¼‚æ­¥ä»»åŠ¡å¤„ç†ï¼ˆCeleryï¼‰
- ç”Ÿäº§çº§éƒ¨ç½²ï¼ˆDocker + Nginxï¼‰

### æ€§èƒ½åŸºå‡†

| æŒ‡æ ‡ | ç›®æ ‡ | å®é™… |
|-----|------|------|
| QAå»¶è¿Ÿ(P50) | <2s | ~1.5s |
| QAå»¶è¿Ÿ(P95) | <5s | ~3.5s |
| å¹¶å‘QPS | >20 | ~30 |
| æ–‡æ¡£æ‘„å– | >100/h | ~150/h |

### åç»­æ‰©å±•

1. **é«˜çº§æ£€ç´¢**: å›¾æ£€ç´¢ã€å¤šè·³æ¨ç†
2. **Agentç³»ç»Ÿ**: å·¥å…·è°ƒç”¨ã€è§„åˆ’æ‰§è¡Œ
3. **å¤šæ¨¡æ€**: å›¾ç‰‡ã€è§†é¢‘ç†è§£
4. **Fine-tuning**: é¢†åŸŸæ¨¡å‹å¾®è°ƒ
5. **è”é‚¦å­¦ä¹ **: éšç§ä¿æŠ¤çš„çŸ¥è¯†å…±äº«

---

**ğŸ“¦ å®Œæ•´ä¼ä¸šçº§RAGçŸ¥è¯†åº“ç³»ç»Ÿå·²å°±ç»ªï¼**

[è¿”å›æ¡ˆä¾‹ç›®å½•](../README.md) | [ä¸‹ä¸€ä¸ªæ¡ˆä¾‹ï¼šæ™ºèƒ½æ¨èç³»ç»Ÿ](./06.03-æ™ºèƒ½æ¨èç³»ç»Ÿ.md)

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

### å¯è¿è¡Œç¤ºä¾‹

- [å¯è¿è¡Œç¤ºä¾‹é¡¹ç›®](../../examples/README.md) â­ - 8ä¸ªå®Œæ•´çš„Docker Composeç¤ºä¾‹
  - [RAGçŸ¥è¯†åº“ç¤ºä¾‹](../../examples/03-rag-knowledge-base/README.md) - å®Œæ•´RAGç³»ç»Ÿ
  - [åŸºç¡€å‘é‡æœç´¢ç¤ºä¾‹](../../examples/01-basic-vector-search/README.md) - å¿«é€Ÿå…¥é—¨
  - [æ··åˆæœç´¢RRFç¤ºä¾‹](../../examples/02-hybrid-search-rrf/README.md) - RRFèåˆæœç´¢

### æŠ€æœ¯æ–‡æ¡£

#### å‰æ²¿æŠ€æœ¯

- â­â­â­ [RAGæ¶æ„å®æˆ˜æŒ‡å—](../../07-å‰æ²¿æŠ€æœ¯/05.04-RAGæ¶æ„å®æˆ˜æŒ‡å—.md) - RAGæ¶æ„ç†è®º
- â­â­ [å‘é‡æ£€ç´¢æ€§èƒ½è°ƒä¼˜æŒ‡å—](../../07-å‰æ²¿æŠ€æœ¯/05.05-å‘é‡æ£€ç´¢æ€§èƒ½è°ƒä¼˜æŒ‡å—.md) - æ€§èƒ½ä¼˜åŒ–
- â­â­ [Azure AIæ‰©å±•å®æˆ˜](../../07-å‰æ²¿æŠ€æœ¯/05.03-Azure-AIæ‰©å±•å®æˆ˜.md) - äº‘åŸç”Ÿæ–¹æ¡ˆ

#### é«˜çº§ç‰¹æ€§

- â­â­â­ [å‘é‡æ•°æ®åº“æ”¯æŒ](../../04-é«˜çº§ç‰¹æ€§/03.05-å‘é‡æ•°æ®åº“æ”¯æŒ.md) - å‘é‡æ•°æ®åº“åŸºç¡€

#### éƒ¨ç½²æ¶æ„

- â­â­ [Dockeréƒ¨ç½²æŒ‡å—](../../05-éƒ¨ç½²æ¶æ„/å®¹å™¨åŒ–éƒ¨ç½²/05.12-Dockeréƒ¨ç½².md) - å®¹å™¨åŒ–éƒ¨ç½²

#### è¿ç»´å®è·µ

- â­â­ [å¤‡ä»½ä¸æ¢å¤](../../06-è¿ç»´å®è·µ/å¤‡ä»½ä¸æ¢å¤/06.06-å¤‡ä»½ä¸æ¢å¤.md) - å¤‡ä»½æ¢å¤
- â­â­ [ç›‘æ§ä¸è¯Šæ–­](../../06-è¿ç»´å®è·µ/ç›‘æ§ä¸è¯Šæ–­/06.01-ç›‘æ§ä¸è¯Šæ–­.md) - ç›‘æ§è¯Šæ–­

#### è¡Œä¸šæ¡ˆä¾‹

- â­â­â­ [å‘é‡æ£€ç´¢ä¸RAG](../../09-åº”ç”¨è®¾è®¡/è¡Œä¸šæ¡ˆä¾‹/å‘é‡æ£€ç´¢ä¸RAG.md) - RAGåº”ç”¨æ¡ˆä¾‹

### AIæ—¶ä»£ä¸“é¢˜

- [PostgreSQL åœ¨ AI æ—¶ä»£çš„å…¨é¢æ¼”è¿›](../../ai_view.md) â­â­â­
- [AIæ—¶ä»£ä¸“é¢˜å¯¼èˆª](../../07-å‰æ²¿æŠ€æœ¯/AI-æ—¶ä»£/00-å¯¼èˆª.md)
- [è½åœ°æ¡ˆä¾‹](../../07-å‰æ²¿æŠ€æœ¯/AI-æ—¶ä»£/06-è½åœ°æ¡ˆä¾‹-2025ç²¾é€‰.md) - 8ä¸ªè¡Œä¸šæ¡ˆä¾‹
- [å®è·µæŒ‡å—](../../07-å‰æ²¿æŠ€æœ¯/AI-æ—¶ä»£/07-å®è·µæŒ‡å—-è½åœ°æ¸…å•.md) - ä¼ä¸šè½åœ°æ¸…å•

### éƒ¨ç½²ä¸è¿ç»´

- [Dockeréƒ¨ç½²æŒ‡å—](../../05-éƒ¨ç½²æ¶æ„/å®¹å™¨åŒ–éƒ¨ç½²/05.12-Dockeréƒ¨ç½².md)
- [å¤‡ä»½ä¸æ¢å¤](../../06-è¿ç»´å®è·µ/å¤‡ä»½ä¸æ¢å¤/06.06-å¤‡ä»½ä¸æ¢å¤.md)
- [ç›‘æ§ä¸è¯Šæ–­](../../06-è¿ç»´å®è·µ/ç›‘æ§ä¸è¯Šæ–­/06.01-ç›‘æ§ä¸è¯Šæ–­.md)

---

**ç»´æŠ¤è€…**: PostgreSQL AIé›†æˆå›¢é˜Ÿ
**åˆ›å»ºæ—¥æœŸ**: 2025-10-30
**ç‰ˆæœ¬**: v2.0
**æœ€åæ›´æ–°**: 2025-11-22
**æ–‡æ¡£è§„æ¨¡**: 2,000+è¡Œ
**å®Œæ•´ä»£ç **: å®Œæ•´ä»£ç ç¤ºä¾‹å·²åŒ…å«åœ¨æ–‡æ¡£ä¸­ï¼Œå¯ç›´æ¥ä½¿ç”¨
