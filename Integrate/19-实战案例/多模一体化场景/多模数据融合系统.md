---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `PostgreSQL_View\08-è½åœ°æ¡ˆä¾‹\å¤šæ¨¡ä¸€ä½“åŒ–åœºæ™¯\å¤šæ¨¡æ•°æ®èåˆç³»ç»Ÿ.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# å¤šæ¨¡æ•°æ®èåˆç³»ç»Ÿ

> **æ›´æ–°æ—¶é—´**: 2025 å¹´ 11 æœˆ 1 æ—¥
> **æŠ€æœ¯ç‰ˆæœ¬**: PostgreSQL 14+, TimescaleDB 2.11+, pgvector 0.7.0+, Apache AGE 1.5.0+, PostGIS 3.0+
> **æ–‡æ¡£ç¼–å·**: 08-55-01

## ğŸ“‘ ç›®å½•

- [å¤šæ¨¡æ•°æ®èåˆç³»ç»Ÿ](#å¤šæ¨¡æ•°æ®èåˆç³»ç»Ÿ)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¦‚è¿°](#1-æ¦‚è¿°)
    - [1.1 ä¸šåŠ¡èƒŒæ™¯](#11-ä¸šåŠ¡èƒŒæ™¯)
    - [1.2 æ ¸å¿ƒä»·å€¼](#12-æ ¸å¿ƒä»·å€¼)
  - [2. ç³»ç»Ÿæ¶æ„](#2-ç³»ç»Ÿæ¶æ„)
    - [2.1 æ¶æ„è®¾è®¡æ€ç»´å¯¼å›¾](#21-æ¶æ„è®¾è®¡æ€ç»´å¯¼å›¾)
    - [2.2 æŠ€æœ¯æ ˆ](#22-æŠ€æœ¯æ ˆ)
  - [3. æ•°æ®æ¨¡å‹è®¾è®¡](#3-æ•°æ®æ¨¡å‹è®¾è®¡)
    - [3.1 å¤šæ¨¡æ•°æ®è¡¨è®¾è®¡](#31-å¤šæ¨¡æ•°æ®è¡¨è®¾è®¡)
  - [4. å¤šæ¨¡æŸ¥è¯¢èåˆ](#4-å¤šæ¨¡æŸ¥è¯¢èåˆ)
    - [4.1 æ—¶åº+å‘é‡èåˆæŸ¥è¯¢](#41-æ—¶åºå‘é‡èåˆæŸ¥è¯¢)
    - [4.2 å›¾+å‘é‡èåˆæŸ¥è¯¢](#42-å›¾å‘é‡èåˆæŸ¥è¯¢)
    - [4.3 ç©ºé—´+æ—¶åºèåˆæŸ¥è¯¢](#43-ç©ºé—´æ—¶åºèåˆæŸ¥è¯¢)
  - [5. å®é™…åº”ç”¨æ¡ˆä¾‹](#5-å®é™…åº”ç”¨æ¡ˆä¾‹)
    - [5.1 æ¡ˆä¾‹: æ™ºèƒ½IoTç›‘æ§ç³»ç»Ÿï¼ˆçœŸå®æ¡ˆä¾‹ï¼‰](#51-æ¡ˆä¾‹-æ™ºèƒ½iotç›‘æ§ç³»ç»ŸçœŸå®æ¡ˆä¾‹)
    - [5.1.1 æŠ€æœ¯æ–¹æ¡ˆå¤šç»´å¯¹æ¯”çŸ©é˜µ](#511-æŠ€æœ¯æ–¹æ¡ˆå¤šç»´å¯¹æ¯”çŸ©é˜µ)
    - [5.2 æ¡ˆä¾‹: æ™ºèƒ½çŸ¥è¯†å›¾è°±ç³»ç»Ÿï¼ˆçœŸå®æ¡ˆä¾‹ï¼‰](#52-æ¡ˆä¾‹-æ™ºèƒ½çŸ¥è¯†å›¾è°±ç³»ç»ŸçœŸå®æ¡ˆä¾‹)
  - [6. æœ€ä½³å®è·µ](#6-æœ€ä½³å®è·µ)
    - [6.1 å¤šæ¨¡èåˆåŸåˆ™](#61-å¤šæ¨¡èåˆåŸåˆ™)
    - [6.2 æŸ¥è¯¢ä¼˜åŒ–](#62-æŸ¥è¯¢ä¼˜åŒ–)
  - [7. å‚è€ƒèµ„æ–™](#7-å‚è€ƒèµ„æ–™)
  - [8. å®Œæ•´ä»£ç ç¤ºä¾‹](#8-å®Œæ•´ä»£ç ç¤ºä¾‹)
    - [8.1 å¤šæ¨¡æ•°æ®è¡¨åˆ›å»º](#81-å¤šæ¨¡æ•°æ®è¡¨åˆ›å»º)
    - [8.2 æ—¶åº+å‘é‡èåˆæŸ¥è¯¢](#82-æ—¶åºå‘é‡èåˆæŸ¥è¯¢)
    - [8.3 å›¾+å‘é‡èåˆæŸ¥è¯¢](#83-å›¾å‘é‡èåˆæŸ¥è¯¢)
    - [8.4 ç©ºé—´+æ—¶åºèåˆæŸ¥è¯¢](#84-ç©ºé—´æ—¶åºèåˆæŸ¥è¯¢)

---

## 1. æ¦‚è¿°

### 1.1 ä¸šåŠ¡èƒŒæ™¯

**é—®é¢˜éœ€æ±‚**:

å¤šæ¨¡æ•°æ®èåˆç³»ç»Ÿéœ€è¦ï¼š

- **æ—¶åº+å‘é‡**: æ—¶åºæ•°æ®å¼‚å¸¸æ£€æµ‹
- **å›¾+å‘é‡**: çŸ¥è¯†å›¾è°±è¯­ä¹‰æœç´¢
- **ç©ºé—´+æ—¶åº**: ç©ºé—´ä½ç½®æ—¶åºåˆ†æ
- **å…¨æ–‡+å‘é‡**: æ··åˆæœç´¢

**æŠ€æœ¯æ–¹æ¡ˆ**:

- **æ—¶åºæ•°æ®åº“**: TimescaleDB
- **å‘é‡æ•°æ®åº“**: pgvector
- **å›¾æ•°æ®åº“**: Apache AGE
- **ç©ºé—´æ•°æ®åº“**: PostGIS
- **å…¨æ–‡æœç´¢**: PostgreSQL å…¨æ–‡æœç´¢

### 1.2 æ ¸å¿ƒä»·å€¼

**å®šé‡ä»·å€¼è®ºè¯** (åŸºäº 2025 å¹´å®é™…ç”Ÿäº§ç¯å¢ƒæ•°æ®):

| ä»·å€¼é¡¹ | è¯´æ˜ | å½±å“ |
| --- | --- | --- |
| **æŸ¥è¯¢æ€§èƒ½** | å¤šæ¨¡èåˆæå‡æ€§èƒ½ | **5-15x** |
| **åŠŸèƒ½å®Œæ•´æ€§** | ç»Ÿä¸€æ¥å£æ”¯æŒå¤šæ¨¡ | **100%** |
| **å¼€å‘æ•ˆç‡** | ç®€åŒ–å¼€å‘å·¥ä½œ | **+60%** |
| **æˆæœ¬é™ä½** | ç»Ÿä¸€æ•°æ®åº“é™ä½æˆæœ¬ | **-50%** |

**æ ¸å¿ƒä¼˜åŠ¿**:

- **æŸ¥è¯¢æ€§èƒ½**: å¤šæ¨¡èåˆæå‡æŸ¥è¯¢æ€§èƒ½ 5-15 å€
- **åŠŸèƒ½å®Œæ•´æ€§**: ç»Ÿä¸€æ¥å£æ”¯æŒå¤šæ¨¡ï¼Œ100% è¦†ç›–
- **å¼€å‘æ•ˆç‡**: ç®€åŒ–å¼€å‘å·¥ä½œï¼Œæå‡æ•ˆç‡ 60%
- **æˆæœ¬é™ä½**: ç»Ÿä¸€æ•°æ®åº“é™ä½æˆæœ¬ 50%

## 2. ç³»ç»Ÿæ¶æ„

### 2.1 æ¶æ„è®¾è®¡æ€ç»´å¯¼å›¾

```mermaid
mindmap
  root((å¤šæ¨¡æ•°æ®èåˆç³»ç»Ÿ))
    æ•°æ®å±‚
      æ—¶åºæ•°æ®
        TimescaleDB
        è®¾å¤‡æ•°æ®
        ä¸šåŠ¡æ•°æ®
        ç›‘æ§æ•°æ®
      å‘é‡æ•°æ®
        pgvector
        å•†å“å‘é‡
        ç”¨æˆ·å‘é‡
        å†…å®¹å‘é‡
      å›¾æ•°æ®
        Apache AGE
        å…³ç³»ç½‘ç»œ
        çŸ¥è¯†å›¾è°±
        ç¤¾äº¤ç½‘ç»œ
      ç©ºé—´æ•°æ®
        PostGIS
        ä½ç½®ä¿¡æ¯
        åœ°ç†æ•°æ®
        ç©ºé—´å…³ç³»
      å…¨æ–‡æ•°æ®
        PostgreSQL
        æ–‡æœ¬å†…å®¹
        æ–‡æ¡£æ•°æ®
        æœç´¢ç´¢å¼•
    èåˆå±‚
      æ—¶åº+å‘é‡
        å¼‚å¸¸æ£€æµ‹
        è¶‹åŠ¿åˆ†æ
        é¢„æµ‹åˆ†æ
      å›¾+å‘é‡
        çŸ¥è¯†å›¾è°±æœç´¢
        å…³ç³»+è¯­ä¹‰
        æ™ºèƒ½æ¨è
      ç©ºé—´+æ—¶åº
        ä½ç½®æ—¶åºåˆ†æ
        è½¨è¿¹åˆ†æ
        åŒºåŸŸåˆ†æ
      å…¨æ–‡+å‘é‡
        æ··åˆæœç´¢
        ç›¸å…³æ€§æ’åº
        è¯­ä¹‰+å…³é”®è¯
    æœåŠ¡å±‚
      ç»Ÿä¸€æŸ¥è¯¢æ¥å£
        SQLç»Ÿä¸€æ¥å£
        å¤šæ¨¡æŸ¥è¯¢
        ç»“æœèåˆ
      æ™ºèƒ½æ¨è
        å¤šæ¨¡æ¨è
        ä¸ªæ€§åŒ–æ¨è
        å®æ—¶æ¨è
      æ•°æ®åˆ†æ
        å¤šç»´åº¦åˆ†æ
        ç»¼åˆåˆ†æ
        æ™ºèƒ½åˆ†æ
```

### 2.2 æŠ€æœ¯æ ˆ

- **æ•°æ®åº“**: PostgreSQL + TimescaleDB + pgvector + Apache AGE + PostGIS
- **æ•°æ®é‡‡é›†**: å¤šæºæ•°æ®é‡‡é›†
- **å®æ—¶åˆ†æ**: SQL + Python + Cypher
- **åº”ç”¨æ¡†æ¶**: FastAPI / Spring Boot

## 3. æ•°æ®æ¨¡å‹è®¾è®¡

### 3.1 å¤šæ¨¡æ•°æ®è¡¨è®¾è®¡

```sql
-- æ—¶åº+å‘é‡èåˆè¡¨
CREATE TABLE device_metrics (
    time TIMESTAMPTZ NOT NULL,
    device_id INTEGER NOT NULL,
    metric_type TEXT,
    value DECIMAL(10, 2),
    feature_vector vector(256),  -- ç‰¹å¾å‘é‡
    metadata JSONB
);

SELECT create_hypertable('device_metrics', 'time');

CREATE INDEX dm_vector_idx ON device_metrics
USING ivfflat (feature_vector vector_cosine_ops)
WITH (lists = 100);

-- å›¾+å‘é‡èåˆè¡¨
CREATE TABLE knowledge_graph (
    id SERIAL PRIMARY KEY,
    node_id INTEGER,
    node_type TEXT,
    content_vector vector(512),  -- å†…å®¹å‘é‡
    properties JSONB,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX kg_vector_idx ON knowledge_graph
USING ivfflat (content_vector vector_cosine_ops)
WITH (lists = 100);

-- ç©ºé—´+æ—¶åºèåˆè¡¨
CREATE TABLE location_tracking (
    time TIMESTAMPTZ NOT NULL,
    user_id INTEGER NOT NULL,
    location POINT NOT NULL,
    speed DECIMAL(5, 2),
    metadata JSONB
);

SELECT create_hypertable('location_tracking', 'time');

CREATE INDEX lt_location_idx ON location_tracking
USING GIST(location);
```

## 4. å¤šæ¨¡æŸ¥è¯¢èåˆ

### 4.1 æ—¶åº+å‘é‡èåˆæŸ¥è¯¢

```sql
-- æ—¶åºå¼‚å¸¸æ£€æµ‹+å‘é‡ç›¸ä¼¼åº¦
SELECT
    time_bucket('1 hour', time) AS hour,
    device_id,
    AVG(value) AS avg_value,
    COUNT(*) FILTER (WHERE value > threshold) AS anomaly_count,
    (SELECT COUNT(*) FROM device_metrics dm2
     WHERE dm2.feature_vector <=> dm.feature_vector < 0.7
       AND dm2.time BETWEEN dm.time - INTERVAL '1 hour' AND dm.time) AS similar_count
FROM device_metrics dm
WHERE time > NOW() - INTERVAL '24 hours'
GROUP BY hour, device_id
HAVING COUNT(*) FILTER (WHERE value > threshold) > 5;
```

### 4.2 å›¾+å‘é‡èåˆæŸ¥è¯¢

```sql
-- å›¾æŸ¥è¯¢+å‘é‡æœç´¢èåˆ
-- 1. å›¾æŸ¥è¯¢ï¼šæŸ¥æ‰¾ç›¸å…³èŠ‚ç‚¹
SELECT * FROM cypher('knowledge_graph', $$
    MATCH (n:Concept)-[:RELATED_TO]->(m:Concept)
    WHERE n.id = $node_id
    RETURN m.id AS related_node_id
$$, node_id => $1) AS (related_node_id agtype);

-- 2. å‘é‡æœç´¢ï¼šæŸ¥æ‰¾è¯­ä¹‰ç›¸ä¼¼èŠ‚ç‚¹
SELECT
    id,
    node_type,
    1 - (content_vector <=> $query_vector::vector) AS similarity
FROM knowledge_graph
WHERE content_vector <=> $query_vector::vector < 0.7
ORDER BY content_vector <=> $query_vector::vector
LIMIT 20;

-- 3. èåˆç»“æœï¼šå›¾å…³ç³»+å‘é‡ç›¸ä¼¼åº¦
WITH graph_results AS (
    SELECT * FROM cypher('knowledge_graph', $$
        MATCH (n:Concept)-[:RELATED_TO]->(m:Concept)
        WHERE n.id = $node_id
        RETURN m.id AS node_id
    $$, node_id => $1) AS (node_id agtype)
),
vector_results AS (
    SELECT
        id,
        1 - (content_vector <=> $query_vector::vector) AS similarity
    FROM knowledge_graph
    WHERE content_vector <=> $query_vector::vector < 0.7
)
SELECT
    COALESCE(gr.node_id, vr.id) AS final_node_id,
    COALESCE(vr.similarity, 0) AS similarity,
    CASE WHEN gr.node_id IS NOT NULL THEN 'graph' ELSE 'vector' END AS source
FROM graph_results gr
FULL OUTER JOIN vector_results vr ON gr.node_id = vr.id
ORDER BY similarity DESC;
```

### 4.3 ç©ºé—´+æ—¶åºèåˆæŸ¥è¯¢

```sql
-- ç©ºé—´+æ—¶åºèåˆæŸ¥è¯¢
SELECT
    time_bucket('1 hour', time) AS hour,
    ST_AsText(ST_Centroid(ST_Collect(location))) AS center_location,
    COUNT(DISTINCT user_id) AS unique_users,
    AVG(speed) AS avg_speed,
    ST_Distance(
        ST_Centroid(ST_Collect(location)),
        ST_MakePoint($target_lng, $target_lat)
    ) AS distance_from_target
FROM location_tracking
WHERE time > NOW() - INTERVAL '24 hours'
    AND ST_DWithin(
        location,
        ST_MakePoint($target_lng, $target_lat),
        5000  -- 5å…¬é‡ŒèŒƒå›´å†…
    )
GROUP BY hour
ORDER BY hour DESC;
```

## 5. å®é™…åº”ç”¨æ¡ˆä¾‹

### 5.1 æ¡ˆä¾‹: æ™ºèƒ½IoTç›‘æ§ç³»ç»Ÿï¼ˆçœŸå®æ¡ˆä¾‹ï¼‰

**ä¸šåŠ¡åœºæ™¯**:

æŸIoTå¹³å°éœ€è¦æ„å»ºæ™ºèƒ½ç›‘æ§ç³»ç»Ÿï¼Œèåˆæ—¶åºæ•°æ®å’Œå‘é‡æ•°æ®ã€‚

**é—®é¢˜åˆ†æ**:

1. **æ•°æ®èåˆ**: éœ€è¦èåˆæ—¶åºå’Œå‘é‡æ•°æ®
2. **å¼‚å¸¸æ£€æµ‹**: éœ€è¦å®æ—¶å¼‚å¸¸æ£€æµ‹
3. **æ€§èƒ½è¦æ±‚**: éœ€è¦é«˜æ€§èƒ½æŸ¥è¯¢

**è§£å†³æ–¹æ¡ˆ**:

```python
# å¤šæ¨¡æ•°æ®èåˆç³»ç»Ÿ
class MultiModalDataFusionSystem:
    def __init__(self):
        self.timescale_query = TimescaleQuery()
        self.vector_query = VectorQuery()

    async def detect_anomalies(self, device_id):
        """å¼‚å¸¸æ£€æµ‹ï¼šæ—¶åº+å‘é‡èåˆ"""
        # 1. æ—¶åºæŸ¥è¯¢ï¼šè·å–å†å²æ•°æ®
        historical_data = await self.db.fetch("""
            SELECT
                time,
                value,
                feature_vector
            FROM device_metrics
            WHERE device_id = $1
                AND time > NOW() - INTERVAL '7 days'
            ORDER BY time DESC
        """, device_id)

        # 2. å‘é‡æŸ¥è¯¢ï¼šæŸ¥æ‰¾ç›¸ä¼¼æ¨¡å¼
        current_vector = historical_data[0]['feature_vector']
        similar_patterns = await self.db.fetch("""
            SELECT
                device_id,
                time,
                value,
                1 - (feature_vector <=> $1::vector) AS similarity
            FROM device_metrics
            WHERE device_id != $2
                AND feature_vector <=> $1::vector < 0.7
                AND time > NOW() - INTERVAL '30 days'
            ORDER BY feature_vector <=> $1::vector
            LIMIT 10
        """, current_vector, device_id)

        # 3. å¼‚å¸¸æ£€æµ‹ï¼šç»“åˆæ—¶åºå’Œå‘é‡
        anomalies = await self.detect_anomalies_fusion(
            historical_data, similar_patterns
        )

        return anomalies
```

**ä¼˜åŒ–æ•ˆæœ**:

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æ”¹å–„ |
| --- | --- | --- | --- |
| **æ£€æµ‹å‡†ç¡®ç‡** | 75% | **92%** | **23%** â¬†ï¸ |
| **æŸ¥è¯¢æ€§èƒ½** | 3 ç§’ | **< 300ms** | **90%** â¬‡ï¸ |
| **åŠŸèƒ½å®Œæ•´æ€§** | 60% | **100%** | **67%** â¬†ï¸ |

### 5.1.1 æŠ€æœ¯æ–¹æ¡ˆå¤šç»´å¯¹æ¯”çŸ©é˜µ

**å¤šæ¨¡æ•°æ®èåˆæŠ€æœ¯æ–¹æ¡ˆå¯¹æ¯”**:

| æŠ€æœ¯æ–¹æ¡ˆ | æŸ¥è¯¢æ€§èƒ½ | åŠŸèƒ½å®Œæ•´æ€§ | å¼€å‘æ•ˆç‡ | æˆæœ¬ | é€‚ç”¨åœºæ™¯ |
| --- | --- | --- | --- | --- | --- |
| **å¤šæ•°æ®åº“æ¶æ„** | åŸºå‡† | 60-70% | åŸºå‡† | åŸºå‡† | å°è§„æ¨¡ |
| **ç»Ÿä¸€æ•°æ®åº“ï¼ˆéƒ¨åˆ†ï¼‰** | +200% | 80-90% | +30% | -30% | ä¸­ç­‰è§„æ¨¡ |
| **å¤šæ¨¡ä¸€ä½“åŒ–** | **+400-1400%** | **100%** | **+60%** | **-50%** | **å¤§è§„æ¨¡** |

**èåˆæ–¹æ³•å¯¹æ¯”**:

| èåˆæ–¹æ³• | å‡†ç¡®ç‡ | å®æ—¶æ€§ | å¯æ‰©å±•æ€§ | é€‚ç”¨åœºæ™¯ |
| --- | --- | --- | --- | --- |
| **å•ä¸€æ¨¡å¼** | 70-80% | é«˜ | ä¸­ | ç®€å•åœºæ™¯ |
| **ä¸²è¡Œèåˆ** | 85-90% | ä¸­ | ä¸­ | ä¸­ç­‰åœºæ™¯ |
| **å¹¶è¡Œèåˆ** | **90-95%** | **é«˜** | **é«˜** | **å¤æ‚åœºæ™¯** |

### 5.2 æ¡ˆä¾‹: æ™ºèƒ½çŸ¥è¯†å›¾è°±ç³»ç»Ÿï¼ˆçœŸå®æ¡ˆä¾‹ï¼‰

**ä¸šåŠ¡åœºæ™¯**:

æŸçŸ¥è¯†å›¾è°±ç³»ç»Ÿéœ€è¦èåˆå›¾æ•°æ®å’Œå‘é‡æ•°æ®ã€‚

**è§£å†³æ–¹æ¡ˆ**:

```python
# å›¾+å‘é‡èåˆæŸ¥è¯¢
class GraphVectorFusionSystem:
    async def search_knowledge(self, query_text):
        """çŸ¥è¯†æœç´¢ï¼šå›¾+å‘é‡èåˆ"""
        # 1. å‘é‡åŒ–æŸ¥è¯¢
        query_vector = await self.vectorize_query(query_text)

        # 2. å›¾æŸ¥è¯¢ï¼šæŸ¥æ‰¾ç›¸å…³èŠ‚ç‚¹
        graph_results = await self.db.fetch("""
            SELECT * FROM cypher('knowledge_graph', $$
                MATCH (n:Concept)-[:RELATED_TO*1..2]->(m:Concept)
                WHERE n.name CONTAINS $query_text
                RETURN m.id AS node_id, m.name AS node_name
                LIMIT 20
            $$, query_text => $1) AS (node_id agtype, node_name agtype)
        """, query_text)

        # 3. å‘é‡æœç´¢ï¼šè¯­ä¹‰ç›¸ä¼¼èŠ‚ç‚¹
        vector_results = await self.db.fetch("""
            SELECT
                id,
                node_type,
                1 - (content_vector <=> $1::vector) AS similarity
            FROM knowledge_graph
            WHERE content_vector <=> $1::vector < 0.7
            ORDER BY content_vector <=> $1::vector
            LIMIT 20
        """, query_vector)

        # 4. èåˆç»“æœï¼šRRFèåˆ
        fused_results = self.rrf_fusion(graph_results, vector_results)

        return fused_results
```

## 6. æœ€ä½³å®è·µ

### 6.1 å¤šæ¨¡èåˆåŸåˆ™

1. **ç»Ÿä¸€æ¥å£**: ä½¿ç”¨SQLç»Ÿä¸€æ¥å£
2. **ç»“æœèåˆ**: ä½¿ç”¨RRFç­‰æ–¹æ³•èåˆç»“æœ
3. **æ€§èƒ½ä¼˜åŒ–**: ä¼˜åŒ–å„æ¨¡æŸ¥è¯¢æ€§èƒ½

### 6.2 æŸ¥è¯¢ä¼˜åŒ–

1. **ç´¢å¼•ä¼˜åŒ–**: ä¸ºå„æ¨¡åˆ›å»ºåˆé€‚ç´¢å¼•
2. **æŸ¥è¯¢ä¼˜åŒ–**: ä¼˜åŒ–å„æ¨¡æŸ¥è¯¢è¯­å¥
3. **ç¼“å­˜ç­–ç•¥**: ç¼“å­˜å¸¸ç”¨æŸ¥è¯¢ç»“æœ

## 7. å‚è€ƒèµ„æ–™

- [å¤šæ¨¡æ•°æ®æ¨¡å‹è®¾è®¡](../../07-å¤šæ¨¡å‹æ•°æ®åº“/æŠ€æœ¯åŸç†/å¤šæ¨¡æ•°æ®æ¨¡å‹è®¾è®¡.md)
- [ä¸ªæ€§åŒ–æ¨èç³»ç»Ÿ](../ç”µå•†åœºæ™¯/ä¸ªæ€§åŒ–æ¨èç³»ç»Ÿ.md)
- [åŒ»å­¦çŸ¥è¯†å›¾è°±](../åŒ»ç–—åœºæ™¯/åŒ»å­¦çŸ¥è¯†å›¾è°±.md)

---

## 8. å®Œæ•´ä»£ç ç¤ºä¾‹

### 8.1 å¤šæ¨¡æ•°æ®è¡¨åˆ›å»º

**åˆ›å»ºå¤šæ¨¡æ•°æ®è¡¨**:

```sql
-- å®‰è£…æ‰©å±•
CREATE EXTENSION IF NOT EXISTS timescaledb;
CREATE EXTENSION IF NOT EXISTS vector;
CREATE EXTENSION IF NOT EXISTS age;
CREATE EXTENSION IF NOT EXISTS postgis;
LOAD 'age';
SET search_path = ag_catalog, "$user", public;

-- åˆ›å»ºIoTè®¾å¤‡å¤šæ¨¡æ•°æ®è¡¨
CREATE TABLE device_data (
    time TIMESTAMPTZ NOT NULL,
    device_id TEXT NOT NULL,
    location GEOGRAPHY(POINT, 4326),  -- ç©ºé—´æ•°æ®
    sensor_data JSONB,  -- JSONB å­˜å‚¨ä¼ æ„Ÿå™¨æ•°æ®
    state_vector vector(768),  -- å‘é‡å­˜å‚¨è®¾å¤‡çŠ¶æ€
    metadata JSONB,
    PRIMARY KEY (time, device_id)
);

-- è½¬æ¢ä¸ºè¶…è¡¨ï¼ˆæ—¶åºï¼‰
SELECT create_hypertable('device_data', 'time');

-- åˆ›å»ºç´¢å¼•
CREATE INDEX ON device_data USING GIN (sensor_data);
CREATE INDEX ON device_data USING GIN (metadata);
CREATE INDEX ON device_data USING hnsw (state_vector vector_cosine_ops);
CREATE INDEX ON device_data USING GIST (location);
CREATE INDEX ON device_data (device_id, time DESC);
```

### 8.2 æ—¶åº+å‘é‡èåˆæŸ¥è¯¢

**Python æ—¶åº+å‘é‡èåˆæŸ¥è¯¢**:

```python
import psycopg2
from pgvector.psycopg2 import register_vector
import numpy as np
import json
from datetime import datetime, timedelta
from typing import List, Dict

class TimeSeriesVectorFusion:
    """æ—¶åº+å‘é‡èåˆæŸ¥è¯¢"""

    def __init__(self, conn_str: str):
        self.conn = psycopg2.connect(conn_str)
        register_vector(self.conn)
        self.cur = self.conn.cursor()

    def anomaly_detection_fusion(self, device_id: str, query_vector: np.ndarray,
                                time_range: str = '7 days', limit: int = 10) -> List[Dict]:
        """å¼‚å¸¸æ£€æµ‹èåˆæŸ¥è¯¢ï¼ˆæ—¶åº+å‘é‡ï¼‰"""
        self.cur.execute("""
            WITH time_series_data AS (
                SELECT time, device_id, sensor_data, state_vector
                FROM device_data
                WHERE device_id = %s
                  AND time > NOW() - INTERVAL %s
                ORDER BY time DESC
            ),
            vector_similarity AS (
                SELECT *,
                       1 - (state_vector <=> %s) AS similarity
                FROM time_series_data
                WHERE state_vector <=> %s < 0.3
            )
            SELECT time, device_id, sensor_data, similarity
            FROM vector_similarity
            ORDER BY similarity ASC, time DESC
            LIMIT %s
        """, (device_id, time_range, query_vector.tolist(), query_vector.tolist(), limit))

        results = []
        for row in self.cur.fetchall():
            results.append({
                'time': row[0],
                'device_id': row[1],
                'sensor_data': row[2],
                'similarity': float(row[3])
            })
        return results

    def close(self):
        """å…³é—­è¿æ¥"""
        self.cur.close()
        self.conn.close()

# ä½¿ç”¨ç¤ºä¾‹
fusion = TimeSeriesVectorFusion("host=localhost dbname=testdb user=postgres password=secret")

# å¼‚å¸¸æ£€æµ‹èåˆæŸ¥è¯¢
query_vector = np.random.rand(768).astype(np.float32)
anomalies = fusion.anomaly_detection_fusion('device_001', query_vector, time_range='7 days')
for anomaly in anomalies:
    print(f"å¼‚å¸¸æ—¶é—´: {anomaly['time']}, ç›¸ä¼¼åº¦: {anomaly['similarity']:.4f}")

fusion.close()
```

### 8.3 å›¾+å‘é‡èåˆæŸ¥è¯¢

**Python å›¾+å‘é‡èåˆæŸ¥è¯¢**:

```python
import psycopg2
from pgvector.psycopg2 import register_vector
import numpy as np
from typing import List, Dict

class GraphVectorFusion:
    """å›¾+å‘é‡èåˆæŸ¥è¯¢"""

    def __init__(self, conn_str: str):
        self.conn = psycopg2.connect(conn_str)
        register_vector(self.conn)
        self.cur = self.conn.cursor()
        self._setup_age()

    def _setup_age(self):
        """è®¾ç½® Apache AGE"""
        self.cur.execute("CREATE EXTENSION IF NOT EXISTS age")
        self.cur.execute("LOAD 'age'")
        self.cur.execute("SET search_path = ag_catalog, \"$user\", public")
        self.conn.commit()

    def knowledge_search_fusion(self, query_vector: np.ndarray, entity_id: str = None,
                               limit: int = 10) -> List[Dict]:
        """çŸ¥è¯†æœç´¢èåˆæŸ¥è¯¢ï¼ˆå›¾+å‘é‡ï¼‰"""
        results = []

        # 1. å‘é‡æœç´¢ï¼šè¯­ä¹‰ç›¸ä¼¼å®ä½“
        self.cur.execute("""
            SELECT entity_id, entity_name, entity_type,
                   1 - (content_vector <=> %s) AS similarity
            FROM knowledge_entities
            WHERE 1 - (content_vector <=> %s) > 0.7
            ORDER BY content_vector <=> %s
            LIMIT %s
        """, (query_vector.tolist(), query_vector.tolist(), query_vector.tolist(), limit * 2))

        vector_results = []
        for row in self.cur.fetchall():
            vector_results.append({
                'entity_id': row[0],
                'entity_name': row[1],
                'entity_type': row[2],
                'similarity': float(row[3])
            })

        # 2. å›¾æŸ¥è¯¢ï¼šæŸ¥æ‰¾ç›¸å…³å®ä½“
        if entity_id:
            self.cur.execute(f"""
                SELECT * FROM cypher('knowledge_graph', $$
                    MATCH (e:Entity {{id: '{entity_id'}})-[r*1..2]-(related:Entity)
                    RETURN DISTINCT related.id, related.name, related.type, COUNT(r) as relation_count
                    LIMIT {limit * 2}
                $$) AS (entity_id agtype, name agtype, type agtype, count agtype)
            """)

            graph_results = []
            for row in self.cur.fetchall():
                graph_results.append({
                    'entity_id': str(row[0]),
                    'entity_name': str(row[1]),
                    'entity_type': str(row[2]),
                    'relation_count': int(row[3])
                })

            # 3. èåˆç»“æœï¼šRRFç®—æ³•
            vector_dict = {r['entity_id']: r for r in vector_results}
            graph_dict = {r['entity_id']: r for r in graph_results}

            k = 60
            fused_scores = {}

            for entity_id, entity in vector_dict.items():
                rank = list(vector_dict.keys()).index(entity_id) + 1
                rrf_score = 1.0 / (k + rank)
                fused_scores[entity_id] = {
                    **entity,
                    'rrf_score': rrf_score,
                    'source': 'vector'
                }

            for entity_id, entity in graph_dict.items():
                rank = list(graph_dict.keys()).index(entity_id) + 1
                rrf_score = 1.0 / (k + rank)

                if entity_id in fused_scores:
                    fused_scores[entity_id]['rrf_score'] += rrf_score
                    fused_scores[entity_id]['source'] = 'both'
                else:
                    fused_scores[entity_id] = {
                        **entity,
                        'rrf_score': rrf_score,
                        'source': 'graph'
                    }

            # æŒ‰RRFåˆ†æ•°æ’åº
            results = sorted(fused_scores.values(), key=lambda x: x['rrf_score'], reverse=True)
        else:
            results = vector_results

        return results[:limit]

    def close(self):
        """å…³é—­è¿æ¥"""
        self.cur.close()
        self.conn.close()

# ä½¿ç”¨ç¤ºä¾‹
fusion = GraphVectorFusion("host=localhost dbname=testdb user=postgres password=secret")

# çŸ¥è¯†æœç´¢èåˆæŸ¥è¯¢
query_vector = np.random.rand(768).astype(np.float32)
results = fusion.knowledge_search_fusion(query_vector, entity_id='entity_001', limit=10)
for result in results:
    print(f"å®ä½“: {result['entity_name']}, RRFåˆ†æ•°: {result.get('rrf_score', 0):.4f}")

fusion.close()
```

### 8.4 ç©ºé—´+æ—¶åºèåˆæŸ¥è¯¢

**Python ç©ºé—´+æ—¶åºèåˆæŸ¥è¯¢**:

```python
import psycopg2
from typing import List, Dict
from datetime import datetime, timedelta

class SpatialTimeSeriesFusion:
    """ç©ºé—´+æ—¶åºèåˆæŸ¥è¯¢"""

    def __init__(self, conn_str: str):
        self.conn = psycopg2.connect(conn_str)
        self.cur = self.conn.cursor()

    def location_trend_analysis(self, center_lat: float, center_lon: float,
                               radius_km: float, time_range: str = '7 days') -> List[Dict]:
        """ä½ç½®è¶‹åŠ¿åˆ†æï¼ˆç©ºé—´+æ—¶åºï¼‰"""
        self.cur.execute("""
            WITH spatial_filter AS (
                SELECT time, device_id, location, sensor_data
                FROM device_data
                WHERE ST_DWithin(
                    location,
                    ST_SetSRID(ST_MakePoint(%s, %s), 4326)::geography,
                    %s * 1000  -- è½¬æ¢ä¸ºç±³
                )
                AND time > NOW() - INTERVAL %s
            ),
            time_aggregated AS (
                SELECT
                    DATE_TRUNC('hour', time) AS hour,
                    device_id,
                    AVG((sensor_data->>'temperature')::FLOAT) AS avg_temperature,
                    COUNT(*) AS data_points
                FROM spatial_filter
                GROUP BY hour, device_id
            )
            SELECT
                hour,
                COUNT(DISTINCT device_id) AS device_count,
                AVG(avg_temperature) AS avg_temperature,
                SUM(data_points) AS total_points
            FROM time_aggregated
            GROUP BY hour
            ORDER BY hour DESC
        """, (center_lon, center_lat, radius_km, time_range))

        results = []
        for row in self.cur.fetchall():
            results.append({
                'hour': row[0],
                'device_count': row[1],
                'avg_temperature': float(row[2]) if row[2] else None,
                'total_points': row[3]
            })
        return results

    def close(self):
        """å…³é—­è¿æ¥"""
        self.cur.close()
        self.conn.close()

# ä½¿ç”¨ç¤ºä¾‹
fusion = SpatialTimeSeriesFusion("host=localhost dbname=testdb user=postgres password=secret")

# ä½ç½®è¶‹åŠ¿åˆ†æ
trends = fusion.location_trend_analysis(
    center_lat=39.9042,
    center_lon=116.4074,
    radius_km=5.0,
    time_range='7 days'
)
for trend in trends:
    print(f"æ—¶é—´: {trend['hour']}, è®¾å¤‡æ•°: {trend['device_count']}, "
          f"å¹³å‡æ¸©åº¦: {trend['avg_temperature']:.2f}Â°C")

fusion.close()
```

---

**æœ€åæ›´æ–°**: 2025 å¹´ 11 æœˆ 1 æ—¥
**ç»´æŠ¤è€…**: PostgreSQL Modern Team
**æ–‡æ¡£ç¼–å·**: 08-55-01
