---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `DataBaseTheory\19-åœºæ™¯æ¡ˆä¾‹åº“\09-æ™ºèƒ½å®¢æœç³»ç»Ÿ\03-æ ¸å¿ƒå®ç°.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# æ¡ˆä¾‹9ï¼šæ™ºèƒ½å®¢æœç³»ç»Ÿ - æ ¸å¿ƒå®ç°

```python
"""
æ™ºèƒ½å®¢æœç³»ç»Ÿ
æŠ€æœ¯æ ˆ: PostgreSQL 18 + pgvector + LangChain
"""

import psycopg2
from psycopg2.extras import RealDictCursor
from langchain.llms import OpenAI
from langchain.embeddings import OpenAIEmbeddings
import numpy as np

class CustomerServiceBot:
    """æ™ºèƒ½å®¢æœæœºå™¨äºº"""

    def __init__(self, conn_str, openai_api_key):
        self.conn = psycopg2.connect(conn_str, cursor_factory=RealDictCursor)
        self.cursor = self.conn.cursor()
        self.llm = OpenAI(api_key=openai_api_key, temperature=0.7)
        self.embeddings = OpenAIEmbeddings(api_key=openai_api_key)

    def answer_question(self, session_id: str, question: str) -> dict:
        """
        å›ç­”ç”¨æˆ·é—®é¢˜

        æµç¨‹:
        1. å‘é‡æ£€ç´¢ç›¸å…³FAQ
        2. è·å–å¯¹è¯å†å²
        3. LLMç”Ÿæˆç­”æ¡ˆ
        4. ä¿å­˜å¯¹è¯
        """

        # 1. å‘é‡æ£€ç´¢
        question_vec = self.embeddings.embed_query(question)
        relevant_faqs = self._retrieve_faqs(question_vec, top_k=3)

        # 2. å¯¹è¯å†å²
        history = self._get_conversation_history(session_id, limit=5)

        # 3. ç”Ÿæˆç­”æ¡ˆ
        answer = self._generate_answer(question, relevant_faqs, history)

        # 4. ä¿å­˜å¯¹è¯
        self._save_conversation(session_id, question, answer)

        return {
            'session_id': session_id,
            'question': question,
            'answer': answer,
            'sources': [faq['faq_id'] for faq in relevant_faqs]
        }

    def _retrieve_faqs(self, query_vec: list, top_k: int = 3) -> list:
        """å‘é‡æ£€ç´¢FAQ"""

        self.cursor.execute("""
            SELECT
                faq_id,
                question,
                answer,
                category,
                1 - (embedding <-> %s::vector) AS similarity
            FROM faqs
            WHERE 1 - (embedding <-> %s::vector) > 0.7
            ORDER BY embedding <-> %s::vector
            LIMIT %s;
        """, (query_vec, query_vec, query_vec, top_k))

        return self.cursor.fetchall()

    def _get_conversation_history(self, session_id: str, limit: int = 5) -> list:
        """è·å–å¯¹è¯å†å²"""

        self.cursor.execute("""
            SELECT question, answer, created_at
            FROM conversations
            WHERE session_id = %s
            ORDER BY created_at DESC
            LIMIT %s;
        """, (session_id, limit))

        return list(reversed(self.cursor.fetchall()))

    def _generate_answer(self, question: str, faqs: list, history: list) -> str:
        """LLMç”Ÿæˆç­”æ¡ˆ"""

        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"Q: {faq['question']}\nA: {faq['answer']}"
            for faq in faqs
        ])

        history_text = "\n".join([
            f"ç”¨æˆ·: {h['question']}\nå®¢æœ: {h['answer']}"
            for h in history
        ])

        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœåŠ©æ‰‹ã€‚åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ç›¸å…³çŸ¥è¯†:
{context}

å¯¹è¯å†å²:
{history_text}

ç”¨æˆ·é—®é¢˜: {question}

è¯·ç»™å‡ºä¸“ä¸šã€å‹å¥½çš„å›ç­”:
"""

        answer = self.llm(prompt)
        return answer.strip()

    def _save_conversation(self, session_id: str, question: str, answer: str):
        """ä¿å­˜å¯¹è¯è®°å½•"""

        self.cursor.execute("""
            INSERT INTO conversations (session_id, question, answer)
            VALUES (%s, %s, %s);
        """, (session_id, question, answer))

        self.conn.commit()

# ============================================================
# æ•°æ®åº“Schema
# ============================================================

"""
-- FAQè¡¨
CREATE TABLE faqs (
    faq_id SERIAL PRIMARY KEY,
    question TEXT NOT NULL,
    answer TEXT NOT NULL,
    category VARCHAR(50),
    embedding vector(768),
    created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_faqs_embedding ON faqs USING hnsw (embedding vector_l2_ops);

-- å¯¹è¯è¡¨
CREATE TABLE conversations (
    conv_id BIGSERIAL PRIMARY KEY,
    session_id VARCHAR(100) NOT NULL,
    question TEXT NOT NULL,
    answer TEXT NOT NULL,
    created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_conversations_session ON conversations (session_id, created_at);

-- ç”¨æˆ·ä¼šè¯è¡¨
CREATE TABLE sessions (
    session_id VARCHAR(100) PRIMARY KEY,
    user_id BIGINT,
    channel VARCHAR(20),
    status VARCHAR(20),
    created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
);
"""

# ============================================================
# FastAPIæ¥å£
# ============================================================

from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

bot = CustomerServiceBot(
    conn_str="dbname=customer_service_db user=postgres",
    openai_api_key="your-api-key"
)

class QuestionRequest(BaseModel):
    session_id: str
    question: str

@app.post("/api/chat")
async def chat(request: QuestionRequest):
    """å¯¹è¯æ¥å£"""
    result = bot.answer_question(request.session_id, request.question)
    return result

@app.get("/api/history/{session_id}")
async def get_history(session_id: str):
    """è·å–å¯¹è¯å†å²"""
    cursor = bot.cursor
    cursor.execute("""
        SELECT question, answer, created_at
        FROM conversations
        WHERE session_id = %s
        ORDER BY created_at;
    """, (session_id,))

    history = cursor.fetchall()
    return {'session_id': session_id, 'history': history}

if __name__ == '__main__':
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8005)
```

---

**è¿”å›**: [æ¡ˆä¾‹9ä¸»é¡µ](./README.md)
