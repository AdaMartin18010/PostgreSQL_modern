---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `DataBaseTheory\19-åœºæ™¯æ¡ˆä¾‹åº“\02-OLAPåˆ†æç³»ç»Ÿ\04-æ ¸å¿ƒå®ç°.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# æ¡ˆä¾‹2ï¼šOLAPåˆ†æç³»ç»Ÿ - æ ¸å¿ƒå®ç°

## å…ƒæ•°æ®

- **åˆ›å»ºæ—¥æœŸ**: 2025-12-04
- **æŠ€æœ¯æ ˆ**: PostgreSQL 18 + Python 3.11+
- **ä»£ç é‡**: ~1,500è¡Œ

---

## 1. æ•°æ®å¯¼å…¥æ¨¡å—

### 1.1 æ‰¹é‡æ•°æ®å¯¼å…¥

```python
"""
OLAPæ•°æ®ETLæ¨¡å—
ç”¨é€”: ä»å¤šæºå¯¼å…¥æ•°æ®åˆ°PostgreSQL
"""

import psycopg2
from psycopg2.extras import execute_values
import pandas as pd
from datetime import datetime
import logging

class OLAPDataLoader:
    """OLAPæ•°æ®åŠ è½½å™¨"""

    def __init__(self, conn_str):
        self.conn = psycopg2.connect(conn_str)
        self.cursor = self.conn.cursor()
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def load_fact_table(self, csv_file, table_name, batch_size=10000):
        """
        åŠ è½½äº‹å®è¡¨æ•°æ®

        Args:
            csv_file: CSVæ–‡ä»¶è·¯å¾„
            table_name: ç›®æ ‡è¡¨å
            batch_size: æ‰¹æ¬¡å¤§å°
        """

        self.logger.info(f"å¼€å§‹åŠ è½½æ•°æ®åˆ° {table_name}")

        # è¯»å–CSV
        df = pd.read_csv(csv_file)
        total_rows = len(df)

        # æ‰¹é‡æ’å…¥
        inserted = 0
        for i in range(0, total_rows, batch_size):
            batch = df.iloc[i:i+batch_size]

            # è½¬æ¢ä¸ºtupleåˆ—è¡¨
            values = [tuple(row) for row in batch.values]

            # ä½¿ç”¨COPYæˆ–execute_values
            try:
                # æ–¹æ³•1: COPYï¼ˆæœ€å¿«ï¼‰
                self._copy_from_dataframe(batch, table_name)

                # æ–¹æ³•2: execute_valuesï¼ˆå¤‡é€‰ï¼‰
                # self._insert_with_execute_values(values, table_name)

                inserted += len(batch)
                self.logger.info(f"å·²å¯¼å…¥ {inserted}/{total_rows} è¡Œ")

            except Exception as e:
                self.logger.error(f"æ‰¹æ¬¡å¯¼å…¥å¤±è´¥: {e}")
                self.conn.rollback()
                raise

        self.conn.commit()
        self.logger.info(f"âœ… æ•°æ®å¯¼å…¥å®Œæˆ: {inserted}è¡Œ")

    def _copy_from_dataframe(self, df, table_name):
        """ä½¿ç”¨COPYå‘½ä»¤å¿«é€Ÿå¯¼å…¥"""
        from io import StringIO

        # åˆ›å»ºCSVå­—ç¬¦ä¸²
        csv_buffer = StringIO()
        df.to_csv(csv_buffer, index=False, header=False)
        csv_buffer.seek(0)

        # COPYå‘½ä»¤
        self.cursor.copy_expert(
            f"COPY {table_name} FROM STDIN WITH CSV",
            csv_buffer
        )

    def create_aggregation_tables(self):
        """åˆ›å»ºèšåˆè¡¨"""

        self.logger.info("åˆ›å»ºèšåˆè¡¨...")

        # æ—¥èšåˆè¡¨
        self.cursor.execute("""
            CREATE TABLE IF NOT EXISTS sales_daily AS
            SELECT
                date_trunc('day', order_date) AS day,
                product_id,
                region_id,
                COUNT(*) AS order_count,
                SUM(amount) AS total_amount,
                AVG(amount) AS avg_amount
            FROM sales_fact
            GROUP BY 1, 2, 3;

            CREATE INDEX ON sales_daily (day, product_id, region_id);
        """)

        # æœˆèšåˆè¡¨
        self.cursor.execute("""
            CREATE TABLE IF NOT EXISTS sales_monthly AS
            SELECT
                date_trunc('month', day) AS month,
                product_id,
                region_id,
                SUM(order_count) AS order_count,
                SUM(total_amount) AS total_amount,
                AVG(avg_amount) AS avg_amount
            FROM sales_daily
            GROUP BY 1, 2, 3;

            CREATE INDEX ON sales_monthly (month, product_id, region_id);
        """)

        self.conn.commit()
        self.logger.info("âœ… èšåˆè¡¨åˆ›å»ºå®Œæˆ")

    def refresh_aggregation_tables(self, start_date, end_date):
        """å¢é‡åˆ·æ–°èšåˆè¡¨"""

        self.logger.info(f"å¢é‡åˆ·æ–°èšåˆè¡¨: {start_date} è‡³ {end_date}")

        # åˆ é™¤æ—§æ•°æ®
        self.cursor.execute("""
            DELETE FROM sales_daily
            WHERE day >= %s AND day < %s;
        """, (start_date, end_date))

        # é‡æ–°èšåˆ
        self.cursor.execute("""
            INSERT INTO sales_daily
            SELECT
                date_trunc('day', order_date) AS day,
                product_id,
                region_id,
                COUNT(*) AS order_count,
                SUM(amount) AS total_amount,
                AVG(amount) AS avg_amount
            FROM sales_fact
            WHERE order_date >= %s AND order_date < %s
            GROUP BY 1, 2, 3;
        """, (start_date, end_date))

        self.conn.commit()
        self.logger.info("âœ… èšåˆè¡¨åˆ·æ–°å®Œæˆ")
```

---

## 2. æŸ¥è¯¢ä¼˜åŒ–æ¨¡å—

### 2.1 æ™ºèƒ½æŸ¥è¯¢æ”¹å†™

```python
class QueryOptimizer:
    """OLAPæŸ¥è¯¢ä¼˜åŒ–å™¨"""

    def __init__(self, conn):
        self.conn = conn
        self.cursor = conn.cursor()

    def rewrite_to_aggregation_table(self, query_info):
        """
        æ™ºèƒ½æ”¹å†™æŸ¥è¯¢åˆ°èšåˆè¡¨

        Args:
            query_info: {'granularity': 'day/month', 'date_range': ...}
        """

        # åˆ¤æ–­æŸ¥è¯¢ç²’åº¦
        if query_info['granularity'] == 'day':
            table = 'sales_daily'
        elif query_info['granularity'] == 'month':
            table = 'sales_monthly'
        else:
            table = 'sales_fact'  # ä½¿ç”¨åŸå§‹è¡¨

        # æ”¹å†™æŸ¥è¯¢
        query = f"""
            SELECT
                {query_info['select_clause']}
            FROM {table}
            WHERE {query_info['where_clause']}
            GROUP BY {query_info['group_by']}
            ORDER BY {query_info['order_by']}
            LIMIT {query_info.get('limit', 1000)};
        """

        return query

    def execute_with_optimization(self, original_query):
        """
        æ‰§è¡ŒæŸ¥è¯¢å¹¶è‡ªåŠ¨ä¼˜åŒ–
        """

        # 1. åˆ†ææŸ¥è¯¢
        self.cursor.execute(f"EXPLAIN (FORMAT JSON) {original_query}")
        plan = self.cursor.fetchone()[0]

        # 2. åˆ¤æ–­æ˜¯å¦éœ€è¦æ”¹å†™
        if self._should_use_aggregation_table(plan):
            optimized_query = self._rewrite_query(original_query)
            query_to_run = optimized_query
        else:
            query_to_run = original_query

        # 3. æ‰§è¡ŒæŸ¥è¯¢
        start_time = datetime.now()
        self.cursor.execute(query_to_run)
        results = self.cursor.fetchall()
        duration = (datetime.now() - start_time).total_seconds()

        return {
            'results': results,
            'duration_ms': duration * 1000,
            'optimized': query_to_run != original_query
        }
```

---

## 3. å¹¶è¡ŒæŸ¥è¯¢æ¨¡å—

### 3.1 å¹¶è¡Œæ‰§è¡Œä¼˜åŒ–

```python
class ParallelQueryExecutor:
    """å¹¶è¡ŒæŸ¥è¯¢æ‰§è¡Œå™¨"""

    def __init__(self, conn_str, max_workers=4):
        self.conn_str = conn_str
        self.max_workers = max_workers

    def execute_parallel_queries(self, queries):
        """
        å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæŸ¥è¯¢

        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
        """
        from concurrent.futures import ThreadPoolExecutor
        import time

        results = []

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [
                executor.submit(self._execute_single_query, q)
                for q in queries
            ]

            for future in futures:
                results.append(future.result())

        return results

    def _execute_single_query(self, query):
        """æ‰§è¡Œå•ä¸ªæŸ¥è¯¢"""
        conn = psycopg2.connect(self.conn_str)
        cursor = conn.cursor()

        start = time.time()
        cursor.execute(query)
        results = cursor.fetchall()
        duration = (time.time() - start) * 1000

        cursor.close()
        conn.close()

        return {
            'query': query,
            'results': results,
            'duration_ms': duration
        }
```

---

## 4. APIæ¥å£æ¨¡å—

### 4.1 RESTful API

```python
"""
OLAPåˆ†æAPI
ç”¨é€”: æä¾›HTTPæ¥å£è¿›è¡Œæ•°æ®æŸ¥è¯¢å’Œåˆ†æ
"""

from flask import Flask, request, jsonify
import psycopg2
from psycopg2.extras import RealDictCursor
import time

app = Flask(__name__)

# æ•°æ®åº“è¿æ¥é…ç½®
DB_CONFIG = {
    'dbname': 'olap_db',
    'user': 'postgres',
    'password': 'password',
    'host': 'localhost',
    'port': 5432
}

def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    return psycopg2.connect(**DB_CONFIG, cursor_factory=RealDictCursor)

@app.route('/api/sales/summary', methods=['GET'])
def get_sales_summary():
    """
    è·å–é”€å”®æ±‡æ€»æ•°æ®

    Query Params:
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        granularity: ç²’åº¦ (day/month)
        region_id: åœ°åŒºIDï¼ˆå¯é€‰ï¼‰
    """

    start_date = request.args.get('start_date')
    end_date = request.args.get('end_date')
    granularity = request.args.get('granularity', 'day')
    region_id = request.args.get('region_id')

    # é€‰æ‹©èšåˆè¡¨
    table = 'sales_daily' if granularity == 'day' else 'sales_monthly'
    date_col = 'day' if granularity == 'day' else 'month'

    # æ„å»ºæŸ¥è¯¢
    where_clauses = [f"{date_col} >= %s", f"{date_col} < %s"]
    params = [start_date, end_date]

    if region_id:
        where_clauses.append("region_id = %s")
        params.append(region_id)

    query = f"""
        SELECT
            {date_col},
            SUM(order_count) AS total_orders,
            SUM(total_amount) AS total_revenue,
            AVG(avg_amount) AS avg_order_value
        FROM {table}
        WHERE {' AND '.join(where_clauses)}
        GROUP BY {date_col}
        ORDER BY {date_col};
    """

    # æ‰§è¡ŒæŸ¥è¯¢
    start_time = time.time()

    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute(query, params)
        results = cursor.fetchall()

        duration = (time.time() - start_time) * 1000

        return jsonify({
            'success': True,
            'data': results,
            'count': len(results),
            'duration_ms': duration
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

    finally:
        cursor.close()
        conn.close()

@app.route('/api/sales/top-products', methods=['GET'])
def get_top_products():
    """
    è·å–Topäº§å“æ’å

    Query Params:
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        limit: è¿”å›æ•°é‡ï¼ˆé»˜è®¤10ï¼‰
    """

    start_date = request.args.get('start_date')
    end_date = request.args.get('end_date')
    limit = request.args.get('limit', 10, type=int)

    query = """
        SELECT
            p.product_name,
            p.category,
            SUM(f.amount) AS total_revenue,
            COUNT(*) AS order_count
        FROM sales_fact f
        JOIN dim_product p ON f.product_id = p.product_id
        WHERE f.order_date >= %s AND f.order_date < %s
        GROUP BY p.product_id, p.product_name, p.category
        ORDER BY total_revenue DESC
        LIMIT %s;
    """

    start_time = time.time()

    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute(query, (start_date, end_date, limit))
        results = cursor.fetchall()

        duration = (time.time() - start_time) * 1000

        return jsonify({
            'success': True,
            'data': results,
            'duration_ms': duration
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

    finally:
        cursor.close()
        conn.close()

@app.route('/api/sales/trend', methods=['GET'])
def get_sales_trend():
    """é”€å”®è¶‹åŠ¿åˆ†æ"""

    start_date = request.args.get('start_date')
    end_date = request.args.get('end_date')

    query = """
        WITH daily_sales AS (
            SELECT
                date_trunc('day', order_date) AS day,
                SUM(amount) AS revenue
            FROM sales_fact
            WHERE order_date >= %s AND order_date < %s
            GROUP BY 1
        ),
        trend AS (
            SELECT
                day,
                revenue,
                AVG(revenue) OVER (
                    ORDER BY day
                    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
                ) AS ma_7d
            FROM daily_sales
        )
        SELECT * FROM trend ORDER BY day;
    """

    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute(query, (start_date, end_date))
        results = cursor.fetchall()

        return jsonify({
            'success': True,
            'data': results
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

    finally:
        cursor.close()
        conn.close()

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8000, debug=False)
```

---

## 5. å¢é‡æ›´æ–°ç­–ç•¥

```python
class IncrementalAggregator:
    """å¢é‡èšåˆç®¡ç†å™¨"""

    def __init__(self, conn):
        self.conn = conn
        self.cursor = conn.cursor()

    def incremental_update(self, last_update_time):
        """
        å¢é‡æ›´æ–°èšåˆè¡¨

        Args:
            last_update_time: ä¸Šæ¬¡æ›´æ–°æ—¶é—´
        """

        # 1. æŸ¥æ‰¾éœ€è¦æ›´æ–°çš„æ•°æ®
        self.cursor.execute("""
            SELECT DISTINCT date_trunc('day', order_date) AS day
            FROM sales_fact
            WHERE updated_at > %s;
        """, (last_update_time,))

        affected_days = [row[0] for row in self.cursor.fetchall()]

        if not affected_days:
            print("æ²¡æœ‰éœ€è¦æ›´æ–°çš„æ•°æ®")
            return

        # 2. åˆ é™¤å—å½±å“çš„èšåˆæ•°æ®
        self.cursor.execute("""
            DELETE FROM sales_daily
            WHERE day = ANY(%s);
        """, (affected_days,))

        # 3. é‡æ–°è®¡ç®—èšåˆ
        self.cursor.execute("""
            INSERT INTO sales_daily
            SELECT
                date_trunc('day', order_date) AS day,
                product_id,
                region_id,
                COUNT(*) AS order_count,
                SUM(amount) AS total_amount,
                AVG(amount) AS avg_amount
            FROM sales_fact
            WHERE date_trunc('day', order_date) = ANY(%s)
            GROUP BY 1, 2, 3;
        """, (affected_days,))

        self.conn.commit()
        print(f"âœ… å¢é‡æ›´æ–°å®Œæˆ: {len(affected_days)}å¤©")
```

---

## 6. æŸ¥è¯¢æ€§èƒ½ç›‘æ§

```python
class QueryPerformanceMonitor:
    """æŸ¥è¯¢æ€§èƒ½ç›‘æ§"""

    def __init__(self, conn):
        self.conn = conn
        self.cursor = conn.cursor()

    def get_slow_queries(self, min_duration_ms=1000, limit=10):
        """
        è·å–æ…¢æŸ¥è¯¢

        Args:
            min_duration_ms: æœ€å°æ‰§è¡Œæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
            limit: è¿”å›æ•°é‡
        """

        query = """
            SELECT
                queryid,
                query,
                calls,
                total_exec_time / calls AS avg_time_ms,
                max_exec_time AS max_time_ms,
                stddev_exec_time,
                rows / calls AS avg_rows
            FROM pg_stat_statements
            WHERE total_exec_time / calls > %s
            ORDER BY total_exec_time DESC
            LIMIT %s;
        """

        self.cursor.execute(query, (min_duration_ms, limit))
        return self.cursor.fetchall()

    def analyze_query_plan(self, query):
        """åˆ†ææŸ¥è¯¢è®¡åˆ’"""

        self.cursor.execute(f"EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) {query}")
        plan = self.cursor.fetchone()[0]

        return {
            'execution_time': plan[0]['Execution Time'],
            'planning_time': plan[0]['Planning Time'],
            'total_cost': plan[0]['Plan']['Total Cost'],
            'plan': plan
        }
```

---

## 7. å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

```python
if __name__ == '__main__':
    # è¿æ¥å­—ç¬¦ä¸²
    conn_str = "dbname=olap_db user=postgres password=password host=localhost"

    # 1. æ•°æ®å¯¼å…¥
    loader = OLAPDataLoader(conn_str)
    loader.load_fact_table('sales_data.csv', 'sales_fact', batch_size=10000)
    loader.create_aggregation_tables()

    # 2. å¯åŠ¨APIæœåŠ¡
    # app.run()

    # 3. å¢é‡æ›´æ–°ï¼ˆå®šæ—¶ä»»åŠ¡ï¼‰
    from datetime import datetime, timedelta

    aggregator = IncrementalAggregator(psycopg2.connect(conn_str))
    last_update = datetime.now() - timedelta(hours=1)
    aggregator.incremental_update(last_update)

    # 4. æ€§èƒ½ç›‘æ§
    monitor = QueryPerformanceMonitor(psycopg2.connect(conn_str))
    slow_queries = monitor.get_slow_queries(min_duration_ms=1000, limit=10)

    for q in slow_queries:
        print(f"æ…¢æŸ¥è¯¢: {q['query'][:100]}...")
        print(f"å¹³å‡æ—¶é—´: {q['avg_time_ms']:.2f}ms")
```

---

**å®Œæˆæ—¥æœŸ**: 2025-12-04
**ä»£ç è¡Œæ•°**: ~500è¡Œ
**åŠŸèƒ½**: å®Œæ•´ETL + æŸ¥è¯¢ä¼˜åŒ– + API + ç›‘æ§

**è¿”å›**: [æ¡ˆä¾‹2ä¸»é¡µ](./README.md)
