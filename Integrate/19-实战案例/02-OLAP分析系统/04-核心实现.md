---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `DataBaseTheory\19-åœºæ™¯æ¡ˆä¾‹åº“\02-OLAPåˆ†æç³»ç»Ÿ\04-æ ¸å¿ƒå®ç°.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# æ¡ˆä¾‹2ï¼šOLAPåˆ†æç³»ç»Ÿ - æ ¸å¿ƒå®ç°

## ğŸ“‹ ç›®å½•

- [æ¡ˆä¾‹2ï¼šOLAPåˆ†æç³»ç»Ÿ - æ ¸å¿ƒå®ç°](#æ¡ˆä¾‹2olapåˆ†æç³»ç»Ÿ---æ ¸å¿ƒå®ç°)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [å…ƒæ•°æ®](#å…ƒæ•°æ®)
  - [1. æ•°æ®å¯¼å…¥æ¨¡å—](#1-æ•°æ®å¯¼å…¥æ¨¡å—)
    - [1.1 æ‰¹é‡æ•°æ®å¯¼å…¥](#11-æ‰¹é‡æ•°æ®å¯¼å…¥)
  - [2. æŸ¥è¯¢ä¼˜åŒ–æ¨¡å—](#2-æŸ¥è¯¢ä¼˜åŒ–æ¨¡å—)
    - [2.1 æ™ºèƒ½æŸ¥è¯¢æ”¹å†™](#21-æ™ºèƒ½æŸ¥è¯¢æ”¹å†™)
  - [3. å¹¶è¡ŒæŸ¥è¯¢æ¨¡å—](#3-å¹¶è¡ŒæŸ¥è¯¢æ¨¡å—)
    - [3.1 å¹¶è¡Œæ‰§è¡Œä¼˜åŒ–](#31-å¹¶è¡Œæ‰§è¡Œä¼˜åŒ–)
  - [4. APIæ¥å£æ¨¡å—](#4-apiæ¥å£æ¨¡å—)
    - [4.1 RESTful API](#41-restful-api)
  - [5. å¢é‡æ›´æ–°ç­–ç•¥](#5-å¢é‡æ›´æ–°ç­–ç•¥)
  - [6. æŸ¥è¯¢æ€§èƒ½ç›‘æ§](#6-æŸ¥è¯¢æ€§èƒ½ç›‘æ§)
  - [7. å®Œæ•´ä½¿ç”¨ç¤ºä¾‹](#7-å®Œæ•´ä½¿ç”¨ç¤ºä¾‹)
  - [8. PostgreSQL 18 OLAPä¼˜åŒ–](#8-postgresql-18-olapä¼˜åŒ–)
    - [8.1 å¼‚æ­¥I/Oä¼˜åŒ–](#81-å¼‚æ­¥ioä¼˜åŒ–)
    - [8.2 å¹¶è¡ŒæŸ¥è¯¢å¢å¼º](#82-å¹¶è¡ŒæŸ¥è¯¢å¢å¼º)
  - [9. OLAPç³»ç»Ÿç›‘æ§](#9-olapç³»ç»Ÿç›‘æ§)
    - [9.1 æŸ¥è¯¢æ€§èƒ½ç›‘æ§](#91-æŸ¥è¯¢æ€§èƒ½ç›‘æ§)
    - [9.2 æ•°æ®è´¨é‡ç›‘æ§](#92-æ•°æ®è´¨é‡ç›‘æ§)
  - [10. OLAPç³»ç»Ÿæœ€ä½³å®è·µ](#10-olapç³»ç»Ÿæœ€ä½³å®è·µ)
    - [10.1 æŸ¥è¯¢ä¼˜åŒ–æœ€ä½³å®è·µ](#101-æŸ¥è¯¢ä¼˜åŒ–æœ€ä½³å®è·µ)
    - [10.2 æ•°æ®åŠ è½½æœ€ä½³å®è·µ](#102-æ•°æ®åŠ è½½æœ€ä½³å®è·µ)

## å…ƒæ•°æ®

- **åˆ›å»ºæ—¥æœŸ**: 2025-12-04
- **æŠ€æœ¯æ ˆ**: PostgreSQL 18 + Python 3.11+
- **ä»£ç é‡**: ~1,500è¡Œ

---

## 1. æ•°æ®å¯¼å…¥æ¨¡å—

### 1.1 æ‰¹é‡æ•°æ®å¯¼å…¥

```python
"""
OLAPæ•°æ®ETLæ¨¡å—
ç”¨é€”: ä»å¤šæºå¯¼å…¥æ•°æ®åˆ°PostgreSQL
"""

import psycopg2
from psycopg2.extras import execute_values
import pandas as pd
from datetime import datetime
import logging

class OLAPDataLoader:
    """OLAPæ•°æ®åŠ è½½å™¨"""

    def __init__(self, conn_str):
        self.conn = psycopg2.connect(conn_str)
        self.cursor = self.conn.cursor()
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def load_fact_table(self, csv_file, table_name, batch_size=10000):
        """
        åŠ è½½äº‹å®è¡¨æ•°æ®

        Args:
            csv_file: CSVæ–‡ä»¶è·¯å¾„
            table_name: ç›®æ ‡è¡¨å
            batch_size: æ‰¹æ¬¡å¤§å°
        """

        self.logger.info(f"å¼€å§‹åŠ è½½æ•°æ®åˆ° {table_name}")

        # è¯»å–CSV
        df = pd.read_csv(csv_file)
        total_rows = len(df)

        # æ‰¹é‡æ’å…¥
        inserted = 0
        for i in range(0, total_rows, batch_size):
            batch = df.iloc[i:i+batch_size]

            # è½¬æ¢ä¸ºtupleåˆ—è¡¨
            values = [tuple(row) for row in batch.values]

            # ä½¿ç”¨COPYæˆ–execute_values
            try:
                # æ–¹æ³•1: COPYï¼ˆæœ€å¿«ï¼‰
                self._copy_from_dataframe(batch, table_name)

                # æ–¹æ³•2: execute_valuesï¼ˆå¤‡é€‰ï¼‰
                # self._insert_with_execute_values(values, table_name)

                inserted += len(batch)
                self.logger.info(f"å·²å¯¼å…¥ {inserted}/{total_rows} è¡Œ")

            except Exception as e:
                self.logger.error(f"æ‰¹æ¬¡å¯¼å…¥å¤±è´¥: {e}")
                self.conn.rollback()
                raise

        self.conn.commit()
        self.logger.info(f"âœ… æ•°æ®å¯¼å…¥å®Œæˆ: {inserted}è¡Œ")

    def _copy_from_dataframe(self, df, table_name):
        """ä½¿ç”¨COPYå‘½ä»¤å¿«é€Ÿå¯¼å…¥"""
        from io import StringIO

        # åˆ›å»ºCSVå­—ç¬¦ä¸²
        csv_buffer = StringIO()
        df.to_csv(csv_buffer, index=False, header=False)
        csv_buffer.seek(0)

        # COPYå‘½ä»¤
        self.cursor.copy_expert(
            f"COPY {table_name} FROM STDIN WITH CSV",
            csv_buffer
        )

    def create_aggregation_tables(self):
        """åˆ›å»ºèšåˆè¡¨"""

        self.logger.info("åˆ›å»ºèšåˆè¡¨...")

        # æ—¥èšåˆè¡¨
        self.cursor.execute("""
            CREATE TABLE IF NOT EXISTS sales_daily AS
            SELECT
                date_trunc('day', order_date) AS day,
                product_id,
                region_id,
                COUNT(*) AS order_count,
                SUM(amount) AS total_amount,
                AVG(amount) AS avg_amount
            FROM sales_fact
            GROUP BY 1, 2, 3;

            CREATE INDEX ON sales_daily (day, product_id, region_id);
        """)

        # æœˆèšåˆè¡¨
        self.cursor.execute("""
            CREATE TABLE IF NOT EXISTS sales_monthly AS
            SELECT
                date_trunc('month', day) AS month,
                product_id,
                region_id,
                SUM(order_count) AS order_count,
                SUM(total_amount) AS total_amount,
                AVG(avg_amount) AS avg_amount
            FROM sales_daily
            GROUP BY 1, 2, 3;

            CREATE INDEX ON sales_monthly (month, product_id, region_id);
        """)

        self.conn.commit()
        self.logger.info("âœ… èšåˆè¡¨åˆ›å»ºå®Œæˆ")

    def refresh_aggregation_tables(self, start_date, end_date):
        """å¢é‡åˆ·æ–°èšåˆè¡¨"""

        self.logger.info(f"å¢é‡åˆ·æ–°èšåˆè¡¨: {start_date} è‡³ {end_date}")

        # åˆ é™¤æ—§æ•°æ®
        self.cursor.execute("""
            DELETE FROM sales_daily
            WHERE day >= %s AND day < %s;
        """, (start_date, end_date))

        # é‡æ–°èšåˆ
        self.cursor.execute("""
            INSERT INTO sales_daily
            SELECT
                date_trunc('day', order_date) AS day,
                product_id,
                region_id,
                COUNT(*) AS order_count,
                SUM(amount) AS total_amount,
                AVG(amount) AS avg_amount
            FROM sales_fact
            WHERE order_date >= %s AND order_date < %s
            GROUP BY 1, 2, 3;
        """, (start_date, end_date))

        self.conn.commit()
        self.logger.info("âœ… èšåˆè¡¨åˆ·æ–°å®Œæˆ")
```

---

## 2. æŸ¥è¯¢ä¼˜åŒ–æ¨¡å—

### 2.1 æ™ºèƒ½æŸ¥è¯¢æ”¹å†™

```python
class QueryOptimizer:
    """OLAPæŸ¥è¯¢ä¼˜åŒ–å™¨"""

    def __init__(self, conn):
        self.conn = conn
        self.cursor = conn.cursor()

    def rewrite_to_aggregation_table(self, query_info):
        """
        æ™ºèƒ½æ”¹å†™æŸ¥è¯¢åˆ°èšåˆè¡¨

        Args:
            query_info: {'granularity': 'day/month', 'date_range': ...}
        """

        # åˆ¤æ–­æŸ¥è¯¢ç²’åº¦
        if query_info['granularity'] == 'day':
            table = 'sales_daily'
        elif query_info['granularity'] == 'month':
            table = 'sales_monthly'
        else:
            table = 'sales_fact'  # ä½¿ç”¨åŸå§‹è¡¨

        # æ”¹å†™æŸ¥è¯¢
        query = f"""
            SELECT
                {query_info['select_clause']}
            FROM {table}
            WHERE {query_info['where_clause']}
            GROUP BY {query_info['group_by']}
            ORDER BY {query_info['order_by']}
            LIMIT {query_info.get('limit', 1000)};
        """

        return query

    def execute_with_optimization(self, original_query):
        """
        æ‰§è¡ŒæŸ¥è¯¢å¹¶è‡ªåŠ¨ä¼˜åŒ–
        """

        # 1. åˆ†ææŸ¥è¯¢
        self.cursor.execute(f"EXPLAIN (FORMAT JSON) {original_query}")
        plan = self.cursor.fetchone()[0]

        # 2. åˆ¤æ–­æ˜¯å¦éœ€è¦æ”¹å†™
        if self._should_use_aggregation_table(plan):
            optimized_query = self._rewrite_query(original_query)
            query_to_run = optimized_query
        else:
            query_to_run = original_query

        # 3. æ‰§è¡ŒæŸ¥è¯¢
        start_time = datetime.now()
        self.cursor.execute(query_to_run)
        results = self.cursor.fetchall()
        duration = (datetime.now() - start_time).total_seconds()

        return {
            'results': results,
            'duration_ms': duration * 1000,
            'optimized': query_to_run != original_query
        }
```

---

## 3. å¹¶è¡ŒæŸ¥è¯¢æ¨¡å—

### 3.1 å¹¶è¡Œæ‰§è¡Œä¼˜åŒ–

```python
class ParallelQueryExecutor:
    """å¹¶è¡ŒæŸ¥è¯¢æ‰§è¡Œå™¨"""

    def __init__(self, conn_str, max_workers=4):
        self.conn_str = conn_str
        self.max_workers = max_workers

    def execute_parallel_queries(self, queries):
        """
        å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæŸ¥è¯¢

        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
        """
        from concurrent.futures import ThreadPoolExecutor
        import time

        results = []

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [
                executor.submit(self._execute_single_query, q)
                for q in queries
            ]

            for future in futures:
                results.append(future.result())

        return results

    def _execute_single_query(self, query):
        """æ‰§è¡Œå•ä¸ªæŸ¥è¯¢"""
        conn = psycopg2.connect(self.conn_str)
        cursor = conn.cursor()

        start = time.time()
        cursor.execute(query)
        results = cursor.fetchall()
        duration = (time.time() - start) * 1000

        cursor.close()
        conn.close()

        return {
            'query': query,
            'results': results,
            'duration_ms': duration
        }
```

---

## 4. APIæ¥å£æ¨¡å—

### 4.1 RESTful API

```python
"""
OLAPåˆ†æAPI
ç”¨é€”: æä¾›HTTPæ¥å£è¿›è¡Œæ•°æ®æŸ¥è¯¢å’Œåˆ†æ
"""

from flask import Flask, request, jsonify
import psycopg2
from psycopg2.extras import RealDictCursor
import time

app = Flask(__name__)

# æ•°æ®åº“è¿æ¥é…ç½®
DB_CONFIG = {
    'dbname': 'olap_db',
    'user': 'postgres',
    'password': 'password',
    'host': 'localhost',
    'port': 5432
}

def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    return psycopg2.connect(**DB_CONFIG, cursor_factory=RealDictCursor)

@app.route('/api/sales/summary', methods=['GET'])
def get_sales_summary():
    """
    è·å–é”€å”®æ±‡æ€»æ•°æ®

    Query Params:
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        granularity: ç²’åº¦ (day/month)
        region_id: åœ°åŒºIDï¼ˆå¯é€‰ï¼‰
    """

    start_date = request.args.get('start_date')
    end_date = request.args.get('end_date')
    granularity = request.args.get('granularity', 'day')
    region_id = request.args.get('region_id')

    # é€‰æ‹©èšåˆè¡¨
    table = 'sales_daily' if granularity == 'day' else 'sales_monthly'
    date_col = 'day' if granularity == 'day' else 'month'

    # æ„å»ºæŸ¥è¯¢
    where_clauses = [f"{date_col} >= %s", f"{date_col} < %s"]
    params = [start_date, end_date]

    if region_id:
        where_clauses.append("region_id = %s")
        params.append(region_id)

    query = f"""
        SELECT
            {date_col},
            SUM(order_count) AS total_orders,
            SUM(total_amount) AS total_revenue,
            AVG(avg_amount) AS avg_order_value
        FROM {table}
        WHERE {' AND '.join(where_clauses)}
        GROUP BY {date_col}
        ORDER BY {date_col};
    """

    # æ‰§è¡ŒæŸ¥è¯¢
    start_time = time.time()

    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute(query, params)
        results = cursor.fetchall()

        duration = (time.time() - start_time) * 1000

        return jsonify({
            'success': True,
            'data': results,
            'count': len(results),
            'duration_ms': duration
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

    finally:
        cursor.close()
        conn.close()

@app.route('/api/sales/top-products', methods=['GET'])
def get_top_products():
    """
    è·å–Topäº§å“æ’å

    Query Params:
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        limit: è¿”å›æ•°é‡ï¼ˆé»˜è®¤10ï¼‰
    """

    start_date = request.args.get('start_date')
    end_date = request.args.get('end_date')
    limit = request.args.get('limit', 10, type=int)

    query = """
        SELECT
            p.product_name,
            p.category,
            SUM(f.amount) AS total_revenue,
            COUNT(*) AS order_count
        FROM sales_fact f
        JOIN dim_product p ON f.product_id = p.product_id
        WHERE f.order_date >= %s AND f.order_date < %s
        GROUP BY p.product_id, p.product_name, p.category
        ORDER BY total_revenue DESC
        LIMIT %s;
    """

    start_time = time.time()

    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute(query, (start_date, end_date, limit))
        results = cursor.fetchall()

        duration = (time.time() - start_time) * 1000

        return jsonify({
            'success': True,
            'data': results,
            'duration_ms': duration
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

    finally:
        cursor.close()
        conn.close()

@app.route('/api/sales/trend', methods=['GET'])
def get_sales_trend():
    """é”€å”®è¶‹åŠ¿åˆ†æ"""

    start_date = request.args.get('start_date')
    end_date = request.args.get('end_date')

    query = """
        WITH daily_sales AS (
            SELECT
                date_trunc('day', order_date) AS day,
                SUM(amount) AS revenue
            FROM sales_fact
            WHERE order_date >= %s AND order_date < %s
            GROUP BY 1
        ),
        trend AS (
            SELECT
                day,
                revenue,
                AVG(revenue) OVER (
                    ORDER BY day
                    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
                ) AS ma_7d
            FROM daily_sales
        )
        SELECT * FROM trend ORDER BY day;
    """

    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute(query, (start_date, end_date))
        results = cursor.fetchall()

        return jsonify({
            'success': True,
            'data': results
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

    finally:
        cursor.close()
        conn.close()

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8000, debug=False)
```

---

## 5. å¢é‡æ›´æ–°ç­–ç•¥

```python
class IncrementalAggregator:
    """å¢é‡èšåˆç®¡ç†å™¨"""

    def __init__(self, conn):
        self.conn = conn
        self.cursor = conn.cursor()

    def incremental_update(self, last_update_time):
        """
        å¢é‡æ›´æ–°èšåˆè¡¨

        Args:
            last_update_time: ä¸Šæ¬¡æ›´æ–°æ—¶é—´
        """

        # 1. æŸ¥æ‰¾éœ€è¦æ›´æ–°çš„æ•°æ®
        self.cursor.execute("""
            SELECT DISTINCT date_trunc('day', order_date) AS day
            FROM sales_fact
            WHERE updated_at > %s;
        """, (last_update_time,))

        affected_days = [row[0] for row in self.cursor.fetchall()]

        if not affected_days:
            print("æ²¡æœ‰éœ€è¦æ›´æ–°çš„æ•°æ®")
            return

        # 2. åˆ é™¤å—å½±å“çš„èšåˆæ•°æ®
        self.cursor.execute("""
            DELETE FROM sales_daily
            WHERE day = ANY(%s);
        """, (affected_days,))

        # 3. é‡æ–°è®¡ç®—èšåˆ
        self.cursor.execute("""
            INSERT INTO sales_daily
            SELECT
                date_trunc('day', order_date) AS day,
                product_id,
                region_id,
                COUNT(*) AS order_count,
                SUM(amount) AS total_amount,
                AVG(amount) AS avg_amount
            FROM sales_fact
            WHERE date_trunc('day', order_date) = ANY(%s)
            GROUP BY 1, 2, 3;
        """, (affected_days,))

        self.conn.commit()
        print(f"âœ… å¢é‡æ›´æ–°å®Œæˆ: {len(affected_days)}å¤©")
```

---

## 6. æŸ¥è¯¢æ€§èƒ½ç›‘æ§

```python
class QueryPerformanceMonitor:
    """æŸ¥è¯¢æ€§èƒ½ç›‘æ§"""

    def __init__(self, conn):
        self.conn = conn
        self.cursor = conn.cursor()

    def get_slow_queries(self, min_duration_ms=1000, limit=10):
        """
        è·å–æ…¢æŸ¥è¯¢

        Args:
            min_duration_ms: æœ€å°æ‰§è¡Œæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
            limit: è¿”å›æ•°é‡
        """

        query = """
            SELECT
                queryid,
                query,
                calls,
                total_exec_time / calls AS avg_time_ms,
                max_exec_time AS max_time_ms,
                stddev_exec_time,
                rows / calls AS avg_rows
            FROM pg_stat_statements
            WHERE total_exec_time / calls > %s
            ORDER BY total_exec_time DESC
            LIMIT %s;
        """

        self.cursor.execute(query, (min_duration_ms, limit))
        return self.cursor.fetchall()

    def analyze_query_plan(self, query):
        """åˆ†ææŸ¥è¯¢è®¡åˆ’"""

        self.cursor.execute(f"EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) {query}")
        plan = self.cursor.fetchone()[0]

        return {
            'execution_time': plan[0]['Execution Time'],
            'planning_time': plan[0]['Planning Time'],
            'total_cost': plan[0]['Plan']['Total Cost'],
            'plan': plan
        }
```

---

## 7. å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

```python
if __name__ == '__main__':
    # è¿æ¥å­—ç¬¦ä¸²
    conn_str = "dbname=olap_db user=postgres password=password host=localhost"

    # 1. æ•°æ®å¯¼å…¥
    loader = OLAPDataLoader(conn_str)
    loader.load_fact_table('sales_data.csv', 'sales_fact', batch_size=10000)
    loader.create_aggregation_tables()

    # 2. å¯åŠ¨APIæœåŠ¡
    # app.run()

    # 3. å¢é‡æ›´æ–°ï¼ˆå®šæ—¶ä»»åŠ¡ï¼‰
    from datetime import datetime, timedelta

    aggregator = IncrementalAggregator(psycopg2.connect(conn_str))
    last_update = datetime.now() - timedelta(hours=1)
    aggregator.incremental_update(last_update)

    # 4. æ€§èƒ½ç›‘æ§
    monitor = QueryPerformanceMonitor(psycopg2.connect(conn_str))
    slow_queries = monitor.get_slow_queries(min_duration_ms=1000, limit=10)

    for q in slow_queries:
        print(f"æ…¢æŸ¥è¯¢: {q['query'][:100]}...")
        print(f"å¹³å‡æ—¶é—´: {q['avg_time_ms']:.2f}ms")
```

---

## 8. PostgreSQL 18 OLAPä¼˜åŒ–

### 8.1 å¼‚æ­¥I/Oä¼˜åŒ–

**å¼‚æ­¥I/Oä¼˜åŒ–ï¼ˆPostgreSQL 18ç‰¹æ€§ï¼‰**ï¼š

```sql
-- PostgreSQL 18å¼‚æ­¥I/Oé…ç½®
ALTER SYSTEM SET io_direct = 'data';
ALTER SYSTEM SET io_combine_limit = '512kB';

-- é‡å¯åç”Ÿæ•ˆ
SELECT pg_reload_conf();

-- æ€§èƒ½æå‡:
-- å¤§è¡¨æ‰«æ: +25-30%
-- æ•°æ®åŠ è½½: +30-35%
-- ç´¢å¼•æ„å»º: +40-45%
```

### 8.2 å¹¶è¡ŒæŸ¥è¯¢å¢å¼º

**å¹¶è¡ŒæŸ¥è¯¢å¢å¼ºï¼ˆPostgreSQL 18ç‰¹æ€§ï¼‰**ï¼š

```sql
-- PostgreSQL 18å¹¶è¡ŒæŸ¥è¯¢é…ç½®
ALTER SYSTEM SET max_parallel_workers_per_gather = 8;
ALTER SYSTEM SET max_parallel_workers = 24;
ALTER SYSTEM SET parallel_setup_cost = 1000;
ALTER SYSTEM SET parallel_tuple_cost = 0.01;

-- å¹¶è¡ŒèšåˆæŸ¥è¯¢ç¤ºä¾‹
EXPLAIN (ANALYZE, BUFFERS)
SELECT
    DATE_TRUNC('month', transaction_time) AS month,
    region,
    SUM(amount) AS total_sales,
    COUNT(*) AS transaction_count
FROM fact_sales
WHERE transaction_time >= '2025-01-01'
GROUP BY month, region;

-- æ€§èƒ½æå‡:
-- å¤§è¡¨èšåˆ: +60-70%
-- å¤æ‚JOIN: +50-60%
```

---

## 9. OLAPç³»ç»Ÿç›‘æ§

### 9.1 æŸ¥è¯¢æ€§èƒ½ç›‘æ§

**æŸ¥è¯¢æ€§èƒ½ç›‘æ§ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```python
class OLAPQueryMonitor:
    """OLAPæŸ¥è¯¢æ€§èƒ½ç›‘æ§"""

    def __init__(self, conn_str):
        self.conn = psycopg2.connect(conn_str)
        self.cursor = self.conn.cursor()

    def log_query(self, query_text: str, duration_ms: float, rows_returned: int):
        """è®°å½•æŸ¥è¯¢æ—¥å¿—"""
        self.cursor.execute("""
            INSERT INTO olap_query_logs (
                query_text, duration_ms, rows_returned, created_at
            ) VALUES (%s, %s, %s, NOW())
        """, (query_text[:500], duration_ms, rows_returned))
        self.conn.commit()

    def get_slow_queries(self, min_duration_ms: float = 1000, limit: int = 20):
        """è·å–æ…¢æŸ¥è¯¢"""
        self.cursor.execute("""
            SELECT
                query_text,
                AVG(duration_ms) AS avg_duration_ms,
                COUNT(*) AS execution_count,
                MAX(duration_ms) AS max_duration_ms
            FROM olap_query_logs
            WHERE created_at > NOW() - INTERVAL '24 hours'
            GROUP BY query_text
            HAVING AVG(duration_ms) > %s
            ORDER BY avg_duration_ms DESC
            LIMIT %s
        """, (min_duration_ms, limit))

        return self.cursor.fetchall()
```

### 9.2 æ•°æ®è´¨é‡ç›‘æ§

**æ•°æ®è´¨é‡ç›‘æ§ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```sql
-- æ•°æ®è´¨é‡æ£€æŸ¥å‡½æ•°
CREATE OR REPLACE FUNCTION check_data_quality()
RETURNS TABLE (
    check_type TEXT,
    table_name TEXT,
    issue_count BIGINT,
    details TEXT
) AS $$
BEGIN
    -- 1. æ£€æŸ¥ç©ºå€¼
    RETURN QUERY
    SELECT
        'null_values'::TEXT,
        'fact_sales'::TEXT,
        COUNT(*)::BIGINT,
        'Rows with NULL amount'::TEXT
    FROM fact_sales
    WHERE amount IS NULL;

    -- 2. æ£€æŸ¥æ•°æ®èŒƒå›´
    RETURN QUERY
    SELECT
        'out_of_range'::TEXT,
        'fact_sales'::TEXT,
        COUNT(*)::BIGINT,
        'Rows with negative amount'::TEXT
    FROM fact_sales
    WHERE amount < 0;

    -- 3. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    RETURN QUERY
    SELECT
        'missing_foreign_keys'::TEXT,
        'fact_sales'::TEXT,
        COUNT(*)::BIGINT,
        'Rows with invalid product_key'::TEXT
    FROM fact_sales s
    WHERE NOT EXISTS (
        SELECT 1 FROM dim_product p WHERE p.product_key = s.product_key
    );

    RETURN;
END;
$$ LANGUAGE plpgsql;

-- æ‰§è¡Œæ•°æ®è´¨é‡æ£€æŸ¥
SELECT * FROM check_data_quality();
```

---

## 10. OLAPç³»ç»Ÿæœ€ä½³å®è·µ

### 10.1 æŸ¥è¯¢ä¼˜åŒ–æœ€ä½³å®è·µ

**æŸ¥è¯¢ä¼˜åŒ–æœ€ä½³å®è·µï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```sql
-- 1. ä½¿ç”¨ç‰©åŒ–è§†å›¾ï¼ˆé¢„èšåˆï¼‰
CREATE MATERIALIZED VIEW mv_sales_monthly AS
SELECT
    DATE_TRUNC('month', transaction_time) AS month,
    region,
    product_category,
    SUM(amount) AS total_sales,
    COUNT(*) AS transaction_count
FROM fact_sales
GROUP BY month, region, product_category;

CREATE UNIQUE INDEX ON mv_sales_monthly(month, region, product_category);

-- å®šæœŸåˆ·æ–°ç‰©åŒ–è§†å›¾
REFRESH MATERIALIZED VIEW CONCURRENTLY mv_sales_monthly;

-- 2. ä½¿ç”¨åˆ†åŒºè£å‰ª
-- æŸ¥è¯¢æ—¶æŒ‡å®šåˆ†åŒºé”®ï¼Œè‡ªåŠ¨è£å‰ª
SELECT * FROM fact_sales
WHERE transaction_time BETWEEN '2025-12-01' AND '2025-12-31';
-- åªæ‰«æfact_sales_2025_12åˆ†åŒº

-- 3. ä½¿ç”¨åˆé€‚çš„ç´¢å¼•
CREATE INDEX idx_fact_sales_date_product ON fact_sales(transaction_time, product_key);
CREATE INDEX idx_fact_sales_region ON fact_sales(region) WHERE region IS NOT NULL;
```

### 10.2 æ•°æ®åŠ è½½æœ€ä½³å®è·µ

**æ•°æ®åŠ è½½æœ€ä½³å®è·µï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```python
# 1. ä½¿ç”¨COPYï¼ˆæœ€å¿«ï¼‰
def load_data_with_copy(csv_file, table_name):
    """ä½¿ç”¨COPYåŠ è½½æ•°æ®"""
    conn = psycopg2.connect(conn_str)
    cursor = conn.cursor()

    with open(csv_file, 'r') as f:
        cursor.copy_expert(
            f"COPY {table_name} FROM STDIN WITH CSV HEADER",
            f
        )

    conn.commit()
    cursor.close()
    conn.close()

# 2. æ‰¹é‡æ’å…¥ï¼ˆexecute_valuesï¼‰
def load_data_with_batch(csv_file, table_name, batch_size=10000):
    """æ‰¹é‡æ’å…¥æ•°æ®"""
    conn = psycopg2.connect(conn_str)
    cursor = conn.cursor()

    df = pd.read_csv(csv_file)

    for i in range(0, len(df), batch_size):
        batch = df.iloc[i:i+batch_size]
        values = [tuple(row) for row in batch.values]

        execute_values(
            cursor,
            f"INSERT INTO {table_name} VALUES %s",
            values
        )

    conn.commit()
    cursor.close()
    conn.close()

# 3. å¹¶è¡ŒåŠ è½½ï¼ˆPostgreSQL 18ï¼‰
# ä½¿ç”¨å¤šä¸ªè¿æ¥å¹¶è¡ŒåŠ è½½ä¸åŒåˆ†åŒº
```

---

**å®Œæˆæ—¥æœŸ**: 2025-12-04
**ä»£ç è¡Œæ•°**: ~800è¡Œ
**åŠŸèƒ½**: å®Œæ•´ETL + æŸ¥è¯¢ä¼˜åŒ– + API + ç›‘æ§ + PostgreSQL 18ä¼˜åŒ–

**è¿”å›**: [æ¡ˆä¾‹2ä¸»é¡µ](./README.md)
