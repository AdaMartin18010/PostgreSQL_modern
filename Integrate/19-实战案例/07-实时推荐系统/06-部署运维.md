---
> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `DataBaseTheory\19-åœºæ™¯æ¡ˆä¾‹åº“\07-å®æ—¶æ¨èç³»ç»Ÿ\06-éƒ¨ç½²è¿ç»´.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜
---
> **âš ï¸ é‡è¦æç¤º**: æœ¬æ–‡æ¡£éµå¾ªæ¡ˆä¾‹æ–‡æ¡£é€šç”¨æ¨¡æ¿æ ¼å¼ã€‚
>
> **æ¨èé˜…è¯»**:
>
> - [æ¡ˆä¾‹æ–‡æ¡£é€šç”¨æ¨¡æ¿](../æ¡ˆä¾‹æ–‡æ¡£é€šç”¨æ¨¡æ¿.md) - é€šç”¨æ¡ˆä¾‹æ–‡æ¡£æ ¼å¼å’Œæœ€ä½³å®è·µ
>
> æœ¬æ–‡æ¡£ä¿ç•™ä½œä¸ºå®æ—¶æ¨èç³»ç»Ÿçš„éƒ¨ç½²è¿ç»´å‚è€ƒã€‚
---

# æ¡ˆä¾‹7ï¼šå®æ—¶æ¨èç³»ç»Ÿ - éƒ¨ç½²è¿ç»´

## å…ƒæ•°æ®

- **åˆ›å»ºæ—¥æœŸ**: 2025-12-22
- **éƒ¨ç½²æ–¹å¼**: Docker + Kubernetes
- **ç›‘æ§**: Prometheus + Grafana
- **ç¼“å­˜**: Redis Cluster
- **æ¶ˆæ¯é˜Ÿåˆ—**: Kafka

---

## 1. Dockeréƒ¨ç½²é…ç½®

### 1.1 Docker Composeé…ç½®

**Docker Composeé…ç½®ï¼ˆå¸¦Rediså’ŒKafkaï¼‰**ï¼š

```yaml
version: '3.8'

services:
  postgres:
    image: pgvector/pgvector:pg18
    container_name: recommendation-db
    environment:
      POSTGRES_DB: recommendation_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
      - ./postgresql.conf:/etc/postgresql/postgresql.conf
    ports:
      - "5432:5432"
    command:
      - "postgres"
      - "-c"
      - "config_file=/etc/postgresql/postgresql.conf"
      - "-c"
      - "shared_buffers=4GB"
      - "-c"
      - "work_mem=256MB"
      - "-c"
      - "max_connections=300"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: recommendation-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --maxmemory 8gb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: recommendation-kafka
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper
    restart: unless-stopped

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: recommendation-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    restart: unless-stopped

  api:
    build:
      context: ./api
      dockerfile: Dockerfile
    container_name: recommendation-api
    environment:
      DB_HOST: postgres
      DB_NAME: recommendation_db
      DB_USER: postgres
      DB_PASSWORD: ${DB_PASSWORD}
      REDIS_HOST: redis
      REDIS_PORT: 6379
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
    ports:
      - "8007:8007"
    depends_on:
      - postgres
      - redis
      - kafka
    restart: unless-stopped

volumes:
  postgres-data:
  redis-data:
```

### 1.2 PostgreSQLä¼˜åŒ–é…ç½®

**PostgreSQLä¼˜åŒ–é…ç½®ï¼ˆPostgreSQL 18ç‰¹æ€§ï¼‰**ï¼š

```ini
# postgresql.conf - å®æ—¶æ¨èç³»ç»Ÿä¼˜åŒ–

# å†…å­˜é…ç½®
shared_buffers = 4GB
work_mem = 256MB
maintenance_work_mem = 1GB
effective_cache_size = 12GB

# è¿æ¥é…ç½®
max_connections = 300
superuser_reserved_connections = 3

# WALé…ç½®
wal_buffers = 32MB
max_wal_size = 8GB
min_wal_size = 2GB
checkpoint_timeout = 20min
checkpoint_completion_target = 0.9

# PostgreSQL 18å¼‚æ­¥I/O
io_direct = data
io_combine_limit = 256kB

# æ³¨æ„ï¼šPostgreSQL 18æ²¡æœ‰å†…ç½®è¿æ¥æ± 
# å»ºè®®ä½¿ç”¨pgBouncerè¿›è¡Œè¿æ¥æ± ç®¡ç†
# pgBounceré…ç½®è¯·å‚è€ƒå•ç‹¬çš„pgbouncer.inié…ç½®æ–‡ä»¶

# æŸ¥è¯¢ä¼˜åŒ–
random_page_cost = 1.1
effective_io_concurrency = 200
default_statistics_target = 100

# pgvectoré…ç½®
shared_preload_libraries = 'vector'

# å¹¶è¡ŒæŸ¥è¯¢ï¼ˆPostgreSQL 18ï¼‰
max_parallel_workers_per_gather = 4
max_parallel_workers = 8
parallel_tuple_cost = 0.01
parallel_setup_cost = 1000

# æ—¥å¿—é…ç½®
log_destination = 'stderr'
logging_collector = on
log_directory = 'log'
log_filename = 'postgresql-%Y-%m-%d.log'
log_rotation_age = 1d
log_rotation_size = 100MB
log_min_duration_statement = 1000  # è®°å½•>1sçš„æŸ¥è¯¢

# ç»Ÿè®¡ä¿¡æ¯
track_io_timing = on
track_functions = all
pg_stat_statements.track = all
```

---

## 2. ç¼“å­˜ç­–ç•¥

### 2.1 Redisç¼“å­˜é…ç½®

**Redisç¼“å­˜é…ç½®ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```python
# redis_config.py
import redis
from redis.sentinel import Sentinel

# Redisè¿æ¥æ± 
redis_pool = redis.ConnectionPool(
    host='redis',
    port=6379,
    max_connections=50,
    decode_responses=True
)

redis_client = redis.Redis(connection_pool=redis_pool)

# ç¼“å­˜é”®ç­–ç•¥
class CacheKeys:
    USER_PROFILE = "user:profile:{user_id}"
    ITEM_FEATURES = "item:features:{item_id}"
    USER_CF_REC = "rec:user_cf:{user_id}"
    ITEM_CF_REC = "rec:item_cf:{item_id}"
    VECTOR_REC = "rec:vector:{user_id}"
    HOT_ITEMS = "rec:hot:items"

    @staticmethod
    def get_ttl(key_type: str) -> int:
        """è·å–ç¼“å­˜TTLï¼ˆç§’ï¼‰"""
        ttl_map = {
            'user_profile': 3600,      # 1å°æ—¶
            'item_features': 7200,      # 2å°æ—¶
            'user_cf': 1800,            # 30åˆ†é’Ÿ
            'item_cf': 1800,            # 30åˆ†é’Ÿ
            'vector': 1800,             # 30åˆ†é’Ÿ
            'hot_items': 300,           # 5åˆ†é’Ÿ
        }
        return ttl_map.get(key_type, 1800)

# ç¼“å­˜è£…é¥°å™¨
from functools import wraps
import json
import hashlib

def cache_result(key_template: str, ttl: int = 1800):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = key_template.format(*args, **kwargs)

            # å°è¯•ä»ç¼“å­˜è·å–
            cached = redis_client.get(cache_key)
            if cached:
                return json.loads(cached)

            # æ‰§è¡Œå‡½æ•°
            result = func(*args, **kwargs)

            # å†™å…¥ç¼“å­˜
            redis_client.setex(cache_key, ttl, json.dumps(result))

            return result
        return wrapper
    return decorator
```

---

## 3. æ¶ˆæ¯é˜Ÿåˆ—é…ç½®

### 3.1 Kafkaä¸»é¢˜é…ç½®

**Kafkaä¸»é¢˜é…ç½®ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```bash
# åˆ›å»ºKafkaä¸»é¢˜
docker exec recommendation-kafka kafka-topics \
  --create \
  --bootstrap-server localhost:9092 \
  --replication-factor 1 \
  --partitions 3 \
  --topic user-behavior

docker exec recommendation-kafka kafka-topics \
  --create \
  --bootstrap-server localhost:9092 \
  --replication-factor 1 \
  --partitions 3 \
  --topic item-updates

# æŸ¥çœ‹ä¸»é¢˜åˆ—è¡¨
docker exec recommendation-kafka kafka-topics \
  --list \
  --bootstrap-server localhost:9092
```

### 3.2 Kafkaæ¶ˆè´¹è€…é…ç½®

**Kafkaæ¶ˆè´¹è€…é…ç½®ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```python
# kafka_consumer.py
from kafka import KafkaConsumer
import json
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BehaviorConsumer:
    """ç”¨æˆ·è¡Œä¸ºæ¶ˆè´¹è€…"""

    def __init__(self, bootstrap_servers: list):
        self.consumer = KafkaConsumer(
            'user-behavior',
            bootstrap_servers=bootstrap_servers,
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            group_id='recommendation-service',
            auto_offset_reset='latest',
            enable_auto_commit=True,
            auto_commit_interval_ms=1000
        )

    def consume(self):
        """æ¶ˆè´¹æ¶ˆæ¯"""
        try:
            for message in self.consumer:
                behavior = message.value
                logger.info(f"æ”¶åˆ°è¡Œä¸ºæ•°æ®: {behavior}")

                # å¤„ç†è¡Œä¸ºæ•°æ®
                self.process_behavior(behavior)

        except Exception as e:
            logger.error(f"æ¶ˆè´¹æ¶ˆæ¯å¤±è´¥: {e}")
            raise

    def process_behavior(self, behavior: dict):
        """å¤„ç†è¡Œä¸ºæ•°æ®"""
        user_id = behavior['user_id']
        item_id = behavior['item_id']
        action = behavior['action']  # view, click, purchase

        # æ›´æ–°ç”¨æˆ·ç”»åƒ
        self.update_user_profile(user_id, item_id, action)

        # æ›´æ–°ç‰©å“ç‰¹å¾
        self.update_item_features(item_id, action)

        # æ¸…é™¤ç›¸å…³ç¼“å­˜
        self.invalidate_cache(user_id, item_id)

    def update_user_profile(self, user_id: int, item_id: int, action: str):
        """æ›´æ–°ç”¨æˆ·ç”»åƒ"""
        # å®ç°ç”¨æˆ·ç”»åƒæ›´æ–°é€»è¾‘
        pass

    def update_item_features(self, item_id: int, action: str):
        """æ›´æ–°ç‰©å“ç‰¹å¾"""
        # å®ç°ç‰©å“ç‰¹å¾æ›´æ–°é€»è¾‘
        pass

    def invalidate_cache(self, user_id: int, item_id: int):
        """æ¸…é™¤ç¼“å­˜"""
        cache_keys = [
            f"rec:user_cf:{user_id}",
            f"rec:item_cf:{item_id}",
            f"rec:vector:{user_id}"
        ]
        for key in cache_keys:
            redis_client.delete(key)
```

---

## 4. ç›‘æ§é…ç½®

### 4.1 Prometheusé…ç½®

**Prometheusé…ç½®ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
    metrics_path: /metrics

  - job_name: 'recommendation-api'
    static_configs:
      - targets: ['api:8007']
    metrics_path: /metrics

  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
    metrics_path: /metrics

  - job_name: 'kafka'
    static_configs:
      - targets: ['kafka-exporter:9308']
    metrics_path: /metrics
```

### 4.2 Grafanaä»ªè¡¨æ¿

**Grafanaä»ªè¡¨æ¿é…ç½®ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```json
{
  "dashboard": {
    "title": "å®æ—¶æ¨èç³»ç»Ÿç›‘æ§",
    "panels": [
      {
        "title": "æ¨èQPS",
        "targets": [
          {
            "expr": "rate(recommendation_requests_total[5m])"
          }
        ]
      },
      {
        "title": "æ¨èå»¶è¿Ÿ",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(recommendation_duration_ms_bucket[5m]))"
          }
        ]
      },
      {
        "title": "å‘é‡æ£€ç´¢å»¶è¿Ÿ",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(vector_search_duration_ms_bucket[5m]))"
          }
        ]
      },
      {
        "title": "ç¼“å­˜å‘½ä¸­ç‡",
        "targets": [
          {
            "expr": "rate(redis_hits_total[5m]) / rate(redis_requests_total[5m])"
          }
        ]
      },
      {
        "title": "Kafkaæ¶ˆè´¹å»¶è¿Ÿ",
        "targets": [
          {
            "expr": "kafka_consumer_lag_sum"
          }
        ]
      }
    ]
  }
}
```

---

## 5. å¤‡ä»½ä¸æ¢å¤

### 5.1 æ•°æ®åº“å¤‡ä»½

**æ•°æ®åº“å¤‡ä»½è„šæœ¬ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```bash
#!/bin/bash
# backup_recommendation.sh - å®æ—¶æ¨èç³»ç»Ÿå¤‡ä»½

set -e

BACKUP_DIR="/backups/recommendation"
DATE=$(date +%Y%m%d_%H%M%S)
RETENTION_DAYS=30

mkdir -p "$BACKUP_DIR"

# æ•°æ®åº“å¤‡ä»½
echo "å¼€å§‹æ•°æ®åº“å¤‡ä»½..."
pg_dump -h postgres -U postgres -d recommendation_db \
  -F c -b -v -f "$BACKUP_DIR/db_$DATE.dump"

# éªŒè¯å¤‡ä»½
if [ $? -eq 0 ]; then
    echo "æ•°æ®åº“å¤‡ä»½æˆåŠŸ: $BACKUP_DIR/db_$DATE.dump"

    # å‹ç¼©å¤‡ä»½
    gzip "$BACKUP_DIR/db_$DATE.dump"

    # æ¸…ç†æ—§å¤‡ä»½
    find "$BACKUP_DIR" -name "db_*.dump.gz" -mtime +$RETENTION_DAYS -delete

    echo "å¤‡ä»½å®Œæˆï¼Œä¿ç•™æœ€è¿‘ $RETENTION_DAYS å¤©çš„å¤‡ä»½"
else
    echo "æ•°æ®åº“å¤‡ä»½å¤±è´¥ï¼"
    exit 1
fi
```

---

## 6. æ€§èƒ½è°ƒä¼˜

### 6.1 æ•°æ®åº“æ€§èƒ½è°ƒä¼˜

**æ•°æ®åº“æ€§èƒ½è°ƒä¼˜ï¼ˆPostgreSQL 18ç‰¹æ€§ï¼‰**ï¼š

```sql
-- 1. åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯
ANALYZE users;
ANALYZE items;
ANALYZE user_behaviors;

-- 2. é‡å»ºç´¢å¼•
REINDEX INDEX CONCURRENTLY idx_items_embedding;
REINDEX INDEX CONCURRENTLY idx_user_behaviors_user_item;

-- 3. æ£€æŸ¥æ…¢æŸ¥è¯¢
SELECT
    query,
    calls,
    total_exec_time,
    mean_exec_time,
    max_exec_time
FROM pg_stat_statements
WHERE mean_exec_time > 1000  -- >1ç§’
ORDER BY mean_exec_time DESC
LIMIT 20;

-- 4. æ£€æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…å†µ
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes
WHERE schemaname = 'public'
ORDER BY idx_scan DESC;
```

### 6.2 ç¼“å­˜ä¼˜åŒ–

**ç¼“å­˜ä¼˜åŒ–ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```python
# 1. é¢„çƒ­ç¼“å­˜
def warmup_cache():
    """é¢„çƒ­ç¼“å­˜"""
    # é¢„çƒ­çƒ­é—¨ç‰©å“
    hot_items = get_hot_items(limit=1000)
    for item in hot_items:
        cache_key = CacheKeys.ITEM_FEATURES.format(item_id=item['item_id'])
        redis_client.setex(
            cache_key,
            CacheKeys.get_ttl('item_features'),
            json.dumps(item)
        )

    # é¢„çƒ­çƒ­é—¨ç”¨æˆ·æ¨è
    hot_users = get_active_users(limit=1000)
    for user in hot_users:
        recommendations = generate_recommendations(user['user_id'])
        cache_key = CacheKeys.USER_CF_REC.format(user_id=user['user_id'])
        redis_client.setex(
            cache_key,
            CacheKeys.get_ttl('user_cf'),
            json.dumps(recommendations)
        )

# 2. æ‰¹é‡è·å–ç¼“å­˜
def batch_get_cache(keys: list) -> dict:
    """æ‰¹é‡è·å–ç¼“å­˜"""
    if not keys:
        return {}

    values = redis_client.mget(keys)
    result = {}
    for key, value in zip(keys, values):
        if value:
            result[key] = json.loads(value)
    return result
```

---

## 7. æ•…éšœå¤„ç†

### 7.1 å¸¸è§æ•…éšœå¤„ç†

**å¸¸è§æ•…éšœå¤„ç†ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```bash
# 1. æ•°æ®åº“è¿æ¥å¤±è´¥
docker exec recommendation-db pg_isready -U postgres

# 2. Redisè¿æ¥å¤±è´¥
docker exec recommendation-redis redis-cli ping

# 3. Kafkaæ¶ˆè´¹å»¶è¿Ÿ
docker exec recommendation-kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 \
  --group recommendation-service \
  --describe

# 4. å‘é‡æ£€ç´¢æ…¢
docker exec recommendation-db psql -U postgres -d recommendation_db -c "
    SELECT indexname, idx_scan, idx_tup_read
    FROM pg_stat_user_indexes
    WHERE indexname = 'idx_items_embedding';
"

# 5. ç¼“å­˜å‘½ä¸­ç‡ä½
# æ£€æŸ¥ç¼“å­˜é…ç½®
docker exec recommendation-redis redis-cli INFO stats | grep keyspace
```

---

## 8. æ‰©å®¹ç­–ç•¥

### 8.1 æ°´å¹³æ‰©å®¹

**æ°´å¹³æ‰©å®¹ç­–ç•¥ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```yaml
# å¢åŠ APIå‰¯æœ¬æ•°
kubectl scale deployment recommendation-api --replicas=5

# æ•°æ®åº“è¯»å†™åˆ†ç¦»ï¼ˆä½¿ç”¨pgBouncerï¼‰
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pgbouncer
spec:
  replicas: 2
  template:
    spec:
      containers:
      - name: pgbouncer
        image: pgbouncer/pgbouncer:latest
        env:
        - name: DATABASES_HOST
          value: postgres
        - name: DATABASES_PORT
          value: "5432"
        - name: DATABASES_DBNAME
          value: recommendation_db
        - name: POOL_MODE
          value: transaction
        - name: MAX_CLIENT_CONN
          value: "1000"
        - name: DEFAULT_POOL_SIZE
          value: "25"
```

### 8.2 Redisé›†ç¾¤

**Redisé›†ç¾¤é…ç½®ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```yaml
# Redis Sentinelé…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-sentinel-config
data:
  sentinel.conf: |
    port 26379
    sentinel monitor mymaster redis-master 6379 2
    sentinel down-after-milliseconds mymaster 5000
    sentinel parallel-syncs mymaster 1
    sentinel failover-timeout mymaster 10000
```

---

## 9. è¿ç»´æ£€æŸ¥æ¸…å•

### 9.1 æ—¥å¸¸æ£€æŸ¥

**æ—¥å¸¸æ£€æŸ¥æ¸…å•ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```bash
#!/bin/bash
# daily_check_recommendation.sh - å®æ—¶æ¨èç³»ç»Ÿæ—¥å¸¸æ£€æŸ¥

echo "=== å®æ—¶æ¨èç³»ç»Ÿæ—¥å¸¸æ£€æŸ¥ ==="

# 1. æ•°æ®åº“è¿æ¥æ£€æŸ¥
echo "1. æ£€æŸ¥æ•°æ®åº“è¿æ¥..."
docker exec recommendation-db pg_isready -U postgres

# 2. Redisè¿æ¥æ£€æŸ¥
echo "2. æ£€æŸ¥Redisè¿æ¥..."
docker exec recommendation-redis redis-cli ping

# 3. Kafkaè¿æ¥æ£€æŸ¥
echo "3. æ£€æŸ¥Kafkaè¿æ¥..."
docker exec recommendation-kafka kafka-topics --list --bootstrap-server localhost:9092

# 4. æ¨èå»¶è¿Ÿæ£€æŸ¥
echo "4. æ£€æŸ¥æ¨èå»¶è¿Ÿ..."
curl -s http://localhost:8007/metrics | grep recommendation_duration_ms

# 5. ç¼“å­˜å‘½ä¸­ç‡æ£€æŸ¥
echo "5. æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡..."
docker exec recommendation-redis redis-cli INFO stats | grep keyspace_hits

# 6. æ•°æ®åº“å¤§å°æ£€æŸ¥
echo "6. æ£€æŸ¥æ•°æ®åº“å¤§å°..."
docker exec recommendation-db psql -U postgres -d recommendation_db -c "
    SELECT pg_size_pretty(pg_database_size('recommendation_db'));
"

echo "=== æ£€æŸ¥å®Œæˆ ==="
```

---

**å®Œæˆ**: å®æ—¶æ¨èç³»ç»Ÿéƒ¨ç½²è¿ç»´
**å­—æ•°**: ~7,000å­—
**æ¶µç›–**: Dockeréƒ¨ç½²ã€ç¼“å­˜ç­–ç•¥ã€æ¶ˆæ¯é˜Ÿåˆ—é…ç½®ã€ç›‘æ§é…ç½®ã€å¤‡ä»½æ¢å¤ã€æ€§èƒ½è°ƒä¼˜ã€æ•…éšœå¤„ç†ã€æ‰©å®¹ç­–ç•¥ã€è¿ç»´æ£€æŸ¥

**è¿”å›**: [æ¡ˆä¾‹7ä¸»é¡µ](./README.md)
