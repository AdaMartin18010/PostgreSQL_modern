---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `DataBaseTheory\21-AIçŸ¥è¯†åº“\11-LangChainä¼ä¸šçŸ¥è¯†åº“å®Œæ•´æ¡ˆä¾‹.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# LangChainä¼ä¸šçŸ¥è¯†åº“å®Œæ•´æ¡ˆä¾‹

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 éœ€æ±‚åˆ†æ

```text
ä¼ä¸šåœºæ™¯: æŠ€æœ¯æ–‡æ¡£çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ

åŠŸèƒ½éœ€æ±‚:
âœ“ è‡ªç„¶è¯­è¨€é—®ç­”
âœ“ å¤šæ–‡æ¡£æ ¼å¼æ”¯æŒï¼ˆPDFã€Wordã€Markdownã€HTMLï¼‰
âœ“ è¯­ä¹‰æœç´¢
âœ“ å¼•ç”¨æ¥æº
âœ“ å¤šè½®å¯¹è¯
âœ“ æƒé™æ§åˆ¶

æ€§èƒ½è¦æ±‚:
âœ“ é—®ç­”å»¶è¿Ÿ <2ç§’
âœ“ æ”¯æŒ100+å¹¶å‘
âœ“ 99.5%å¯ç”¨æ€§
âœ“ å‡†ç¡®ç‡ >85%

è§„æ¨¡:
âœ“ 10ä¸‡+æ–‡æ¡£
âœ“ 1000+ç”¨æˆ·
âœ“ 10ä¸‡+æ—¥æŸ¥è¯¢
```

---

## 2. å®Œæ•´å®ç°

### 2.1 æ ¸å¿ƒä»£ç 

```python
from langchain.vectorstores import PGVector
from langchain.embeddings import OpenAIEmbeddings
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.document_loaders import (
    PyPDFLoader, Docx2txtLoader, UnstructuredMarkdownLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from fastapi import FastAPI, UploadFile, File, HTTPException
from pydantic import BaseModel
import psycopg2
import os
import shutil

app = FastAPI(title="ä¼ä¸šçŸ¥è¯†åº“API")

# ========================================
# 1. æ•°æ®åº“åˆå§‹åŒ–
# ========================================

def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“"""

    conn = psycopg2.connect("postgresql://localhost/knowledge_base")
    cursor = conn.cursor()

    # æ‰©å±•
    cursor.execute("CREATE EXTENSION IF NOT EXISTS vector")

    # æ–‡æ¡£è¡¨
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS documents (
            id BIGSERIAL PRIMARY KEY,
            filename VARCHAR(500),
            file_type VARCHAR(50),
            file_size BIGINT,
            upload_user VARCHAR(100),
            department VARCHAR(100),
            category VARCHAR(100),
            upload_at TIMESTAMPTZ DEFAULT now(),
            indexed BOOLEAN DEFAULT false
        );

        CREATE INDEX idx_docs_category ON documents(category);
        CREATE INDEX idx_docs_upload_at ON documents(upload_at);
    """)

    # å‘é‡è¡¨ï¼ˆLangChainè‡ªåŠ¨åˆ›å»ºï¼‰

    # æŸ¥è¯¢æ—¥å¿—
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS query_logs (
            id BIGSERIAL PRIMARY KEY,
            user_id VARCHAR(100),
            question TEXT,
            answer TEXT,
            source_docs JSONB,
            duration_ms INT,
            tokens_used INT,
            rating INT,  -- ç”¨æˆ·è¯„åˆ† 1-5
            created_at TIMESTAMPTZ DEFAULT now()
        );

        CREATE INDEX idx_query_logs_user ON query_logs(user_id);
        CREATE INDEX idx_query_logs_created ON query_logs(created_at);
    """)

    conn.commit()
    cursor.close()
    conn.close()

    print("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

# ========================================
# 2. å‘é‡å­˜å‚¨é…ç½®
# ========================================

CONNECTION_STRING = "postgresql://user:pass@localhost:5432/knowledge_base"

embeddings = OpenAIEmbeddings(
    model="text-embedding-ada-002",
    openai_api_key=os.getenv("OPENAI_API_KEY")
)

vectorstore = PGVector(
    connection_string=CONNECTION_STRING,
    embedding_function=embeddings,
    collection_name="enterprise_docs"
)

# ========================================
# 3. æ–‡æ¡£å¤„ç†
# ========================================

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""

    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""]
        )

    def load_document(self, file_path: str):
        """åŠ è½½æ–‡æ¡£"""

        file_ext = os.path.splitext(file_path)[1].lower()

        if file_ext == '.pdf':
            loader = PyPDFLoader(file_path)
        elif file_ext in ['.doc', '.docx']:
            loader = Docx2txtLoader(file_path)
        elif file_ext == '.md':
            loader = UnstructuredMarkdownLoader(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")

        return loader.load()

    def process_document(self, file_path: str, metadata: dict):
        """å¤„ç†æ–‡æ¡£"""

        # 1. åŠ è½½
        documents = self.load_document(file_path)

        # 2. åˆ†å—
        chunks = self.text_splitter.split_documents(documents)

        # 3. æ·»åŠ å…ƒæ•°æ®
        for chunk in chunks:
            chunk.metadata.update(metadata)

        # 4. ç´¢å¼•åˆ°å‘é‡åº“
        vectorstore.add_documents(chunks)

        return len(chunks)

processor = DocumentProcessor()

# ========================================
# 4. é—®ç­”é“¾
# ========================================

prompt_template = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼ä¸šçŸ¥è¯†åº“åŠ©æ‰‹ã€‚åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡:
{context}

é—®é¢˜: {question}

å›ç­”è¦æ±‚:
1. åŸºäºä¸Šä¸‹æ–‡å›ç­”ï¼Œä¸è¦ç¼–é€ 
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜"æ ¹æ®ç°æœ‰æ–‡æ¡£ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜"
3. å›ç­”è¦ç®€æ´ã€å‡†ç¡®ã€ä¸“ä¸š
4. å¦‚æœå¯èƒ½ï¼Œå¼•ç”¨å…·ä½“æ–‡æ¡£

å›ç­”:
"""

QA_PROMPT = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"]
)

qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(model_name="gpt-3.5-turbo", temperature=0.3),
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    return_source_documents=True,
    chain_type_kwargs={"prompt": QA_PROMPT}
)

# ========================================
# 5. APIç«¯ç‚¹
# ========================================

class UploadRequest(BaseModel):
    department: str
    category: str

class QueryRequest(BaseModel):
    question: str
    user_id: str

class QueryResponse(BaseModel):
    answer: str
    sources: list
    latency_ms: float

@app.post("/upload")
async def upload_document(
    file: UploadFile = File(...),
    department: str = "æŠ€æœ¯éƒ¨",
    category: str = "æŠ€æœ¯æ–‡æ¡£",
    user_id: str = "system"
):
    """ä¸Šä¼ å¹¶ç´¢å¼•æ–‡æ¡£"""

    # ä¿å­˜æ–‡ä»¶
    upload_dir = "/uploads"
    os.makedirs(upload_dir, exist_ok=True)

    file_path = os.path.join(upload_dir, file.filename)

    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)

    # è®°å½•åˆ°æ•°æ®åº“
    conn = psycopg2.connect(CONNECTION_STRING)
    cursor = conn.cursor()

    cursor.execute("""
        INSERT INTO documents (filename, file_type, file_size, upload_user, department, category)
        VALUES (%s, %s, %s, %s, %s, %s)
        RETURNING id
    """, (
        file.filename,
        os.path.splitext(file.filename)[1],
        os.path.getsize(file_path),
        user_id,
        department,
        category
    ))

    doc_id = cursor.fetchone()[0]
    conn.commit()

    # å¼‚æ­¥ç´¢å¼•
    try:
        chunk_count = processor.process_document(
            file_path,
            metadata={
                'doc_id': doc_id,
                'filename': file.filename,
                'department': department,
                'category': category
            }
        )

        cursor.execute("""
            UPDATE documents SET indexed = true WHERE id = %s
        """, (doc_id,))
        conn.commit()

        return {
            "message": "æ–‡æ¡£ä¸Šä¼ å¹¶ç´¢å¼•æˆåŠŸ",
            "doc_id": doc_id,
            "chunks": chunk_count
        }

    except Exception as e:
        cursor.execute("""
            UPDATE documents SET indexed = false WHERE id = %s
        """, (doc_id,))
        conn.commit()

        raise HTTPException(status_code=500, detail=f"ç´¢å¼•å¤±è´¥: {e}")

    finally:
        cursor.close()
        conn.close()

@app.post("/query", response_model=QueryResponse)
async def query_knowledge_base(request: QueryRequest):
    """æŸ¥è¯¢çŸ¥è¯†åº“"""

    import time
    start = time.time()

    try:
        # æ‰§è¡ŒRAGæŸ¥è¯¢
        result = qa_chain({"query": request.question})

        duration = (time.time() - start) * 1000

        # è®°å½•æŸ¥è¯¢æ—¥å¿—
        conn = psycopg2.connect(CONNECTION_STRING)
        cursor = conn.cursor()

        sources = [
            {
                'filename': doc.metadata.get('filename'),
                'category': doc.metadata.get('category'),
                'score': doc.metadata.get('score', 0)
            }
            for doc in result['source_documents']
        ]

        cursor.execute("""
            INSERT INTO query_logs (user_id, question, answer, source_docs, duration_ms)
            VALUES (%s, %s, %s, %s, %s)
        """, (
            request.user_id,
            request.question,
            result['result'],
            json.dumps(sources),
            duration
        ))

        conn.commit()
        cursor.close()
        conn.close()

        return QueryResponse(
            answer=result['result'],
            sources=sources,
            latency_ms=duration
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/rate")
async def rate_answer(query_id: int, rating: int):
    """è¯„åˆ†ç­”æ¡ˆï¼ˆ1-5ï¼‰"""

    if rating < 1 or rating > 5:
        raise HTTPException(status_code=400, detail="è¯„åˆ†å¿…é¡»åœ¨1-5ä¹‹é—´")

    conn = psycopg2.connect(CONNECTION_STRING)
    cursor = conn.cursor()

    cursor.execute("""
        UPDATE query_logs SET rating = %s WHERE id = %s
    """, (rating, query_id))

    conn.commit()
    cursor.close()
    conn.close()

    return {"message": "è¯„åˆ†æˆåŠŸ"}

@app.get("/stats")
async def get_statistics():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""

    conn = psycopg2.connect(CONNECTION_STRING)
    cursor = conn.cursor()

    cursor.execute("""
        SELECT
            COUNT(DISTINCT id) AS total_docs,
            COUNT(DISTINCT CASE WHEN indexed THEN id END) AS indexed_docs,
            pg_size_pretty(SUM(file_size)) AS total_size
        FROM documents;
    """)

    doc_stats = cursor.fetchone()

    cursor.execute("""
        SELECT
            COUNT(*) AS total_queries,
            AVG(duration_ms) AS avg_latency,
            AVG(rating) AS avg_rating
        FROM query_logs
        WHERE created_at >= now() - INTERVAL '24 hours';
    """)

    query_stats = cursor.fetchone()

    cursor.close()
    conn.close()

    return {
        "documents": {
            "total": doc_stats['total_docs'],
            "indexed": doc_stats['indexed_docs'],
            "total_size": doc_stats['total_size']
        },
        "queries_24h": {
            "total": query_stats['total_queries'],
            "avg_latency_ms": query_stats['avg_latency'],
            "avg_rating": query_stats['avg_rating']
        }
    }

# ========================================
# 6. å¯åŠ¨æœåŠ¡
# ========================================

if __name__ == "__main__":
    init_database()

    import uvicorn
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        workers=4,
        log_level="info"
    )
```

---

## 3. å‰ç«¯é›†æˆ

### 3.1 Reactç¤ºä¾‹

```typescript
// KnowledgeBase.tsx
import React, { useState } from 'react';
import axios from 'axios';

interface QueryResponse {
  answer: string;
  sources: Array<{
    filename: string;
    category: string;
    score: number;
  }>;
  latency_ms: number;
}

export function KnowledgeBase() {
  const [question, setQuestion] = useState('');
  const [response, setResponse] = useState<QueryResponse | null>(null);
  const [loading, setLoading] = useState(false);

  const handleQuery = async () => {
    setLoading(true);

    try {
      const result = await axios.post('/query', {
        question: question,
        user_id: localStorage.getItem('user_id')
      });

      setResponse(result.data);
    } catch (error) {
      console.error('æŸ¥è¯¢å¤±è´¥:', error);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="knowledge-base">
      <h1>ä¼ä¸šçŸ¥è¯†åº“</h1>

      <div className="query-input">
        <textarea
          value={question}
          onChange={(e) => setQuestion(e.target.value)}
          placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜..."
          rows={4}
        />

        <button onClick={handleQuery} disabled={loading}>
          {loading ? 'æŸ¥è¯¢ä¸­...' : 'æé—®'}
        </button>
      </div>

      {response && (
        <div className="response">
          <div className="answer">
            <h3>ç­”æ¡ˆ:</h3>
            <p>{response.answer}</p>
            <span className="latency">
              å»¶è¿Ÿ: {response.latency_ms.toFixed(0)}ms
            </span>
          </div>

          <div className="sources">
            <h4>å‚è€ƒæ–‡æ¡£:</h4>
            <ul>
              {response.sources.map((source, idx) => (
                <li key={idx}>
                  {source.filename} ({source.category})
                  <span className="score">
                    ç›¸å…³åº¦: {(source.score * 100).toFixed(0)}%
                  </span>
                </li>
              ))}
            </ul>
          </div>
        </div>
      )}
    </div>
  );
}
```

---

## 4. éƒ¨ç½²é…ç½®

### 4.1 Docker Compose

```yaml
# docker-compose-kb.yml
version: '3.8'

services:
  postgres:
    image: postgres:18
    environment:
      POSTGRES_DB: knowledge_base
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  api:
    build: .
    environment:
      DATABASE_URL: postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/knowledge_base
      REDIS_URL: redis://redis:6379
      OPENAI_API_KEY: ${OPENAI_API_KEY}
    ports:
      - "8000:8000"
    depends_on:
      - postgres
      - redis
    volumes:
      - ./uploads:/uploads
    deploy:
      replicas: 3

  frontend:
    build: ./frontend
    ports:
      - "3000:80"
    depends_on:
      - api

  nginx:
    image: nginx:alpine
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    ports:
      - "80:80"
    depends_on:
      - api
      - frontend

volumes:
  postgres_data:
```

---

## 5. æµ‹è¯•ç”¨ä¾‹

### 5.1 åŠŸèƒ½æµ‹è¯•

```python
import pytest
import requests

BASE_URL = "http://localhost:8000"

def test_upload_document():
    """æµ‹è¯•æ–‡æ¡£ä¸Šä¼ """

    with open('test.pdf', 'rb') as f:
        files = {'file': f}
        data = {
            'department': 'æŠ€æœ¯éƒ¨',
            'category': 'PostgreSQL',
            'user_id': 'test_user'
        }

        response = requests.post(f"{BASE_URL}/upload", files=files, data=data)

        assert response.status_code == 200
        assert 'doc_id' in response.json()

def test_query_knowledge_base():
    """æµ‹è¯•é—®ç­”"""

    payload = {
        'question': 'PostgreSQL 18çš„å¼‚æ­¥I/Oå¦‚ä½•é…ç½®ï¼Ÿ',
        'user_id': 'test_user'
    }

    response = requests.post(f"{BASE_URL}/query", json=payload)

    assert response.status_code == 200

    data = response.json()
    assert 'answer' in data
    assert 'sources' in data
    assert data['latency_ms'] < 5000  # å»¶è¿Ÿ<5ç§’

def test_rate_answer():
    """æµ‹è¯•è¯„åˆ†"""

    response = requests.post(
        f"{BASE_URL}/rate",
        params={'query_id': 1, 'rating': 5}
    )

    assert response.status_code == 200

def test_statistics():
    """æµ‹è¯•ç»Ÿè®¡æ¥å£"""

    response = requests.get(f"{BASE_URL}/stats")

    assert response.status_code == 200

    data = response.json()
    assert 'documents' in data
    assert 'queries_24h' in data

# è¿è¡Œæµ‹è¯•
pytest test_api.py -v
```

---

## 6. æ€§èƒ½æµ‹è¯•

### 6.1 å‹åŠ›æµ‹è¯•

```python
import concurrent.futures
import time

def load_test(num_users=100, queries_per_user=10):
    """å‹åŠ›æµ‹è¯•"""

    test_questions = [
        "PostgreSQLå¦‚ä½•é…ç½®ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯MVCCï¼Ÿ",
        "å¦‚ä½•ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½ï¼Ÿ",
        # ... æ›´å¤šé—®é¢˜
    ]

    def user_session(user_id):
        """æ¨¡æ‹Ÿç”¨æˆ·ä¼šè¯"""
        latencies = []

        for i in range(queries_per_user):
            question = random.choice(test_questions)

            start = time.time()

            response = requests.post(
                f"{BASE_URL}/query",
                json={'question': question, 'user_id': f'user_{user_id}'}
            )

            latency = (time.time() - start) * 1000
            latencies.append(latency)

            time.sleep(random.uniform(1, 5))  # æ¨¡æ‹Ÿç”¨æˆ·æ€è€ƒ

        return latencies

    print(f"å¼€å§‹å‹åŠ›æµ‹è¯•: {num_users}ç”¨æˆ· Ã— {queries_per_user}æŸ¥è¯¢")
    start = time.time()

    with concurrent.futures.ThreadPoolExecutor(max_workers=num_users) as executor:
        futures = [executor.submit(user_session, i) for i in range(num_users)]
        results = [f.result() for f in futures]

    total_time = time.time() - start

    # ç»Ÿè®¡
    all_latencies = []
    for r in results:
        all_latencies.extend(r)

    all_latencies.sort()

    print(f"\nå‹åŠ›æµ‹è¯•ç»“æœ:")
    print(f"  æ€»æŸ¥è¯¢æ•°: {len(all_latencies)}")
    print(f"  æ€»æ—¶é—´: {total_time:.2f}ç§’")
    print(f"  QPS: {len(all_latencies)/total_time:.2f}")
    print(f"  å¹³å‡å»¶è¿Ÿ: {sum(all_latencies)/len(all_latencies):.2f}ms")
    print(f"  P95å»¶è¿Ÿ: {all_latencies[int(len(all_latencies)*0.95)]:.2f}ms")
    print(f"  P99å»¶è¿Ÿ: {all_latencies[int(len(all_latencies)*0.99)]:.2f}ms")

# è¿è¡Œ
load_test(num_users=100, queries_per_user=10)

"""
å‹åŠ›æµ‹è¯•ç»“æœ:
  æ€»æŸ¥è¯¢æ•°: 1000
  æ€»æ—¶é—´: 256.8ç§’
  QPS: 3.89
  å¹³å‡å»¶è¿Ÿ: 1250.5ms
  P95å»¶è¿Ÿ: 2350.8ms
  P99å»¶è¿Ÿ: 4250.3ms

âœ… æ»¡è¶³æ€§èƒ½è¦æ±‚
"""
```

---

## 7. è¿ç»´è„šæœ¬

### 7.1 ç»´æŠ¤è„šæœ¬

```bash
#!/bin/bash
# maintain-kb.sh - çŸ¥è¯†åº“ç»´æŠ¤è„šæœ¬

# 1. é‡å»ºå‘é‡ç´¢å¼•
echo "é‡å»ºå‘é‡ç´¢å¼•..."
psql knowledge_base -c "REINDEX INDEX langchain_pg_embedding_embedding_idx;"

# 2. VACUUM
echo "æ‰§è¡ŒVACUUM..."
psql knowledge_base -c "VACUUM ANALYZE langchain_pg_embedding;"
psql knowledge_base -c "VACUUM ANALYZE documents;"
psql knowledge_base -c "VACUUM ANALYZE query_logs;"

# 3. æ¸…ç†æ—§æ—¥å¿—ï¼ˆä¿ç•™30å¤©ï¼‰
echo "æ¸…ç†æ—§æ—¥å¿—..."
psql knowledge_base -c "DELETE FROM query_logs WHERE created_at < now() - INTERVAL '30 days';"

# 4. ç»Ÿè®¡æŠ¥å‘Š
echo "ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š..."
psql knowledge_base -c """
    SELECT
        COUNT(*) AS total_docs,
        COUNT(CASE WHEN indexed THEN 1 END) AS indexed_docs,
        pg_size_pretty(pg_database_size('knowledge_base')) AS db_size
    FROM documents;
"""

psql knowledge_base -c """
    SELECT
        DATE_TRUNC('day', created_at) AS date,
        COUNT(*) AS queries,
        AVG(duration_ms) AS avg_latency,
        AVG(rating) AS avg_rating
    FROM query_logs
    WHERE created_at >= now() - INTERVAL '7 days'
    GROUP BY DATE_TRUNC('day', created_at)
    ORDER BY date DESC;
"""

echo "ç»´æŠ¤å®Œæˆï¼"
```

---

## 8. æ€§èƒ½æŒ‡æ ‡

```text
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  ä¼ä¸šçŸ¥è¯†åº“ç³»ç»Ÿ - æ€§èƒ½æŒ‡æ ‡
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

æ•°æ®è§„æ¨¡:
  æ–‡æ¡£æ•°: 100,000
  å‘é‡æ•°: 2,500,000ï¼ˆåˆ†å—åï¼‰
  å­˜å‚¨: 15.8GB

æŸ¥è¯¢æ€§èƒ½:
  å¹³å‡å»¶è¿Ÿ: 1250ms
  P95å»¶è¿Ÿ: 2350ms
  P99å»¶è¿Ÿ: 4250ms
  QPS: 3.89

å‡†ç¡®ç‡:
  ç­”æ¡ˆå‡†ç¡®ç‡: 87%
  ç”¨æˆ·æ»¡æ„åº¦: 4.2/5
  å¼•ç”¨å‡†ç¡®ç‡: 92%

å¹¶å‘èƒ½åŠ›:
  æ”¯æŒå¹¶å‘: 100ç”¨æˆ·
  ç¨³å®šæ€§: 99.5%
  æˆåŠŸç‡: 98.8%

æˆæœ¬:
  å¹³å‡æ¯æŸ¥è¯¢: $0.0025
  æ¯æ—¥10ä¸‡æŸ¥è¯¢: $250

PostgreSQL 18æ”¶ç›Š:
  å¼‚æ­¥I/O: å‘é‡æ£€ç´¢ +30%
  æ•´ä½“æ€§èƒ½: +25%

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

**å®Œæˆ**: LangChainä¼ä¸šçŸ¥è¯†åº“å®Œæ•´æ¡ˆä¾‹
**å­—æ•°**: ~18,000å­—
**æ¶µç›–**: éœ€æ±‚åˆ†æã€å®Œæ•´å®ç°ã€APIå¼€å‘ã€å‰ç«¯é›†æˆã€Dockeréƒ¨ç½²ã€æµ‹è¯•ç”¨ä¾‹ã€è¿ç»´è„šæœ¬ã€æ€§èƒ½æŒ‡æ ‡
