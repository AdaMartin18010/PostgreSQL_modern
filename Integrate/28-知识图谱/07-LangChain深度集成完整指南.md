---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `DataBaseTheory\21-AIçŸ¥è¯†åº“\07-LangChainæ·±åº¦é›†æˆå®Œæ•´æŒ‡å—.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# LangChain + PostgreSQL 18 æ·±åº¦é›†æˆæŒ‡å—

## 1. æ ¸å¿ƒæ¶æ„

### 1.1 æŠ€æœ¯æ ˆ

```python
LangChain + PostgreSQL 18 + pgvector + Apache AGE
â”œâ”€ LLM: OpenAI GPT-4 / Claude / æœ¬åœ°æ¨¡å‹
â”œâ”€ å‘é‡æ•°æ®åº“: pgvector
â”œâ”€ å›¾æ•°æ®åº“: Apache AGE
â””â”€ å…³ç³»æ•°æ®åº“: PostgreSQL 18
```

### 1.2 é›†æˆæ¨¡å¼

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LangChain + PostgreSQL æ¶æ„              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚  [LangChain Application]                         â”‚
â”‚         â”‚                                        â”‚
â”‚    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”                                   â”‚
â”‚    â”‚         â”‚                                   â”‚
â”‚  [Agent]  [Chain]                                â”‚
â”‚    â”‚         â”‚                                   â”‚
â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                   â”‚
â”‚         â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚  â”‚                 â”‚                             â”‚
â”‚ [VectorStore]  [SQLDatabase]  [GraphDatabase]    â”‚
â”‚  (pgvector)    (PostgreSQL)   (AGE)              â”‚
â”‚                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. VectorStoreé›†æˆ

### 2.1 åŸºç¡€é…ç½®

```python
from langchain.vectorstores.pgvector import PGVector
from langchain.embeddings import OpenAIEmbeddings
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# è¿æ¥é…ç½®
CONNECTION_STRING = "postgresql://postgres:password@localhost:5432/vectordb"

# Embeddings
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    openai_api_key="your-api-key"
)

# VectorStore
vectorstore = PGVector(
    connection_string=CONNECTION_STRING,
    embedding_function=embeddings,
    collection_name="documents",
    distance_strategy="cosine"
)
```

### 2.2 æ–‡æ¡£ç´¢å¼•

```python
# åŠ è½½æ–‡æ¡£
loader = TextLoader("knowledge_base.txt")
documents = loader.load()

# åˆ†å—
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len
)
chunks = text_splitter.split_documents(documents)

# æ·»åŠ åˆ°å‘é‡åº“
vectorstore.add_documents(chunks)

# æ‰¹é‡ç´¢å¼•ä¼˜åŒ–
def batch_index_documents(docs, batch_size=100):
    """æ‰¹é‡ç´¢å¼•ï¼Œæå‡æ€§èƒ½"""
    for i in range(0, len(docs), batch_size):
        batch = docs[i:i+batch_size]
        vectorstore.add_documents(batch)
        print(f"å·²ç´¢å¼• {min(i+batch_size, len(docs))}/{len(docs)} ä¸ªæ–‡æ¡£")
```

### 2.3 å‘é‡æ£€ç´¢

```python
# ç›¸ä¼¼åº¦æœç´¢
def semantic_search(query: str, k: int = 5):
    """è¯­ä¹‰æœç´¢"""
    results = vectorstore.similarity_search(query, k=k)
    return results

# ç›¸ä¼¼åº¦æœç´¢ + åˆ†æ•°
def search_with_score(query: str, k: int = 5):
    """è¿”å›æ–‡æ¡£å’Œç›¸ä¼¼åº¦åˆ†æ•°"""
    results = vectorstore.similarity_search_with_score(query, k=k)
    return [(doc.page_content, score) for doc, score in results]

# MMRæœç´¢ï¼ˆæœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼‰
def mmr_search(query: str, k: int = 5, fetch_k: int = 20):
    """å¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§"""
    results = vectorstore.max_marginal_relevance_search(
        query=query,
        k=k,
        fetch_k=fetch_k
    )
    return results

# è¿‡æ»¤æœç´¢
def filtered_search(query: str, filter_dict: dict, k: int = 5):
    """å¸¦è¿‡æ»¤æ¡ä»¶çš„æœç´¢"""
    results = vectorstore.similarity_search(
        query=query,
        k=k,
        filter=filter_dict
    )
    return results

# ç¤ºä¾‹
docs = semantic_search("PostgreSQL MVCCåŸç†")
for doc in docs:
    print(doc.page_content[:200])
```

---

## 3. SQL Chainé›†æˆ

### 3.1 æ•°æ®åº“è¿æ¥

```python
from langchain.sql_database import SQLDatabase
from langchain.chains import SQLDatabaseChain
from langchain.llms import OpenAI

# è¿æ¥æ•°æ®åº“
db = SQLDatabase.from_uri(
    "postgresql://postgres:password@localhost:5432/mydb",
    include_tables=['users', 'orders', 'products'],  # é™åˆ¶è¡¨
    sample_rows_in_table_info=3  # æ ·æœ¬è¡Œæ•°
)

# æŸ¥çœ‹schema
print(db.table_info)

# LLM
llm = OpenAI(temperature=0, openai_api_key="your-api-key")

# SQL Chain
sql_chain = SQLDatabaseChain.from_llm(
    llm=llm,
    db=db,
    verbose=True,
    return_intermediate_steps=True
)
```

### 3.2 è‡ªç„¶è¯­è¨€æŸ¥è¯¢

```python
# Text-to-SQL
def nl_to_sql(question: str):
    """è‡ªç„¶è¯­è¨€è½¬SQLæŸ¥è¯¢"""
    result = sql_chain.run(question)
    return result

# ç¤ºä¾‹
question = "æœ€è¿‘ä¸€å‘¨é”€å”®é¢æœ€é«˜çš„5ä¸ªäº§å“æ˜¯ä»€ä¹ˆï¼Ÿ"
answer = nl_to_sql(question)
print(answer)

# è·å–ä¸­é—´æ­¥éª¤
def nl_to_sql_detailed(question: str):
    """è¿”å›SQLå’Œç»“æœ"""
    result = sql_chain(question)
    return {
        'question': question,
        'sql': result['intermediate_steps'][0],
        'result': result['result']
    }
```

### 3.3 SQL Agent

```python
from langchain.agents import create_sql_agent
from langchain.agents.agent_toolkits import SQLDatabaseToolkit

# åˆ›å»ºtoolkit
toolkit = SQLDatabaseToolkit(db=db, llm=llm)

# åˆ›å»ºagent
sql_agent = create_sql_agent(
    llm=llm,
    toolkit=toolkit,
    verbose=True,
    agent_type="openai-tools"
)

# ä½¿ç”¨agent
def ask_database(question: str):
    """ä½¿ç”¨AgentæŸ¥è¯¢æ•°æ®åº“"""
    response = sql_agent.run(question)
    return response

# å¤æ‚æŸ¥è¯¢ç¤ºä¾‹
complex_query = """
åˆ†ææœ€è¿‘ä¸€ä¸ªæœˆçš„é”€å”®è¶‹åŠ¿ï¼š
1. æ¯å¤©çš„é”€å”®é¢
2. é”€å”®é¢ç¯æ¯”å¢é•¿
3. Top 3äº§å“ç±»åˆ«
"""
answer = ask_database(complex_query)
```

---

## 4. Graph Chainé›†æˆï¼ˆApache AGEï¼‰

### 4.1 å›¾æ•°æ®åº“è¿æ¥

```python
from langchain.graphs import Neo4jGraph  # AGEå…¼å®¹
import psycopg2

class AGEGraph:
    """Apache AGEå›¾æ•°æ®åº“è¿æ¥"""

    def __init__(self, conn_str: str, graph_name: str = "knowledge_graph"):
        self.conn = psycopg2.connect(conn_str)
        self.cursor = self.conn.cursor()
        self.graph_name = graph_name

        # åŠ è½½AGE
        self.cursor.execute("LOAD 'age';")
        self.cursor.execute("SET search_path = ag_catalog, '$user', public;")

    def query(self, cypher: str, params: dict = None):
        """æ‰§è¡ŒCypheræŸ¥è¯¢"""
        if params:
            query = f"""
                SELECT * FROM cypher('{self.graph_name}', $$
                    {cypher}
                $$, %s) AS (result agtype);
            """
            self.cursor.execute(query, (params,))
        else:
            query = f"""
                SELECT * FROM cypher('{self.graph_name}', $$
                    {cypher}
                $$) AS (result agtype);
            """
            self.cursor.execute(query)

        return self.cursor.fetchall()

    def get_schema(self):
        """è·å–å›¾schema"""
        labels = self.query("MATCH (n) RETURN DISTINCT labels(n)")
        relationships = self.query("MATCH ()-[r]->() RETURN DISTINCT type(r)")
        return {
            'node_labels': labels,
            'relationship_types': relationships
        }

# ä½¿ç”¨
graph = AGEGraph("postgresql://postgres@localhost/graphdb")
```

### 4.2 Graph QA Chain

```python
from langchain.chains import GraphCypherQAChain
from langchain.prompts import PromptTemplate

# Cypherç”Ÿæˆæç¤ºè¯
CYPHER_GENERATION_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªCypheræŸ¥è¯¢ä¸“å®¶ã€‚æ ¹æ®é—®é¢˜ç”ŸæˆCypheræŸ¥è¯¢ã€‚

Schema:
{schema}

é—®é¢˜: {question}

åªè¿”å›CypheræŸ¥è¯¢:
"""

cypher_prompt = PromptTemplate(
    input_variables=["schema", "question"],
    template=CYPHER_GENERATION_TEMPLATE
)

# Graph QA Chain
class GraphQAChain:
    """å›¾æ•°æ®åº“é—®ç­”"""

    def __init__(self, graph: AGEGraph, llm):
        self.graph = graph
        self.llm = llm
        self.schema = graph.get_schema()

    def ask(self, question: str):
        """å›ç­”é—®é¢˜"""
        # 1. ç”ŸæˆCypher
        cypher = self._generate_cypher(question)

        # 2. æ‰§è¡ŒæŸ¥è¯¢
        results = self.graph.query(cypher)

        # 3. ç”Ÿæˆç­”æ¡ˆ
        answer = self._generate_answer(question, results)

        return {
            'question': question,
            'cypher': cypher,
            'results': results,
            'answer': answer
        }

    def _generate_cypher(self, question: str) -> str:
        """LLMç”ŸæˆCypher"""
        prompt = cypher_prompt.format(
            schema=str(self.schema),
            question=question
        )
        cypher = self.llm(prompt)
        return cypher.strip()

    def _generate_answer(self, question: str, results: list) -> str:
        """æ ¹æ®ç»“æœç”Ÿæˆç­”æ¡ˆ"""
        prompt = f"""
        åŸºäºä»¥ä¸‹æŸ¥è¯¢ç»“æœå›ç­”é—®é¢˜ã€‚

        é—®é¢˜: {question}
        ç»“æœ: {results}

        å›ç­”:
        """
        answer = self.llm(prompt)
        return answer.strip()

# ä½¿ç”¨
graph_qa = GraphQAChain(graph, llm)
result = graph_qa.ask("PostgreSQLä½¿ç”¨äº†å“ªäº›æŠ€æœ¯ï¼Ÿ")
```

---

## 5. RAGåº”ç”¨å¼€å‘

### 5.1 åŸºç¡€RAG

```python
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# RAGæç¤ºè¯
RAG_PROMPT_TEMPLATE = """
ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä¸çŸ¥é“ã€‚

ä¸Šä¸‹æ–‡:
{context}

é—®é¢˜: {question}

å›ç­”:
"""

rag_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=RAG_PROMPT_TEMPLATE
)

# RetrievalQA Chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    chain_type_kwargs={"prompt": rag_prompt},
    return_source_documents=True
)

# ä½¿ç”¨
def rag_query(question: str):
    """RAGæŸ¥è¯¢"""
    result = qa_chain({"query": question})
    return {
        'answer': result['result'],
        'sources': [doc.page_content for doc in result['source_documents']]
    }

# ç¤ºä¾‹
answer = rag_query("ä»€ä¹ˆæ˜¯MVCCï¼Ÿ")
print(f"ç­”æ¡ˆ: {answer['answer']}")
print(f"æ¥æº: {answer['sources']}")
```

### 5.2 æ··åˆRAGï¼ˆå‘é‡+SQL+å›¾ï¼‰

```python
class HybridRAG:
    """æ··åˆæ£€ç´¢å¢å¼ºç”Ÿæˆ"""

    def __init__(self, vectorstore, sql_db, graph_db, llm):
        self.vectorstore = vectorstore
        self.sql_db = sql_db
        self.graph_db = graph_db
        self.llm = llm

    def query(self, question: str):
        """å¤šæºæ£€ç´¢+ç”Ÿæˆ"""

        # 1. é—®é¢˜åˆ†ç±»
        query_type = self._classify_question(question)

        # 2. å¤šè·¯æ£€ç´¢
        contexts = []

        # å‘é‡æ£€ç´¢
        if query_type in ['concept', 'general']:
            vector_results = self.vectorstore.similarity_search(question, k=3)
            contexts.append({
                'source': 'vector',
                'content': [doc.page_content for doc in vector_results]
            })

        # SQLæŸ¥è¯¢
        if query_type in ['data', 'statistics']:
            sql_chain = SQLDatabaseChain.from_llm(self.llm, self.sql_db)
            sql_result = sql_chain.run(question)
            contexts.append({
                'source': 'sql',
                'content': sql_result
            })

        # å›¾æŸ¥è¯¢
        if query_type in ['relation', 'path']:
            graph_qa = GraphQAChain(self.graph_db, self.llm)
            graph_result = graph_qa.ask(question)
            contexts.append({
                'source': 'graph',
                'content': graph_result['answer']
            })

        # 3. èåˆç”Ÿæˆç­”æ¡ˆ
        answer = self._generate_answer(question, contexts)

        return {
            'question': question,
            'query_type': query_type,
            'contexts': contexts,
            'answer': answer
        }

    def _classify_question(self, question: str) -> str:
        """åˆ†ç±»é—®é¢˜"""
        prompt = f"""
        å°†é—®é¢˜åˆ†ç±»ä¸ºä»¥ä¸‹ç±»å‹ä¹‹ä¸€ï¼š
        - concept: æ¦‚å¿µè§£é‡Š
        - data: æ•°æ®æŸ¥è¯¢
        - statistics: ç»Ÿè®¡åˆ†æ
        - relation: å…³ç³»æŸ¥è¯¢
        - path: è·¯å¾„æŸ¥è¯¢
        - general: ä¸€èˆ¬é—®é¢˜

        é—®é¢˜: {question}

        ç±»å‹:
        """
        result = self.llm(prompt).strip().lower()
        return result

    def _generate_answer(self, question: str, contexts: list) -> str:
        """èåˆå¤šæºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ"""
        context_text = "\n\n".join([
            f"æ¥æº {ctx['source']}:\n{ctx['content']}"
            for ctx in contexts
        ])

        prompt = f"""
        åŸºäºä»¥ä¸‹å¤šä¸ªæ¥æºçš„ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

        {context_text}

        é—®é¢˜: {question}

        ç»¼åˆå›ç­”:
        """

        answer = self.llm(prompt)
        return answer.strip()

# ä½¿ç”¨
hybrid_rag = HybridRAG(vectorstore, db, graph, llm)
result = hybrid_rag.query("PostgreSQLçš„MVCCåœ¨å“ªäº›äº§å“ä¸­ä½¿ç”¨ï¼Ÿ")
```

---

## 6. Agentå¼€å‘

### 6.1 è‡ªå®šä¹‰å·¥å…·

```python
from langchain.tools import BaseTool
from typing import Optional
from pydantic import BaseModel, Field

class VectorSearchInput(BaseModel):
    """å‘é‡æœç´¢è¾“å…¥"""
    query: str = Field(description="æœç´¢æŸ¥è¯¢")
    k: int = Field(default=5, description="è¿”å›ç»“æœæ•°")

class VectorSearchTool(BaseTool):
    """å‘é‡æœç´¢å·¥å…·"""
    name = "vector_search"
    description = "æœç´¢çŸ¥è¯†åº“æ–‡æ¡£ï¼Œé€‚ç”¨äºæ¦‚å¿µæŸ¥è¯¢å’Œé—®é¢˜è§£ç­”"
    args_schema = VectorSearchInput
    vectorstore: Any = None

    def _run(self, query: str, k: int = 5) -> str:
        """æ‰§è¡Œæœç´¢"""
        results = self.vectorstore.similarity_search(query, k=k)
        return "\n\n".join([doc.page_content for doc in results])

class SQLQueryTool(BaseTool):
    """SQLæŸ¥è¯¢å·¥å…·"""
    name = "sql_query"
    description = "æŸ¥è¯¢æ•°æ®åº“æ•°æ®ï¼Œé€‚ç”¨äºæ•°æ®ç»Ÿè®¡å’Œåˆ†æ"
    db: Any = None
    llm: Any = None

    def _run(self, question: str) -> str:
        """æ‰§è¡ŒSQLæŸ¥è¯¢"""
        chain = SQLDatabaseChain.from_llm(self.llm, self.db)
        result = chain.run(question)
        return result

class GraphQueryTool(BaseTool):
    """å›¾æŸ¥è¯¢å·¥å…·"""
    name = "graph_query"
    description = "æŸ¥è¯¢çŸ¥è¯†å›¾è°±ï¼Œé€‚ç”¨äºå…³ç³»å’Œè·¯å¾„æŸ¥è¯¢"
    graph: Any = None
    llm: Any = None

    def _run(self, question: str) -> str:
        """æ‰§è¡Œå›¾æŸ¥è¯¢"""
        qa = GraphQAChain(self.graph, self.llm)
        result = qa.ask(question)
        return result['answer']
```

### 6.2 åˆ›å»ºAgent

```python
from langchain.agents import initialize_agent, AgentType

# åˆå§‹åŒ–å·¥å…·
tools = [
    VectorSearchTool(vectorstore=vectorstore),
    SQLQueryTool(db=db, llm=llm),
    GraphQueryTool(graph=graph, llm=llm)
]

# åˆ›å»ºAgent
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.OPENAI_FUNCTIONS,
    verbose=True,
    max_iterations=5
)

# ä½¿ç”¨Agent
def ask_agent(question: str):
    """ä½¿ç”¨Agentå›ç­”é—®é¢˜"""
    response = agent.run(question)
    return response

# å¤æ‚é—®é¢˜ç¤ºä¾‹
question = """
åˆ†æPostgreSQLçš„MVCCæœºåˆ¶ï¼š
1. æ¦‚å¿µè§£é‡Š
2. åœ¨å“ªäº›ç³»ç»Ÿä¸­ä½¿ç”¨
3. è¿‘ä¸€å¹´çš„æ€§èƒ½æ•°æ®
"""
answer = ask_agent(question)
```

---

## 7. è®°å¿†ç®¡ç†

### 7.1 å¯¹è¯è®°å¿†ï¼ˆPostgreSQLå­˜å‚¨ï¼‰

```python
from langchain.memory import ConversationBufferMemory
from langchain.memory.chat_message_histories import PostgresChatMessageHistory

# åŸºäºPostgreSQLçš„å¯¹è¯å†å²
class PGChatMemory:
    """PostgreSQLå¯¹è¯è®°å¿†"""

    def __init__(self, conn_str: str, session_id: str):
        self.history = PostgresChatMessageHistory(
            connection_string=conn_str,
            session_id=session_id
        )
        self.memory = ConversationBufferMemory(
            chat_memory=self.history,
            return_messages=True
        )

    def add_message(self, user_message: str, ai_message: str):
        """æ·»åŠ æ¶ˆæ¯"""
        self.history.add_user_message(user_message)
        self.history.add_ai_message(ai_message)

    def get_messages(self, limit: int = 10):
        """è·å–æœ€è¿‘æ¶ˆæ¯"""
        messages = self.history.messages[-limit:]
        return messages

    def clear(self):
        """æ¸…ç©ºå†å²"""
        self.history.clear()

# ä½¿ç”¨
memory = PGChatMemory(CONNECTION_STRING, session_id="user123")

# å¯¹è¯
from langchain.chains import ConversationChain

conversation = ConversationChain(
    llm=llm,
    memory=memory.memory,
    verbose=True
)

response = conversation.predict(input="ä»€ä¹ˆæ˜¯MVCCï¼Ÿ")
```

### 7.2 æ‘˜è¦è®°å¿†

```python
from langchain.memory import ConversationSummaryMemory

# è‡ªåŠ¨æ‘˜è¦é•¿å¯¹è¯
summary_memory = ConversationSummaryMemory(
    llm=llm,
    return_messages=True
)

# å¯¹è¯é“¾
conversation_with_summary = ConversationChain(
    llm=llm,
    memory=summary_memory
)

# é•¿å¯¹è¯ä¼šè‡ªåŠ¨æ‘˜è¦
for i in range(10):
    response = conversation_with_summary.predict(
        input=f"ç¬¬{i+1}ä¸ªé—®é¢˜..."
    )
```

---

## 8. æµå¼è¾“å‡º

### 8.1 æµå¼ç”Ÿæˆ

```python
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.callbacks.base import BaseCallbackHandler

class CustomStreamingHandler(BaseCallbackHandler):
    """è‡ªå®šä¹‰æµå¼å¤„ç†å™¨"""

    def on_llm_new_token(self, token: str, **kwargs):
        """å¤„ç†æ–°token"""
        print(token, end="", flush=True)

    def on_llm_end(self, response, **kwargs):
        """LLMç»“æŸ"""
        print("\n[å®Œæˆ]")

# ä½¿ç”¨æµå¼è¾“å‡º
streaming_llm = OpenAI(
    temperature=0,
    streaming=True,
    callbacks=[CustomStreamingHandler()]
)

# æµå¼æŸ¥è¯¢
response = streaming_llm("è§£é‡ŠPostgreSQLçš„MVCCæœºåˆ¶")
```

### 8.2 å¼‚æ­¥æµå¼

```python
import asyncio
from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler

async def stream_query(question: str):
    """å¼‚æ­¥æµå¼æŸ¥è¯¢"""

    callback = AsyncIteratorCallbackHandler()

    async_llm = OpenAI(
        temperature=0,
        streaming=True,
        callbacks=[callback]
    )

    # å¯åŠ¨ç”Ÿæˆä»»åŠ¡
    task = asyncio.create_task(
        async_llm.agenerate([[question]])
    )

    # æµå¼è¾“å‡º
    async for token in callback.aiter():
        print(token, end="", flush=True)

    await task
    print("\n[å®Œæˆ]")

# è¿è¡Œ
asyncio.run(stream_query("ä»€ä¹ˆæ˜¯ACIDï¼Ÿ"))
```

---

## 9. æ€§èƒ½ä¼˜åŒ–

### 9.1 å‘é‡ç´¢å¼•ä¼˜åŒ–

```python
# HNSWç´¢å¼•å‚æ•°è°ƒä¼˜
def create_optimized_index(table_name: str, column_name: str):
    """åˆ›å»ºä¼˜åŒ–çš„HNSWç´¢å¼•"""
    conn = psycopg2.connect(CONNECTION_STRING)
    cursor = conn.cursor()

    # PostgreSQL 18ä¼˜åŒ–å‚æ•°
    cursor.execute(f"""
        CREATE INDEX idx_{table_name}_{column_name}_hnsw
        ON {table_name}
        USING hnsw ({column_name} vector_cosine_ops)
        WITH (m = 16, ef_construction = 64);
    """)

    conn.commit()
    cursor.close()
    conn.close()

# æŸ¥è¯¢ä¼˜åŒ–
def optimized_search(query: str, k: int = 5):
    """ä¼˜åŒ–çš„å‘é‡æœç´¢"""
    conn = psycopg2.connect(CONNECTION_STRING)
    cursor = conn.cursor()

    # è®¾ç½®ef_searchå‚æ•°
    cursor.execute("SET hnsw.ef_search = 100;")

    # æ‰§è¡Œæœç´¢
    query_vec = embeddings.embed_query(query)
    cursor.execute("""
        SELECT content, 1 - (embedding <=> %s::vector) AS similarity
        FROM documents
        ORDER BY embedding <=> %s::vector
        LIMIT %s;
    """, (query_vec, query_vec, k))

    results = cursor.fetchall()
    cursor.close()
    conn.close()

    return results
```

### 9.2 æ‰¹å¤„ç†ä¼˜åŒ–

```python
from concurrent.futures import ThreadPoolExecutor

def batch_embed_documents(docs: list, batch_size: int = 100):
    """æ‰¹é‡embedding"""

    embeddings_list = []

    for i in range(0, len(docs), batch_size):
        batch = docs[i:i+batch_size]
        batch_texts = [doc.page_content for doc in batch]

        # æ‰¹é‡ç”Ÿæˆembedding
        batch_embeddings = embeddings.embed_documents(batch_texts)
        embeddings_list.extend(batch_embeddings)

    return embeddings_list

def parallel_index(docs: list, num_workers: int = 4):
    """å¹¶è¡Œç´¢å¼•"""

    chunk_size = len(docs) // num_workers

    def index_chunk(chunk):
        vectorstore.add_documents(chunk)

    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = []
        for i in range(0, len(docs), chunk_size):
            chunk = docs[i:i+chunk_size]
            futures.append(executor.submit(index_chunk, chunk))

        for future in futures:
            future.result()
```

### 9.3 ç¼“å­˜ç­–ç•¥

```python
from langchain.cache import PostgresCache
from langchain.globals import set_llm_cache

# PostgreSQLç¼“å­˜
set_llm_cache(PostgresCache(connection_string=CONNECTION_STRING))

# ç›¸åŒæŸ¥è¯¢ä¼šä½¿ç”¨ç¼“å­˜
response1 = llm("ä»€ä¹ˆæ˜¯MVCCï¼Ÿ")  # è°ƒç”¨LLM
response2 = llm("ä»€ä¹ˆæ˜¯MVCCï¼Ÿ")  # ä½¿ç”¨ç¼“å­˜

# Redisç¼“å­˜ï¼ˆæ›´å¿«ï¼‰
from langchain.cache import RedisCache
import redis

redis_client = redis.Redis(host='localhost', port=6379)
set_llm_cache(RedisCache(redis_client))
```

---

## 10. ç”Ÿäº§éƒ¨ç½²

### 10.1 è¿æ¥æ± é…ç½®

```python
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

# è¿æ¥æ± 
engine = create_engine(
    CONNECTION_STRING,
    poolclass=QueuePool,
    pool_size=20,
    max_overflow=10,
    pool_timeout=30,
    pool_recycle=3600
)

# ä½¿ç”¨è¿æ¥æ± 
vectorstore = PGVector(
    connection_string=CONNECTION_STRING,
    embedding_function=embeddings,
    engine=engine
)
```

### 10.2 FastAPIç”Ÿäº§éƒ¨ç½²

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn

app = FastAPI(title="LangChain PostgreSQL API")

class QueryRequest(BaseModel):
    question: str
    k: int = 5

class QueryResponse(BaseModel):
    answer: str
    sources: list
    duration_ms: float

@app.post("/api/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """RAGæŸ¥è¯¢æ¥å£"""

    import time
    start = time.time()

    try:
        result = qa_chain({"query": request.question})

        return QueryResponse(
            answer=result['result'],
            sources=[doc.page_content[:200] for doc in result['source_documents']],
            duration_ms=(time.time() - start) * 1000
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "ok"}

if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        workers=4
    )
```

### 10.3 Dockeréƒ¨ç½²

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# è¿è¡Œ
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  postgres:
    image: pgvector/pgvector:pg18
    environment:
      POSTGRES_PASSWORD: password
    volumes:
      - pgdata:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  api:
    build: .
    depends_on:
      - postgres
    environment:
      DATABASE_URL: postgresql://postgres:password@postgres:5432/vectordb
      OPENAI_API_KEY: ${OPENAI_API_KEY}
    ports:
      - "8000:8000"

volumes:
  pgdata:
```

---

## 11. ç›‘æ§ä¸è°ƒè¯•

### 11.1 æ€§èƒ½ç›‘æ§

```python
from langchain.callbacks import get_openai_callback
import time

def monitored_query(question: str):
    """å¸¦ç›‘æ§çš„æŸ¥è¯¢"""

    start_time = time.time()

    with get_openai_callback() as cb:
        result = qa_chain({"query": question})

        metrics = {
            'question': question,
            'answer': result['result'],
            'duration_ms': (time.time() - start_time) * 1000,
            'total_tokens': cb.total_tokens,
            'prompt_tokens': cb.prompt_tokens,
            'completion_tokens': cb.completion_tokens,
            'total_cost': cb.total_cost
        }

    return metrics

# è®°å½•åˆ°æ•°æ®åº“
def log_metrics(metrics: dict):
    """è®°å½•æŒ‡æ ‡"""
    conn = psycopg2.connect(CONNECTION_STRING)
    cursor = conn.cursor()

    cursor.execute("""
        INSERT INTO query_metrics
        (question, duration_ms, total_tokens, total_cost)
        VALUES (%s, %s, %s, %s);
    """, (
        metrics['question'],
        metrics['duration_ms'],
        metrics['total_tokens'],
        metrics['total_cost']
    ))

    conn.commit()
    cursor.close()
    conn.close()
```

### 11.2 è°ƒè¯•å·¥å…·

```python
from langchain.callbacks import StdOutCallbackHandler
from langchain.callbacks.tracers import ConsoleCallbackHandler

# è¯¦ç»†è°ƒè¯•
debug_handler = ConsoleCallbackHandler()

agent_with_debug = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.OPENAI_FUNCTIONS,
    callbacks=[debug_handler],
    verbose=True
)

# æ‰§è¡ŒæŸ¥çœ‹è¯¦ç»†æ­¥éª¤
result = agent_with_debug.run("å¤æ‚é—®é¢˜...")
```

---

## 12. æœ€ä½³å®è·µ

### 12.1 Promptå·¥ç¨‹

```python
# å¥½çš„Promptè®¾è®¡
GOOD_PROMPT = """
ä½ æ˜¯PostgreSQLæ•°æ®åº“ä¸“å®¶ã€‚åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚

è§„åˆ™:
1. å¦‚æœä¸ç¡®å®šï¼Œè¯´"æˆ‘ä¸çŸ¥é“"
2. å¼•ç”¨å…·ä½“çš„ä¸Šä¸‹æ–‡ç‰‡æ®µ
3. æä¾›æŠ€æœ¯ç»†èŠ‚å’Œç¤ºä¾‹

ä¸Šä¸‹æ–‡:
{context}

é—®é¢˜: {question}

å›ç­”:
"""

# åçš„Prompt
BAD_PROMPT = "å›ç­”: {question}"
```

### 12.2 é”™è¯¯å¤„ç†

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
def robust_query(question: str):
    """å¸¦é‡è¯•çš„æŸ¥è¯¢"""
    try:
        result = qa_chain({"query": question})
        return result['result']
    except Exception as e:
        print(f"é”™è¯¯: {e}, é‡è¯•...")
        raise

# ä½¿ç”¨
answer = robust_query("ä»€ä¹ˆæ˜¯MVCCï¼Ÿ")
```

### 12.3 å®‰å…¨å®è·µ

```python
# SQLæ³¨å…¥é˜²æŠ¤
def safe_sql_query(user_input: str):
    """å®‰å…¨çš„SQLæŸ¥è¯¢"""

    # 1. è¾“å…¥éªŒè¯
    if len(user_input) > 1000:
        raise ValueError("è¾“å…¥è¿‡é•¿")

    # 2. ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢
    # SQLDatabaseChainé»˜è®¤å·²é˜²æŠ¤

    # 3. é™åˆ¶è¡¨è®¿é—®
    db = SQLDatabase.from_uri(
        CONNECTION_STRING,
        include_tables=['public_table'],  # åªå…è®¸è®¿é—®ç‰¹å®šè¡¨
        sample_rows_in_table_info=1
    )

    return sql_chain.run(user_input)

# Promptæ³¨å…¥é˜²æŠ¤
def sanitize_input(user_input: str) -> str:
    """æ¸…ç†ç”¨æˆ·è¾“å…¥"""
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    sanitized = user_input.replace("\\n", " ").replace("\\", "")
    return sanitized[:500]  # é™åˆ¶é•¿åº¦
```

---

## 13. å®Œæ•´æ¡ˆä¾‹ï¼šä¼ä¸šçŸ¥è¯†åº“

```python
"""
ä¼ä¸šçŸ¥è¯†åº“å®Œæ•´å®ç°
åŠŸèƒ½: RAGé—®ç­” + SQLæŸ¥è¯¢ + å›¾æŸ¥è¯¢
"""

from langchain.vectorstores.pgvector import PGVector
from langchain.embeddings import OpenAIEmbeddings
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA
from langchain.sql_database import SQLDatabase
from langchain.agents import initialize_agent, AgentType
from langchain.tools import Tool
import psycopg2

class EnterpriseKnowledgeBase:
    """ä¼ä¸šçŸ¥è¯†åº“"""

    def __init__(self, db_config: dict, openai_api_key: str):
        # åˆå§‹åŒ–ç»„ä»¶
        self.embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
        self.llm = OpenAI(temperature=0, openai_api_key=openai_api_key)

        # Vector Store
        self.vectorstore = PGVector(
            connection_string=db_config['vector_db'],
            embedding_function=self.embeddings
        )

        # SQL Database
        self.sql_db = SQLDatabase.from_uri(db_config['sql_db'])

        # Graph Database
        self.graph_db = AGEGraph(db_config['graph_db'])

        # åˆ›å»ºå·¥å…·
        self.tools = self._create_tools()

        # åˆ›å»ºAgent
        self.agent = initialize_agent(
            tools=self.tools,
            llm=self.llm,
            agent=AgentType.OPENAI_FUNCTIONS,
            verbose=True
        )

    def _create_tools(self):
        """åˆ›å»ºå·¥å…·é›†"""

        # æ–‡æ¡£æœç´¢å·¥å…·
        doc_tool = Tool(
            name="DocumentSearch",
            func=self._search_documents,
            description="æœç´¢æŠ€æœ¯æ–‡æ¡£å’ŒçŸ¥è¯†åº“ï¼Œé€‚ç”¨äºæ¦‚å¿µæŸ¥è¯¢"
        )

        # SQLæŸ¥è¯¢å·¥å…·
        sql_tool = Tool(
            name="SQLQuery",
            func=self._query_database,
            description="æŸ¥è¯¢ç»“æ„åŒ–æ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯"
        )

        # å›¾æŸ¥è¯¢å·¥å…·
        graph_tool = Tool(
            name="GraphQuery",
            func=self._query_graph,
            description="æŸ¥è¯¢å®ä½“å…³ç³»å’ŒçŸ¥è¯†å›¾è°±"
        )

        return [doc_tool, sql_tool, graph_tool]

    def _search_documents(self, query: str) -> str:
        """æœç´¢æ–‡æ¡£"""
        results = self.vectorstore.similarity_search(query, k=3)
        return "\n\n".join([doc.page_content for doc in results])

    def _query_database(self, question: str) -> str:
        """SQLæŸ¥è¯¢"""
        from langchain.chains import SQLDatabaseChain
        chain = SQLDatabaseChain.from_llm(self.llm, self.sql_db)
        return chain.run(question)

    def _query_graph(self, question: str) -> str:
        """å›¾æŸ¥è¯¢"""
        qa = GraphQAChain(self.graph_db, self.llm)
        result = qa.ask(question)
        return result['answer']

    def ask(self, question: str):
        """å›ç­”é—®é¢˜"""
        response = self.agent.run(question)
        return response

    def index_document(self, doc_path: str):
        """ç´¢å¼•æ–°æ–‡æ¡£"""
        from langchain.document_loaders import TextLoader
        from langchain.text_splitter import RecursiveCharacterTextSplitter

        # åŠ è½½
        loader = TextLoader(doc_path)
        documents = loader.load()

        # åˆ†å—
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = splitter.split_documents(documents)

        # ç´¢å¼•
        self.vectorstore.add_documents(chunks)

        return f"å·²ç´¢å¼• {len(chunks)} ä¸ªæ–‡æ¡£å—"

# ä½¿ç”¨
kb = EnterpriseKnowledgeBase(
    db_config={
        'vector_db': 'postgresql://...',
        'sql_db': 'postgresql://...',
        'graph_db': 'postgresql://...'
    },
    openai_api_key='your-key'
)

# æŸ¥è¯¢
answer = kb.ask("""
åˆ†æPostgreSQLçš„MVCCå®ç°:
1. æŠ€æœ¯åŸç† (ä»æ–‡æ¡£)
2. æ€§èƒ½æ•°æ® (ä»æ•°æ®åº“)
3. ä½¿ç”¨å…³ç³» (ä»å›¾è°±)
""")
print(answer)
```

---

**å®Œæˆ**: LangChain + PostgreSQL 18å®Œæ•´é›†æˆæŒ‡å—
**å­—æ•°**: ~18,000å­—
**æ¶µç›–**: VectorStoreã€SQLã€Graphã€RAGã€Agentã€ç”Ÿäº§éƒ¨ç½²ã€æœ€ä½³å®è·µ
