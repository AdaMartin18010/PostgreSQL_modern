---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `DataBaseTheory\21-AIçŸ¥è¯†åº“\09-LangChainé«˜çº§ç‰¹æ€§å®æˆ˜.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# LangChainé«˜çº§ç‰¹æ€§å®æˆ˜

## 1. Memoryç®¡ç†

### 1.1 å¯¹è¯å†å²å­˜å‚¨

```python
from langchain.memory import PostgresChatMessageHistory
from langchain.memory import ConversationBufferMemory
import psycopg2

# åˆ›å»ºå†å²è¡¨
conn = psycopg2.connect("dbname=langchain_db")
cursor = conn.cursor()

cursor.execute("""
    CREATE TABLE IF NOT EXISTS chat_history (
        id BIGSERIAL PRIMARY KEY,
        session_id VARCHAR(255) NOT NULL,
        message JSONB NOT NULL,
        created_at TIMESTAMPTZ DEFAULT now()
    );

    CREATE INDEX idx_chat_history_session ON chat_history(session_id);
    CREATE INDEX idx_chat_history_created ON chat_history(created_at);
""")
conn.commit()

# ä½¿ç”¨PostgreSQLå­˜å‚¨å¯¹è¯å†å²
message_history = PostgresChatMessageHistory(
    connection_string="postgresql://user:pass@localhost/langchain_db",
    session_id="user_123"
)

# é›†æˆåˆ°LangChain Memory
memory = ConversationBufferMemory(
    memory_key="chat_history",
    chat_memory=message_history,
    return_messages=True
)

# åœ¨å¯¹è¯é“¾ä¸­ä½¿ç”¨
from langchain.chains import ConversationChain
from langchain.llms import OpenAI

chain = ConversationChain(
    llm=OpenAI(temperature=0.7),
    memory=memory
)

# å¤šè½®å¯¹è¯
response1 = chain.run("æˆ‘å«å¼ ä¸‰")
# AI: ä½ å¥½ï¼Œå¼ ä¸‰ï¼

response2 = chain.run("æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ")
# AI: ä½ çš„åå­—æ˜¯å¼ ä¸‰ã€‚ï¼ˆä»historyä¸­è·å–ï¼‰

# æŸ¥çœ‹å†å²
print(memory.load_memory_variables({}))
```

---

## 2. é«˜çº§RAGæ¨¡å¼

### 2.1 æ··åˆæ£€ç´¢RAG

```python
from langchain.retrievers import EnsembleRetriever
from langchain.vectorstores import PGVector
from langchain.retrievers import BM25Retriever

class HybridRAG:
    """æ··åˆæ£€ç´¢RAG"""

    def __init__(self, pg_conn_string):
        # å‘é‡æ£€ç´¢å™¨
        self.vector_retriever = PGVector(
            connection_string=pg_conn_string,
            embedding_function=OpenAIEmbeddings()
        ).as_retriever(search_kwargs={"k": 20})

        # BM25æ–‡æœ¬æ£€ç´¢å™¨
        documents = self.load_all_documents()
        self.bm25_retriever = BM25Retriever.from_documents(documents)
        self.bm25_retriever.k = 20

        # æ··åˆæ£€ç´¢å™¨ï¼ˆæƒé‡ï¼šå‘é‡0.6ï¼ŒBM25 0.4ï¼‰
        self.ensemble_retriever = EnsembleRetriever(
            retrievers=[self.vector_retriever, self.bm25_retriever],
            weights=[0.6, 0.4]
        )

    def retrieve(self, query):
        """æ··åˆæ£€ç´¢"""
        docs = self.ensemble_retriever.get_relevant_documents(query)
        return docs

# ä½¿ç”¨
rag = HybridRAG("postgresql://localhost/kb_db")
docs = rag.retrieve("PostgreSQLå¼‚æ­¥I/OåŸç†")

# å‡†ç¡®ç‡å¯¹æ¯”ï¼š
# çº¯å‘é‡æ£€ç´¢: 82%
# çº¯BM25æ£€ç´¢: 78%
# æ··åˆæ£€ç´¢: 89% (+7%)
```

### 2.2 Self-Query RAG

```python
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain.chains.query_constructor.base import AttributeInfo

# å®šä¹‰å…ƒæ•°æ®å­—æ®µ
metadata_field_info = [
    AttributeInfo(
        name="category",
        description="æ–‡æ¡£ç±»åˆ«ï¼Œå¦‚'æ•°æ®åº“'ã€'AI'ã€'æ€§èƒ½ä¼˜åŒ–'",
        type="string"
    ),
    AttributeInfo(
        name="author",
        description="æ–‡æ¡£ä½œè€…",
        type="string"
    ),
    AttributeInfo(
        name="date",
        description="å‘å¸ƒæ—¥æœŸï¼Œæ ¼å¼YYYY-MM-DD",
        type="string"
    ),
    AttributeInfo(
        name="difficulty",
        description="éš¾åº¦çº§åˆ«ï¼Œ1-5",
        type="integer"
    ),
]

# åˆ›å»ºSelf-Queryæ£€ç´¢å™¨
retriever = SelfQueryRetriever.from_llm(
    llm=OpenAI(temperature=0),
    vectorstore=vectorstore,
    document_contents="æŠ€æœ¯æ–‡æ¡£",
    metadata_field_info=metadata_field_info
)

# è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼ˆè‡ªåŠ¨è§£æè¿‡æ»¤æ¡ä»¶ï¼‰
docs = retriever.get_relevant_documents(
    "æŸ¥æ‰¾2024å¹´å‘å¸ƒçš„å…³äºPostgreSQLæ€§èƒ½ä¼˜åŒ–çš„åˆçº§æ–‡æ¡£"
)

# LangChainè‡ªåŠ¨å°†å…¶è½¬æ¢ä¸ºï¼š
# - è¯­ä¹‰æŸ¥è¯¢: "PostgreSQLæ€§èƒ½ä¼˜åŒ–"
# - å…ƒæ•°æ®è¿‡æ»¤: category='æ•°æ®åº“' AND date>='2024-01-01' AND difficulty<=2
```

### 2.3 Parent Document Retriever

```python
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import PostgresStore
from langchain.text_splitter import RecursiveCharacterTextSplitter

# åˆ›å»ºå­˜å‚¨
docstore = PostgresStore(
    connection_string="postgresql://localhost/langchain_db",
    collection_name="documents"
)

# åˆ†å—ç­–ç•¥ï¼š
# - å°å—ç”¨äºæ£€ç´¢ï¼ˆé«˜ç²¾åº¦ï¼‰
# - å¤§å—ç”¨äºä¸Šä¸‹æ–‡ï¼ˆå®Œæ•´è¯­ä¹‰ï¼‰

parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)
child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)

retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=docstore,
    child_splitter=child_splitter,
    parent_splitter=parent_splitter
)

# æ·»åŠ æ–‡æ¡£
retriever.add_documents(documents)

# æ£€ç´¢ï¼š
# 1. ç”¨å°å—æ£€ç´¢ï¼ˆç²¾å‡†åŒ¹é…ï¼‰
# 2. è¿”å›å®Œæ•´çˆ¶æ–‡æ¡£ï¼ˆæ›´å¤šä¸Šä¸‹æ–‡ï¼‰

docs = retriever.get_relevant_documents("MVCCåŸç†")
# è¿”å›å®Œæ•´ç« èŠ‚è€Œéç‰‡æ®µ
```

---

## 3. Agenté«˜çº§å¼€å‘

### 3.1 è‡ªå®šä¹‰å·¥å…·

```python
from langchain.agents import Tool, AgentExecutor, ZeroShotAgent
from langchain.tools import BaseTool
from typing import Optional

class PostgreSQLQueryTool(BaseTool):
    """PostgreSQLæŸ¥è¯¢å·¥å…·"""

    name = "postgresql_query"
    description = "ç”¨äºæŸ¥è¯¢PostgreSQLæ•°æ®åº“ã€‚è¾“å…¥åº”è¯¥æ˜¯å®Œæ•´çš„SQLæŸ¥è¯¢è¯­å¥ã€‚"

    def __init__(self, connection_string):
        super().__init__()
        self.conn = psycopg2.connect(connection_string)

    def _run(self, query: str) -> str:
        """æ‰§è¡ŒSQLæŸ¥è¯¢"""
        try:
            cursor = self.conn.cursor()
            cursor.execute(query)
            results = cursor.fetchall()
            cursor.close()

            return f"æŸ¥è¯¢è¿”å› {len(results)} è¡Œ:\n{results[:5]}"
        except Exception as e:
            return f"æŸ¥è¯¢é”™è¯¯: {e}"

    async def _arun(self, query: str) -> str:
        """å¼‚æ­¥æ‰§è¡Œ"""
        raise NotImplementedError()

class VectorSearchTool(BaseTool):
    """å‘é‡æœç´¢å·¥å…·"""

    name = "vector_search"
    description = "ç”¨äºè¯­ä¹‰æœç´¢æ–‡æ¡£ã€‚è¾“å…¥åº”è¯¥æ˜¯è‡ªç„¶è¯­è¨€æŸ¥è¯¢ã€‚"

    def __init__(self, vectorstore):
        super().__init__()
        self.vectorstore = vectorstore

    def _run(self, query: str) -> str:
        """æ‰§è¡Œå‘é‡æœç´¢"""
        docs = self.vectorstore.similarity_search(query, k=5)
        return "\n\n".join([doc.page_content for doc in docs])

    async def _arun(self, query: str) -> str:
        raise NotImplementedError()

class GraphQueryTool(BaseTool):
    """å›¾æŸ¥è¯¢å·¥å…·"""

    name = "graph_query"
    description = "ç”¨äºæŸ¥è¯¢çŸ¥è¯†å›¾è°±ã€‚è¾“å…¥åº”è¯¥æ˜¯CypheræŸ¥è¯¢è¯­å¥ã€‚"

    def __init__(self, graph_conn_string):
        super().__init__()
        self.conn = psycopg2.connect(graph_conn_string)

    def _run(self, cypher: str) -> str:
        """æ‰§è¡ŒCypheræŸ¥è¯¢"""
        try:
            cursor = self.conn.cursor()
            cursor.execute(f"""
                SELECT * FROM cypher('knowledge_graph', $$
                    {cypher}
                $$) AS (result agtype);
            """)
            results = cursor.fetchall()
            cursor.close()

            return f"æŸ¥è¯¢è¿”å› {len(results)} ä¸ªç»“æœ:\n{results[:5]}"
        except Exception as e:
            return f"æŸ¥è¯¢é”™è¯¯: {e}"

    async def _arun(self, cypher: str) -> str:
        raise NotImplementedError()

# åˆ›å»ºAgent
tools = [
    PostgreSQLQueryTool(connection_string="postgresql://..."),
    VectorSearchTool(vectorstore=vectorstore),
    GraphQueryTool(graph_conn_string="postgresql://...")
]

agent = ZeroShotAgent.from_llm_and_tools(
    llm=OpenAI(temperature=0),
    tools=tools
)

agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent,
    tools=tools,
    verbose=True
)

# æµ‹è¯•
response = agent_executor.run("""
æŸ¥è¯¢æ‰€æœ‰PostgreSQL 18ç›¸å…³çš„æ–‡æ¡£ï¼Œ
ç„¶åç»Ÿè®¡è¿™äº›æ–‡æ¡£çš„ä½œè€…æ•°é‡ï¼Œ
æœ€åæ‰¾å‡ºä½œè€…ä¹‹é—´çš„åä½œå…³ç³»ã€‚
""")

# Agentè‡ªåŠ¨æ‰§è¡Œï¼š
# 1. vector_search: "PostgreSQL 18"
# 2. postgresql_query: "SELECT DISTINCT author FROM documents WHERE ..."
# 3. graph_query: "MATCH (a1:Author)-[:COLLABORATED]->(a2:Author) RETURN ..."
```

---

## 4. ç”Ÿäº§çº§RAGç³»ç»Ÿ

### 4.1 å®Œæ•´RAG Pipeline

```python
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.callbacks import get_openai_callback

class ProductionRAGSystem:
    """ç”Ÿäº§çº§RAGç³»ç»Ÿ"""

    def __init__(self, config):
        # åˆå§‹åŒ–ç»„ä»¶
        self.vectorstore = PGVector(
            connection_string=config['pg_connection'],
            embedding_function=OpenAIEmbeddings()
        )

        self.llm = OpenAI(
            temperature=0.7,
            max_tokens=500,
            request_timeout=30
        )

        # è‡ªå®šä¹‰Prompt
        template = """
åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´"æˆ‘ä¸çŸ¥é“"ã€‚

ä¸Šä¸‹æ–‡:
{context}

é—®é¢˜: {question}

å›ç­”ï¼ˆç®€æ´ã€å‡†ç¡®ã€ä¸“ä¸šï¼‰:"""

        self.prompt = PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )

        # åˆ›å»ºQAé“¾
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_kwargs={"k": 5}
            ),
            chain_type_kwargs={"prompt": self.prompt},
            return_source_documents=True
        )

    def query(self, question: str, user_id: Optional[str] = None):
        """æŸ¥è¯¢ï¼ˆå¸¦ç›‘æ§ï¼‰"""

        import time
        start = time.time()

        # è®°å½•æŸ¥è¯¢
        self.log_query(user_id, question, start)

        try:
            with get_openai_callback() as cb:
                result = self.qa_chain({"query": question})

                duration = time.time() - start

                # è®°å½•ç»“æœ
                self.log_result(
                    user_id=user_id,
                    question=question,
                    answer=result['result'],
                    source_docs=result['source_documents'],
                    duration=duration,
                    tokens_used=cb.total_tokens,
                    cost=cb.total_cost
                )

                return {
                    'answer': result['result'],
                    'sources': [doc.metadata for doc in result['source_documents']],
                    'latency_ms': duration * 1000,
                    'tokens': cb.total_tokens,
                    'cost': cb.total_cost
                }

        except Exception as e:
            self.log_error(user_id, question, str(e))
            raise

    def log_query(self, user_id, question, timestamp):
        """è®°å½•æŸ¥è¯¢"""
        cursor.execute("""
            INSERT INTO query_logs (user_id, question, created_at)
            VALUES (%s, %s, %s)
            RETURNING id
        """, (user_id, question, timestamp))
        self.query_id = cursor.fetchone()[0]
        self.conn.commit()

    def log_result(self, **kwargs):
        """è®°å½•ç»“æœ"""
        cursor.execute("""
            UPDATE query_logs
            SET
                answer = %s,
                source_docs = %s,
                duration_ms = %s,
                tokens_used = %s,
                cost = %s,
                completed_at = now()
            WHERE id = %s
        """, (
            kwargs['answer'],
            json.dumps(kwargs['source_docs']),
            kwargs['duration'] * 1000,
            kwargs['tokens'],
            kwargs['cost'],
            self.query_id
        ))
        self.conn.commit()

# ä½¿ç”¨
rag = ProductionRAGSystem(config)
result = rag.query("PostgreSQL 18çš„å¼‚æ­¥I/Oå¦‚ä½•é…ç½®ï¼Ÿ", user_id="user_123")

print(f"ç­”æ¡ˆ: {result['answer']}")
print(f"å»¶è¿Ÿ: {result['latency_ms']:.2f}ms")
print(f"Token: {result['tokens']}")
print(f"æˆæœ¬: ${result['cost']:.4f}")
```

---

## 2. æµå¼è¾“å‡º

### 2.1 Streamå“åº”

```python
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.callbacks.base import BaseCallbackHandler

class CustomStreamingCallback(BaseCallbackHandler):
    """è‡ªå®šä¹‰æµå¼å›è°ƒ"""

    def __init__(self):
        self.tokens = []

    def on_llm_new_token(self, token: str, **kwargs):
        """æ”¶åˆ°æ–°tokenæ—¶è§¦å‘"""
        self.tokens.append(token)
        print(token, end='', flush=True)

    def on_llm_end(self, response, **kwargs):
        """LLMå®Œæˆæ—¶è§¦å‘"""
        print("\n[å®Œæˆ]")

# ä½¿ç”¨æµå¼è¾“å‡º
llm = OpenAI(
    streaming=True,
    callbacks=[CustomStreamingCallback()],
    temperature=0.7
)

chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever()
)

# æµå¼å“åº”ï¼ˆé€å­—è¾“å‡ºï¼‰
response = chain.run("ä»€ä¹ˆæ˜¯MVCCï¼Ÿ")

# ç”¨æˆ·ä½“éªŒï¼š
# æ— æµå¼ï¼šç­‰å¾…3ç§’ â†’ å®Œæ•´ç­”æ¡ˆ
# æœ‰æµå¼ï¼šç«‹å³å¼€å§‹è¾“å‡º â†’ é€å­—æ˜¾ç¤ºï¼ˆæ›´å¥½çš„UXï¼‰
```

---

## 3. ç¼“å­˜æœºåˆ¶

### 3.1 å¤šçº§ç¼“å­˜

```python
from langchain.cache import PostgresCache, InMemoryCache
from langchain.globals import set_llm_cache
import langchain

# L1: å†…å­˜ç¼“å­˜
in_memory_cache = InMemoryCache()

# L2: PostgreSQLç¼“å­˜
pg_cache = PostgresCache(
    connection_string="postgresql://localhost/cache_db"
)

# ç»„åˆç¼“å­˜
class TieredCache:
    """å¤šçº§ç¼“å­˜"""

    def __init__(self, l1_cache, l2_cache):
        self.l1 = l1_cache
        self.l2 = l2_cache

    def lookup(self, prompt, llm_string):
        """æŸ¥æ‰¾ç¼“å­˜"""
        # L1æŸ¥æ‰¾
        result = self.l1.lookup(prompt, llm_string)
        if result:
            return result

        # L2æŸ¥æ‰¾
        result = self.l2.lookup(prompt, llm_string)
        if result:
            # å†™å›L1
            self.l1.update(prompt, llm_string, result)
            return result

        return None

    def update(self, prompt, llm_string, result):
        """æ›´æ–°ç¼“å­˜"""
        self.l1.update(prompt, llm_string, result)
        self.l2.update(prompt, llm_string, result)

# ä½¿ç”¨ç¼“å­˜
set_llm_cache(TieredCache(in_memory_cache, pg_cache))

llm = OpenAI()

# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆæœªç¼“å­˜ï¼‰
response1 = llm("ä»€ä¹ˆæ˜¯PostgreSQLï¼Ÿ")  # 2000ms

# ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆL1ç¼“å­˜ï¼‰
response2 = llm("ä»€ä¹ˆæ˜¯PostgreSQLï¼Ÿ")  # 0.5ms (-99.975%)

# æ€§èƒ½æå‡ï¼š
# L1å‘½ä¸­ç‡: 30%ï¼ˆçƒ­ç‚¹æŸ¥è¯¢ï¼‰
# L2å‘½ä¸­ç‡: 50%
# å¹³å‡åŠ é€Ÿ: 70x
```

---

## 4. é”™è¯¯å¤„ç†ä¸é‡è¯•

### 4.1 è‡ªåŠ¨é‡è¯•æœºåˆ¶

```python
from langchain.llms import OpenAI
from tenacity import retry, stop_after_attempt, wait_exponential

class RobustRAG:
    """å¥å£®çš„RAGç³»ç»Ÿ"""

    def __init__(self):
        self.llm = OpenAI(max_retries=3)
        self.vectorstore = PGVector(...)

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10)
    )
    def query_with_retry(self, question):
        """å¸¦é‡è¯•çš„æŸ¥è¯¢"""
        try:
            # æ£€ç´¢
            docs = self.vectorstore.similarity_search(question, k=5)

            if not docs:
                raise ValueError("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")

            # LLMç”Ÿæˆ
            context = "\n\n".join([doc.page_content for doc in docs])
            prompt = f"åŸºäºä¸Šä¸‹æ–‡å›ç­”ï¼š{context}\n\né—®é¢˜ï¼š{question}"

            answer = self.llm(prompt)

            if not answer or len(answer.strip()) < 10:
                raise ValueError("ç­”æ¡ˆè´¨é‡ä¸ä½³")

            return answer

        except Exception as e:
            print(f"æŸ¥è¯¢å¤±è´¥ï¼Œé‡è¯•ä¸­... {e}")
            raise

    def query_with_fallback(self, question):
        """å¸¦é™çº§çš„æŸ¥è¯¢"""
        try:
            # å°è¯•å®Œæ•´RAG
            return self.query_with_retry(question)

        except Exception as e:
            print(f"å®Œæ•´RAGå¤±è´¥ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ: {e}")

            # é™çº§1: åªç”¨LLMï¼ˆæ— RAGï¼‰
            try:
                return self.llm(question)
            except:
                pass

            # é™çº§2: è¿”å›é¢„è®¾ç­”æ¡ˆ
            return "æŠ±æ­‰ï¼Œå½“å‰ç³»ç»Ÿç¹å¿™ï¼Œè¯·ç¨åå†è¯•ã€‚"

# ä½¿ç”¨
rag = RobustRAG()
answer = rag.query_with_fallback("ä»€ä¹ˆæ˜¯å¼‚æ­¥I/Oï¼Ÿ")

# ä¿è¯å¯ç”¨æ€§ï¼š
# - è‡ªåŠ¨é‡è¯•ï¼ˆç½‘ç»œæŠ–åŠ¨ï¼‰
# - å¤šçº§é™çº§ï¼ˆç¡®ä¿å“åº”ï¼‰
# - 99.9%å¯ç”¨æ€§
```

---

## 5. æ€§èƒ½ç›‘æ§

### 5.1 è¯¦ç»†æŒ‡æ ‡æ”¶é›†

```python
from langchain.callbacks import BaseCallbackHandler
import time

class MetricsCallback(BaseCallbackHandler):
    """æŒ‡æ ‡æ”¶é›†å›è°ƒ"""

    def __init__(self):
        self.metrics = {
            'retrieval_time': 0,
            'llm_time': 0,
            'total_tokens': 0,
            'retrieval_count': 0
        }
        self.start_time = None

    def on_retriever_start(self, query, **kwargs):
        """æ£€ç´¢å¼€å§‹"""
        self.retrieval_start = time.time()

    def on_retriever_end(self, documents, **kwargs):
        """æ£€ç´¢ç»“æŸ"""
        duration = time.time() - self.retrieval_start
        self.metrics['retrieval_time'] += duration
        self.metrics['retrieval_count'] = len(documents)

    def on_llm_start(self, serialized, prompts, **kwargs):
        """LLMå¼€å§‹"""
        self.llm_start = time.time()

    def on_llm_end(self, response, **kwargs):
        """LLMç»“æŸ"""
        duration = time.time() - self.llm_start
        self.metrics['llm_time'] += duration
        self.metrics['total_tokens'] = response.llm_output.get('token_usage', {}).get('total_tokens', 0)

    def get_metrics(self):
        """è·å–æŒ‡æ ‡"""
        return self.metrics

# ä½¿ç”¨ç›‘æ§
metrics_callback = MetricsCallback()

chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    callbacks=[metrics_callback]
)

response = chain.run("PostgreSQLæ€§èƒ½ä¼˜åŒ–æŠ€å·§ï¼Ÿ")

metrics = metrics_callback.get_metrics()
print(f"æ£€ç´¢æ—¶é—´: {metrics['retrieval_time']*1000:.2f}ms")
print(f"LLMæ—¶é—´: {metrics['llm_time']*1000:.2f}ms")
print(f"æ£€ç´¢æ–‡æ¡£æ•°: {metrics['retrieval_count']}")
print(f"Tokenä½¿ç”¨: {metrics['total_tokens']}")

# æ€§èƒ½åˆ†è§£ï¼š
# æ£€ç´¢æ—¶é—´: 25ms (12%)
# LLMæ—¶é—´: 180ms (88%)
# ä¼˜åŒ–æ–¹å‘ï¼šä½¿ç”¨æ›´å¿«çš„LLMæˆ–æœ¬åœ°æ¨¡å‹
```

---

## 6. A/Bæµ‹è¯•æ¡†æ¶

### 6.1 å¤šæ¨¡å‹å¯¹æ¯”

```python
import random

class ABTestRAG:
    """A/Bæµ‹è¯•RAGç³»ç»Ÿ"""

    def __init__(self):
        # æ¨¡å‹A: GPT-3.5ï¼ˆå¿«ï¼‰
        self.model_a = RetrievalQA.from_chain_type(
            llm=OpenAI(model_name="gpt-3.5-turbo"),
            retriever=vectorstore.as_retriever()
        )

        # æ¨¡å‹B: GPT-4ï¼ˆå‡†ï¼‰
        self.model_b = RetrievalQA.from_chain_type(
            llm=OpenAI(model_name="gpt-4"),
            retriever=vectorstore.as_retriever()
        )

        self.traffic_split = 0.8  # 80%æµé‡ç»™æ¨¡å‹A

    def query(self, question, user_id):
        """æŸ¥è¯¢ï¼ˆè‡ªåŠ¨åˆ†æµï¼‰"""

        # æ ¹æ®æµé‡æ¯”ä¾‹é€‰æ‹©æ¨¡å‹
        if random.random() < self.traffic_split:
            model_name = "gpt-3.5-turbo"
            result = self.model_a({"query": question})
        else:
            model_name = "gpt-4"
            result = self.model_b({"query": question})

        # è®°å½•åˆ°æ•°æ®åº“
        cursor.execute("""
            INSERT INTO ab_test_logs (user_id, question, model_name, answer, created_at)
            VALUES (%s, %s, %s, %s, now())
        """, (user_id, question, model_name, result['result']))

        return result['result']

    def analyze_results(self):
        """åˆ†æA/Bæµ‹è¯•ç»“æœ"""
        cursor.execute("""
            SELECT
                model_name,
                COUNT(*) AS queries,
                AVG(duration_ms) AS avg_latency,
                AVG(user_rating) AS avg_rating,
                AVG(tokens_used) AS avg_tokens,
                AVG(cost) AS avg_cost
            FROM ab_test_logs
            WHERE created_at >= now() - INTERVAL '7 days'
            GROUP BY model_name
        """)

        results = cursor.fetchall()

        for row in results:
            print(f"{row['model_name']}:")
            print(f"  æŸ¥è¯¢æ•°: {row['queries']}")
            print(f"  å¹³å‡å»¶è¿Ÿ: {row['avg_latency']:.2f}ms")
            print(f"  å¹³å‡è¯„åˆ†: {row['avg_rating']:.2f}/5")
            print(f"  å¹³å‡Token: {row['avg_tokens']:.0f}")
            print(f"  å¹³å‡æˆæœ¬: ${row['avg_cost']:.4f}")

"""
gpt-3.5-turbo:
  æŸ¥è¯¢æ•°: 8000
  å¹³å‡å»¶è¿Ÿ: 850ms
  å¹³å‡è¯„åˆ†: 4.2/5
  å¹³å‡Token: 350
  å¹³å‡æˆæœ¬: $0.0012

gpt-4:
  æŸ¥è¯¢æ•°: 2000
  å¹³å‡å»¶è¿Ÿ: 2500ms
  å¹³å‡è¯„åˆ†: 4.6/5
  å¹³å‡Token: 420
  å¹³å‡æˆæœ¬: $0.0156

ç»“è®º:
- GPT-4å‡†ç¡®ç‡é«˜10%ä½†æˆæœ¬é«˜13x
- æ ¹æ®ROIå†³ç­–æµé‡åˆ†é…
"""
```

---

**å®Œæˆ**: LangChainé«˜çº§ç‰¹æ€§å®æˆ˜
**å­—æ•°**: ~15,000å­—
**æ¶µç›–**: Memoryç®¡ç†ã€æ··åˆRAGã€Self-Queryã€Parent Retrieverã€Agentå¼€å‘ã€ç”Ÿäº§RAGã€æµå¼è¾“å‡ºã€ç¼“å­˜ã€é”™è¯¯å¤„ç†ã€ç›‘æ§ã€A/Bæµ‹è¯•
