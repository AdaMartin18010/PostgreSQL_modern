---
> **ğŸ“‹ æ–‡æ¡£æ¥æº**: æ–°å¢æ·±åŒ–æ–‡æ¡£
> **ğŸ“… åˆ›å»ºæ—¥æœŸ**: 2025-01
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºæ·±åº¦è¡¥å……ï¼Œæ·±åŒ–RAGæŠ€æœ¯æ ˆ

---

# RAGé«˜çº§æŠ€æœ¯å®Œæ•´æŒ‡å—

## å…ƒæ•°æ®

- **æ–‡æ¡£ç‰ˆæœ¬**: v1.0
- **åˆ›å»ºæ—¥æœŸ**: 2025-01
- **æŠ€æœ¯æ ˆ**: PostgreSQL 16+ | pgvector 0.7+ | LangChain 0.2+ | OpenAI/Anthropic API | Cross-Encoders
- **éš¾åº¦çº§åˆ«**: â­â­â­â­â­ (ä¸“å®¶çº§)
- **é¢„è®¡é˜…è¯»**: 150åˆ†é’Ÿ
- **å‰ç½®è¦æ±‚**: ç†Ÿæ‚‰åŸºç¡€RAGå’ŒKGå¢å¼ºRAGæ¶æ„

---

## ğŸ“‹ å®Œæ•´ç›®å½•

- [RAGé«˜çº§æŠ€æœ¯å®Œæ•´æŒ‡å—](#ragé«˜çº§æŠ€æœ¯å®Œæ•´æŒ‡å—)
  - [å…ƒæ•°æ®](#å…ƒæ•°æ®)
  - [ğŸ“‹ å®Œæ•´ç›®å½•](#-å®Œæ•´ç›®å½•)
  - [1. æŸ¥è¯¢é‡å†™ä¸æ‰©å±•](#1-æŸ¥è¯¢é‡å†™ä¸æ‰©å±•)
    - [1.1 æŸ¥è¯¢é‡å†™ç­–ç•¥](#11-æŸ¥è¯¢é‡å†™ç­–ç•¥)
      - [é—®é¢˜è¯Šæ–­å¼é‡å†™](#é—®é¢˜è¯Šæ–­å¼é‡å†™)
      - [å¤šè§†è§’é‡å†™](#å¤šè§†è§’é‡å†™)
    - [1.2 æŸ¥è¯¢æ‰©å±•æŠ€æœ¯](#12-æŸ¥è¯¢æ‰©å±•æŠ€æœ¯)
      - [åŸºäºçŸ¥è¯†å›¾è°±çš„æŸ¥è¯¢æ‰©å±•](#åŸºäºçŸ¥è¯†å›¾è°±çš„æŸ¥è¯¢æ‰©å±•)
      - [åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„æŸ¥è¯¢æ‰©å±•](#åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„æŸ¥è¯¢æ‰©å±•)
    - [1.3 LLMé©±åŠ¨çš„æŸ¥è¯¢ä¼˜åŒ–](#13-llmé©±åŠ¨çš„æŸ¥è¯¢ä¼˜åŒ–)
  - [2. å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ](#2-å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ)
    - [2.1 ç²—æ’-ç²¾æ’æ¶æ„](#21-ç²—æ’-ç²¾æ’æ¶æ„)
    - [2.2 å¬å›ç­–ç•¥](#22-å¬å›ç­–ç•¥)
      - [æ··åˆå¬å›](#æ··åˆå¬å›)
    - [2.3 ç²¾æ’æ¨¡å‹](#23-ç²¾æ’æ¨¡å‹)
      - [Cross-Encoderç²¾æ’](#cross-encoderç²¾æ’)
  - [3. é‡æ’åºæŠ€æœ¯(Re-ranking)](#3-é‡æ’åºæŠ€æœ¯re-ranking)
    - [3.1 Cross-Encoderé‡æ’åº](#31-cross-encoderé‡æ’åº)
      - [é«˜çº§Cross-Encoderé…ç½®](#é«˜çº§cross-encoderé…ç½®)
    - [3.2 æ··åˆé‡æ’åºç­–ç•¥](#32-æ··åˆé‡æ’åºç­–ç•¥)
    - [3.3 ä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡æ’åº](#33-ä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡æ’åº)
  - [4. Self-RAGæ¶æ„](#4-self-ragæ¶æ„)
    - [4.1 Self-RAGåŸç†](#41-self-ragåŸç†)
      - [Self-RAGæ ¸å¿ƒç»„ä»¶](#self-ragæ ¸å¿ƒç»„ä»¶)
  - [5. Agentic RAG](#5-agentic-rag)
    - [5.1 å¤šAgentåä½œ](#51-å¤šagentåä½œ)
      - [å¤šAgentæ¶æ„](#å¤šagentæ¶æ„)
      - [ä¸“é—¨åŒ–Agentå®ç°](#ä¸“é—¨åŒ–agentå®ç°)
    - [5.2 å·¥å…·è°ƒç”¨ä¸è§„åˆ’](#52-å·¥å…·è°ƒç”¨ä¸è§„åˆ’)
      - [å·¥å…·ç³»ç»Ÿ](#å·¥å…·ç³»ç»Ÿ)
    - [5.3 è¿­ä»£ä¼˜åŒ–](#53-è¿­ä»£ä¼˜åŒ–)
      - [è¿­ä»£ä¼˜åŒ–æ¡†æ¶](#è¿­ä»£ä¼˜åŒ–æ¡†æ¶)
  - [7. RAGè¯„ä¼°ä½“ç³»](#7-ragè¯„ä¼°ä½“ç³»)
    - [7.1 è¯„ä¼°æŒ‡æ ‡](#71-è¯„ä¼°æŒ‡æ ‡)
      - [æ£€ç´¢è´¨é‡æŒ‡æ ‡](#æ£€ç´¢è´¨é‡æŒ‡æ ‡)
      - [ç”Ÿæˆè´¨é‡æŒ‡æ ‡](#ç”Ÿæˆè´¨é‡æŒ‡æ ‡)
    - [7.2 åŸºå‡†æµ‹è¯•](#72-åŸºå‡†æµ‹è¯•)
      - [MTEBåŸºå‡†æµ‹è¯•](#mtebåŸºå‡†æµ‹è¯•)
      - [è‡ªå®šä¹‰è¯„ä¼°æ¡†æ¶](#è‡ªå®šä¹‰è¯„ä¼°æ¡†æ¶)
    - [7.3 æŒç»­ä¼˜åŒ–](#73-æŒç»­ä¼˜åŒ–)
      - [A/Bæµ‹è¯•æ¡†æ¶](#abæµ‹è¯•æ¡†æ¶)
      - [åé¦ˆå¾ªç¯ä¼˜åŒ–](#åé¦ˆå¾ªç¯ä¼˜åŒ–)
  - [6. å¤šæ¨¡æ€RAG](#6-å¤šæ¨¡æ€rag)
    - [6.1 å›¾æ–‡æ··åˆæ£€ç´¢](#61-å›¾æ–‡æ··åˆæ£€ç´¢)
      - [å¤šæ¨¡æ€æ£€ç´¢æ¶æ„](#å¤šæ¨¡æ€æ£€ç´¢æ¶æ„)
    - [6.2 å¤šæ¨¡æ€å‘é‡è¡¨ç¤º](#62-å¤šæ¨¡æ€å‘é‡è¡¨ç¤º)
      - [CLIPæ¨¡å‹é›†æˆ](#clipæ¨¡å‹é›†æˆ)
      - [å¤šæ¨¡æ€å‘é‡å­˜å‚¨](#å¤šæ¨¡æ€å‘é‡å­˜å‚¨)
    - [6.3 è·¨æ¨¡æ€å¯¹é½](#63-è·¨æ¨¡æ€å¯¹é½)
      - [è·¨æ¨¡æ€æ£€ç´¢ä¼˜åŒ–](#è·¨æ¨¡æ€æ£€ç´¢ä¼˜åŒ–)
      - [å¤šæ¨¡æ€èåˆæ£€ç´¢](#å¤šæ¨¡æ€èåˆæ£€ç´¢)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)
  - [ğŸ“ æ›´æ–°æ—¥å¿—](#-æ›´æ–°æ—¥å¿—)

---

## 1. æŸ¥è¯¢é‡å†™ä¸æ‰©å±•

### 1.1 æŸ¥è¯¢é‡å†™ç­–ç•¥

æŸ¥è¯¢é‡å†™æ˜¯æå‡RAGç³»ç»Ÿæ£€ç´¢è´¨é‡çš„å…³é”®æŠ€æœ¯ï¼Œé€šè¿‡æ”¹å†™åŸå§‹æŸ¥è¯¢æ¥æé«˜æ£€ç´¢çš„å‡†ç¡®æ€§å’Œå¬å›ç‡ã€‚

#### é—®é¢˜è¯Šæ–­å¼é‡å†™

```python
class QueryRewriter:
    """æŸ¥è¯¢é‡å†™å™¨"""

    def __init__(self, llm):
        self.llm = llm

    def diagnose_and_rewrite(self, query: str) -> Dict[str, Any]:
        """
        è¯Šæ–­æŸ¥è¯¢é—®é¢˜å¹¶é‡å†™

        Returns:
            {
                'original': åŸå§‹æŸ¥è¯¢,
                'diagnosis': é—®é¢˜è¯Šæ–­,
                'rewritten_queries': é‡å†™åçš„æŸ¥è¯¢åˆ—è¡¨,
                'strategy': ä½¿ç”¨çš„ç­–ç•¥
            }
        """

        diagnosis_prompt = f"""
        åˆ†æä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢ï¼Œè¯†åˆ«æ½œåœ¨é—®é¢˜å¹¶é‡å†™ï¼š

        åŸå§‹æŸ¥è¯¢: {query}

        è¯·åˆ†æï¼š
        1. æŸ¥è¯¢æ˜¯å¦è¿‡äºç®€çŸ­æˆ–æ¨¡ç³Šï¼Ÿ
        2. æ˜¯å¦åŒ…å«ä¸“ä¸šæœ¯è¯­éœ€è¦å±•å¼€ï¼Ÿ
        3. æ˜¯å¦éšå«å¤šä¸ªå­é—®é¢˜ï¼Ÿ
        4. æ˜¯å¦éœ€è¦æ·»åŠ é¢†åŸŸä¸Šä¸‹æ–‡ï¼Ÿ

        è¯·æä¾›3-5ä¸ªé‡å†™ç‰ˆæœ¬ï¼Œæ¯ä¸ªç‰ˆæœ¬é’ˆå¯¹ä¸åŒçš„æ£€ç´¢ç­–ç•¥ã€‚
        """

        response = self.llm.chat.completions.create(
            model="gpt-4-turbo",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ã€‚"},
                {"role": "user", "content": diagnosis_prompt}
            ],
            temperature=0.7
        )

        # è§£æé‡å†™ç»“æœ
        rewritten = self._parse_rewritten_queries(response.choices[0].message.content)

        return {
            'original': query,
            'rewritten_queries': rewritten,
            'strategy': 'diagnosis'
        }

    def _parse_rewritten_queries(self, content: str) -> List[str]:
        """è§£æLLMè¿”å›çš„é‡å†™æŸ¥è¯¢"""
        import re
        # æå–ç¼–å·çš„æŸ¥è¯¢
        queries = re.findall(r'\d+[\.\)]\s*(.+)', content)
        return queries[:5]  # è¿”å›å‰5ä¸ª
```

#### å¤šè§†è§’é‡å†™

```python
class MultiPerspectiveRewriter:
    """å¤šè§†è§’æŸ¥è¯¢é‡å†™"""

    def __init__(self, llm):
        self.llm = llm
        self.perspectives = [
            'general',      # é€šç”¨è§†è§’
            'technical',    # æŠ€æœ¯è§†è§’
            'practical',    # å®ç”¨è§†è§’
            'comparative',  # å¯¹æ¯”è§†è§’
            'troubleshooting'  # é—®é¢˜æ’æŸ¥è§†è§’
        ]

    def rewrite_from_perspectives(self, query: str) -> Dict[str, str]:
        """
        ä»ä¸åŒè§†è§’é‡å†™æŸ¥è¯¢

        Returns:
            {perspective: rewritten_query}
        """
        rewritten = {}

        for perspective in self.perspectives:
            prompt = f"""
            ä»{perspective}è§†è§’é‡å†™ä»¥ä¸‹æŸ¥è¯¢ï¼Œä½¿å…¶æ›´é€‚åˆæ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼š

            åŸå§‹æŸ¥è¯¢: {query}

            è§†è§’è¯´æ˜:
            - general: é€šç”¨ã€å®½æ³›çš„è¡¨è¾¾
            - technical: å¼ºè°ƒæŠ€æœ¯ç»†èŠ‚å’Œæœ¯è¯­
            - practical: å¼ºè°ƒå®é™…åº”ç”¨å’Œæ“ä½œ
            - comparative: å¼ºè°ƒå¯¹æ¯”å’Œå·®å¼‚
            - troubleshooting: å¼ºè°ƒé—®é¢˜è§£å†³å’Œæ•…éšœæ’æŸ¥

            é‡å†™æŸ¥è¯¢:
            """

            response = self.llm.chat.completions.create(
                model="gpt-4-turbo",
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.8
            )

            rewritten[perspective] = response.choices[0].message.content.strip()

        return rewritten
```

### 1.2 æŸ¥è¯¢æ‰©å±•æŠ€æœ¯

æŸ¥è¯¢æ‰©å±•é€šè¿‡æ·»åŠ ç›¸å…³æœ¯è¯­ã€åŒä¹‰è¯å’Œä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æå‡å¬å›ç‡ã€‚

#### åŸºäºçŸ¥è¯†å›¾è°±çš„æŸ¥è¯¢æ‰©å±•

```python
class KGQueryExpander:
    """åŸºäºçŸ¥è¯†å›¾è°±çš„æŸ¥è¯¢æ‰©å±•"""

    def __init__(self, graph_conn, graph_name: str):
        self.graph_conn = graph_conn
        self.graph_name = graph_name
        self.cursor = graph_conn.cursor()

    def expand_with_related_entities(self, query: str, entities: List[str]) -> List[str]:
        """
        ä½¿ç”¨çŸ¥è¯†å›¾è°±æ‰©å±•æŸ¥è¯¢ï¼Œæ·»åŠ ç›¸å…³å®ä½“

        Args:
            query: åŸå§‹æŸ¥è¯¢
            entities: è¯†åˆ«å‡ºçš„å®ä½“åˆ—è¡¨

        Returns:
            æ‰©å±•åçš„æŸ¥è¯¢åˆ—è¡¨
        """
        expanded_queries = [query]  # ä¿ç•™åŸå§‹æŸ¥è¯¢

        for entity in entities:
            # æŸ¥æ‰¾å®ä½“çš„ç›¸å…³å®ä½“ï¼ˆ1-2è·³ï¼‰
            related = self._get_related_entities(entity, max_hops=2)

            # ç”Ÿæˆæ‰©å±•æŸ¥è¯¢
            for related_entity in related[:3]:  # æ¯ä¸ªå®ä½“å–å‰3ä¸ªç›¸å…³å®ä½“
                expanded = f"{query} {related_entity['label']}"
                expanded_queries.append(expanded)

        return expanded_queries

    def _get_related_entities(self, entity: str, max_hops: int = 2) -> List[Dict]:
        """è·å–å®ä½“çš„ç›¸å…³å®ä½“"""
        self.cursor.execute(f"""
            SELECT * FROM cypher('{self.graph_name}', $$
                MATCH (e:Entity {{name: '{entity}'}})
                MATCH path = (e)-[*1..{max_hops}]-(related:Entity)
                RETURN DISTINCT related.name as name, related.label as label
                ORDER BY length(path)
                LIMIT 10
            $$) AS (name text, label text);
        """)

        return [{'name': row[0], 'label': row[1]} for row in self.cursor.fetchall()]
```

#### åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„æŸ¥è¯¢æ‰©å±•

```python
class VectorQueryExpander:
    """åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„æŸ¥è¯¢æ‰©å±•"""

    def __init__(self, embedding_model, vector_db):
        self.embedding_model = embedding_model
        self.vector_db = vector_db

    def expand_with_similar_terms(self, query: str, top_k: int = 5) -> List[str]:
        """
        ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æŸ¥æ‰¾ç›¸ä¼¼æœ¯è¯­æ‰©å±•æŸ¥è¯¢

        Args:
            query: åŸå§‹æŸ¥è¯¢
            top_k: è¿”å›çš„ç›¸ä¼¼æœ¯è¯­æ•°

        Returns:
            æ‰©å±•åçš„æŸ¥è¯¢åˆ—è¡¨
        """
        # æŸ¥è¯¢å‘é‡
        query_emb = self.embedding_model.encode(query)

        # åœ¨æœ¯è¯­åº“ä¸­æŸ¥æ‰¾ç›¸ä¼¼æœ¯è¯­
        similar_terms = self.vector_db.similarity_search_by_vector(
            query_emb,
            k=top_k
        )

        expanded_queries = [query]
        for term_doc in similar_terms:
            expanded = f"{query} {term_doc.page_content}"
            expanded_queries.append(expanded)

        return expanded_queries
```

### 1.3 LLMé©±åŠ¨çš„æŸ¥è¯¢ä¼˜åŒ–

ä½¿ç”¨LLMçš„æ¨ç†èƒ½åŠ›è¿›è¡Œæ™ºèƒ½æŸ¥è¯¢ä¼˜åŒ–ã€‚

```python
class LLMQueryOptimizer:
    """LLMé©±åŠ¨çš„æŸ¥è¯¢ä¼˜åŒ–å™¨"""

    def __init__(self, llm, few_shot_examples: List[Dict] = None):
        self.llm = llm
        self.few_shot_examples = few_shot_examples or []

    def optimize_query(self, query: str, context: Dict = None) -> Dict[str, Any]:
        """
        ä¼˜åŒ–æŸ¥è¯¢ï¼Œç”Ÿæˆå¤šä¸ªä¼˜åŒ–ç‰ˆæœ¬

        Args:
            query: åŸå§‹æŸ¥è¯¢
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚é¢†åŸŸã€å†å²æŸ¥è¯¢ç­‰ï¼‰

        Returns:
            {
                'original': åŸå§‹æŸ¥è¯¢,
                'optimized': ä¼˜åŒ–åçš„ä¸»æŸ¥è¯¢,
                'variants': æŸ¥è¯¢å˜ä½“åˆ—è¡¨,
                'keywords': æå–çš„å…³é”®è¯,
                'intent': æ„å›¾è¯†åˆ«
            }
        """

        # æ„å»ºFew-Shotç¤ºä¾‹
        examples_text = "\n\n".join([
            f"åŸå§‹: {ex['original']}\nä¼˜åŒ–: {ex['optimized']}\næ„å›¾: {ex['intent']}"
            for ex in self.few_shot_examples[:3]
        ])

        context_info = ""
        if context:
            context_info = f"\nä¸Šä¸‹æ–‡: {context}"

        prompt = f"""
        ä¼˜åŒ–ä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢ï¼Œä½¿å…¶æ›´é€‚åˆæ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼š

        {examples_text}

        åŸå§‹æŸ¥è¯¢: {query}{context_info}

        è¯·ï¼š
        1. è¯†åˆ«æŸ¥è¯¢æ„å›¾
        2. æå–å…³é”®ä¿¡æ¯
        3. ç”Ÿæˆä¼˜åŒ–åçš„æŸ¥è¯¢
        4. æä¾›2-3ä¸ªæŸ¥è¯¢å˜ä½“

        æ ¼å¼ï¼š
        æ„å›¾: <æ„å›¾>
        å…³é”®è¯: <å…³é”®è¯åˆ—è¡¨>
        ä¼˜åŒ–æŸ¥è¯¢: <ä¼˜åŒ–åçš„æŸ¥è¯¢>
        å˜ä½“1: <å˜ä½“1>
        å˜ä½“2: <å˜ä½“2>
        """

        response = self.llm.chat.completions.create(
            model="gpt-4-turbo",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ï¼Œæ“…é•¿å°†ç”¨æˆ·æŸ¥è¯¢æ”¹å†™ä¸ºæœ€é€‚åˆæ£€ç´¢çš„å½¢å¼ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7
        )

        # è§£æç»“æœ
        result = self._parse_optimization_result(response.choices[0].message.content)
        result['original'] = query

        return result

    def _parse_optimization_result(self, content: str) -> Dict[str, Any]:
        """è§£æä¼˜åŒ–ç»“æœ"""
        import re

        intent_match = re.search(r'æ„å›¾:\s*(.+)', content)
        keywords_match = re.search(r'å…³é”®è¯:\s*(.+)', content)
        optimized_match = re.search(r'ä¼˜åŒ–æŸ¥è¯¢:\s*(.+)', content)
        variants = re.findall(r'å˜ä½“\d+:\s*(.+)', content)

        return {
            'intent': intent_match.group(1).strip() if intent_match else None,
            'keywords': keywords_match.group(1).strip().split(',') if keywords_match else [],
            'optimized': optimized_match.group(1).strip() if optimized_match else None,
            'variants': [v.strip() for v in variants]
        }
```

---

## 2. å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ

### 2.1 ç²—æ’-ç²¾æ’æ¶æ„

å¤šé˜¶æ®µæ£€ç´¢é€šè¿‡ç²—æ’ï¼ˆå¬å›å¤§é‡å€™é€‰ï¼‰å’Œç²¾æ’ï¼ˆç²¾ç¡®æ’åºï¼‰ä¸¤é˜¶æ®µæå‡æ£€ç´¢è´¨é‡ã€‚

```python
class TwoStageRetriever:
    """ä¸¤é˜¶æ®µæ£€ç´¢å™¨"""

    def __init__(
        self,
        vector_store,      # å‘é‡æ•°æ®åº“ï¼ˆç²—æ’ï¼‰
        reranker=None,     # é‡æ’åºæ¨¡å‹ï¼ˆç²¾æ’ï¼‰
        top_k_coarse=100,  # ç²—æ’å¬å›æ•°
        top_k_final=10     # æœ€ç»ˆè¿”å›æ•°
    ):
        self.vector_store = vector_store
        self.reranker = reranker
        self.top_k_coarse = top_k_coarse
        self.top_k_final = top_k_final

    def retrieve(self, query: str, filters: Dict = None) -> List[Dict]:
        """
        ä¸¤é˜¶æ®µæ£€ç´¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            filters: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶

        Returns:
            æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        """
        # é˜¶æ®µ1: ç²—æ’ - å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢
        coarse_results = self._coarse_retrieve(query, filters)

        if not self.reranker:
            # æ— é‡æ’åºæ¨¡å‹ï¼Œç›´æ¥è¿”å›ç²—æ’ç»“æœ
            return coarse_results[:self.top_k_final]

        # é˜¶æ®µ2: ç²¾æ’ - é‡æ’åº
        reranked_results = self._fine_rerank(query, coarse_results)

        return reranked_results[:self.top_k_final]

    def _coarse_retrieve(self, query: str, filters: Dict = None) -> List[Dict]:
        """ç²—æ’ï¼šå‘é‡ç›¸ä¼¼åº¦æ£€ç´¢"""
        # å‘é‡æ£€ç´¢
        docs = self.vector_store.similarity_search_with_score(
            query,
            k=self.top_k_coarse,
            filter=filters
        )

        results = []
        for doc, score in docs:
            results.append({
                'content': doc.page_content,
                'metadata': doc.metadata,
                'score': float(score),
                'source': 'vector'
            })

        return results

    def _fine_rerank(self, query: str, candidates: List[Dict]) -> List[Dict]:
        """ç²¾æ’ï¼šä½¿ç”¨é‡æ’åºæ¨¡å‹"""
        if not self.reranker:
            return candidates

        # å‡†å¤‡é‡æ’åºè¾“å…¥
        pairs = [(query, item['content']) for item in candidates]

        # é‡æ’åºå¾—åˆ†
        rerank_scores = self.reranker.predict(pairs)

        # åˆå¹¶åˆ†æ•°å¹¶æ’åº
        for i, item in enumerate(candidates):
            # ç»¼åˆåˆ†æ•°ï¼ˆå¯åŠ æƒå¹³å‡ï¼‰
            item['rerank_score'] = float(rerank_scores[i])
            item['final_score'] = 0.3 * item['score'] + 0.7 * item['rerank_score']

        # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
        return sorted(candidates, key=lambda x: x['final_score'], reverse=True)
```

### 2.2 å¬å›ç­–ç•¥

#### æ··åˆå¬å›

```python
class HybridRecallStrategy:
    """æ··åˆå¬å›ç­–ç•¥"""

    def __init__(self, vector_store, keyword_search=None, graph_search=None):
        self.vector_store = vector_store      # å‘é‡æ£€ç´¢
        self.keyword_search = keyword_search  # å…³é”®è¯æ£€ç´¢ï¼ˆå¦‚BM25ï¼‰
        self.graph_search = graph_search      # å›¾æ£€ç´¢

    def hybrid_recall(
        self,
        query: str,
        vector_weight=0.6,
        keyword_weight=0.3,
        graph_weight=0.1
    ) -> List[Dict]:
        """
        æ··åˆå¬å›

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            vector_weight: å‘é‡æ£€ç´¢æƒé‡
            keyword_weight: å…³é”®è¯æ£€ç´¢æƒé‡
            graph_weight: å›¾æ£€ç´¢æƒé‡
        """
        all_results = {}

        # 1. å‘é‡æ£€ç´¢
        if self.vector_store:
            vector_results = self._vector_recall(query, top_k=50)
            for item in vector_results:
                doc_id = item['id']
                if doc_id not in all_results:
                    all_results[doc_id] = item
                    all_results[doc_id]['scores'] = {}
                all_results[doc_id]['scores']['vector'] = item['score'] * vector_weight

        # 2. å…³é”®è¯æ£€ç´¢
        if self.keyword_search:
            keyword_results = self._keyword_recall(query, top_k=50)
            for item in keyword_results:
                doc_id = item['id']
                if doc_id not in all_results:
                    all_results[doc_id] = item
                    all_results[doc_id]['scores'] = {}
                all_results[doc_id]['scores']['keyword'] = item['score'] * keyword_weight

        # 3. å›¾æ£€ç´¢
        if self.graph_search:
            graph_results = self._graph_recall(query, top_k=30)
            for item in graph_results:
                doc_id = item['id']
                if doc_id not in all_results:
                    all_results[doc_id] = item
                    all_results[doc_id]['scores'] = {}
                all_results[doc_id]['scores']['graph'] = item['score'] * graph_weight

        # èåˆåˆ†æ•°
        for doc_id, item in all_results.items():
            item['final_score'] = sum(item['scores'].values())

        # æ’åºå¹¶è¿”å›
        sorted_results = sorted(
            all_results.values(),
            key=lambda x: x['final_score'],
            reverse=True
        )

        return sorted_results[:100]  # è¿”å›Top 100ç”¨äºç²¾æ’

    def _vector_recall(self, query: str, top_k: int) -> List[Dict]:
        """å‘é‡æ£€ç´¢å¬å›"""
        docs = self.vector_store.similarity_search_with_score(query, k=top_k)
        return [
            {
                'id': f"vec_{i}",
                'content': doc.page_content,
                'metadata': doc.metadata,
                'score': float(score)
            }
            for i, (doc, score) in enumerate(docs)
        ]

    def _keyword_recall(self, query: str, top_k: int) -> List[Dict]:
        """å…³é”®è¯æ£€ç´¢å¬å›ï¼ˆBM25ç­‰ï¼‰"""
        # è¿™é‡Œä½¿ç”¨PostgreSQLå…¨æ–‡æœç´¢ä½œä¸ºç¤ºä¾‹
        # å®é™…å¯ä»¥ä½¿ç”¨Elasticsearchã€OpenSearchç­‰
        if not self.keyword_search:
            return []

        results = self.keyword_search.search(query, top_k=top_k)
        return [
            {
                'id': f"kw_{i}",
                'content': result['content'],
                'metadata': result['metadata'],
                'score': float(result['score'])
            }
            for i, result in enumerate(results)
        ]

    def _graph_recall(self, query: str, top_k: int) -> List[Dict]:
        """å›¾æ£€ç´¢å¬å›"""
        if not self.graph_search:
            return []

        # å®ä½“è¯†åˆ«
        entities = self.graph_search.extract_entities(query)

        # å­å›¾æ£€ç´¢
        subgraph_results = self.graph_search.retrieve_subgraph(entities, top_k=top_k)

        return [
            {
                'id': f"graph_{i}",
                'content': result['text'],
                'metadata': result['metadata'],
                'score': float(result['relevance'])
            }
            for i, result in enumerate(subgraph_results)
        ]
```

### 2.3 ç²¾æ’æ¨¡å‹

#### Cross-Encoderç²¾æ’

```python
from sentence_transformers import CrossEncoder

class CrossEncoderReranker:
    """Cross-Encoderé‡æ’åºå™¨"""

    def __init__(self, model_name: str = 'cross-encoder/ms-marco-MiniLM-L-12-v2'):
        """
        åˆå§‹åŒ–Cross-Encoder

        Args:
            model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
                - cross-encoder/ms-marco-MiniLM-L-12-v2 (å¿«é€Ÿ)
                - cross-encoder/ms-marco-MiniLM-L-6-v2 (æ›´å¿«)
                - cross-encoder/ms-marco-electra-base (æ›´å‡†ç¡®)
        """
        self.model = CrossEncoder(model_name, max_length=512)

    def rerank(self, query: str, candidates: List[Dict], top_k: int = 10) -> List[Dict]:
        """
        é‡æ’åºå€™é€‰æ–‡æ¡£

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            candidates: å€™é€‰æ–‡æ¡£åˆ—è¡¨
            top_k: è¿”å›Top Kç»“æœ

        Returns:
            é‡æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        """
        # å‡†å¤‡è¾“å…¥å¯¹
        pairs = [(query, item['content']) for item in candidates]

        # æ‰¹é‡é¢„æµ‹
        scores = self.model.predict(pairs)

        # æ·»åŠ é‡æ’åºåˆ†æ•°
        for i, item in enumerate(candidates):
            item['rerank_score'] = float(scores[i])

        # æŒ‰é‡æ’åºåˆ†æ•°æ’åº
        reranked = sorted(candidates, key=lambda x: x['rerank_score'], reverse=True)

        return reranked[:top_k]
```

---

## 3. é‡æ’åºæŠ€æœ¯(Re-ranking)

### 3.1 Cross-Encoderé‡æ’åº

Cross-Encoderæ˜¯å½“å‰æœ€å¸¸ç”¨çš„é‡æ’åºæŠ€æœ¯ï¼Œé€šè¿‡è”åˆç¼–ç æŸ¥è¯¢-æ–‡æ¡£å¯¹è·å¾—ç²¾ç¡®çš„ç›¸å…³æ€§åˆ†æ•°ã€‚

#### é«˜çº§Cross-Encoderé…ç½®

```python
class AdvancedCrossEncoderReranker:
    """é«˜çº§Cross-Encoderé‡æ’åºå™¨"""

    def __init__(
        self,
        model_name: str = 'cross-encoder/ms-marco-MiniLM-L-12-v2',
        batch_size: int = 32,
        device: str = 'cuda'
    ):
        self.model = CrossEncoder(model_name, device=device)
        self.batch_size = batch_size

    def rerank_with_metadata(
        self,
        query: str,
        candidates: List[Dict],
        metadata_weight: float = 0.1
    ) -> List[Dict]:
        """
        ç»“åˆå…ƒæ•°æ®çš„é‡æ’åº

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            candidates: å€™é€‰æ–‡æ¡£
            metadata_weight: å…ƒæ•°æ®æƒé‡ï¼ˆç”¨äºæœ€ç»ˆåˆ†æ•°èåˆï¼‰
        """
        # æ„å»ºæŸ¥è¯¢-æ–‡æ¡£å¯¹
        pairs = []
        for item in candidates:
            # å¯ä»¥å°†å…ƒæ•°æ®ä¿¡æ¯èå…¥æ–‡æ¡£æ–‡æœ¬
            enhanced_content = self._enhance_with_metadata(item)
            pairs.append((query, enhanced_content))

        # æ‰¹é‡é¢„æµ‹
        scores = self.model.predict(
            pairs,
            batch_size=self.batch_size,
            show_progress_bar=True
        )

        # èåˆåˆ†æ•°
        for i, item in enumerate(candidates):
            rerank_score = float(scores[i])
            metadata_score = self._compute_metadata_score(item, query)

            # åŠ æƒèåˆ
            item['rerank_score'] = rerank_score
            item['metadata_score'] = metadata_score
            item['final_score'] = (1 - metadata_weight) * rerank_score + metadata_weight * metadata_score

        # æ’åº
        return sorted(candidates, key=lambda x: x['final_score'], reverse=True)

    def _enhance_with_metadata(self, item: Dict) -> str:
        """ä½¿ç”¨å…ƒæ•°æ®å¢å¼ºæ–‡æ¡£å†…å®¹"""
        content = item['content']
        metadata = item.get('metadata', {})

        # æ·»åŠ å…ƒæ•°æ®ä¿¡æ¯ï¼ˆå¦‚ç±»åˆ«ã€æ ‡ç­¾ç­‰ï¼‰
        metadata_text = ""
        if 'category' in metadata:
            metadata_text += f"[ç±»åˆ«: {metadata['category']}] "
        if 'tags' in metadata:
            tags = ', '.join(metadata['tags']) if isinstance(metadata['tags'], list) else metadata['tags']
            metadata_text += f"[æ ‡ç­¾: {tags}] "

        return f"{metadata_text}{content}"

    def _compute_metadata_score(self, item: Dict, query: str) -> float:
        """è®¡ç®—åŸºäºå…ƒæ•°æ®çš„ç›¸å…³æ€§åˆ†æ•°"""
        metadata = item.get('metadata', {})

        # ç®€å•çš„å…ƒæ•°æ®åŒ¹é…åˆ†æ•°
        score = 0.0

        # ç±»åˆ«åŒ¹é…
        if 'category' in metadata and metadata['category'].lower() in query.lower():
            score += 0.3

        # æ ‡ç­¾åŒ¹é…
        if 'tags' in metadata:
            tags = metadata['tags'] if isinstance(metadata['tags'], list) else [metadata['tags']]
            matched_tags = sum(1 for tag in tags if tag.lower() in query.lower())
            score += 0.2 * min(matched_tags / len(tags), 1.0)

        return min(score, 1.0)
```

### 3.2 æ··åˆé‡æ’åºç­–ç•¥

ç»“åˆå¤šç§é‡æ’åºæ–¹æ³•çš„æ··åˆç­–ç•¥ã€‚

```python
class HybridRerankingStrategy:
    """æ··åˆé‡æ’åºç­–ç•¥"""

    def __init__(
        self,
        cross_encoder=None,
        llm_reranker=None,
        keyword_reranker=None
    ):
        self.cross_encoder = cross_encoder
        self.llm_reranker = llm_reranker
        self.keyword_reranker = keyword_reranker

    def hybrid_rerank(
        self,
        query: str,
        candidates: List[Dict],
        strategy: str = 'adaptive'
    ) -> List[Dict]:
        """
        æ··åˆé‡æ’åº

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            candidates: å€™é€‰æ–‡æ¡£
            strategy: ç­–ç•¥
                - 'adaptive': è‡ªé€‚åº”é€‰æ‹©
                - 'ensemble': é›†æˆæ‰€æœ‰æ–¹æ³•
                - 'cascade': çº§è”ï¼ˆå…ˆç”¨å¿«é€Ÿæ–¹æ³•ï¼Œå†ç”¨ç²¾ç¡®æ–¹æ³•ï¼‰
        """
        if strategy == 'adaptive':
            return self._adaptive_rerank(query, candidates)
        elif strategy == 'ensemble':
            return self._ensemble_rerank(query, candidates)
        elif strategy == 'cascade':
            return self._cascade_rerank(query, candidates)
        else:
            raise ValueError(f"Unknown strategy: {strategy}")

    def _adaptive_rerank(self, query: str, candidates: List[Dict]) -> List[Dict]:
        """è‡ªé€‚åº”é‡æ’åºï¼šæ ¹æ®æŸ¥è¯¢ç‰¹å¾é€‰æ‹©æ–¹æ³•"""
        # æŸ¥è¯¢é•¿åº¦
        query_length = len(query.split())

        # å€™é€‰æ•°é‡
        num_candidates = len(candidates)

        if num_candidates <= 20 and query_length <= 20:
            # å°‘é‡å€™é€‰ä¸”æŸ¥è¯¢ç®€å•ï¼šä½¿ç”¨Cross-Encoder
            if self.cross_encoder:
                return self.cross_encoder.rerank(query, candidates)
        elif num_candidates > 100:
            # å¤§é‡å€™é€‰ï¼šå…ˆå…³é”®è¯è¿‡æ»¤ï¼Œå†Cross-Encoder
            if self.keyword_reranker:
                filtered = self.keyword_reranker.filter(query, candidates, top_k=50)
                if self.cross_encoder:
                    return self.cross_encoder.rerank(query, filtered)
        else:
            # ä¸­ç­‰è§„æ¨¡ï¼šä½¿ç”¨LLMé‡æ’åºï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.llm_reranker:
                return self.llm_reranker.rerank(query, candidates)
            elif self.cross_encoder:
                return self.cross_encoder.rerank(query, candidates)

        # é»˜è®¤ï¼šè¿”å›åŸé¡ºåº
        return candidates

    def _ensemble_rerank(self, query: str, candidates: List[Dict]) -> List[Dict]:
        """é›†æˆé‡æ’åºï¼šç»¼åˆå¤šç§æ–¹æ³•"""
        scores_dict = {}

        # Cross-Encoderåˆ†æ•°
        if self.cross_encoder:
            ce_results = self.cross_encoder.rerank(query, candidates)
            for i, item in enumerate(ce_results):
                doc_id = item.get('id', i)
                scores_dict[doc_id] = scores_dict.get(doc_id, {})
                scores_dict[doc_id]['ce'] = item['rerank_score']

        # LLMé‡æ’åºåˆ†æ•°
        if self.llm_reranker:
            llm_results = self.llm_reranker.rerank(query, candidates)
            for i, item in enumerate(llm_results):
                doc_id = item.get('id', i)
                scores_dict[doc_id] = scores_dict.get(doc_id, {})
                scores_dict[doc_id]['llm'] = item['rerank_score']

        # åŠ æƒèåˆ
        weights = {'ce': 0.7, 'llm': 0.3}
        for item in candidates:
            doc_id = item.get('id', candidates.index(item))
            final_score = sum(
                scores_dict.get(doc_id, {}).get(method, 0) * weight
                for method, weight in weights.items()
            )
            item['final_score'] = final_score

        # æ’åº
        return sorted(candidates, key=lambda x: x['final_score'], reverse=True)

    def _cascade_rerank(self, query: str, candidates: List[Dict]) -> List[Dict]:
        """çº§è”é‡æ’åºï¼šå…ˆç”¨å¿«é€Ÿæ–¹æ³•ç­›é€‰ï¼Œå†ç”¨ç²¾ç¡®æ–¹æ³•"""
        # ç¬¬ä¸€å±‚ï¼šå…³é”®è¯å¿«é€Ÿç­›é€‰
        if self.keyword_reranker:
            filtered = self.keyword_reranker.filter(query, candidates, top_k=50)
        else:
            filtered = candidates[:50]

        # ç¬¬äºŒå±‚ï¼šCross-Encoderç²¾ç¡®æ’åº
        if self.cross_encoder:
            return self.cross_encoder.rerank(query, filtered)
        else:
            return filtered
```

### 3.3 ä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡æ’åº

è€ƒè™‘ç”¨æˆ·ä¸Šä¸‹æ–‡å’Œå†å²çš„é‡æ’åºã€‚

```python
class ContextAwareReranker:
    """ä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡æ’åº"""

    def __init__(self, base_reranker, context_encoder=None):
        self.base_reranker = base_reranker
        self.context_encoder = context_encoder

    def rerank_with_context(
        self,
        query: str,
        candidates: List[Dict],
        user_context: Dict = None,
        conversation_history: List[Dict] = None
    ) -> List[Dict]:
        """
        è€ƒè™‘ä¸Šä¸‹æ–‡çš„é‡æ’åº

        Args:
            query: å½“å‰æŸ¥è¯¢
            candidates: å€™é€‰æ–‡æ¡£
            user_context: ç”¨æˆ·ä¸Šä¸‹æ–‡ï¼ˆåå¥½ã€å†å²ç­‰ï¼‰
            conversation_history: å¯¹è¯å†å²
        """
        # ä½¿ç”¨åŸºç¡€é‡æ’åºè·å¾—åˆå§‹åˆ†æ•°
        reranked = self.base_reranker.rerank(query, candidates)

        # ä¸Šä¸‹æ–‡å¢å¼º
        if user_context or conversation_history:
            for item in reranked:
                context_score = self._compute_context_score(
                    item,
                    query,
                    user_context,
                    conversation_history
                )
                # èåˆä¸Šä¸‹æ–‡åˆ†æ•°
                item['context_score'] = context_score
                item['final_score'] = 0.7 * item['rerank_score'] + 0.3 * context_score

            # é‡æ–°æ’åº
            reranked = sorted(reranked, key=lambda x: x['final_score'], reverse=True)

        return reranked

    def _compute_context_score(
        self,
        item: Dict,
        query: str,
        user_context: Dict,
        conversation_history: List[Dict]
    ) -> float:
        """è®¡ç®—ä¸Šä¸‹æ–‡ç›¸å…³åˆ†æ•°"""
        score = 0.0

        # ç”¨æˆ·åå¥½åŒ¹é…
        if user_context:
            preferences = user_context.get('preferences', [])
            item_tags = item.get('metadata', {}).get('tags', [])
            if isinstance(item_tags, str):
                item_tags = [item_tags]

            # æ ‡ç­¾åŒ¹é…
            matched = sum(1 for pref in preferences if pref in item_tags)
            score += 0.3 * (matched / max(len(preferences), 1))

        # å¯¹è¯å†å²ç›¸å…³æ€§
        if conversation_history:
            # è®¡ç®—ä¸å†å²æŸ¥è¯¢çš„ç›¸å…³æ€§
            history_queries = [h['query'] for h in conversation_history[-3:]]
            history_embedding = self.context_encoder.encode(' '.join(history_queries))
            item_embedding = self.context_encoder.encode(item['content'])

            similarity = np.dot(history_embedding, item_embedding) / (
                np.linalg.norm(history_embedding) * np.linalg.norm(item_embedding)
            )
            score += 0.2 * similarity

        return min(score, 1.0)
```

---

## 4. Self-RAGæ¶æ„

### 4.1 Self-RAGåŸç†

Self-RAGé€šè¿‡è‡ªé€‚åº”çš„æ£€ç´¢å’Œç”Ÿæˆå†³ç­–ï¼ŒåŠ¨æ€å†³å®šæ˜¯å¦éœ€è¦æ£€ç´¢ã€æ£€ç´¢ä»€ä¹ˆå†…å®¹ã€å¦‚ä½•ç”Ÿæˆç­”æ¡ˆã€‚

#### Self-RAGæ ¸å¿ƒç»„ä»¶

```python
class SelfRAG:
    """Self-RAGå®ç°"""

    def __init__(
        self,
        llm,
        retriever,
        critique_llm=None
    ):
        self.llm = llm
        self.retriever = retriever
        self.critique_llm = critique_llm or llm

    def generate(self, query: str, max_iterations: int = 3) -> Dict[str, Any]:
        """
        Self-RAGç”Ÿæˆæµç¨‹

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        """
        context = []
        retrieved_docs = []
        generation_history = []

        for iteration in range(max_iterations):
            # 1. å†³å®šæ˜¯å¦éœ€è¦æ£€ç´¢
            retrieve_decision = self._decide_retrieve(query, context, generation_history)

            if retrieve_decision['should_retrieve']:
                # 2. å†³å®šæ£€ç´¢ä»€ä¹ˆ
                retrieval_query = self._decide_retrieval_query(
                    query,
                    context,
                    generation_history
                )

                # 3. æ‰§è¡Œæ£€ç´¢
                docs = self.retriever.retrieve(retrieval_query, top_k=5)
                retrieved_docs.extend(docs)
                context.extend([doc['content'] for doc in docs])

            # 4. ç”Ÿæˆç­”æ¡ˆç‰‡æ®µ
            generation_result = self._generate_with_critique(
                query,
                context,
                generation_history
            )

            generation_history.append(generation_result)

            # 5. åˆ¤æ–­æ˜¯å¦ç»§ç»­
            if generation_result['is_complete']:
                break

        # 6. æœ€ç»ˆç­”æ¡ˆç”Ÿæˆ
        final_answer = self._finalize_answer(query, context, generation_history)

        return {
            'answer': final_answer,
            'retrieved_docs': retrieved_docs,
            'generation_history': generation_history,
            'iterations': len(generation_history)
        }

    def _decide_retrieve(
        self,
        query: str,
        context: List[str],
        history: List[Dict]
    ) -> Dict[str, Any]:
        """å†³å®šæ˜¯å¦éœ€è¦æ£€ç´¢"""
        prompt = f"""
        åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢å¤–éƒ¨æ–‡æ¡£æ¥å›ç­”ä»¥ä¸‹æŸ¥è¯¢ã€‚

        æŸ¥è¯¢: {query}
        å·²æœ‰ä¸Šä¸‹æ–‡: {len(context)} ä¸ªæ–‡æ¡£ç‰‡æ®µ

        å¦‚æœæŸ¥è¯¢éœ€è¦ï¼š
        - äº‹å®æ€§ä¿¡æ¯
        - æœ€æ–°æ•°æ®
        - ç‰¹å®šé¢†åŸŸçŸ¥è¯†
        åˆ™å›ç­” YES

        å¦‚æœå·²æœ‰ä¸Šä¸‹æ–‡è¶³å¤Ÿå›ç­”ï¼Œå›ç­” NO

        å›ç­”æ ¼å¼: YES/NO
        ç†ç”±: <ç®€çŸ­ç†ç”±>
        """

        response = self.critique_llm.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )

        result_text = response.choices[0].message.content
        should_retrieve = "YES" in result_text.upper()

        return {
            'should_retrieve': should_retrieve,
            'reasoning': result_text
        }

    def _decide_retrieval_query(
        self,
        query: str,
        context: List[str],
        history: List[Dict]
    ) -> str:
        """å†³å®šæ£€ç´¢æŸ¥è¯¢ï¼ˆå¯èƒ½éœ€è¦æ”¹å†™ï¼‰"""
        # å¦‚æœæœ‰ç”Ÿæˆå†å²ï¼Œå¯èƒ½éœ€è¦åŸºäºå†å²ç”Ÿæˆæ–°çš„æ£€ç´¢æŸ¥è¯¢
        if history:
            prompt = f"""
            åŸºäºåŸå§‹æŸ¥è¯¢å’Œå·²ç”Ÿæˆçš„å†…å®¹ï¼Œå†³å®šä¸‹ä¸€æ­¥åº”è¯¥æ£€ç´¢ä»€ä¹ˆä¿¡æ¯ã€‚

            åŸå§‹æŸ¥è¯¢: {query}
            å·²ç”Ÿæˆå†…å®¹: {history[-1]['text']}

            ç”Ÿæˆä¸€ä¸ªä¼˜åŒ–åçš„æ£€ç´¢æŸ¥è¯¢:
            """

            response = self.llm.chat.completions.create(
                model="gpt-4-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )

            return response.choices[0].message.content.strip()
        else:
            return query

    def _generate_with_critique(
        self,
        query: str,
        context: List[str],
        history: List[Dict]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå¹¶è‡ªæˆ‘æ‰¹åˆ¤"""
        # ç”Ÿæˆ
        context_text = "\n\n".join(context[-5:])  # ä½¿ç”¨æœ€è¿‘5ä¸ªæ–‡æ¡£

        generation_prompt = f"""
        åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”æŸ¥è¯¢ã€‚

        æŸ¥è¯¢: {query}
        ä¸Šä¸‹æ–‡:
        {context_text}

        å›ç­”:
        """

        generation_response = self.llm.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": generation_prompt}],
            temperature=0.7
        )

        generated_text = generation_response.choices[0].message.content

        # è‡ªæˆ‘æ‰¹åˆ¤
        critique = self._critique_generation(
            query,
            context_text,
            generated_text
        )

        return {
            'text': generated_text,
            'critique': critique,
            'is_complete': critique['is_sufficient'],
            'needs_revision': critique['needs_revision']
        }

    def _critique_generation(
        self,
        query: str,
        context: str,
        generated: str
    ) -> Dict[str, Any]:
        """æ‰¹åˆ¤ç”Ÿæˆçš„å†…å®¹"""
        prompt = f"""
        æ‰¹åˆ¤ä»¥ä¸‹ç”Ÿæˆçš„å›ç­”ï¼š

        æŸ¥è¯¢: {query}
        ä¸Šä¸‹æ–‡: {context[:500]}
        ç”Ÿæˆå›ç­”: {generated}

        è¯„ä¼°ï¼š
        1. å›ç­”æ˜¯å¦å……åˆ†ï¼Ÿ(YES/NO)
        2. æ˜¯å¦å‡†ç¡®å¼•ç”¨ä¸Šä¸‹æ–‡ï¼Ÿ(YES/NO)
        3. æ˜¯å¦éœ€è¦ä¿®æ­£ï¼Ÿ(YES/NO)
        4. æ˜¯å¦éœ€è¦æ›´å¤šä¿¡æ¯ï¼Ÿ(YES/NO)
        """

        response = self.critique_llm.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )

        result_text = response.choices[0].message.content

        return {
            'is_sufficient': "YES" in result_text.upper() and "å……åˆ†" in result_text or "sufficient" in result_text.lower(),
            'is_accurate': "YES" in result_text.upper() and "å‡†ç¡®" in result_text or "accurate" in result_text.lower(),
            'needs_revision': "YES" in result_text.upper() and "ä¿®æ­£" in result_text or "revision" in result_text.lower(),
            'needs_more_info': "YES" in result_text.upper() and "æ›´å¤šä¿¡æ¯" in result_text or "more information" in result_text.lower(),
            'reasoning': result_text
        }

    def _finalize_answer(
        self,
        query: str,
        context: List[str],
        history: List[Dict]
    ) -> str:
        """æœ€ç»ˆç­”æ¡ˆç”Ÿæˆ"""
        # æ•´åˆæ‰€æœ‰ç”Ÿæˆç‰‡æ®µ
        all_texts = [h['text'] for h in history]

        prompt = f"""
        æ•´åˆä»¥ä¸‹å†…å®¹ç‰‡æ®µï¼Œç”Ÿæˆä¸€ä¸ªå®Œæ•´ã€è¿è´¯çš„å›ç­”ã€‚

        æŸ¥è¯¢: {query}

        å†…å®¹ç‰‡æ®µ:
        {chr(10).join([f"{i+1}. {text}" for i, text in enumerate(all_texts)])}

        ç”Ÿæˆæœ€ç»ˆå›ç­”:
        """

        response = self.llm.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )

        return response.choices[0].message.content
```

---

## 5. Agentic RAG

### 5.1 å¤šAgentåä½œ

Agentic RAGé€šè¿‡å¤šä¸ªä¸“é—¨çš„Agentåä½œï¼Œå®ç°æ›´æ™ºèƒ½çš„æ£€ç´¢å’Œç”Ÿæˆã€‚æ¯ä¸ªAgentè´Ÿè´£ç‰¹å®šçš„ä»»åŠ¡ï¼Œé€šè¿‡åä½œå®Œæˆå¤æ‚çš„æŸ¥è¯¢ã€‚

#### å¤šAgentæ¶æ„

```python
from typing import List, Dict, Any
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain.tools import Tool
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

class MultiAgentRAG:
    """å¤šAgentåä½œRAGç³»ç»Ÿ"""

    def __init__(
        self,
        llm,
        retriever_agent,
        query_agent,
        synthesis_agent,
        validator_agent
    ):
        self.llm = llm
        self.retriever_agent = retriever_agent
        self.query_agent = query_agent
        self.synthesis_agent = synthesis_agent
        self.validator_agent = validator_agent

    def process_query(self, query: str) -> Dict[str, Any]:
        """
        å¤šAgentåä½œå¤„ç†æŸ¥è¯¢

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢

        Returns:
            æœ€ç»ˆç­”æ¡ˆå’Œä¸­é—´ç»“æœ
        """
        # 1. Query Agent: åˆ†ææŸ¥è¯¢ï¼Œåˆ¶å®šè®¡åˆ’
        query_plan = self.query_agent.analyze(query)

        # 2. Retriever Agent: æ‰§è¡Œæ£€ç´¢
        retrieved_docs = []
        for sub_query in query_plan['sub_queries']:
            docs = self.retriever_agent.retrieve(sub_query)
            retrieved_docs.extend(docs)

        # 3. Synthesis Agent: ç»¼åˆä¿¡æ¯ç”Ÿæˆç­”æ¡ˆ
        answer = self.synthesis_agent.synthesize(
            query,
            retrieved_docs,
            query_plan
        )

        # 4. Validator Agent: éªŒè¯ç­”æ¡ˆ
        validation = self.validator_agent.validate(
            query,
            answer,
            retrieved_docs
        )

        # å¦‚æœéªŒè¯å¤±è´¥ï¼Œé‡æ–°æ£€ç´¢å’Œç”Ÿæˆ
        if not validation['is_valid']:
            # ä½¿ç”¨åé¦ˆæ”¹è¿›æ£€ç´¢
            improved_docs = self.retriever_agent.retrieve_with_feedback(
                query,
                validation['feedback']
            )
            answer = self.synthesis_agent.synthesize(
                query,
                improved_docs,
                query_plan
            )

        return {
            'answer': answer,
            'query_plan': query_plan,
            'retrieved_docs': retrieved_docs,
            'validation': validation
        }
```

#### ä¸“é—¨åŒ–Agentå®ç°

```python
class QueryAnalysisAgent:
    """æŸ¥è¯¢åˆ†æAgent"""

    def __init__(self, llm):
        self.llm = llm

    def analyze(self, query: str) -> Dict[str, Any]:
        """åˆ†ææŸ¥è¯¢ï¼Œç”Ÿæˆæ‰§è¡Œè®¡åˆ’"""
        prompt = f"""
        åˆ†æä»¥ä¸‹æŸ¥è¯¢ï¼Œç”Ÿæˆæ‰§è¡Œè®¡åˆ’ï¼š

        æŸ¥è¯¢: {query}

        è¯·ï¼š
        1. è¯†åˆ«æŸ¥è¯¢æ„å›¾ï¼ˆä¿¡æ¯æ£€ç´¢ã€è®¡ç®—ã€å¯¹æ¯”ç­‰ï¼‰
        2. åˆ†è§£ä¸ºå­æŸ¥è¯¢ï¼ˆå¦‚æœéœ€è¦ï¼‰
        3. ç¡®å®šæ£€ç´¢ç­–ç•¥
        4. ä¼°è®¡å¤æ‚åº¦

        è¿”å›JSONæ ¼å¼ï¼š
        {{
            "intent": "æŸ¥è¯¢æ„å›¾",
            "sub_queries": ["å­æŸ¥è¯¢1", "å­æŸ¥è¯¢2"],
            "retrieval_strategy": "æ£€ç´¢ç­–ç•¥",
            "complexity": "å¤æ‚åº¦"
        }}
        """

        response = self.llm.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )

        import json
        return json.loads(response.choices[0].message.content)

class RetrievalAgent:
    """æ£€ç´¢Agent"""

    def __init__(self, vector_store, graph_store, keyword_search):
        self.vector_store = vector_store
        self.graph_store = graph_store
        self.keyword_search = keyword_search

    def retrieve(self, query: str, strategy: str = 'hybrid') -> List[Dict]:
        """æ‰§è¡Œæ£€ç´¢"""
        results = []

        if strategy in ['hybrid', 'vector']:
            vector_results = self.vector_store.similarity_search(query, k=5)
            results.extend([
                {'content': doc.page_content, 'source': 'vector', 'score': 0.9}
                for doc in vector_results
            ])

        if strategy in ['hybrid', 'keyword']:
            keyword_results = self.keyword_search.search(query, k=5)
            results.extend([
                {'content': result['text'], 'source': 'keyword', 'score': 0.8}
                for result in keyword_results
            ])

        if strategy in ['hybrid', 'graph']:
            graph_results = self.graph_store.query(query)
            results.extend([
                {'content': result['text'], 'source': 'graph', 'score': 0.85}
                for result in graph_results
            ])

        # å»é‡å’Œæ’åº
        return self._deduplicate_and_rank(results)

    def _deduplicate_and_rank(self, results: List[Dict]) -> List[Dict]:
        """å»é‡å¹¶æ’åº"""
        seen = set()
        unique_results = []
        for result in results:
            content_hash = hash(result['content'][:100])  # ä½¿ç”¨å‰100å­—ç¬¦ä½œä¸ºhash
            if content_hash not in seen:
                seen.add(content_hash)
                unique_results.append(result)

        # æŒ‰åˆ†æ•°æ’åº
        return sorted(unique_results, key=lambda x: x['score'], reverse=True)[:10]

class SynthesisAgent:
    """ç»¼åˆAgent"""

    def __init__(self, llm):
        self.llm = llm

    def synthesize(
        self,
        query: str,
        docs: List[Dict],
        plan: Dict[str, Any]
    ) -> str:
        """ç»¼åˆä¿¡æ¯ç”Ÿæˆç­”æ¡ˆ"""
        context = "\n\n".join([
            f"[æ¥æº: {doc['source']}]\n{doc['content']}"
            for doc in docs[:10]
        ])

        prompt = f"""
        åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”æŸ¥è¯¢ã€‚

        æŸ¥è¯¢: {query}
        æ‰§è¡Œè®¡åˆ’: {plan}

        ä¸Šä¸‹æ–‡:
        {context}

        è¯·ç”Ÿæˆä¸€ä¸ªå‡†ç¡®ã€å®Œæ•´çš„ç­”æ¡ˆã€‚
        """

        response = self.llm.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )

        return response.choices[0].message.content

class ValidatorAgent:
    """éªŒè¯Agent"""

    def __init__(self, llm, fact_checker=None):
        self.llm = llm
        self.fact_checker = fact_checker

    def validate(
        self,
        query: str,
        answer: str,
        docs: List[Dict]
    ) -> Dict[str, Any]:
        """éªŒè¯ç­”æ¡ˆ"""
        prompt = f"""
        éªŒè¯ä»¥ä¸‹ç­”æ¡ˆæ˜¯å¦æ­£ç¡®ã€å®Œæ•´ã€‚

        æŸ¥è¯¢: {query}
        ç­”æ¡ˆ: {answer}
        æ”¯æŒæ–‡æ¡£æ•°: {len(docs)}

        è¯·è¯„ä¼°ï¼š
        1. ç­”æ¡ˆæ˜¯å¦å‡†ç¡®ï¼Ÿ
        2. ç­”æ¡ˆæ˜¯å¦å®Œæ•´ï¼Ÿ
        3. ç­”æ¡ˆæ˜¯å¦æœ‰äº‹å®ä¾æ®ï¼Ÿ
        4. æ˜¯å¦éœ€è¦æ›´å¤šä¿¡æ¯ï¼Ÿ

        è¿”å›JSONæ ¼å¼ï¼š
        {{
            "is_valid": true/false,
            "confidence": 0.0-1.0,
            "issues": ["é—®é¢˜åˆ—è¡¨"],
            "feedback": "æ”¹è¿›å»ºè®®"
        }}
        """

        response = self.llm.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )

        import json
        return json.loads(response.choices[0].message.content)
```

### 5.2 å·¥å…·è°ƒç”¨ä¸è§„åˆ’

Agentic RAGé€šè¿‡å·¥å…·è°ƒç”¨æ‰©å±•èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ•°æ®åº“æŸ¥è¯¢ã€APIè°ƒç”¨ã€è®¡ç®—å·¥å…·ç­‰ã€‚

#### å·¥å…·ç³»ç»Ÿ

```python
from langchain.tools import Tool
from langchain.agents import initialize_agent, AgentType

class ToolBasedRAG:
    """åŸºäºå·¥å…·çš„RAGç³»ç»Ÿ"""

    def __init__(self, llm, vector_store):
        self.llm = llm
        self.vector_store = vector_store
        self.tools = self._create_tools()
        self.agent = initialize_agent(
            self.tools,
            llm,
            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
            verbose=True
        )

    def _create_tools(self) -> List[Tool]:
        """åˆ›å»ºå·¥å…·é›†"""
        tools = [
            Tool(
                name="å‘é‡æ£€ç´¢",
                func=self._vector_search,
                description="ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢ç›¸å…³æ–‡æ¡£ã€‚è¾“å…¥ï¼šæŸ¥è¯¢æ–‡æœ¬"
            ),
            Tool(
                name="SQLæŸ¥è¯¢",
                func=self._sql_query,
                description="æ‰§è¡ŒSQLæŸ¥è¯¢æ•°æ®åº“ã€‚è¾“å…¥ï¼šSQLæŸ¥è¯¢è¯­å¥"
            ),
            Tool(
                name="å›¾æŸ¥è¯¢",
                func=self._graph_query,
                description="æŸ¥è¯¢çŸ¥è¯†å›¾è°±ã€‚è¾“å…¥ï¼šCypheræŸ¥è¯¢è¯­å¥"
            ),
            Tool(
                name="è®¡ç®—å™¨",
                func=self._calculator,
                description="æ‰§è¡Œæ•°å­¦è®¡ç®—ã€‚è¾“å…¥ï¼šæ•°å­¦è¡¨è¾¾å¼"
            ),
            Tool(
                name="å®ä½“è¯†åˆ«",
                func=self._entity_recognition,
                description="è¯†åˆ«æ–‡æœ¬ä¸­çš„å®ä½“ã€‚è¾“å…¥ï¼šæ–‡æœ¬"
            )
        ]
        return tools

    def _vector_search(self, query: str) -> str:
        """å‘é‡æ£€ç´¢å·¥å…·"""
        docs = self.vector_store.similarity_search(query, k=5)
        return "\n".join([doc.page_content for doc in docs])

    def _sql_query(self, sql: str) -> str:
        """SQLæŸ¥è¯¢å·¥å…·"""
        # æ‰§è¡ŒSQLæŸ¥è¯¢
        # è¿™é‡Œåº”è¯¥è¿æ¥æ•°æ®åº“æ‰§è¡ŒæŸ¥è¯¢
        try:
            # result = execute_sql(sql)
            return f"SQLæŸ¥è¯¢ç»“æœ: {sql}"
        except Exception as e:
            return f"SQLæŸ¥è¯¢é”™è¯¯: {str(e)}"

    def _graph_query(self, cypher: str) -> str:
        """å›¾æŸ¥è¯¢å·¥å…·"""
        # æ‰§è¡ŒCypheræŸ¥è¯¢
        try:
            # result = execute_cypher(cypher)
            return f"å›¾æŸ¥è¯¢ç»“æœ: {cypher}"
        except Exception as e:
            return f"å›¾æŸ¥è¯¢é”™è¯¯: {str(e)}"

    def _calculator(self, expression: str) -> str:
        """è®¡ç®—å™¨å·¥å…·"""
        try:
            result = eval(expression)  # æ³¨æ„ï¼šç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨æ›´å®‰å…¨çš„evalæ›¿ä»£
            return str(result)
        except Exception as e:
            return f"è®¡ç®—é”™è¯¯: {str(e)}"

    def _entity_recognition(self, text: str) -> str:
        """å®ä½“è¯†åˆ«å·¥å…·"""
        # ä½¿ç”¨NERæ¨¡å‹è¯†åˆ«å®ä½“
        # entities = ner_model.predict(text)
        return f"è¯†åˆ«åˆ°çš„å®ä½“: {text}"

    def query(self, question: str) -> str:
        """ä½¿ç”¨å·¥å…·å›ç­”æŸ¥è¯¢"""
        return self.agent.run(question)
```

### 5.3 è¿­ä»£ä¼˜åŒ–

Agentic RAGé€šè¿‡è¿­ä»£ä¼˜åŒ–ä¸æ–­æå‡ç­”æ¡ˆè´¨é‡ã€‚

#### è¿­ä»£ä¼˜åŒ–æ¡†æ¶

```python
class IterativeRAG:
    """è¿­ä»£ä¼˜åŒ–RAG"""

    def __init__(self, base_rag, max_iterations: int = 3):
        self.base_rag = base_rag
        self.max_iterations = max_iterations

    def iterative_improve(self, query: str) -> Dict[str, Any]:
        """è¿­ä»£æ”¹è¿›ç­”æ¡ˆ"""
        history = []
        current_answer = None

        for iteration in range(self.max_iterations):
            # ç”Ÿæˆæˆ–æ”¹è¿›ç­”æ¡ˆ
            if iteration == 0:
                # ç¬¬ä¸€æ¬¡ï¼šåŸºç¡€æ£€ç´¢å’Œç”Ÿæˆ
                result = self.base_rag.query(query)
                current_answer = result['answer']
            else:
                # åç»­è¿­ä»£ï¼šåŸºäºåé¦ˆæ”¹è¿›
                feedback = self._generate_feedback(query, current_answer, history)
                improved_result = self.base_rag.query_with_feedback(query, feedback)
                current_answer = improved_result['answer']

            # è¯„ä¼°ç­”æ¡ˆè´¨é‡
            quality_score = self._evaluate_answer(query, current_answer)

            history.append({
                'iteration': iteration,
                'answer': current_answer,
                'quality_score': quality_score
            })

            # å¦‚æœè´¨é‡è¶³å¤Ÿé«˜ï¼Œåœæ­¢è¿­ä»£
            if quality_score > 0.9:
                break

        return {
            'final_answer': current_answer,
            'iterations': len(history),
            'history': history
        }

    def _generate_feedback(
        self,
        query: str,
        current_answer: str,
        history: List[Dict]
    ) -> str:
        """ç”Ÿæˆæ”¹è¿›åé¦ˆ"""
        prompt = f"""
        åˆ†æå½“å‰ç­”æ¡ˆï¼Œç”Ÿæˆæ”¹è¿›å»ºè®®ã€‚

        æŸ¥è¯¢: {query}
        å½“å‰ç­”æ¡ˆ: {current_answer}
        å†å²è¿­ä»£: {len(history)}

        è¯·æŒ‡å‡ºï¼š
        1. ç­”æ¡ˆçš„é—®é¢˜
        2. éœ€è¦æ”¹è¿›çš„åœ°æ–¹
        3. åº”è¯¥æ£€ç´¢å“ªäº›é¢å¤–ä¿¡æ¯
        """

        response = self.base_rag.llm.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )

        return response.choices[0].message.content

    def _evaluate_answer(self, query: str, answer: str) -> float:
        """è¯„ä¼°ç­”æ¡ˆè´¨é‡"""
        prompt = f"""
        è¯„ä¼°ç­”æ¡ˆè´¨é‡ï¼ˆ0-1åˆ†ï¼‰ã€‚

        æŸ¥è¯¢: {query}
        ç­”æ¡ˆ: {answer}

        è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„åˆ†ï¼š
        1. å‡†ç¡®æ€§ï¼ˆ0.3ï¼‰
        2. å®Œæ•´æ€§ï¼ˆ0.3ï¼‰
        3. ç›¸å…³æ€§ï¼ˆ0.2ï¼‰
        4. å¯è¯»æ€§ï¼ˆ0.2ï¼‰

        è¿”å›0-1ä¹‹é—´çš„åˆ†æ•°ã€‚
        """

        response = self.base_rag.llm.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )

        try:
            score = float(response.choices[0].message.content)
            return max(0.0, min(1.0, score))
        except:
            return 0.5
```

---

## 7. RAGè¯„ä¼°ä½“ç³»

### 7.1 è¯„ä¼°æŒ‡æ ‡

RAGç³»ç»Ÿçš„è¯„ä¼°éœ€è¦ä»å¤šä¸ªç»´åº¦è¿›è¡Œï¼ŒåŒ…æ‹¬æ£€ç´¢è´¨é‡ã€ç”Ÿæˆè´¨é‡å’Œæ•´ä½“æ•ˆæœã€‚

#### æ£€ç´¢è´¨é‡æŒ‡æ ‡

```python
from typing import List, Dict, Set
import numpy as np

class RetrievalMetrics:
    """æ£€ç´¢è´¨é‡è¯„ä¼°æŒ‡æ ‡"""

    @staticmethod
    def precision_at_k(retrieved: List[str], relevant: Set[str], k: int) -> float:
        """
        Precision@K: å‰Kä¸ªç»“æœä¸­ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹

        Args:
            retrieved: æ£€ç´¢åˆ°çš„æ–‡æ¡£IDåˆ—è¡¨
            relevant: ç›¸å…³æ–‡æ¡£IDé›†åˆ
            k: è¯„ä¼°å‰Kä¸ªç»“æœ

        Returns:
            Precision@Kåˆ†æ•°
        """
        retrieved_k = retrieved[:k]
        relevant_retrieved = len([doc for doc in retrieved_k if doc in relevant])
        return relevant_retrieved / k if k > 0 else 0.0

    @staticmethod
    def recall_at_k(retrieved: List[str], relevant: Set[str], k: int) -> float:
        """
        Recall@K: å‰Kä¸ªç»“æœä¸­ç›¸å…³æ–‡æ¡£å æ‰€æœ‰ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹

        Args:
            retrieved: æ£€ç´¢åˆ°çš„æ–‡æ¡£IDåˆ—è¡¨
            relevant: ç›¸å…³æ–‡æ¡£IDé›†åˆ
            k: è¯„ä¼°å‰Kä¸ªç»“æœ

        Returns:
            Recall@Kåˆ†æ•°
        """
        if len(relevant) == 0:
            return 0.0

        retrieved_k = retrieved[:k]
        relevant_retrieved = len([doc for doc in retrieved_k if doc in relevant])
        return relevant_retrieved / len(relevant)

    @staticmethod
    def f1_at_k(retrieved: List[str], relevant: Set[str], k: int) -> float:
        """F1@K: Precisionå’ŒRecallçš„è°ƒå’Œå¹³å‡"""
        precision = RetrievalMetrics.precision_at_k(retrieved, relevant, k)
        recall = RetrievalMetrics.recall_at_k(retrieved, relevant, k)

        if precision + recall == 0:
            return 0.0

        return 2 * precision * recall / (precision + recall)

    @staticmethod
    def mrr(retrieved_lists: List[List[str]], relevant_sets: List[Set[str]]) -> float:
        """
        MRR (Mean Reciprocal Rank): å¹³å‡å€’æ•°æ’å

        Args:
            retrieved_lists: æ¯ä¸ªæŸ¥è¯¢çš„æ£€ç´¢ç»“æœåˆ—è¡¨
            relevant_sets: æ¯ä¸ªæŸ¥è¯¢çš„ç›¸å…³æ–‡æ¡£é›†åˆ

        Returns:
            MRRåˆ†æ•°
        """
        reciprocal_ranks = []

        for retrieved, relevant in zip(retrieved_lists, relevant_sets):
            if len(relevant) == 0:
                continue

            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„æ’å
            rank = None
            for i, doc in enumerate(retrieved, 1):
                if doc in relevant:
                    rank = i
                    break

            if rank is not None:
                reciprocal_ranks.append(1.0 / rank)
            else:
                reciprocal_ranks.append(0.0)

        return np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0

    @staticmethod
    def ndcg_at_k(
        retrieved: List[str],
        relevant: Set[str],
        k: int,
        gains: Dict[str, float] = None
    ) -> float:
        """
        NDCG@K (Normalized Discounted Cumulative Gain)

        Args:
            retrieved: æ£€ç´¢åˆ°çš„æ–‡æ¡£IDåˆ—è¡¨
            relevant: ç›¸å…³æ–‡æ¡£IDé›†åˆï¼ˆæˆ–å¸¦åˆ†æ•°çš„å­—å…¸ï¼‰
            k: è¯„ä¼°å‰Kä¸ªç»“æœ
            gains: æ–‡æ¡£çš„ç›¸å…³æ€§åˆ†æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤1.0ï¼‰

        Returns:
            NDCG@Kåˆ†æ•°
        """
        if gains is None:
            gains = {doc: 1.0 for doc in relevant}

        # è®¡ç®—DCG@K
        dcg = 0.0
        for i, doc in enumerate(retrieved[:k], 1):
            if doc in gains:
                gain = gains[doc]
                dcg += gain / np.log2(i + 1)

        # è®¡ç®—IDCG@Kï¼ˆç†æƒ³æƒ…å†µä¸‹çš„DCGï¼‰
        ideal_gains = sorted(gains.values(), reverse=True)[:k]
        idcg = sum(gain / np.log2(i + 2) for i, gain in enumerate(ideal_gains))

        # NDCG = DCG / IDCG
        return dcg / idcg if idcg > 0 else 0.0

    @staticmethod
    def evaluate_all(
        retrieved_lists: List[List[str]],
        relevant_sets: List[Set[str]],
        k_values: List[int] = [1, 5, 10]
    ) -> Dict[str, float]:
        """è¯„ä¼°æ‰€æœ‰æŒ‡æ ‡"""
        results = {}

        # MRR
        results['MRR'] = RetrievalMetrics.mrr(retrieved_lists, relevant_sets)

        # Precision, Recall, F1, NDCG @ K
        for k in k_values:
            precisions = [
                RetrievalMetrics.precision_at_k(ret, rel, k)
                for ret, rel in zip(retrieved_lists, relevant_sets)
            ]
            recalls = [
                RetrievalMetrics.recall_at_k(ret, rel, k)
                for ret, rel in zip(retrieved_lists, relevant_sets)
            ]
            f1s = [
                RetrievalMetrics.f1_at_k(ret, rel, k)
                for ret, rel in zip(retrieved_lists, relevant_sets)
            ]
            ndcgs = [
                RetrievalMetrics.ndcg_at_k(ret, rel, k)
                for ret, rel in zip(retrieved_lists, relevant_sets)
            ]

            results[f'Precision@{k}'] = np.mean(precisions)
            results[f'Recall@{k}'] = np.mean(recalls)
            results[f'F1@{k}'] = np.mean(f1s)
            results[f'NDCG@{k}'] = np.mean(ndcgs)

        return results

# ä½¿ç”¨ç¤ºä¾‹
retrieved = [
    ['doc1', 'doc2', 'doc3', 'doc4', 'doc5'],  # æŸ¥è¯¢1çš„æ£€ç´¢ç»“æœ
    ['doc2', 'doc1', 'doc5', 'doc6', 'doc7']   # æŸ¥è¯¢2çš„æ£€ç´¢ç»“æœ
]
relevant = [
    {'doc1', 'doc3'},  # æŸ¥è¯¢1çš„ç›¸å…³æ–‡æ¡£
    {'doc2', 'doc5'}   # æŸ¥è¯¢2çš„ç›¸å…³æ–‡æ¡£
]

metrics = RetrievalMetrics.evaluate_all(retrieved, relevant, k_values=[1, 5, 10])
print(metrics)
```

#### ç”Ÿæˆè´¨é‡æŒ‡æ ‡

```python
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import openai

class GenerationMetrics:
    """ç”Ÿæˆè´¨é‡è¯„ä¼°æŒ‡æ ‡"""

    def __init__(self):
        self.rouge_scorer = rouge_scorer.RougeScorer(
            ['rouge1', 'rouge2', 'rougeL'],
            use_stemmer=True
        )
        self.smoothing = SmoothingFunction().method1

    def rouge_score(self, generated: str, reference: str) -> Dict[str, float]:
        """
        ROUGEåˆ†æ•°ï¼šè¯„ä¼°ç”Ÿæˆæ–‡æœ¬çš„æ‘˜è¦è´¨é‡

        Args:
            generated: ç”Ÿæˆçš„æ–‡æœ¬
            reference: å‚è€ƒæ–‡æœ¬

        Returns:
            ROUGE-1, ROUGE-2, ROUGE-Låˆ†æ•°
        """
        scores = self.rouge_scorer.score(reference, generated)
        return {
            'rouge1': scores['rouge1'].fmeasure,
            'rouge2': scores['rouge2'].fmeasure,
            'rougeL': scores['rougeL'].fmeasure
        }

    def bleu_score(self, generated: str, reference: str) -> float:
        """
        BLEUåˆ†æ•°ï¼šè¯„ä¼°ç”Ÿæˆæ–‡æœ¬ä¸å‚è€ƒæ–‡æœ¬çš„ç›¸ä¼¼åº¦

        Args:
            generated: ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆåˆ†è¯åçš„åˆ—è¡¨ï¼‰
            reference: å‚è€ƒæ–‡æœ¬ï¼ˆåˆ†è¯åçš„åˆ—è¡¨ï¼‰

        Returns:
            BLEUåˆ†æ•°
        """
        # ç®€å•ç¤ºä¾‹ï¼Œå®é™…ä½¿ç”¨éœ€è¦åˆ†è¯
        generated_tokens = generated.split()
        reference_tokens = [reference.split()]

        return sentence_bleu(
            reference_tokens,
            generated_tokens,
            smoothing_function=self.smoothing
        )

    def semantic_similarity(
        self,
        generated: str,
        reference: str,
        embedding_model
    ) -> float:
        """
        è¯­ä¹‰ç›¸ä¼¼åº¦ï¼šä½¿ç”¨åµŒå…¥å‘é‡è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦

        Args:
            generated: ç”Ÿæˆçš„æ–‡æœ¬
            reference: å‚è€ƒæ–‡æœ¬
            embedding_model: åµŒå…¥æ¨¡å‹

        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦åˆ†æ•°
        """
        gen_emb = embedding_model.encode(generated)
        ref_emb = embedding_model.encode(reference)

        similarity = np.dot(gen_emb, ref_emb) / (
            np.linalg.norm(gen_emb) * np.linalg.norm(ref_emb)
        )

        return float(similarity)

    def faithfulness_score(
        self,
        generated: str,
        source_docs: List[str],
        llm
    ) -> float:
        """
        å¿ å®åº¦åˆ†æ•°ï¼šè¯„ä¼°ç”Ÿæˆå†…å®¹æ˜¯å¦å¿ å®äºæºæ–‡æ¡£

        Args:
            generated: ç”Ÿæˆçš„æ–‡æœ¬
            source_docs: æºæ–‡æ¡£åˆ—è¡¨
            llm: LLMæ¨¡å‹ï¼ˆç”¨äºéªŒè¯ï¼‰

        Returns:
            å¿ å®åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        prompt = f"""
        è¯„ä¼°ä»¥ä¸‹ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦å¿ å®äºæä¾›çš„æºæ–‡æ¡£ã€‚

        æºæ–‡æ¡£:
        {chr(10).join([f"{i+1}. {doc[:200]}" for i, doc in enumerate(source_docs[:3])])}

        ç”Ÿæˆçš„ç­”æ¡ˆ:
        {generated}

        è¯·è¯„ä¼°ï¼š
        1. ç­”æ¡ˆä¸­çš„æ‰€æœ‰äº‹å®æ˜¯å¦éƒ½åœ¨æºæ–‡æ¡£ä¸­ï¼Ÿ
        2. ç­”æ¡ˆæ˜¯å¦æ·»åŠ äº†æºæ–‡æ¡£ä¸­æ²¡æœ‰çš„ä¿¡æ¯ï¼Ÿ
        3. ç­”æ¡ˆæ˜¯å¦æ›²è§£äº†æºæ–‡æ¡£çš„æ„æ€ï¼Ÿ

        è¿”å›0-1ä¹‹é—´çš„åˆ†æ•°ï¼ˆ1è¡¨ç¤ºå®Œå…¨å¿ å®ï¼‰ï¼š
        """

        response = llm.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )

        try:
            score = float(response.choices[0].message.content)
            return max(0.0, min(1.0, score))
        except:
            return 0.5

    def answer_relevancy(
        self,
        query: str,
        answer: str,
        embedding_model
    ) -> float:
        """
        ç­”æ¡ˆç›¸å…³æ€§ï¼šè¯„ä¼°ç­”æ¡ˆä¸æŸ¥è¯¢çš„ç›¸å…³æ€§

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            answer: ç”Ÿæˆçš„ç­”æ¡ˆ
            embedding_model: åµŒå…¥æ¨¡å‹

        Returns:
            ç›¸å…³æ€§åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        query_emb = embedding_model.encode(query)
        answer_emb = embedding_model.encode(answer)

        similarity = np.dot(query_emb, answer_emb) / (
            np.linalg.norm(query_emb) * np.linalg.norm(answer_emb)
        )

        return float(similarity)
```

### 7.2 åŸºå‡†æµ‹è¯•

ä½¿ç”¨æ ‡å‡†åŸºå‡†æµ‹è¯•è¯„ä¼°RAGç³»ç»Ÿçš„æ€§èƒ½ã€‚

#### MTEBåŸºå‡†æµ‹è¯•

```python
from mteb import MTEB
from sentence_transformers import SentenceTransformer

class RAGEvaluator:
    """RAGç³»ç»Ÿè¯„ä¼°å™¨"""

    def __init__(self, embedding_model, retriever):
        self.embedding_model = embedding_model
        self.retriever = retriever

    def evaluate_on_mteb(self, task_name: str = "Retrieval"):
        """
        åœ¨MTEBåŸºå‡†ä¸Šè¯„ä¼°åµŒå…¥æ¨¡å‹

        Args:
            task_name: ä»»åŠ¡åç§°ï¼ˆå¦‚"Retrieval", "Clustering"ç­‰ï¼‰
        """
        evaluation = MTEB(task_types=[task_name])

        results = evaluation.run(
            self.embedding_model,
            output_folder=f"results/{task_name}",
            eval_splits=["test"]
        )

        return results

    def evaluate_on_beir(
        self,
        dataset_name: str = "nfcorpus",
        split: str = "test"
    ) -> Dict[str, float]:
        """
        åœ¨BEIRåŸºå‡†ä¸Šè¯„ä¼°æ£€ç´¢ç³»ç»Ÿ

        Args:
            dataset_name: BEIRæ•°æ®é›†åç§°
            split: æ•°æ®é›†åˆ†å‰²ï¼ˆtrain/test/devï¼‰

        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        from beir import util, dataset
        from beir.retrieval.evaluation import EvaluateRetrieval
        from beir.retrieval.search.dense import DenseRetrievalExactSearch

        # ä¸‹è½½æ•°æ®é›†
        url = f"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset_name}.zip"
        data_path = util.download_and_unzip(url, "datasets")

        # åŠ è½½æ•°æ®é›†
        corpus, queries, qrels = dataset.load_beir_data(data_path, split=split)

        # åˆ›å»ºæ£€ç´¢å™¨
        model = DenseRetrievalExactSearch(
            self.embedding_model,
            batch_size=128
        )

        # æ‰§è¡Œæ£€ç´¢
        retriever = EvaluateRetrieval(model, score_function="cos_sim")
        results = retriever.retrieve(corpus, queries)

        # è¯„ä¼°
        metrics = retriever.evaluate(qrels, results, k_values=[1, 3, 5, 10])

        return metrics
```

#### è‡ªå®šä¹‰è¯„ä¼°æ¡†æ¶

```python
class CustomRAGEvaluator:
    """è‡ªå®šä¹‰RAGè¯„ä¼°æ¡†æ¶"""

    def __init__(self, rag_system):
        self.rag_system = rag_system

    def evaluate_dataset(
        self,
        test_set: List[Dict[str, Any]],
        metrics_config: Dict[str, bool] = None
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°RAGç³»ç»Ÿåœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°

        Args:
            test_set: æµ‹è¯•é›†ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«query, expected_answer, relevant_docs
            metrics_config: è¯„ä¼°æŒ‡æ ‡é…ç½®

        Returns:
            è¯„ä¼°ç»“æœ
        """
        if metrics_config is None:
            metrics_config = {
                'retrieval': True,
                'generation': True,
                'end_to_end': True
            }

        results = {
            'retrieval_metrics': {},
            'generation_metrics': {},
            'end_to_end_metrics': {}
        }

        retrieval_results = []
        generation_results = []

        for sample in test_set:
            query = sample['query']
            expected_answer = sample['expected_answer']
            relevant_docs = set(sample['relevant_docs'])

            # æ‰§è¡ŒRAGæŸ¥è¯¢
            rag_result = self.rag_system.query(query)

            # è¯„ä¼°æ£€ç´¢è´¨é‡
            if metrics_config['retrieval']:
                retrieved_docs = [doc['id'] for doc in rag_result['retrieved_docs']]
                retrieval_results.append({
                    'retrieved': retrieved_docs,
                    'relevant': relevant_docs
                })

            # è¯„ä¼°ç”Ÿæˆè´¨é‡
            if metrics_config['generation']:
                gen_metrics = GenerationMetrics()
                rouge = gen_metrics.rouge_score(
                    rag_result['answer'],
                    expected_answer
                )
                generation_results.append({
                    'rouge1': rouge['rouge1'],
                    'rouge2': rouge['rouge2'],
                    'rougeL': rouge['rougeL']
                })

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        if metrics_config['retrieval'] and retrieval_results:
            retrieved_lists = [r['retrieved'] for r in retrieval_results]
            relevant_sets = [r['relevant'] for r in retrieval_results]
            results['retrieval_metrics'] = RetrievalMetrics.evaluate_all(
                retrieved_lists,
                relevant_sets
            )

        if metrics_config['generation'] and generation_results:
            results['generation_metrics'] = {
                'rouge1': np.mean([r['rouge1'] for r in generation_results]),
                'rouge2': np.mean([r['rouge2'] for r in generation_results]),
                'rougeL': np.mean([r['rougeL'] for r in generation_results])
            }

        return results
```

### 7.3 æŒç»­ä¼˜åŒ–

å»ºç«‹æŒç»­ä¼˜åŒ–æœºåˆ¶ï¼Œä¸æ–­æå‡RAGç³»ç»Ÿæ€§èƒ½ã€‚

#### A/Bæµ‹è¯•æ¡†æ¶

```python
class RAGABTest:
    """RAGç³»ç»ŸA/Bæµ‹è¯•æ¡†æ¶"""

    def __init__(self, system_a, system_b, traffic_split: float = 0.5):
        self.system_a = system_a
        self.system_b = system_b
        self.traffic_split = traffic_split  # Aç»„æµé‡æ¯”ä¾‹
        self.results_a = []
        self.results_b = []

    def query(self, query: str, user_id: str) -> Dict[str, Any]:
        """
        æ ¹æ®ç”¨æˆ·IDåˆ†é…æµé‡ï¼Œæ‰§è¡ŒæŸ¥è¯¢

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            user_id: ç”¨æˆ·IDï¼ˆç”¨äºæµé‡åˆ†é…ï¼‰

        Returns:
            æŸ¥è¯¢ç»“æœå’Œå®éªŒç»„ä¿¡æ¯
        """
        # åŸºäºç”¨æˆ·IDçš„å“ˆå¸Œåˆ†é…æµé‡
        import hashlib
        user_hash = int(hashlib.md5(user_id.encode()).hexdigest(), 16)
        is_group_a = (user_hash % 100) < (self.traffic_split * 100)

        if is_group_a:
            result = self.system_a.query(query)
            result['experiment_group'] = 'A'
            self.results_a.append({
                'query': query,
                'user_id': user_id,
                'result': result
            })
        else:
            result = self.system_b.query(query)
            result['experiment_group'] = 'B'
            self.results_b.append({
                'query': query,
                'user_id': user_id,
                'result': result
            })

        return result

    def compare_results(self) -> Dict[str, Any]:
        """æ¯”è¾ƒA/Bä¸¤ç»„çš„ç»“æœ"""
        # è¿™é‡Œå¯ä»¥è®¡ç®—å„ç§æŒ‡æ ‡
        # ä¾‹å¦‚ï¼šå“åº”æ—¶é—´ã€ç”¨æˆ·æ»¡æ„åº¦ã€ç‚¹å‡»ç‡ç­‰

        return {
            'group_a_count': len(self.results_a),
            'group_b_count': len(self.results_b),
            'metrics': {
                # å¯ä»¥æ·»åŠ å…·ä½“çš„æ¯”è¾ƒæŒ‡æ ‡
            }
        }
```

#### åé¦ˆå¾ªç¯ä¼˜åŒ–

```python
class FeedbackLoop:
    """åé¦ˆå¾ªç¯ä¼˜åŒ–æœºåˆ¶"""

    def __init__(self, rag_system, feedback_store):
        self.rag_system = rag_system
        self.feedback_store = feedback_store  # åé¦ˆå­˜å‚¨ï¼ˆå¦‚æ•°æ®åº“ï¼‰

    def collect_feedback(
        self,
        query: str,
        answer: str,
        retrieved_docs: List[Dict],
        user_feedback: Dict[str, Any]
    ):
        """
        æ”¶é›†ç”¨æˆ·åé¦ˆ

        Args:
            query: æŸ¥è¯¢
            answer: ç”Ÿæˆçš„ç­”æ¡ˆ
            retrieved_docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£
            user_feedback: ç”¨æˆ·åé¦ˆï¼ˆå¦‚è¯„åˆ†ã€ç›¸å…³æ€§æ ‡æ³¨ç­‰ï¼‰
        """
        feedback_record = {
            'query': query,
            'answer': answer,
            'retrieved_docs': retrieved_docs,
            'user_rating': user_feedback.get('rating'),
            'is_relevant': user_feedback.get('is_relevant'),
            'timestamp': datetime.now()
        }

        self.feedback_store.save(feedback_record)

    def analyze_feedback(self, time_window: int = 7) -> Dict[str, Any]:
        """
        åˆ†æåé¦ˆæ•°æ®ï¼Œè¯†åˆ«æ”¹è¿›ç‚¹

        Args:
            time_window: åˆ†ææ—¶é—´çª—å£ï¼ˆå¤©ï¼‰
        """
        # è·å–æ—¶é—´çª—å£å†…çš„åé¦ˆ
        feedbacks = self.feedback_store.get_recent(time_window)

        # åˆ†æä½è¯„åˆ†æŸ¥è¯¢
        low_rated = [f for f in feedbacks if f.get('user_rating', 5) < 3]

        # åˆ†æä¸ç›¸å…³æ–‡æ¡£
        irrelevant_docs = [
            f for f in feedbacks
            if not f.get('is_relevant', True)
        ]

        # è¯†åˆ«é—®é¢˜æ¨¡å¼
        problems = {
            'low_rated_queries': low_rated[:10],  # Top 10é—®é¢˜æŸ¥è¯¢
            'irrelevant_docs_count': len(irrelevant_docs),
            'average_rating': np.mean([f.get('user_rating', 5) for f in feedbacks])
        }

        return problems

    def optimize_based_on_feedback(self, problems: Dict[str, Any]):
        """
        åŸºäºåé¦ˆä¼˜åŒ–ç³»ç»Ÿ

        Args:
            problems: åˆ†æå‡ºçš„é—®é¢˜
        """
        # æ ¹æ®é—®é¢˜è°ƒæ•´æ£€ç´¢ç­–ç•¥
        if problems['irrelevant_docs_count'] > 100:
            # æé«˜æ£€ç´¢é˜ˆå€¼
            self.rag_system.retriever.score_threshold *= 1.1

        # ä¼˜åŒ–æŸ¥è¯¢é‡å†™
        if problems['average_rating'] < 3.5:
            # å¯ç”¨æ›´ç§¯æçš„æŸ¥è¯¢é‡å†™
            self.rag_system.query_rewriter.enabled = True

        # è°ƒæ•´é‡æ’åºæƒé‡
        # æ ¹æ®åé¦ˆè°ƒæ•´é‡æ’åºæ¨¡å‹çš„æƒé‡
        if problems['low_rated_queries']:
            # åˆ†æä½è¯„åˆ†æŸ¥è¯¢çš„æ¨¡å¼ï¼Œè°ƒæ•´é‡æ’åºç­–ç•¥
            pass
```

---

## 6. å¤šæ¨¡æ€RAG

### 6.1 å›¾æ–‡æ··åˆæ£€ç´¢

å¤šæ¨¡æ€RAGæ”¯æŒæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰å¤šç§æ¨¡æ€çš„æ··åˆæ£€ç´¢å’Œç”Ÿæˆã€‚

#### å¤šæ¨¡æ€æ£€ç´¢æ¶æ„

```python
from typing import List, Dict, Union
import numpy as np
from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel

class MultimodalRAG:
    """å¤šæ¨¡æ€RAGç³»ç»Ÿ"""

    def __init__(
        self,
        text_embedding_model,
        image_embedding_model,
        vector_store,
        llm
    ):
        self.text_embedding_model = text_embedding_model
        self.image_embedding_model = image_embedding_model
        self.vector_store = vector_store
        self.llm = llm

        # CLIPæ¨¡å‹ç”¨äºå›¾æ–‡å¯¹é½
        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

    def retrieve_multimodal(
        self,
        query: Union[str, Image.Image],
        top_k: int = 10,
        modalities: List[str] = ['text', 'image']
    ) -> Dict[str, List[Dict]]:
        """
        å¤šæ¨¡æ€æ£€ç´¢

        Args:
            query: æŸ¥è¯¢ï¼ˆæ–‡æœ¬æˆ–å›¾åƒï¼‰
            top_k: è¿”å›Top Kç»“æœ
            modalities: æ£€ç´¢çš„æ¨¡æ€åˆ—è¡¨

        Returns:
            {modality: [æ£€ç´¢ç»“æœ]}
        """
        results = {}

        # æ–‡æœ¬æŸ¥è¯¢
        if isinstance(query, str):
            query_text = query

            # æ–‡æœ¬æ£€ç´¢
            if 'text' in modalities:
                text_results = self._retrieve_text(query_text, top_k)
                results['text'] = text_results

            # å›¾åƒæ£€ç´¢ï¼ˆä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢ï¼‰
            if 'image' in modalities:
                image_results = self._retrieve_images_by_text(query_text, top_k)
                results['image'] = image_results

        # å›¾åƒæŸ¥è¯¢
        elif isinstance(query, Image.Image):
            query_image = query

            # å›¾åƒæ£€ç´¢
            if 'image' in modalities:
                image_results = self._retrieve_images_by_image(query_image, top_k)
                results['image'] = image_results

            # æ–‡æœ¬æ£€ç´¢ï¼ˆä½¿ç”¨å›¾åƒæŸ¥è¯¢ï¼‰
            if 'text' in modalities:
                text_results = self._retrieve_text_by_image(query_image, top_k)
                results['text'] = text_results

        return results

    def _retrieve_text(self, query: str, top_k: int) -> List[Dict]:
        """æ–‡æœ¬æ£€ç´¢"""
        docs = self.vector_store.similarity_search(query, k=top_k)
        return [
            {
                'content': doc.page_content,
                'metadata': doc.metadata,
                'modality': 'text',
                'score': 0.9
            }
            for doc in docs
        ]

    def _retrieve_images_by_text(self, query: str, top_k: int) -> List[Dict]:
        """ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢æ£€ç´¢å›¾åƒ"""
        # ä½¿ç”¨CLIPå°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡
        inputs = self.clip_processor(text=[query], return_tensors="pt", padding=True)
        text_emb = self.clip_model.get_text_features(**inputs)
        text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)

        # åœ¨å›¾åƒå‘é‡åº“ä¸­æœç´¢
        # è¿™é‡Œå‡è®¾å›¾åƒå‘é‡å·²å­˜å‚¨åœ¨vector_storeä¸­
        image_results = self.vector_store.similarity_search_by_vector(
            text_emb[0].detach().numpy(),
            k=top_k
        )

        return [
            {
                'content': result['image_path'],
                'metadata': result['metadata'],
                'modality': 'image',
                'score': float(result['similarity'])
            }
            for result in image_results
        ]

    def _retrieve_images_by_image(self, query_image: Image.Image, top_k: int) -> List[Dict]:
        """ä½¿ç”¨å›¾åƒæŸ¥è¯¢æ£€ç´¢å›¾åƒ"""
        # ä½¿ç”¨CLIPå°†å›¾åƒç¼–ç ä¸ºå‘é‡
        inputs = self.clip_processor(images=[query_image], return_tensors="pt")
        image_emb = self.clip_model.get_image_features(**inputs)
        image_emb = image_emb / image_emb.norm(dim=-1, keepdim=True)

        # åœ¨å›¾åƒå‘é‡åº“ä¸­æœç´¢
        image_results = self.vector_store.similarity_search_by_vector(
            image_emb[0].detach().numpy(),
            k=top_k
        )

        return [
            {
                'content': result['image_path'],
                'metadata': result['metadata'],
                'modality': 'image',
                'score': float(result['similarity'])
            }
            for result in image_results
        ]

    def _retrieve_text_by_image(self, query_image: Image.Image, top_k: int) -> List[Dict]:
        """ä½¿ç”¨å›¾åƒæŸ¥è¯¢æ£€ç´¢æ–‡æœ¬"""
        # ä½¿ç”¨CLIPå°†å›¾åƒç¼–ç ä¸ºå‘é‡
        inputs = self.clip_processor(images=[query_image], return_tensors="pt")
        image_emb = self.clip_model.get_image_features(**inputs)
        image_emb = image_emb / image_emb.norm(dim=-1, keepdim=True)

        # åœ¨æ–‡æœ¬å‘é‡åº“ä¸­æœç´¢ï¼ˆä½¿ç”¨å›¾åƒå‘é‡ï¼‰
        text_results = self.vector_store.similarity_search_by_vector(
            image_emb[0].detach().numpy(),
            k=top_k
        )

        return [
            {
                'content': result['text'],
                'metadata': result['metadata'],
                'modality': 'text',
                'score': float(result['similarity'])
            }
            for result in text_results
        ]

    def generate_multimodal_answer(
        self,
        query: Union[str, Image.Image],
        retrieved_items: Dict[str, List[Dict]]
    ) -> str:
        """ç”Ÿæˆå¤šæ¨¡æ€ç­”æ¡ˆ"""
        # æ„å»ºå¤šæ¨¡æ€ä¸Šä¸‹æ–‡
        context_parts = []

        # æ–‡æœ¬ä¸Šä¸‹æ–‡
        if 'text' in retrieved_items:
            text_context = "\n\n".join([
                f"æ–‡æœ¬ç‰‡æ®µ {i+1}: {item['content']}"
                for i, item in enumerate(retrieved_items['text'][:5])
            ])
            context_parts.append(f"ç›¸å…³æ–‡æœ¬:\n{text_context}")

        # å›¾åƒä¸Šä¸‹æ–‡ï¼ˆæè¿°ï¼‰
        if 'image' in retrieved_items:
            image_descriptions = []
            for item in retrieved_items['image'][:5]:
                # ä½¿ç”¨å›¾åƒæè¿°æ¨¡å‹ç”Ÿæˆæè¿°
                description = self._describe_image(item['content'])
                image_descriptions.append(f"å›¾åƒ {len(image_descriptions)+1}: {description}")
            context_parts.append(f"ç›¸å…³å›¾åƒ:\n{chr(10).join(image_descriptions)}")

        context = "\n\n".join(context_parts)

        # ç”Ÿæˆç­”æ¡ˆ
        query_text = query if isinstance(query, str) else "è¿™å¼ å›¾ç‰‡çš„å†…å®¹"

        prompt = f"""
        åŸºäºä»¥ä¸‹å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å›ç­”æŸ¥è¯¢ã€‚

        æŸ¥è¯¢: {query_text}

        {context}

        è¯·ç”Ÿæˆä¸€ä¸ªå‡†ç¡®ã€å®Œæ•´çš„ç­”æ¡ˆï¼Œå¦‚æœæ¶‰åŠå›¾åƒå†…å®¹ï¼Œè¯·è¯¦ç»†æè¿°ã€‚
        """

        response = self.llm.chat.completions.create(
            model="gpt-4-vision-preview",  # æ”¯æŒå¤šæ¨¡æ€çš„æ¨¡å‹
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )

        return response.choices[0].message.content

    def _describe_image(self, image_path: str) -> str:
        """ç”Ÿæˆå›¾åƒæè¿°"""
        # ä½¿ç”¨å›¾åƒæè¿°æ¨¡å‹ï¼ˆå¦‚BLIPã€GPT-4Vç­‰ï¼‰
        # è¿™é‡Œç®€åŒ–å¤„ç†
        return f"å›¾åƒå†…å®¹æè¿°: {image_path}"
```

### 6.2 å¤šæ¨¡æ€å‘é‡è¡¨ç¤º

#### CLIPæ¨¡å‹é›†æˆ

```python
class CLIPMultimodalEncoder:
    """CLIPå¤šæ¨¡æ€ç¼–ç å™¨"""

    def __init__(self, model_name: str = "openai/clip-vit-base-patch32"):
        self.model = CLIPModel.from_pretrained(model_name)
        self.processor = CLIPProcessor.from_pretrained(model_name)

    def encode_text(self, texts: List[str]) -> np.ndarray:
        """ç¼–ç æ–‡æœ¬ä¸ºå‘é‡"""
        inputs = self.processor(text=texts, return_tensors="pt", padding=True)
        with torch.no_grad():
            text_features = self.model.get_text_features(**inputs)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        return text_features.numpy()

    def encode_images(self, images: List[Image.Image]) -> np.ndarray:
        """ç¼–ç å›¾åƒä¸ºå‘é‡"""
        inputs = self.processor(images=images, return_tensors="pt")
        with torch.no_grad():
            image_features = self.model.get_image_features(**inputs)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        return image_features.numpy()

    def compute_similarity(
        self,
        text_emb: np.ndarray,
        image_emb: np.ndarray
    ) -> float:
        """è®¡ç®—æ–‡æœ¬å’Œå›¾åƒçš„ç›¸ä¼¼åº¦"""
        similarity = np.dot(text_emb, image_emb.T)
        return float(similarity[0][0])
```

#### å¤šæ¨¡æ€å‘é‡å­˜å‚¨

```python
import psycopg2
from pgvector.psycopg2 import register_vector

class MultimodalVectorStore:
    """å¤šæ¨¡æ€å‘é‡å­˜å‚¨"""

    def __init__(self, db_config, embedding_dim: int = 512):
        self.conn = psycopg2.connect(**db_config)
        register_vector(self.conn)
        self.cursor = self.conn.cursor()
        self.embedding_dim = embedding_dim
        self._init_schema()

    def _init_schema(self):
        """åˆå§‹åŒ–æ•°æ®åº“æ¨¡å¼"""
        self.cursor.execute("CREATE EXTENSION IF NOT EXISTS vector;")

        # å¤šæ¨¡æ€æ–‡æ¡£è¡¨
        self.cursor.execute(f"""
            CREATE TABLE IF NOT EXISTS multimodal_documents (
                id SERIAL PRIMARY KEY,
                content TEXT,
                content_type VARCHAR(20),  -- 'text' or 'image'
                content_path TEXT,  -- æ–‡ä»¶è·¯å¾„ï¼ˆå›¾åƒï¼‰
                embedding vector({self.embedding_dim}),
                metadata JSONB,
                created_at TIMESTAMPTZ DEFAULT NOW()
            );
        """)

        # åˆ›å»ºå‘é‡ç´¢å¼•
        self.cursor.execute("""
            CREATE INDEX IF NOT EXISTS multimodal_documents_embedding_idx
            ON multimodal_documents
            USING hnsw (embedding vector_cosine_ops);
        """)

        self.conn.commit()

    def add_text(self, text: str, embedding: np.ndarray, metadata: Dict = None):
        """æ·»åŠ æ–‡æœ¬æ–‡æ¡£"""
        self.cursor.execute("""
            INSERT INTO multimodal_documents (content, content_type, embedding, metadata)
            VALUES (%s, 'text', %s, %s);
        """, (text, embedding.tolist(), json.dumps(metadata or {})))
        self.conn.commit()

    def add_image(self, image_path: str, embedding: np.ndarray, metadata: Dict = None):
        """æ·»åŠ å›¾åƒæ–‡æ¡£"""
        self.cursor.execute("""
            INSERT INTO multimodal_documents (content_path, content_type, embedding, metadata)
            VALUES (%s, 'image', %s, %s);
        """, (image_path, embedding.tolist(), json.dumps(metadata or {})))
        self.conn.commit()

    def search(
        self,
        query_embedding: np.ndarray,
        content_type: str = None,
        top_k: int = 10
    ) -> List[Dict]:
        """å¤šæ¨¡æ€æœç´¢"""
        type_filter = f"AND content_type = '{content_type}'" if content_type else ""

        self.cursor.execute(f"""
            SELECT id, content, content_path, content_type, metadata,
                   1 - (embedding <=> %s::vector) AS similarity
            FROM multimodal_documents
            WHERE 1=1 {type_filter}
            ORDER BY embedding <=> %s::vector
            LIMIT %s;
        """, (query_embedding.tolist(), query_embedding.tolist(), top_k))

        results = []
        for row in self.cursor.fetchall():
            results.append({
                'id': row[0],
                'content': row[1],
                'content_path': row[2],
                'content_type': row[3],
                'metadata': row[4],
                'similarity': float(row[5])
            })

        return results
```

### 6.3 è·¨æ¨¡æ€å¯¹é½

#### è·¨æ¨¡æ€æ£€ç´¢ä¼˜åŒ–

```python
class CrossModalAlignment:
    """è·¨æ¨¡æ€å¯¹é½ä¼˜åŒ–"""

    def __init__(self, clip_model, alignment_model=None):
        self.clip_model = clip_model
        self.alignment_model = alignment_model

    def align_text_image(
        self,
        text_emb: np.ndarray,
        image_emb: np.ndarray,
        alignment_weight: float = 0.5
    ) -> np.ndarray:
        """
        å¯¹é½æ–‡æœ¬å’Œå›¾åƒå‘é‡

        Args:
            text_emb: æ–‡æœ¬å‘é‡
            image_emb: å›¾åƒå‘é‡
            alignment_weight: å¯¹é½æƒé‡

        Returns:
            å¯¹é½åçš„å‘é‡
        """
        # è®¡ç®—å¯¹é½å‘é‡
        alignment_vector = alignment_weight * text_emb + (1 - alignment_weight) * image_emb

        # å½’ä¸€åŒ–
        alignment_vector = alignment_vector / np.linalg.norm(alignment_vector)

        return alignment_vector

    def cross_modal_retrieval(
        self,
        query_embedding: np.ndarray,
        target_modality: str,
        vector_store: MultimodalVectorStore,
        top_k: int = 10
    ) -> List[Dict]:
        """
        è·¨æ¨¡æ€æ£€ç´¢

        Args:
            query_embedding: æŸ¥è¯¢å‘é‡ï¼ˆå¯ä»¥æ˜¯æ–‡æœ¬æˆ–å›¾åƒï¼‰
            target_modality: ç›®æ ‡æ¨¡æ€ï¼ˆ'text' or 'image'ï¼‰
            vector_store: å‘é‡å­˜å‚¨
            top_k: è¿”å›Top Kç»“æœ
        """
        # å¦‚æœæŸ¥è¯¢å’Œç›®æ ‡æ¨¡æ€ä¸åŒï¼Œè¿›è¡Œå¯¹é½
        if target_modality == 'image':
            # æ–‡æœ¬æŸ¥è¯¢å›¾åƒï¼šä½¿ç”¨CLIPå¯¹é½
            aligned_embedding = self._align_for_image_retrieval(query_embedding)
        else:
            # å›¾åƒæŸ¥è¯¢æ–‡æœ¬ï¼šä½¿ç”¨CLIPå¯¹é½
            aligned_embedding = self._align_for_text_retrieval(query_embedding)

        # æ£€ç´¢
        results = vector_store.search(
            aligned_embedding,
            content_type=target_modality,
            top_k=top_k
        )

        return results

    def _align_for_image_retrieval(self, text_emb: np.ndarray) -> np.ndarray:
        """ä¸ºå›¾åƒæ£€ç´¢å¯¹é½æ–‡æœ¬å‘é‡"""
        # ä½¿ç”¨CLIPçš„å¯¹é½èƒ½åŠ›
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„å¯¹é½æ¨¡å‹
        return text_emb

    def _align_for_text_retrieval(self, image_emb: np.ndarray) -> np.ndarray:
        """ä¸ºæ–‡æœ¬æ£€ç´¢å¯¹é½å›¾åƒå‘é‡"""
        # ä½¿ç”¨CLIPçš„å¯¹é½èƒ½åŠ›
        return image_emb
```

#### å¤šæ¨¡æ€èåˆæ£€ç´¢

```python
class MultimodalFusionRetriever:
    """å¤šæ¨¡æ€èåˆæ£€ç´¢å™¨"""

    def __init__(
        self,
        text_retriever,
        image_retriever,
        fusion_strategy: str = 'weighted'
    ):
        self.text_retriever = text_retriever
        self.image_retriever = image_retriever
        self.fusion_strategy = fusion_strategy

    def fused_retrieve(
        self,
        query: Union[str, Image.Image],
        top_k: int = 10,
        text_weight: float = 0.6,
        image_weight: float = 0.4
    ) -> List[Dict]:
        """
        èåˆæ£€ç´¢

        Args:
            query: æŸ¥è¯¢ï¼ˆæ–‡æœ¬æˆ–å›¾åƒï¼‰
            top_k: è¿”å›Top Kç»“æœ
            text_weight: æ–‡æœ¬æ£€ç´¢æƒé‡
            image_weight: å›¾åƒæ£€ç´¢æƒé‡
        """
        # æ–‡æœ¬æ£€ç´¢
        if isinstance(query, str):
            text_results = self.text_retriever.retrieve(query, top_k=top_k * 2)
            # ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢å›¾åƒ
            image_results = self.image_retriever.retrieve_by_text(query, top_k=top_k * 2)
        else:
            # å›¾åƒæŸ¥è¯¢
            image_results = self.image_retriever.retrieve_by_image(query, top_k=top_k * 2)
            # ä½¿ç”¨å›¾åƒæŸ¥è¯¢æ–‡æœ¬
            text_results = self.text_retriever.retrieve_by_image(query, top_k=top_k * 2)

        # èåˆç»“æœ
        if self.fusion_strategy == 'weighted':
            return self._weighted_fusion(text_results, image_results, text_weight, image_weight, top_k)
        elif self.fusion_strategy == 'rrf':
            return self._rrf_fusion(text_results, image_results, top_k)
        else:
            return self._simple_merge(text_results, image_results, top_k)

    def _weighted_fusion(
        self,
        text_results: List[Dict],
        image_results: List[Dict],
        text_weight: float,
        image_weight: float,
        top_k: int
    ) -> List[Dict]:
        """åŠ æƒèåˆ"""
        all_items = {}

        # æ·»åŠ æ–‡æœ¬ç»“æœ
        for item in text_results:
            item_id = item.get('id', f"text_{len(all_items)}")
            if item_id not in all_items:
                all_items[item_id] = item
                all_items[item_id]['final_score'] = 0.0
            all_items[item_id]['final_score'] += item['score'] * text_weight

        # æ·»åŠ å›¾åƒç»“æœ
        for item in image_results:
            item_id = item.get('id', f"image_{len(all_items)}")
            if item_id not in all_items:
                all_items[item_id] = item
                all_items[item_id]['final_score'] = 0.0
            all_items[item_id]['final_score'] += item['score'] * image_weight

        # æ’åº
        sorted_items = sorted(
            all_items.values(),
            key=lambda x: x['final_score'],
            reverse=True
        )

        return sorted_items[:top_k]

    def _rrf_fusion(
        self,
        text_results: List[Dict],
        image_results: List[Dict],
        top_k: int,
        k: int = 60
    ) -> List[Dict]:
        """RRFèåˆ"""
        all_items = {}

        # æ–‡æœ¬ç»“æœæ’å
        for rank, item in enumerate(text_results, 1):
            item_id = item.get('id', f"text_{rank}")
            if item_id not in all_items:
                all_items[item_id] = item
                all_items[item_id]['rrf_score'] = 0.0
            all_items[item_id]['rrf_score'] += 1.0 / (k + rank)

        # å›¾åƒç»“æœæ’å
        for rank, item in enumerate(image_results, 1):
            item_id = item.get('id', f"image_{rank}")
            if item_id not in all_items:
                all_items[item_id] = item
                all_items[item_id]['rrf_score'] = 0.0
            all_items[item_id]['rrf_score'] += 1.0 / (k + rank)

        # æ’åº
        sorted_items = sorted(
            all_items.values(),
            key=lambda x: x['rrf_score'],
            reverse=True
        )

        return sorted_items[:top_k]

    def _simple_merge(
        self,
        text_results: List[Dict],
        image_results: List[Dict],
        top_k: int
    ) -> List[Dict]:
        """ç®€å•åˆå¹¶"""
        merged = text_results + image_results
        # å»é‡
        seen = set()
        unique = []
        for item in merged:
            item_id = item.get('id', hash(item.get('content', '')))
            if item_id not in seen:
                seen.add(item_id)
                unique.append(item)

        # æŒ‰åˆ†æ•°æ’åº
        sorted_items = sorted(unique, key=lambda x: x.get('score', 0), reverse=True)
        return sorted_items[:top_k]
```

---

## ğŸ“š å‚è€ƒèµ„æº

1. **Self-RAGè®ºæ–‡**: <https://arxiv.org/abs/2310.11511>
2. **RAGè¯„ä¼°**: <https://github.com/langchain-ai/ragas>
3. **Cross-Encoderæ¨¡å‹**: <https://www.sbert.net/docs/pretrained-cross-encoders.html>
4. **Query Expansion**: <https://arxiv.org/abs/2305.03653>

---

## ğŸ“ æ›´æ–°æ—¥å¿—

- **v1.0** (2025-01): åˆå§‹ç‰ˆæœ¬
  - æŸ¥è¯¢é‡å†™ä¸æ‰©å±•
  - å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ
  - Cross-Encoderé‡æ’åº
  - Self-RAGæ¶æ„
  - Agentic RAG
  - å¤šæ¨¡æ€RAG
  - RAGè¯„ä¼°ä½“ç³»

---

**çŠ¶æ€**: âœ… **æ–‡æ¡£å®Œæˆ** | [è¿”å›ç›®å½•](./README.md)
