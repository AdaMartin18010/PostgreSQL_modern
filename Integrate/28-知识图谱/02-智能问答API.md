---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `DataBaseTheory\21-AIçŸ¥è¯†åº“\02-æ™ºèƒ½é—®ç­”API.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# PostgreSQLæ™ºèƒ½é—®ç­”API

> **åŸºäºå‘é‡æ£€ç´¢**

---

## æ¶æ„è®¾è®¡

```sql
-- å®‰è£…pgvectoræ‰©å±•
CREATE EXTENSION vector;

-- çŸ¥è¯†åº“è¡¨
CREATE TABLE kb_documents (
    doc_id SERIAL PRIMARY KEY,
    title TEXT,
    content TEXT,
    doc_type VARCHAR(50),  -- feature/tutorial/troubleshooting
    pg_version VARCHAR(20),
    embedding vector(1536),  -- OpenAI embedding
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- å‘é‡ç´¢å¼•ï¼ˆHNSWï¼‰
CREATE INDEX idx_kb_embedding
ON kb_documents USING hnsw (embedding vector_cosine_ops);
```

---

## å‘é‡åŒ–æ–‡æ¡£

```python
import openai
import psycopg2

def embed_document(text):
    """ç”Ÿæˆæ–‡æ¡£embedding"""
    response = openai.Embedding.create(
        model="text-embedding-ada-002",
        input=text
    )
    return response['data'][0]['embedding']

def insert_document(title, content, doc_type):
    """æ’å…¥çŸ¥è¯†åº“"""
    embedding = embed_document(content)

    conn = psycopg2.connect("...")
    cur = conn.cursor()

    cur.execute("""
        INSERT INTO kb_documents (title, content, doc_type, embedding)
        VALUES (%s, %s, %s, %s)
    """, (title, content, doc_type, embedding))

    conn.commit()

# æ’å…¥PostgreSQL 18æ–‡æ¡£
insert_document(
    "å¼‚æ­¥I/Oç‰¹æ€§",
    "PostgreSQL 18å¼•å…¥å¼‚æ­¥I/Oï¼Œæå‡ååé‡30-70%...",
    "feature"
)
```

---

## æ™ºèƒ½é—®ç­”

```python
def ask_question(question):
    """æ™ºèƒ½é—®ç­”"""
    # 1. å‘é‡åŒ–é—®é¢˜
    q_embedding = embed_document(question)

    # 2. å‘é‡æ£€ç´¢ï¼ˆ<10msï¼‰
    cur.execute("""
        SELECT
            doc_id,
            title,
            content,
            1 - (embedding <=> %s::vector) as similarity
        FROM kb_documents
        WHERE 1 - (embedding <=> %s::vector) > 0.7  -- ç›¸ä¼¼åº¦é˜ˆå€¼
        ORDER BY embedding <=> %s::vector
        LIMIT 5
    """, (q_embedding, q_embedding, q_embedding))

    docs = cur.fetchall()

    # 3. æ„é€ prompt
    context = "\n\n".join([doc[2] for doc in docs])

    prompt = f"""
    åŸºäºä»¥ä¸‹PostgreSQLæ–‡æ¡£å›ç­”é—®é¢˜ï¼š

    {context}

    é—®é¢˜ï¼š{question}

    å›ç­”ï¼š
    """

    # 4. ç”Ÿæˆç­”æ¡ˆ
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content

# ä½¿ç”¨
answer = ask_question("å¦‚ä½•ä¼˜åŒ–PostgreSQLçš„è¿æ¥æ€§èƒ½ï¼Ÿ")
print(answer)
```

---

## APIæ¥å£

```python
from fastapi import FastAPI

app = FastAPI()

@app.post("/api/ask")
async def api_ask(question: str):
    """é—®ç­”API"""
    answer = ask_question(question)
    return {"answer": answer}

@app.post("/api/search")
async def api_search(query: str):
    """å‘é‡æ£€ç´¢API"""
    embedding = embed_document(query)
    # æ£€ç´¢é€»è¾‘...
    return {"results": [...]}
```

---

## 4. é«˜çº§æ£€ç´¢åŠŸèƒ½

### 4.1 æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å…³é”®è¯ï¼‰

```python
def hybrid_search(query: str, top_k: int = 5):
    """æ··åˆæ£€ç´¢ï¼šå‘é‡æ£€ç´¢ + å…³é”®è¯æ£€ç´¢"""
    # 1. å‘é‡æ£€ç´¢
    q_embedding = embed_document(query)

    vector_results = cur.execute("""
        SELECT doc_id, title, content,
               1 - (embedding <=> %s::vector) as similarity
        FROM kb_documents
        ORDER BY embedding <=> %s::vector
        LIMIT %s
    """, (q_embedding, q_embedding, top_k))

    # 2. å…³é”®è¯æ£€ç´¢ï¼ˆå…¨æ–‡æœç´¢ï¼‰
    keyword_results = cur.execute("""
        SELECT doc_id, title, content,
               ts_rank(to_tsvector('english', content),
                       plainto_tsquery('english', %s)) as rank
        FROM kb_documents
        WHERE to_tsvector('english', content) @@ plainto_tsquery('english', %s)
        ORDER BY rank DESC
        LIMIT %s
    """, (query, query, top_k))

    # 3. åˆå¹¶ç»“æœï¼ˆåŠ æƒï¼‰
    combined_results = []
    vector_dict = {r[0]: r for r in vector_results}
    keyword_dict = {r[0]: r for r in keyword_results}

    for doc_id in set(list(vector_dict.keys()) + list(keyword_dict.keys())):
        vector_score = vector_dict.get(doc_id, [None, None, None, 0])[3]
        keyword_score = keyword_dict.get(doc_id, [None, None, None, 0])[3]

        # åŠ æƒåˆå¹¶ï¼ˆå‘é‡70%ï¼Œå…³é”®è¯30%ï¼‰
        combined_score = vector_score * 0.7 + keyword_score * 0.3

        combined_results.append({
            'doc_id': doc_id,
            'title': vector_dict.get(doc_id, keyword_dict[doc_id])[1],
            'content': vector_dict.get(doc_id, keyword_dict[doc_id])[2],
            'score': combined_score
        })

    return sorted(combined_results, key=lambda x: x['score'], reverse=True)
```

### 4.2 ä¸Šä¸‹æ–‡å¢å¼ºæ£€ç´¢

```python
def contextual_search(query: str, context: str, top_k: int = 5):
    """ä¸Šä¸‹æ–‡å¢å¼ºæ£€ç´¢"""
    # ç»“åˆæŸ¥è¯¢å’Œä¸Šä¸‹æ–‡ç”Ÿæˆembedding
    combined_text = f"Query: {query}\nContext: {context}"
    q_embedding = embed_document(combined_text)

    results = cur.execute("""
        SELECT doc_id, title, content,
               1 - (embedding <=> %s::vector) as similarity
        FROM kb_documents
        WHERE 1 - (embedding <=> %s::vector) > 0.6
        ORDER BY embedding <=> %s::vector
        LIMIT %s
    """, (q_embedding, q_embedding, q_embedding, top_k))

    return results
```

---

## 5. æ€§èƒ½ä¼˜åŒ–

### 5.1 å‘é‡ç´¢å¼•ä¼˜åŒ–

```sql
-- HNSWç´¢å¼•å‚æ•°ä¼˜åŒ–
CREATE INDEX idx_kb_embedding_optimized
ON kb_documents USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- æŸ¥è¯¢æ—¶è®¾ç½®ef_searchå‚æ•°
SET hnsw.ef_search = 100;  -- å¹³è¡¡å‡†ç¡®æ€§å’Œæ€§èƒ½

-- æ€§èƒ½æµ‹è¯•
EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT doc_id, title, 1 - (embedding <=> %s::vector) as similarity
FROM kb_documents
ORDER BY embedding <=> %s::vector
LIMIT 5;
```

### 5.2 ç¼“å­˜ç­–ç•¥

```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=1000)
def cached_embed_document(text_hash: str, text: str):
    """ç¼“å­˜æ–‡æ¡£embedding"""
    return embed_document(text)

def get_document_embedding(text: str):
    """è·å–æ–‡æ¡£embeddingï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    text_hash = hashlib.md5(text.encode()).hexdigest()
    return cached_embed_document(text_hash, text)

# ä½¿ç”¨ç¼“å­˜
embedding = get_document_embedding("PostgreSQL 18å¼‚æ­¥I/Oç‰¹æ€§")
```

---

## 6. ç›‘æ§å’Œè¯Šæ–­

### 6.1 æŸ¥è¯¢æ€§èƒ½ç›‘æ§

```sql
-- åˆ›å»ºæŸ¥è¯¢æ—¥å¿—è¡¨
CREATE TABLE IF NOT EXISTS query_log (
    log_id SERIAL PRIMARY KEY,
    query_text TEXT,
    response_time_ms NUMERIC,
    result_count INT,
    query_type VARCHAR(50),  -- 'vector', 'keyword', 'hybrid'
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- æŸ¥è¯¢æ€§èƒ½ç»Ÿè®¡ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰
CREATE OR REPLACE FUNCTION get_query_performance_stats()
RETURNS TABLE (
    query_type VARCHAR(50),
    avg_response_time_ms NUMERIC,
    p95_response_time_ms NUMERIC,
    p99_response_time_ms NUMERIC,
    total_queries BIGINT
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        ql.query_type,
        ROUND(AVG(ql.response_time_ms), 2) AS avg_response_time_ms,
        ROUND(PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY ql.response_time_ms), 2) AS p95_response_time_ms,
        ROUND(PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY ql.response_time_ms), 2) AS p99_response_time_ms,
        COUNT(*) AS total_queries
    FROM query_log ql
    WHERE ql.created_at > NOW() - INTERVAL '24 hours'
    GROUP BY ql.query_type
    ORDER BY ql.query_type;

EXCEPTION
    WHEN OTHERS THEN
        RAISE EXCEPTION 'è·å–æŸ¥è¯¢æ€§èƒ½ç»Ÿè®¡å¤±è´¥: %', SQLERRM;
END;
$$ LANGUAGE plpgsql;

-- æŸ¥è¯¢æ€§èƒ½ç»Ÿè®¡
SELECT * FROM get_query_performance_stats();
```

### 6.2 æ£€ç´¢è´¨é‡è¯„ä¼°

```python
def evaluate_retrieval_quality(query: str, expected_docs: list, top_k: int = 5):
    """è¯„ä¼°æ£€ç´¢è´¨é‡ï¼ˆç²¾ç¡®ç‡ã€å¬å›ç‡ï¼‰"""
    # æ‰§è¡Œæ£€ç´¢
    results = hybrid_search(query, top_k)
    retrieved_doc_ids = [r['doc_id'] for r in results]

    # è®¡ç®—ç²¾ç¡®ç‡
    precision = len(set(retrieved_doc_ids) & set(expected_docs)) / len(retrieved_doc_ids) if retrieved_doc_ids else 0

    # è®¡ç®—å¬å›ç‡
    recall = len(set(retrieved_doc_ids) & set(expected_docs)) / len(expected_docs) if expected_docs else 0

    # F1åˆ†æ•°
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    return {
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score,
        'retrieved_count': len(retrieved_doc_ids),
        'expected_count': len(expected_docs)
    }
```

---

## 7. APIå¢å¼ºåŠŸèƒ½

### 7.1 æ‰¹é‡é—®ç­”

```python
@app.post("/api/batch_ask")
async def api_batch_ask(questions: list[str]):
    """æ‰¹é‡é—®ç­”API"""
    results = []
    for question in questions:
        answer = ask_question(question)
        results.append({
            'question': question,
            'answer': answer
        })
    return {"results": results}
```

### 7.2 ç›¸ä¼¼é—®é¢˜æ¨è

```python
@app.post("/api/similar_questions")
async def api_similar_questions(question: str, top_k: int = 5):
    """ç›¸ä¼¼é—®é¢˜æ¨è"""
    q_embedding = embed_document(question)

    similar_questions = cur.execute("""
        SELECT question_text,
               1 - (question_embedding <=> %s::vector) as similarity
        FROM question_history
        WHERE 1 - (question_embedding <=> %s::vector) > 0.8
        ORDER BY question_embedding <=> %s::vector
        LIMIT %s
    """, (q_embedding, q_embedding, q_embedding, top_k))

    return {"similar_questions": similar_questions}
```

---

**æ–‡æ¡£å®Œæˆ** âœ…
