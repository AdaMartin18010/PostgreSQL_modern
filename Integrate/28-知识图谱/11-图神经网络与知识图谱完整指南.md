---
> **ğŸ“‹ æ–‡æ¡£æ¥æº**: æ–°å¢æ·±åŒ–æ–‡æ¡£
> **ğŸ“… åˆ›å»ºæ—¥æœŸ**: 2025-01
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºæ·±åº¦è¡¥å……ï¼Œæ·±åŒ–å›¾ç¥ç»ç½‘ç»œæŠ€æœ¯æ ˆ

---

# å›¾ç¥ç»ç½‘ç»œä¸çŸ¥è¯†å›¾è°±å®Œæ•´æŒ‡å—

## å…ƒæ•°æ®

- **æ–‡æ¡£ç‰ˆæœ¬**: v1.0
- **åˆ›å»ºæ—¥æœŸ**: 2025-01
- **æŠ€æœ¯æ ˆ**: PostgreSQL 16+ | Apache AGE 1.5+ | PyTorch Geometric | DGL | TensorFlow GNN
- **éš¾åº¦çº§åˆ«**: â­â­â­â­â­ (ä¸“å®¶çº§)
- **é¢„è®¡é˜…è¯»**: 180åˆ†é’Ÿ
- **å‰ç½®è¦æ±‚**: ç†Ÿæ‚‰å›¾æ•°æ®åº“ã€æ·±åº¦å­¦ä¹ å’ŒçŸ¥è¯†å›¾è°±åŸºç¡€

---

## ğŸ“‹ å®Œæ•´ç›®å½•

- [å›¾ç¥ç»ç½‘ç»œä¸çŸ¥è¯†å›¾è°±å®Œæ•´æŒ‡å—](#å›¾ç¥ç»ç½‘ç»œä¸çŸ¥è¯†å›¾è°±å®Œæ•´æŒ‡å—)
  - [å…ƒæ•°æ®](#å…ƒæ•°æ®)
  - [ğŸ“‹ å®Œæ•´ç›®å½•](#-å®Œæ•´ç›®å½•)
  - [1. GNNåŸºç¡€ç†è®º](#1-gnnåŸºç¡€ç†è®º)
    - [1.1 å›¾ç¥ç»ç½‘ç»œæ¦‚è¿°](#11-å›¾ç¥ç»ç½‘ç»œæ¦‚è¿°)
      - [ä¸ºä»€ä¹ˆéœ€è¦GNNï¼Ÿ](#ä¸ºä»€ä¹ˆéœ€è¦gnn)
    - [1.2 GNNæ ¸å¿ƒæ¦‚å¿µ](#12-gnnæ ¸å¿ƒæ¦‚å¿µ)
      - [æ¶ˆæ¯ä¼ é€’æœºåˆ¶](#æ¶ˆæ¯ä¼ é€’æœºåˆ¶)
      - [å›¾è¡¨ç¤ºå­¦ä¹ ](#å›¾è¡¨ç¤ºå­¦ä¹ )
    - [1.3 GNNæ¶æ„ç±»å‹](#13-gnnæ¶æ„ç±»å‹)
      - [å·ç§¯ç±»GNN](#å·ç§¯ç±»gnn)
      - [æ³¨æ„åŠ›ç±»GNN](#æ³¨æ„åŠ›ç±»gnn)
      - [é‡‡æ ·ç±»GNN](#é‡‡æ ·ç±»gnn)
  - [2. çŸ¥è¯†å›¾è°±ä¸­çš„GNNåº”ç”¨](#2-çŸ¥è¯†å›¾è°±ä¸­çš„gnnåº”ç”¨)
    - [2.1 èŠ‚ç‚¹åˆ†ç±»](#21-èŠ‚ç‚¹åˆ†ç±»)
      - [å®ç°ç¤ºä¾‹](#å®ç°ç¤ºä¾‹)
    - [2.2 å…³ç³»é¢„æµ‹ï¼ˆé“¾æ¥é¢„æµ‹ï¼‰](#22-å…³ç³»é¢„æµ‹é“¾æ¥é¢„æµ‹)
      - [TransEé£æ ¼çš„é“¾æ¥é¢„æµ‹](#transeé£æ ¼çš„é“¾æ¥é¢„æµ‹)
      - [GNN-basedé“¾æ¥é¢„æµ‹](#gnn-basedé“¾æ¥é¢„æµ‹)
    - [2.3 å®ä½“å¯¹é½](#23-å®ä½“å¯¹é½)
    - [2.4 çŸ¥è¯†å›¾è°±è¡¥å…¨](#24-çŸ¥è¯†å›¾è°±è¡¥å…¨)
  - [3. GNNæ¨¡å‹è¯¦è§£](#3-gnnæ¨¡å‹è¯¦è§£)
    - [3.1 GCN (Graph Convolutional Network)](#31-gcn-graph-convolutional-network)
      - [å®ç°](#å®ç°)
    - [3.2 GraphSAGE](#32-graphsage)
    - [3.3 GAT (Graph Attention Network)](#33-gat-graph-attention-network)
    - [3.4 RGCN (Relational GCN)](#34-rgcn-relational-gcn)
  - [4. çŸ¥è¯†å›¾è°±åµŒå…¥](#4-çŸ¥è¯†å›¾è°±åµŒå…¥)
    - [4.1 TransEä¸Transå®¶æ—](#41-transeä¸transå®¶æ—)
    - [4.2 ComplExä¸RotatE](#42-complexä¸rotate)
      - [ComplExåµŒå…¥](#complexåµŒå…¥)
      - [RotatEåµŒå…¥](#rotateåµŒå…¥)
    - [4.3 GNN-basedåµŒå…¥](#43-gnn-basedåµŒå…¥)
  - [5. å®è·µæ¡ˆä¾‹](#5-å®è·µæ¡ˆä¾‹)
    - [5.1 ä½¿ç”¨PyTorch Geometricå®ç°GCN](#51-ä½¿ç”¨pytorch-geometricå®ç°gcn)
    - [5.2 çŸ¥è¯†å›¾è°±è¡¥å…¨å®æˆ˜](#52-çŸ¥è¯†å›¾è°±è¡¥å…¨å®æˆ˜)
    - [5.3 ä¸Apache AGEé›†æˆ](#53-ä¸apache-ageé›†æˆ)
  - [6. æ€§èƒ½ä¼˜åŒ–](#6-æ€§èƒ½ä¼˜åŒ–)
    - [6.1 å¤§è§„æ¨¡å›¾å¤„ç†](#61-å¤§è§„æ¨¡å›¾å¤„ç†)
      - [GraphSAINTé‡‡æ ·](#graphsainté‡‡æ ·)
      - [Cluster-GCN](#cluster-gcn)
    - [6.2 é‡‡æ ·ç­–ç•¥](#62-é‡‡æ ·ç­–ç•¥)
      - [é‚»å±…é‡‡æ ·](#é‚»å±…é‡‡æ ·)
    - [6.3 åˆ†å¸ƒå¼è®­ç»ƒ](#63-åˆ†å¸ƒå¼è®­ç»ƒ)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)
  - [ğŸ“ æ›´æ–°æ—¥å¿—](#-æ›´æ–°æ—¥å¿—)

---

## 1. GNNåŸºç¡€ç†è®º

### 1.1 å›¾ç¥ç»ç½‘ç»œæ¦‚è¿°

å›¾ç¥ç»ç½‘ç»œ(Graph Neural Networks, GNN)æ˜¯ä¸“é—¨å¤„ç†å›¾ç»“æ„æ•°æ®çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚åœ¨çŸ¥è¯†å›¾è°±é¢†åŸŸï¼ŒGNNèƒ½å¤Ÿå­¦ä¹ å®ä½“å’Œå…³ç³»çš„è¡¨ç¤ºï¼Œå¹¶è¿›è¡Œå„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚

#### ä¸ºä»€ä¹ˆéœ€è¦GNNï¼Ÿ

```text
ä¼ ç»Ÿæ–¹æ³•å±€é™æ€§:
âŒ æ— æ³•å¤„ç†å›¾ç»“æ„ä¿¡æ¯
âŒ éš¾ä»¥æ•è·é‚»å±…èŠ‚ç‚¹çš„å½±å“
âŒ ç¼ºä¹ç»“æ„æ„ŸçŸ¥èƒ½åŠ›

GNNä¼˜åŠ¿:
âœ… ç›´æ¥åœ¨å›¾ç»“æ„ä¸Šæ“ä½œ
âœ… é€šè¿‡æ¶ˆæ¯ä¼ é€’èšåˆé‚»å±…ä¿¡æ¯
âœ… å­¦ä¹ ç»“æ„æ„ŸçŸ¥çš„èŠ‚ç‚¹è¡¨ç¤º
âœ… ç«¯åˆ°ç«¯å¯è®­ç»ƒ
```

### 1.2 GNNæ ¸å¿ƒæ¦‚å¿µ

#### æ¶ˆæ¯ä¼ é€’æœºåˆ¶

GNNçš„æ ¸å¿ƒæ˜¯æ¶ˆæ¯ä¼ é€’(Message Passing)æœºåˆ¶ï¼š

```python
class MessagePassing:
    """
    æ¶ˆæ¯ä¼ é€’åŸºç¡€æ¡†æ¶

    æ ¸å¿ƒå…¬å¼:
    h_v^(l+1) = UPDATE(h_v^(l), AGGREGATE({MESSAGE(h_u^(l), h_v^(l), e_uv) | u âˆˆ N(v)}))
    """

    def message(self, x_i, x_j, edge_attr):
        """
        æ¶ˆæ¯å‡½æ•°: ç”Ÿæˆä»èŠ‚ç‚¹jåˆ°èŠ‚ç‚¹içš„æ¶ˆæ¯

        Args:
            x_i: ç›®æ ‡èŠ‚ç‚¹içš„ç‰¹å¾
            x_j: æºèŠ‚ç‚¹jçš„ç‰¹å¾
            edge_attr: è¾¹çš„ç‰¹å¾

        Returns:
            æ¶ˆæ¯å‘é‡
        """
        # åŸºç¡€å®ç°ï¼šä»…ä¼ é€’é‚»å±…ç‰¹å¾
        return x_j

    def aggregate(self, messages, index, dim_size):
        """
        èšåˆå‡½æ•°: èšåˆæ¥è‡ªæ‰€æœ‰é‚»å±…çš„æ¶ˆæ¯

        Args:
            messages: æ‰€æœ‰æ¶ˆæ¯
            index: ç›®æ ‡èŠ‚ç‚¹ç´¢å¼•
            dim_size: èŠ‚ç‚¹æ•°é‡

        Returns:
            èšåˆåçš„æ¶ˆæ¯
        """
        # åŸºç¡€å®ç°ï¼šæ±‚å’Œèšåˆ
        from torch_scatter import scatter
        return scatter(messages, index, dim=0, dim_size=dim_size, reduce='sum')

    def update(self, aggr_out, x):
        """
        æ›´æ–°å‡½æ•°: æ›´æ–°èŠ‚ç‚¹è¡¨ç¤º

        Args:
            aggr_out: èšåˆåçš„æ¶ˆæ¯
            x: åŸå§‹èŠ‚ç‚¹ç‰¹å¾

        Returns:
            æ›´æ–°åçš„èŠ‚ç‚¹ç‰¹å¾
        """
        # åŸºç¡€å®ç°ï¼šç›´æ¥ä½¿ç”¨èšåˆç»“æœ
        return aggr_out
```

#### å›¾è¡¨ç¤ºå­¦ä¹ 

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import add_self_loops, degree

class BasicGNN(MessagePassing):
    """åŸºç¡€GNNå±‚"""

    def __init__(self, in_channels, out_channels):
        super(BasicGNN, self).__init__(aggr='add')  # èšåˆæ–¹å¼: sum
        self.lin = nn.Linear(in_channels, out_channels)

    def forward(self, x, edge_index):
        """
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾çŸ©é˜µ [N, in_channels]
            edge_index: è¾¹ç´¢å¼• [2, E]
        """
        # æ·»åŠ è‡ªç¯
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))

        # ç‰¹å¾å˜æ¢
        x = self.lin(x)

        # æ¶ˆæ¯ä¼ é€’
        return self.propagate(edge_index, x=x)

    def message(self, x_j):
        """æ¶ˆæ¯: ä¼ é€’é‚»å±…ç‰¹å¾"""
        return x_j
```

### 1.3 GNNæ¶æ„ç±»å‹

#### å·ç§¯ç±»GNN

åŸºäºå›¾å·ç§¯çš„æ¨¡å‹ï¼Œé€šè¿‡å·ç§¯æ“ä½œèšåˆé‚»å±…ä¿¡æ¯ï¼š

```python
from torch_geometric.nn import GCNConv

class GCNLayer(nn.Module):
    """å›¾å·ç§¯å±‚"""

    def __init__(self, in_channels, out_channels):
        super(GCNLayer, self).__init__()
        self.conv = GCNConv(in_channels, out_channels)
        self.bn = nn.BatchNorm1d(out_channels)
        self.relu = nn.ReLU()

    def forward(self, x, edge_index):
        x = self.conv(x, edge_index)
        x = self.bn(x)
        x = self.relu(x)
        return x
```

#### æ³¨æ„åŠ›ç±»GNN

ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€å­¦ä¹ é‚»å±…æƒé‡ï¼š

```python
from torch_geometric.nn import GATConv

class GATLayer(nn.Module):
    """å›¾æ³¨æ„åŠ›å±‚"""

    def __init__(self, in_channels, out_channels, heads=8, dropout=0.6):
        super(GATLayer, self).__init__()
        self.conv = GATConv(
            in_channels,
            out_channels,
            heads=heads,
            dropout=dropout,
            concat=True
        )

    def forward(self, x, edge_index):
        return self.conv(x, edge_index)
```

#### é‡‡æ ·ç±»GNN

é€šè¿‡é‡‡æ ·é‚»å±…èŠ‚ç‚¹å¤„ç†å¤§è§„æ¨¡å›¾ï¼š

```python
from torch_geometric.nn import SAGEConv

class GraphSAGELayer(nn.Module):
    """GraphSAGEå±‚"""

    def __init__(self, in_channels, out_channels):
        super(GraphSAGELayer, self).__init__()
        self.conv = SAGEConv(in_channels, out_channels)

    def forward(self, x, edge_index):
        return self.conv(x, edge_index)
```

---

## 2. çŸ¥è¯†å›¾è°±ä¸­çš„GNNåº”ç”¨

### 2.1 èŠ‚ç‚¹åˆ†ç±»

ä½¿ç”¨GNNå¯¹çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“è¿›è¡Œåˆ†ç±»ã€‚

#### å®ç°ç¤ºä¾‹

```python
import torch
import torch.nn as nn
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data

class KGNodeClassifier(nn.Module):
    """çŸ¥è¯†å›¾è°±èŠ‚ç‚¹åˆ†ç±»å™¨"""

    def __init__(self, num_features, hidden_dim, num_classes, num_layers=2):
        super(KGNodeClassifier, self).__init__()
        self.num_layers = num_layers

        # GCNå±‚
        self.convs = nn.ModuleList()
        self.convs.append(GCNConv(num_features, hidden_dim))
        for _ in range(num_layers - 2):
            self.convs.append(GCNConv(hidden_dim, hidden_dim))
        if num_layers > 1:
            self.convs.append(GCNConv(hidden_dim, num_classes))
        else:
            self.convs.append(GCNConv(num_features, num_classes))

        self.dropout = nn.Dropout(0.5)

    def forward(self, x, edge_index):
        """å‰å‘ä¼ æ’­"""
        for i, conv in enumerate(self.convs[:-1]):
            x = conv(x, edge_index)
            x = F.relu(x)
            x = self.dropout(x)

        x = self.convs[-1](x, edge_index)
        return F.log_softmax(x, dim=1)

# ä½¿ç”¨ç¤ºä¾‹
def train_node_classifier(graph_data, labels, train_mask, val_mask, test_mask):
    """è®­ç»ƒèŠ‚ç‚¹åˆ†ç±»å™¨"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = KGNodeClassifier(
        num_features=graph_data.num_features,
        hidden_dim=64,
        num_classes=labels.max().item() + 1,
        num_layers=2
    ).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)
    criterion = nn.NLLLoss()

    graph_data = graph_data.to(device)
    labels = labels.to(device)

    for epoch in range(200):
        model.train()
        optimizer.zero_grad()

        # å‰å‘ä¼ æ’­
        out = model(graph_data.x, graph_data.edge_index)

        # æŸå¤±è®¡ç®—ï¼ˆä»…è®­ç»ƒé›†ï¼‰
        loss = criterion(out[train_mask], labels[train_mask])

        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()

        # éªŒè¯
        if epoch % 10 == 0:
            model.eval()
            with torch.no_grad():
                val_out = model(graph_data.x, graph_data.edge_index)
                val_loss = criterion(val_out[val_mask], labels[val_mask])
                val_pred = val_out[val_mask].argmax(dim=1)
                val_acc = (val_pred == labels[val_mask]).sum().item() / val_mask.sum().item()

                print(f'Epoch {epoch}, Loss: {loss.item():.4f}, '
                      f'Val Loss: {val_loss.item():.4f}, Val Acc: {val_acc:.4f}')

    return model
```

### 2.2 å…³ç³»é¢„æµ‹ï¼ˆé“¾æ¥é¢„æµ‹ï¼‰

é¢„æµ‹çŸ¥è¯†å›¾è°±ä¸­ç¼ºå¤±çš„å…³ç³»ï¼ˆä¸‰å…ƒç»„ï¼‰ã€‚

#### TransEé£æ ¼çš„é“¾æ¥é¢„æµ‹

```python
class LinkPredictor(nn.Module):
    """é“¾æ¥é¢„æµ‹å™¨ï¼ˆåŸºäºåµŒå…¥ï¼‰"""

    def __init__(self, num_entities, num_relations, embedding_dim=100):
        super(LinkPredictor, self).__init__()
        self.embedding_dim = embedding_dim

        # å®ä½“åµŒå…¥
        self.entity_embedding = nn.Embedding(num_entities, embedding_dim)
        # å…³ç³»åµŒå…¥
        self.relation_embedding = nn.Embedding(num_relations, embedding_dim)

        # åˆå§‹åŒ–
        nn.init.xavier_uniform_(self.entity_embedding.weight.data)
        nn.init.xavier_uniform_(self.relation_embedding.weight.data)

    def forward(self, head, relation, tail):
        """
        TransEè¯„åˆ†å‡½æ•°: f(h, r, t) = -||h + r - t||

        Args:
            head: å¤´å®ä½“ID [batch_size]
            relation: å…³ç³»ID [batch_size]
            tail: å°¾å®ä½“ID [batch_size]
        """
        h = self.entity_embedding(head)  # [batch_size, embedding_dim]
        r = self.relation_embedding(relation)  # [batch_size, embedding_dim]
        t = self.entity_embedding(tail)  # [batch_size, embedding_dim]

        # TransEè¯„åˆ†
        score = h + r - t
        score = torch.norm(score, p=2, dim=1)
        return -score  # è´Ÿè·ç¦»ä½œä¸ºåˆ†æ•°ï¼ˆè¶Šå°è¶Šå¥½ï¼‰

# è®­ç»ƒ
def train_link_predictor(model, train_triples, num_epochs=100):
    """è®­ç»ƒé“¾æ¥é¢„æµ‹å™¨"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0

        for batch in train_triples:  # batch: (head, relation, tail, neg_head, neg_tail)
            head, relation, tail, neg_head, neg_tail = batch
            head, relation, tail = head.to(device), relation.to(device), tail.to(device)
            neg_head, neg_tail = neg_head.to(device), neg_tail.to(device)

            optimizer.zero_grad()

            # æ­£æ ·æœ¬åˆ†æ•°
            pos_score = model(head, relation, tail)

            # è´Ÿæ ·æœ¬åˆ†æ•°ï¼ˆéšæœºæ›¿æ¢å¤´æˆ–å°¾å®ä½“ï¼‰
            neg_score_head = model(neg_head, relation, tail)
            neg_score_tail = model(head, relation, neg_tail)

            # å¯¹æ¯”æŸå¤±ï¼ˆMargin Lossï¼‰
            margin = 1.0
            loss = F.relu(margin - pos_score + neg_score_head).mean() + \
                   F.relu(margin - pos_score + neg_score_tail).mean()

            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        if epoch % 10 == 0:
            print(f'Epoch {epoch}, Loss: {total_loss / len(train_triples):.4f}')

    return model
```

#### GNN-basedé“¾æ¥é¢„æµ‹

```python
from torch_geometric.nn import RGCNConv

class GNNLinkPredictor(nn.Module):
    """åŸºäºGNNçš„é“¾æ¥é¢„æµ‹å™¨"""

    def __init__(self, num_entities, num_relations, embedding_dim=64, hidden_dim=64):
        super(GNNLinkPredictor, self).__init__()
        self.num_entities = num_entities
        self.num_relations = num_relations

        # å®ä½“åµŒå…¥ï¼ˆåˆå§‹åŒ–ï¼‰
        self.entity_embedding = nn.Embedding(num_entities, embedding_dim)

        # RGCNå±‚ï¼ˆå…³ç³»å›¾å·ç§¯ï¼‰
        self.conv1 = RGCNConv(embedding_dim, hidden_dim, num_relations, num_bases=30)
        self.conv2 = RGCNConv(hidden_dim, embedding_dim, num_relations, num_bases=30)

        self.dropout = nn.Dropout(0.2)

    def forward(self, x, edge_index, edge_type):
        """é€šè¿‡RGCNè·å–å®ä½“è¡¨ç¤º"""
        x = self.entity_embedding(x)
        x = self.conv1(x, edge_index, edge_type)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.conv2(x, edge_index, edge_type)
        return x

    def predict(self, entity_embeddings, head, relation, tail):
        """é¢„æµ‹é“¾æ¥"""
        h = entity_embeddings[head]
        r = self.relation_embedding(relation)  # éœ€è¦å®šä¹‰å…³ç³»åµŒå…¥
        t = entity_embeddings[tail]

        # DistMultè¯„åˆ†å‡½æ•°
        score = torch.sum(h * r * t, dim=1)
        return score
```

### 2.3 å®ä½“å¯¹é½

ä½¿ç”¨GNNå¯¹é½ä¸åŒçŸ¥è¯†å›¾è°±ä¸­çš„ç›¸åŒå®ä½“ã€‚

```python
class EntityAlignmentGNN(nn.Module):
    """å®ä½“å¯¹é½GNN"""

    def __init__(self, num_features, hidden_dim, embedding_dim):
        super(EntityAlignmentGNN, self).__init__()
        self.conv1 = GCNConv(num_features, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, embedding_dim)

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return F.normalize(x, p=2, dim=1)  # L2å½’ä¸€åŒ–

def align_entities(model, graph1, graph2, seed_alignments):
    """
    å¯¹é½ä¸¤ä¸ªçŸ¥è¯†å›¾è°±çš„å®ä½“

    Args:
        model: GNNæ¨¡å‹
        graph1: ç¬¬ä¸€ä¸ªçŸ¥è¯†å›¾è°±
        graph2: ç¬¬äºŒä¸ªçŸ¥è¯†å›¾è°±
        seed_alignments: ç§å­å¯¹é½å¯¹ [(entity1_id, entity2_id), ...]
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    # è·å–å®ä½“åµŒå…¥
    with torch.no_grad():
        emb1 = model(graph1.x, graph1.edge_index).cpu().numpy()
        emb2 = model(graph2.x, graph2.edge_index).cpu().numpy()

    # ä½¿ç”¨ç§å­å¯¹é½è®­ç»ƒå¯¹é½æ¨¡å‹ï¼ˆå¦‚çº¿æ€§å˜æ¢ï¼‰
    from sklearn.linear_model import LinearRegression

    seed_emb1 = emb1[[e1 for e1, e2 in seed_alignments]]
    seed_emb2 = emb2[[e2 for e1, e2 in seed_alignments]]

    align_model = LinearRegression()
    align_model.fit(seed_emb1, seed_emb2)

    # å¯¹é½æ‰€æœ‰å®ä½“
    aligned_emb2 = align_model.predict(emb1)

    # æŸ¥æ‰¾æœ€è¿‘é‚»
    from sklearn.metrics.pairwise import cosine_similarity
    similarity = cosine_similarity(emb2, aligned_emb2)

    # è¿”å›å¯¹é½ç»“æœ
    alignments = []
    for i in range(len(emb1)):
        best_match = similarity[:, i].argmax()
        alignments.append((i, best_match, similarity[best_match, i]))

    return alignments
```

### 2.4 çŸ¥è¯†å›¾è°±è¡¥å…¨

ä½¿ç”¨GNNè¡¥å…¨ç¼ºå¤±çš„ä¸‰å…ƒç»„ã€‚

```python
class KGCompletionGNN(nn.Module):
    """çŸ¥è¯†å›¾è°±è¡¥å…¨GNN"""

    def __init__(self, num_entities, num_relations, embedding_dim=100):
        super(KGCompletionGNN, self).__init__()
        self.embedding_dim = embedding_dim

        # å®ä½“å’Œå…³ç³»åµŒå…¥
        self.entity_embedding = nn.Embedding(num_entities, embedding_dim)
        self.relation_embedding = nn.Embedding(num_relations, embedding_dim)

        # GNNå±‚
        self.conv1 = RGCNConv(embedding_dim, embedding_dim, num_relations)
        self.conv2 = RGCNConv(embedding_dim, embedding_dim, num_relations)

    def forward(self, edge_index, edge_type):
        """é€šè¿‡RGCNè·å–å®ä½“è¡¨ç¤º"""
        x = self.entity_embedding.weight
        x = self.conv1(x, edge_index, edge_type)
        x = F.relu(x)
        x = self.conv2(x, edge_index, edge_type)
        return x

    def score(self, head, relation, tail):
        """ComplExè¯„åˆ†å‡½æ•°"""
        h = self.entity_embedding(head)
        r = self.relation_embedding(relation)
        t = self.entity_embedding(tail)

        # ComplEx: Re(<h, r, conj(t)>)
        score = torch.sum(
            torch.real(torch.conj(h) * r * torch.conj(t)),
            dim=1
        )
        return score

def complete_kg(model, graph, query_head, query_relation, k=10):
    """
    è¡¥å…¨çŸ¥è¯†å›¾è°±

    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        query_head: æŸ¥è¯¢å¤´å®ä½“
        query_relation: æŸ¥è¯¢å…³ç³»
        k: è¿”å›Top Kå€™é€‰

    Returns:
        Top Kå°¾å®ä½“å€™é€‰
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.eval()

    # è·å–å®ä½“è¡¨ç¤º
    with torch.no_grad():
        entity_embeddings = model(graph.edge_index, graph.edge_type)

    h = entity_embeddings[query_head]
    r = model.relation_embedding(torch.tensor([query_relation]).to(device))

    # è®¡ç®—ä¸æ‰€æœ‰å®ä½“çš„åˆ†æ•°
    scores = model.score(
        torch.full((entity_embeddings.size(0),), query_head).to(device),
        torch.full((entity_embeddings.size(0),), query_relation).to(device),
        torch.arange(entity_embeddings.size(0)).to(device)
    )

    # è¿”å›Top K
    top_k_scores, top_k_indices = torch.topk(scores, k)
    return top_k_indices.cpu().tolist(), top_k_scores.cpu().tolist()
```

---

## 3. GNNæ¨¡å‹è¯¦è§£

### 3.1 GCN (Graph Convolutional Network)

GCNæ˜¯æœ€åŸºç¡€çš„å›¾å·ç§¯ç½‘ç»œã€‚

#### å®ç°

```python
class GCN(nn.Module):
    """å›¾å·ç§¯ç½‘ç»œ"""

    def __init__(self, num_features, hidden_dim, num_classes, num_layers=2, dropout=0.5):
        super(GCN, self).__init__()
        self.convs = nn.ModuleList()

        # ç¬¬ä¸€å±‚
        self.convs.append(GCNConv(num_features, hidden_dim))

        # ä¸­é—´å±‚
        for _ in range(num_layers - 2):
            self.convs.append(GCNConv(hidden_dim, hidden_dim))

        # è¾“å‡ºå±‚
        self.convs.append(GCNConv(hidden_dim, num_classes))

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, edge_index):
        for i, conv in enumerate(self.convs[:-1]):
            x = conv(x, edge_index)
            x = F.relu(x)
            x = self.dropout(x)

        x = self.convs[-1](x, edge_index)
        return F.log_softmax(x, dim=1)
```

### 3.2 GraphSAGE

GraphSAGEé€šè¿‡é‡‡æ ·å’Œèšåˆé‚»å±…èŠ‚ç‚¹å¤„ç†å¤§è§„æ¨¡å›¾ã€‚

```python
from torch_geometric.nn import SAGEConv

class GraphSAGE(nn.Module):
    """GraphSAGEæ¨¡å‹"""

    def __init__(self, num_features, hidden_dim, num_classes, num_layers=2):
        super(GraphSAGE, self).__init__()
        self.convs = nn.ModuleList()

        self.convs.append(SAGEConv(num_features, hidden_dim))
        for _ in range(num_layers - 2):
            self.convs.append(SAGEConv(hidden_dim, hidden_dim))
        self.convs.append(SAGEConv(hidden_dim, num_classes))

    def forward(self, x, edge_index):
        for i, conv in enumerate(self.convs[:-1]):
            x = conv(x, edge_index)
            x = F.relu(x)
        x = self.convs[-1](x, edge_index)
        return F.log_softmax(x, dim=1)
```

### 3.3 GAT (Graph Attention Network)

GATä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ é‚»å±…æƒé‡ã€‚

```python
from torch_geometric.nn import GATConv

class GAT(nn.Module):
    """å›¾æ³¨æ„åŠ›ç½‘ç»œ"""

    def __init__(self, num_features, hidden_dim, num_classes, heads=8, dropout=0.6):
        super(GAT, self).__init__()
        self.conv1 = GATConv(num_features, hidden_dim, heads=heads, dropout=dropout)
        self.conv2 = GATConv(hidden_dim * heads, num_classes, heads=1, dropout=dropout)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, edge_index):
        x = self.dropout(x)
        x = F.elu(self.conv1(x, edge_index))
        x = self.dropout(x)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)
```

### 3.4 RGCN (Relational GCN)

RGCNä¸“é—¨å¤„ç†å¤šå…³ç³»å›¾ï¼ˆå¦‚çŸ¥è¯†å›¾è°±ï¼‰ã€‚

```python
from torch_geometric.nn import RGCNConv

class RGCN(nn.Module):
    """å…³ç³»å›¾å·ç§¯ç½‘ç»œ"""

    def __init__(self, num_entities, num_relations, embedding_dim, hidden_dim, num_classes):
        super(RGCN, self).__init__()
        self.entity_embedding = nn.Embedding(num_entities, embedding_dim)
        self.conv1 = RGCNConv(embedding_dim, hidden_dim, num_relations, num_bases=30)
        self.conv2 = RGCNConv(hidden_dim, num_classes, num_relations, num_bases=30)

    def forward(self, edge_index, edge_type):
        x = self.entity_embedding.weight
        x = F.relu(self.conv1(x, edge_index, edge_type))
        x = self.conv2(x, edge_index, edge_type)
        return F.log_softmax(x, dim=1)
```

---

## 4. çŸ¥è¯†å›¾è°±åµŒå…¥

### 4.1 TransEä¸Transå®¶æ—

TransEæ˜¯ç»å…¸çš„KGåµŒå…¥æ–¹æ³•ã€‚

```python
class TransE(nn.Module):
    """TransEæ¨¡å‹"""

    def __init__(self, num_entities, num_relations, embedding_dim=100, margin=1.0):
        super(TransE, self).__init__()
        self.margin = margin
        self.embedding_dim = embedding_dim

        self.entity_embedding = nn.Embedding(num_entities, embedding_dim)
        self.relation_embedding = nn.Embedding(num_relations, embedding_dim)

        # åˆå§‹åŒ–
        nn.init.xavier_uniform_(self.entity_embedding.weight.data)
        nn.init.xavier_uniform_(self.relation_embedding.weight.data)

    def forward(self, head, relation, tail):
        """TransEè¯„åˆ†: -||h + r - t||"""
        h = F.normalize(self.entity_embedding(head), p=2, dim=1)
        r = self.relation_embedding(relation)
        t = F.normalize(self.entity_embedding(tail), p=2, dim=1)

        score = h + r - t
        return -torch.norm(score, p=2, dim=1)
```

### 4.2 ComplExä¸RotatE

#### ComplExåµŒå…¥

ComplExä½¿ç”¨å¤æ•°åµŒå…¥å¤„ç†å¯¹ç§°å’Œåå¯¹ç§°å…³ç³»ã€‚

```python
class ComplEx(nn.Module):
    """ComplExæ¨¡å‹"""

    def __init__(self, num_entities, num_relations, embedding_dim=100):
        super(ComplEx, self).__init__()
        self.embedding_dim = embedding_dim

        # å¤æ•°åµŒå…¥ï¼šå®éƒ¨å’Œè™šéƒ¨
        self.entity_emb_real = nn.Embedding(num_entities, embedding_dim)
        self.entity_emb_imag = nn.Embedding(num_entities, embedding_dim)
        self.relation_emb_real = nn.Embedding(num_relations, embedding_dim)
        self.relation_emb_imag = nn.Embedding(num_relations, embedding_dim)

        # åˆå§‹åŒ–
        nn.init.xavier_uniform_(self.entity_emb_real.weight.data)
        nn.init.xavier_uniform_(self.entity_emb_imag.weight.data)
        nn.init.xavier_uniform_(self.relation_emb_real.weight.data)
        nn.init.xavier_uniform_(self.relation_emb_imag.weight.data)

    def forward(self, head, relation, tail):
        """
        ComplExè¯„åˆ†: Re(<h, r, conj(t)>)

        å…¶ä¸­ <h, r, conj(t)> æ˜¯å¤æ•°ä¸‰å…ƒç»„çš„å†…ç§¯
        """
        h_real = self.entity_emb_real(head)
        h_imag = self.entity_emb_imag(head)
        r_real = self.relation_emb_real(relation)
        r_imag = self.relation_emb_imag(relation)
        t_real = self.entity_emb_real(tail)
        t_imag = self.entity_emb_imag(tail)

        # è®¡ç®—å¤æ•°å†…ç§¯: <h, r, conj(t)>
        # = (h_real * r_real * t_real + h_imag * r_imag * t_real +
        #    h_real * r_imag * t_imag - h_imag * r_real * t_imag)
        score = (
            torch.sum(h_real * r_real * t_real, dim=1) +
            torch.sum(h_imag * r_imag * t_real, dim=1) +
            torch.sum(h_real * r_imag * t_imag, dim=1) -
            torch.sum(h_imag * r_real * t_imag, dim=1)
        )

        return score
```

#### RotatEåµŒå…¥

RotatEä½¿ç”¨æ—‹è½¬åµŒå…¥å¤„ç†å„ç§å…³ç³»æ¨¡å¼ã€‚

```python
class RotatE(nn.Module):
    """RotatEæ¨¡å‹"""

    def __init__(self, num_entities, num_relations, embedding_dim=100, gamma=12.0):
        super(RotatE, self).__init__()
        self.embedding_dim = embedding_dim
        self.gamma = gamma

        # å®ä½“åµŒå…¥ï¼ˆå¤æ•°ï¼‰
        self.entity_emb = nn.Embedding(num_entities, embedding_dim * 2)
        # å…³ç³»åµŒå…¥ï¼ˆè§’åº¦ï¼‰
        self.relation_emb = nn.Embedding(num_relations, embedding_dim)

        # åˆå§‹åŒ–
        nn.init.uniform_(self.entity_emb.weight.data, -1, 1)
        nn.init.uniform_(self.relation_emb.weight.data, -1, 1)

        # å½’ä¸€åŒ–å®ä½“åµŒå…¥
        self.entity_emb.weight.data = F.normalize(self.entity_emb.weight.data, p=2, dim=1)

    def forward(self, head, relation, tail):
        """
        RotatEè¯„åˆ†: -||h âˆ˜ r - t||

        å…¶ä¸­ âˆ˜ æ˜¯Hadamardç§¯ï¼ˆé€å…ƒç´ ç›¸ä¹˜ï¼‰
        """
        # å°†å®ä½“åµŒå…¥åˆ†ä¸ºå®éƒ¨å’Œè™šéƒ¨
        head_real, head_imag = torch.chunk(self.entity_emb(head), 2, dim=1)
        tail_real, tail_imag = torch.chunk(self.entity_emb(tail), 2, dim=1)

        # å…³ç³»è§’åº¦
        relation_angle = self.relation_emb(relation)

        # æ—‹è½¬ï¼šh âˆ˜ r = (h_real * cos(r) - h_imag * sin(r),
        #                h_real * sin(r) + h_imag * cos(r))
        rotated_real = head_real * torch.cos(relation_angle) - head_imag * torch.sin(relation_angle)
        rotated_imag = head_real * torch.sin(relation_angle) + head_imag * torch.cos(relation_angle)

        # è®¡ç®—è·ç¦»
        diff_real = rotated_real - tail_real
        diff_imag = rotated_imag - tail_imag
        distance = torch.norm(torch.stack([diff_real, diff_imag], dim=0), p=2, dim=0).sum(dim=1)

        # è¯„åˆ†
        score = self.gamma - distance

        return score
```

### 4.3 GNN-basedåµŒå…¥

ä½¿ç”¨GNNç”Ÿæˆå®ä½“åµŒå…¥ï¼Œç»“åˆç»“æ„ä¿¡æ¯ã€‚

```python
class GNNBasedEmbedding(nn.Module):
    """åŸºäºGNNçš„çŸ¥è¯†å›¾è°±åµŒå…¥"""

    def __init__(
        self,
        num_entities,
        num_relations,
        embedding_dim=100,
        hidden_dim=64,
        num_layers=2
    ):
        super(GNNBasedEmbedding, self).__init__()
        self.num_entities = num_entities
        self.num_relations = num_relations
        self.embedding_dim = embedding_dim

        # åˆå§‹å®ä½“åµŒå…¥
        self.entity_embedding = nn.Embedding(num_entities, embedding_dim)

        # RGCNå±‚
        self.convs = nn.ModuleList()
        self.convs.append(RGCNConv(embedding_dim, hidden_dim, num_relations, num_bases=30))
        for _ in range(num_layers - 2):
            self.convs.append(RGCNConv(hidden_dim, hidden_dim, num_relations, num_bases=30))
        self.convs.append(RGCNConv(hidden_dim, embedding_dim, num_relations, num_bases=30))

        # å…³ç³»åµŒå…¥
        self.relation_embedding = nn.Embedding(num_relations, embedding_dim)

    def forward(self, edge_index, edge_type):
        """é€šè¿‡RGCNè·å–å®ä½“åµŒå…¥"""
        x = self.entity_embedding.weight
        for i, conv in enumerate(self.convs[:-1]):
            x = conv(x, edge_index, edge_type)
            x = F.relu(x)
        x = self.convs[-1](x, edge_index, edge_type)
        return x

    def score(self, head, relation, tail, entity_embeddings):
        """DistMultè¯„åˆ†å‡½æ•°"""
        h = entity_embeddings[head]
        r = self.relation_embedding(relation)
        t = entity_embeddings[tail]

        score = torch.sum(h * r * t, dim=1)
        return score
```

---

## 5. å®è·µæ¡ˆä¾‹

### 5.1 ä½¿ç”¨PyTorch Geometricå®ç°GCN

å®Œæ•´çš„çŸ¥è¯†å›¾è°±èŠ‚ç‚¹åˆ†ç±»æ¡ˆä¾‹ã€‚

```python
import torch
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
from torch_geometric.loader import DataLoader

# å‡†å¤‡æ•°æ®
def create_kg_data(entities, relations, labels):
    """åˆ›å»ºçŸ¥è¯†å›¾è°±æ•°æ®"""
    # æ„å»ºè¾¹ç´¢å¼•
    edge_index = []
    edge_type = []

    for head, relation, tail in relations:
        edge_index.append([head, tail])
        edge_type.append(relation)

    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
    edge_type = torch.tensor(edge_type, dtype=torch.long)

    # èŠ‚ç‚¹ç‰¹å¾ï¼ˆå¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒåµŒå…¥æˆ–éšæœºåˆå§‹åŒ–ï¼‰
    x = torch.randn(len(entities), 128)  # 128ç»´ç‰¹å¾

    # æ ‡ç­¾
    y = torch.tensor(labels, dtype=torch.long)

    # è®­ç»ƒ/éªŒè¯/æµ‹è¯•åˆ†å‰²
    train_mask = torch.zeros(len(entities), dtype=torch.bool)
    val_mask = torch.zeros(len(entities), dtype=torch.bool)
    test_mask = torch.zeros(len(entities), dtype=torch.bool)

    # 80%è®­ç»ƒï¼Œ10%éªŒè¯ï¼Œ10%æµ‹è¯•
    train_mask[:int(len(entities) * 0.8)] = True
    val_mask[int(len(entities) * 0.8):int(len(entities) * 0.9)] = True
    test_mask[int(len(entities) * 0.9):] = True

    data = Data(x=x, edge_index=edge_index, y=y,
                train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)

    return data

# è®­ç»ƒæ¨¡å‹
def train_gnn_model(data, num_classes, epochs=200):
    """è®­ç»ƒGNNæ¨¡å‹"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    model = GCN(
        num_features=data.x.size(1),
        hidden_dim=64,
        num_classes=num_classes,
        num_layers=2
    ).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)
    criterion = nn.NLLLoss()

    data = data.to(device)

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()

        out = model(data.x, data.edge_index)
        loss = criterion(out[data.train_mask], data.y[data.train_mask])

        loss.backward()
        optimizer.step()

        # éªŒè¯
        if epoch % 10 == 0:
            model.eval()
            with torch.no_grad():
                val_out = model(data.x, data.edge_index)
                val_loss = criterion(val_out[data.val_mask], data.y[data.val_mask])
                val_pred = val_out[data.val_mask].argmax(dim=1)
                val_acc = (val_pred == data.y[data.val_mask]).sum().item() / data.val_mask.sum().item()

                print(f'Epoch {epoch}, Loss: {loss.item():.4f}, '
                      f'Val Loss: {val_loss.item():.4f}, Val Acc: {val_acc:.4f}')

    return model
```

### 5.2 çŸ¥è¯†å›¾è°±è¡¥å…¨å®æˆ˜

ä½¿ç”¨RGCNè¿›è¡ŒçŸ¥è¯†å›¾è°±è¡¥å…¨ã€‚

```python
def kg_completion_with_rgcn(graph_data, train_triples, test_triples):
    """ä½¿ç”¨RGCNè¿›è¡ŒçŸ¥è¯†å›¾è°±è¡¥å…¨"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # åˆ›å»ºRGCNæ¨¡å‹
    model = RGCN(
        num_entities=graph_data.num_nodes,
        num_relations=graph_data.num_relations,
        embedding_dim=100,
        hidden_dim=64,
        num_classes=graph_data.num_nodes  # é¢„æµ‹å°¾å®ä½“
    ).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    # è®­ç»ƒ
    for epoch in range(100):
        model.train()
        optimizer.zero_grad()

        # è·å–å®ä½“åµŒå…¥
        entity_embeddings = model(graph_data.edge_index, graph_data.edge_type)

        # è®¡ç®—è®­ç»ƒæŸå¤±
        total_loss = 0
        for head, relation, tail, neg_tail in train_triples:
            # æ­£æ ·æœ¬åˆ†æ•°
            pos_score = model.score(
                torch.tensor([head]).to(device),
                torch.tensor([relation]).to(device),
                torch.tensor([tail]).to(device),
                entity_embeddings
            )

            # è´Ÿæ ·æœ¬åˆ†æ•°
            neg_score = model.score(
                torch.tensor([head]).to(device),
                torch.tensor([relation]).to(device),
                torch.tensor([neg_tail]).to(device),
                entity_embeddings
            )

            # Margin Loss
            loss = F.relu(1.0 - pos_score + neg_score)
            total_loss += loss

        total_loss.backward()
        optimizer.step()

        if epoch % 10 == 0:
            print(f'Epoch {epoch}, Loss: {total_loss.item():.4f}')

    return model
```

### 5.3 ä¸Apache AGEé›†æˆ

å°†GNNæ¨¡å‹ä¸Apache AGEé›†æˆï¼Œå®ç°ç«¯åˆ°ç«¯çš„çŸ¥è¯†å›¾è°±åº”ç”¨ã€‚

```python
import psycopg2
from psycopg2.extras import RealDictCursor

class AGE_GNN_Integration:
    """Apache AGEä¸GNNé›†æˆ"""

    def __init__(self, db_config, graph_name: str):
        self.conn = psycopg2.connect(**db_config)
        self.cursor = self.conn.cursor(cursor_factory=RealDictCursor)
        self.graph_name = graph_name

    def export_to_pyg(self) -> Data:
        """ä»Apache AGEå¯¼å‡ºæ•°æ®åˆ°PyTorch Geometricæ ¼å¼"""
        # è·å–æ‰€æœ‰èŠ‚ç‚¹
        self.cursor.execute(f"""
            SELECT * FROM cypher('{self.graph_name}', $$
                MATCH (n)
                RETURN id(n) as id, labels(n) as labels, properties(n) as properties
            $$) AS (id bigint, labels text[], properties jsonb);
        """)
        nodes = self.cursor.fetchall()

        # è·å–æ‰€æœ‰è¾¹
        self.cursor.execute(f"""
            SELECT * FROM cypher('{self.graph_name}', $$
                MATCH (a)-[r]->(b)
                RETURN id(a) as source, id(b) as target, type(r) as relation, properties(r) as properties
            $$) AS (source bigint, target bigint, relation text, properties jsonb);
        """)
        edges = self.cursor.fetchall()

        # æ„å»ºèŠ‚ç‚¹ç‰¹å¾
        num_nodes = len(nodes)
        node_features = torch.randn(num_nodes, 128)  # å¯ä»¥ä½¿ç”¨èŠ‚ç‚¹å±æ€§ä½œä¸ºç‰¹å¾

        # æ„å»ºè¾¹ç´¢å¼•
        edge_index = []
        edge_type = []
        relation_to_id = {}

        for edge in edges:
            source = edge['source']
            target = edge['target']
            relation = edge['relation']

            if relation not in relation_to_id:
                relation_to_id[relation] = len(relation_to_id)

            edge_index.append([source, target])
            edge_type.append(relation_to_id[relation])

        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
        edge_type = torch.tensor(edge_type, dtype=torch.long)

        # åˆ›å»ºPyGæ•°æ®å¯¹è±¡
        data = Data(x=node_features, edge_index=edge_index, edge_type=edge_type)

        return data, relation_to_id

    def import_embeddings(self, entity_embeddings: torch.Tensor, entity_ids: List[int]):
        """å°†GNNç”Ÿæˆçš„åµŒå…¥å¯¼å…¥Apache AGE"""
        for i, entity_id in enumerate(entity_ids):
            embedding = entity_embeddings[i].detach().cpu().numpy().tolist()

            # æ›´æ–°èŠ‚ç‚¹å±æ€§
            self.cursor.execute(f"""
                SELECT * FROM cypher('{self.graph_name}', $$
                    MATCH (n)
                    WHERE id(n) = {entity_id}
                    SET n.embedding = {embedding}
                    RETURN n
                $$) AS (n agtype);
            """)

        self.conn.commit()

    def query_with_embeddings(self, query_entity_id: int, top_k: int = 10):
        """ä½¿ç”¨åµŒå…¥è¿›è¡Œç›¸ä¼¼å®ä½“æŸ¥è¯¢"""
        # è·å–æŸ¥è¯¢å®ä½“çš„åµŒå…¥
        self.cursor.execute(f"""
            SELECT * FROM cypher('{self.graph_name}', $$
                MATCH (n)
                WHERE id(n) = {query_entity_id}
                RETURN n.embedding as embedding
            $$) AS (embedding float[]);
        """)
        result = self.cursor.fetchone()
        query_embedding = result['embedding']

        # æŸ¥æ‰¾ç›¸ä¼¼å®ä½“ï¼ˆä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦ï¼‰
        self.cursor.execute(f"""
            SELECT * FROM cypher('{self.graph_name}', $$
                MATCH (n)
                WHERE n.embedding IS NOT NULL
                WITH n,
                     [x IN range(0, size(n.embedding)-1) |
                      (n.embedding[x] - {query_embedding}[x])^2] as diffs
                WITH n, sqrt(reduce(total = 0.0, diff IN diffs | total + diff)) as distance
                ORDER BY distance
                LIMIT {top_k}
                RETURN id(n) as id, labels(n) as labels, distance
            $$) AS (id bigint, labels text[], distance float);
        """)

        return self.cursor.fetchall()
```

---

## 6. æ€§èƒ½ä¼˜åŒ–

### 6.1 å¤§è§„æ¨¡å›¾å¤„ç†

#### GraphSAINTé‡‡æ ·

```python
from torch_geometric.loader import GraphSAINTSampler

class LargeScaleGNN:
    """å¤§è§„æ¨¡å›¾GNN"""

    def __init__(self, model, data):
        self.model = model
        self.data = data

    def train_with_sampling(self, epochs=100):
        """ä½¿ç”¨é‡‡æ ·è®­ç»ƒ"""
        # GraphSAINTé‡‡æ ·å™¨
        loader = GraphSAINTSampler(
            self.data,
            batch_size=2000,
            num_steps=5,
            sample_coverage=100
        )

        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)

        for epoch in range(epochs):
            for subgraph in loader:
                self.model.train()
                optimizer.zero_grad()

                out = self.model(subgraph.x, subgraph.edge_index)
                loss = F.nll_loss(out[subgraph.train_mask], subgraph.y[subgraph.train_mask])

                loss.backward()
                optimizer.step()
```

#### Cluster-GCN

```python
from torch_geometric.loader import ClusterData, ClusterLoader

def train_with_cluster_gcn(data, model, epochs=100):
    """ä½¿ç”¨Cluster-GCNè®­ç»ƒ"""
    # å›¾èšç±»
    cluster_data = ClusterData(data, num_parts=10, recursive=False)
    loader = ClusterLoader(cluster_data, batch_size=1, shuffle=True)

    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    for epoch in range(epochs):
        for subgraph in loader:
            model.train()
            optimizer.zero_grad()

            out = model(subgraph.x, subgraph.edge_index)
            loss = F.nll_loss(out[subgraph.train_mask], subgraph.y[subgraph.train_mask])

            loss.backward()
            optimizer.step()
```

### 6.2 é‡‡æ ·ç­–ç•¥

#### é‚»å±…é‡‡æ ·

```python
from torch_geometric.loader import NeighborSampler

def train_with_neighbor_sampling(data, model, epochs=100):
    """ä½¿ç”¨é‚»å±…é‡‡æ ·è®­ç»ƒ"""
    train_loader = NeighborSampler(
        data.edge_index,
        node_idx=data.train_mask,
        sizes=[25, 10],  # æ¯å±‚é‡‡æ ·25å’Œ10ä¸ªé‚»å±…
        batch_size=1024,
        shuffle=True
    )

    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    for epoch in range(epochs):
        for batch_size, n_id, adjs in train_loader:
            optimizer.zero_grad()

            # è·å–å­å›¾èŠ‚ç‚¹ç‰¹å¾
            x = data.x[n_id]

            # å‰å‘ä¼ æ’­ï¼ˆé€šè¿‡é‡‡æ ·é‚»æ¥ï¼‰
            out = model(x, adjs)

            # è®¡ç®—æŸå¤±
            loss = F.nll_loss(out, data.y[n_id[:batch_size]])
            loss.backward()
            optimizer.step()
```

### 6.3 åˆ†å¸ƒå¼è®­ç»ƒ

```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel

def distributed_train(model, data, world_size=4):
    """åˆ†å¸ƒå¼è®­ç»ƒ"""
    # åˆå§‹åŒ–è¿›ç¨‹ç»„
    dist.init_process_group("nccl")
    rank = dist.get_rank()

    # åˆ›å»ºåˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨
    train_sampler = torch.utils.data.distributed.DistributedSampler(
        data,
        num_replicas=world_size,
        rank=rank
    )

    # åŒ…è£…æ¨¡å‹
    model = DistributedDataParallel(model)

    # è®­ç»ƒå¾ªç¯
    for epoch in range(100):
        train_sampler.set_epoch(epoch)
        # ... è®­ç»ƒä»£ç 
```

## ğŸ“š å‚è€ƒèµ„æº

1. **PyTorch Geometric**: <https://pytorch-geometric.readthedocs.io/>
2. **DGL**: <https://www.dgl.ai/>
3. **GNNç»¼è¿°**: <https://arxiv.org/abs/1812.08434>
4. **KGåµŒå…¥ç»¼è¿°**: <https://arxiv.org/abs/2003.08019>

---

## ğŸ“ æ›´æ–°æ—¥å¿—

- **v1.0** (2025-01): åˆå§‹ç‰ˆæœ¬
  - GNNåŸºç¡€ç†è®º
  - çŸ¥è¯†å›¾è°±ä¸­çš„GNNåº”ç”¨
  - å¸¸ç”¨GNNæ¨¡å‹å®ç°ï¼ˆGCNã€GraphSAGEã€GATã€RGCNï¼‰
  - çŸ¥è¯†å›¾è°±åµŒå…¥ï¼ˆTransEã€ComplExã€RotatEã€GNN-basedï¼‰
  - å®è·µæ¡ˆä¾‹ï¼ˆèŠ‚ç‚¹åˆ†ç±»ã€é“¾æ¥é¢„æµ‹ã€ä¸Apache AGEé›†æˆï¼‰
  - æ€§èƒ½ä¼˜åŒ–ï¼ˆå¤§è§„æ¨¡å›¾å¤„ç†ã€é‡‡æ ·ç­–ç•¥ã€åˆ†å¸ƒå¼è®­ç»ƒï¼‰

---

**çŠ¶æ€**: âœ… **æ–‡æ¡£å®Œæˆ** | [è¿”å›ç›®å½•](./README.md)
