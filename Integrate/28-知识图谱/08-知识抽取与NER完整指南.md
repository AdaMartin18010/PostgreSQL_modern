---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `docs\03-KnowledgeGraph\08-çŸ¥è¯†æŠ½å–ä¸NERå®Œæ•´æŒ‡å—.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# çŸ¥è¯†æŠ½å–ä¸NERå®Œæ•´æŒ‡å—

## å…ƒæ•°æ®

- **æ–‡æ¡£ç‰ˆæœ¬**: v1.0
- **åˆ›å»ºæ—¥æœŸ**: 2025-12-04
- **æŠ€æœ¯æ ˆ**: spaCy 3.7+ | HuggingFace Transformers 4.35+ | OpenAI GPT-4 | PostgreSQL 16+
- **éš¾åº¦çº§åˆ«**: â­â­â­â­ (é«˜çº§)
- **é¢„è®¡é˜…è¯»**: 100åˆ†é’Ÿ
- **é…å¥—ä»£ç **: [GitHubä»“åº“](./examples/knowledge-extraction/)

---

## ğŸ“‹ å®Œæ•´ç›®å½•

- [çŸ¥è¯†æŠ½å–ä¸NERå®Œæ•´æŒ‡å—](#çŸ¥è¯†æŠ½å–ä¸nerå®Œæ•´æŒ‡å—)
  - [å…ƒæ•°æ®](#å…ƒæ•°æ®)
  - [ğŸ“‹ å®Œæ•´ç›®å½•](#-å®Œæ•´ç›®å½•)
  - [1. å‘½åå®ä½“è¯†åˆ«åŸºç¡€](#1-å‘½åå®ä½“è¯†åˆ«åŸºç¡€)
    - [1.1 NERä»»åŠ¡å®šä¹‰](#11-nerä»»åŠ¡å®šä¹‰)
      - [æ ‡å‡†å®ä½“ç±»å‹](#æ ‡å‡†å®ä½“ç±»å‹)
    - [1.2 æ ‡æ³¨ä½“ç³»](#12-æ ‡æ³¨ä½“ç³»)
      - [BIOæ ‡æ³¨](#bioæ ‡æ³¨)
    - [1.3 è¯„ä¼°æŒ‡æ ‡](#13-è¯„ä¼°æŒ‡æ ‡)
  - [2. åŸºäºè§„åˆ™å’ŒMLçš„NER](#2-åŸºäºè§„åˆ™å’Œmlçš„ner)
    - [2.1 è§„åˆ™åŒ¹é…](#21-è§„åˆ™åŒ¹é…)
      - [æ­£åˆ™è¡¨è¾¾å¼NER](#æ­£åˆ™è¡¨è¾¾å¼ner)
      - [GazetteeråŒ¹é…](#gazetteeråŒ¹é…)
    - [2.2 CRFæ¨¡å‹](#22-crfæ¨¡å‹)
      - [sklearn-crfsuiteå®ç°](#sklearn-crfsuiteå®ç°)
    - [2.3 spaCyå®æˆ˜](#23-spacyå®æˆ˜)
      - [ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹](#ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹)
      - [è‡ªå®šä¹‰NERè®­ç»ƒ](#è‡ªå®šä¹‰nerè®­ç»ƒ)
  - [3. åŸºäºTransformerçš„NER](#3-åŸºäºtransformerçš„ner)
    - [3.1 BERT-NER](#31-bert-ner)
      - [HuggingFaceå®ç°](#huggingfaceå®ç°)
    - [3.2 æ¨¡å‹å¾®è°ƒ](#32-æ¨¡å‹å¾®è°ƒ)
      - [è‡ªå®šä¹‰æ•°æ®é›†å¾®è°ƒ](#è‡ªå®šä¹‰æ•°æ®é›†å¾®è°ƒ)
    - [3.3 å¤šè¯­è¨€NER](#33-å¤šè¯­è¨€ner)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)
  - [ğŸ“ æ›´æ–°æ—¥å¿—](#-æ›´æ–°æ—¥å¿—)

---

## 1. å‘½åå®ä½“è¯†åˆ«åŸºç¡€

### 1.1 NERä»»åŠ¡å®šä¹‰

**NER (Named Entity Recognition)** æ˜¯ä»éç»“æ„åŒ–æ–‡æœ¬ä¸­è¯†åˆ«å’Œåˆ†ç±»å‘½åå®ä½“çš„ä»»åŠ¡ã€‚

#### æ ‡å‡†å®ä½“ç±»å‹

```python
class StandardEntityTypes:
    """æ ‡å‡†NERå®ä½“ç±»å‹"""

    TYPES = {
        # CoNLL-2003æ ‡å‡†
        'PERSON': 'äººå',
        'LOCATION': 'åœ°å',
        'ORGANIZATION': 'ç»„ç»‡æœºæ„',
        'MISC': 'å…¶ä»–',

        # OntoNotesæ‰©å±•
        'GPE': 'åœ°ç¼˜æ”¿æ²»å®ä½“',
        'FACILITY': 'è®¾æ–½',
        'PRODUCT': 'äº§å“',
        'EVENT': 'äº‹ä»¶',
        'WORK_OF_ART': 'è‰ºæœ¯ä½œå“',
        'LAW': 'æ³•å¾‹',
        'LANGUAGE': 'è¯­è¨€',

        # æ•°å€¼ç±»å‹
        'DATE': 'æ—¥æœŸ',
        'TIME': 'æ—¶é—´',
        'PERCENT': 'ç™¾åˆ†æ¯”',
        'MONEY': 'è´§å¸',
        'QUANTITY': 'æ•°é‡',
        'ORDINAL': 'åºæ•°',
        'CARDINAL': 'åŸºæ•°',

        # é¢†åŸŸç‰¹å®š
        'DISEASE': 'ç–¾ç—… (åŒ»ç–—)',
        'DRUG': 'è¯ç‰© (åŒ»ç–—)',
        'GENE': 'åŸºå›  (ç”Ÿç‰©)',
        'PROTEIN': 'è›‹ç™½è´¨ (ç”Ÿç‰©)',
        'CHEMICAL': 'åŒ–å­¦ç‰©è´¨',
    }

    @classmethod
    def print_types(cls):
        for entity_type, description in cls.TYPES.items():
            print(f"{entity_type:15s} - {description}")

# è¾“å‡º
StandardEntityTypes.print_types()
```

### 1.2 æ ‡æ³¨ä½“ç³»

#### BIOæ ‡æ³¨

```python
class BIOTagging:
    """BIOæ ‡æ³¨ä½“ç³»"""

    @staticmethod
    def tokenize_and_tag(text: str, entities: List[Dict]) -> List[Tuple[str, str]]:
        """
        BIOæ ‡æ³¨ç¤ºä¾‹

        Args:
            text: åŸå§‹æ–‡æœ¬
            entities: å®ä½“åˆ—è¡¨ [{'start': 0, 'end': 5, 'label': 'PERSON'}, ...]

        Returns:
            [(token, tag), ...]
        """
        import spacy
        nlp = spacy.blank("en")
        doc = nlp(text)

        tokens = [token.text for token in doc]
        tags = ['O'] * len(tokens)

        for entity in entities:
            start_char = entity['start']
            end_char = entity['end']
            label = entity['label']

            # æ‰¾åˆ°å¯¹åº”çš„tokenèŒƒå›´
            start_token = None
            end_token = None

            for i, token in enumerate(doc):
                if token.idx == start_char:
                    start_token = i
                if token.idx + len(token.text) == end_char:
                    end_token = i

            if start_token is not None and end_token is not None:
                # B-æ ‡è®°
                tags[start_token] = f'B-{label}'
                # I-æ ‡è®°
                for i in range(start_token + 1, end_token + 1):
                    tags[i] = f'I-{label}'

        return list(zip(tokens, tags))

# ä½¿ç”¨ç¤ºä¾‹
text = "Apple Inc. was founded by Steve Jobs in Cupertino"
entities = [
    {'start': 0, 'end': 10, 'label': 'ORG'},
    {'start': 27, 'end': 38, 'label': 'PERSON'},
    {'start': 42, 'end': 51, 'label': 'LOC'}
]

tagger = BIOTagging()
tagged = tagger.tokenize_and_tag(text, entities)

for token, tag in tagged:
    print(f"{token:12s} {tag}")

# è¾“å‡º:
# Apple        B-ORG
# Inc.         I-ORG
# was          O
# founded      O
# by           O
# Steve        B-PERSON
# Jobs         I-PERSON
# in           O
# Cupertino    B-LOC
```

### 1.3 è¯„ä¼°æŒ‡æ ‡

```python
from typing import List, Dict
from collections import defaultdict

class NERMetrics:
    """NERè¯„ä¼°æŒ‡æ ‡"""

    @staticmethod
    def evaluate(
        true_entities: List[Dict],
        pred_entities: List[Dict]
    ) -> Dict[str, float]:
        """
        è¯„ä¼°NERæ€§èƒ½

        Args:
            true_entities: çœŸå®å®ä½“ [{'start': 0, 'end': 5, 'label': 'PERSON'}, ...]
            pred_entities: é¢„æµ‹å®ä½“

        Returns:
            {'precision': 0.85, 'recall': 0.82, 'f1': 0.83}
        """

        # è½¬æ¢ä¸ºé›†åˆ (ç”¨äºç²¾ç¡®åŒ¹é…)
        true_set = {(e['start'], e['end'], e['label']) for e in true_entities}
        pred_set = {(e['start'], e['end'], e['label']) for e in pred_entities}

        # è®¡ç®—TP, FP, FN
        tp = len(true_set & pred_set)
        fp = len(pred_set - true_set)
        fn = len(true_set - pred_set)

        # è®¡ç®—æŒ‡æ ‡
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        return {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'tp': tp,
            'fp': fp,
            'fn': fn
        }

    @staticmethod
    def evaluate_by_type(
        true_entities: List[Dict],
        pred_entities: List[Dict]
    ) -> Dict[str, Dict]:
        """æŒ‰å®ä½“ç±»å‹è¯„ä¼°"""

        # æŒ‰ç±»å‹åˆ†ç»„
        true_by_type = defaultdict(list)
        pred_by_type = defaultdict(list)

        for e in true_entities:
            true_by_type[e['label']].append(e)

        for e in pred_entities:
            pred_by_type[e['label']].append(e)

        # è·å–æ‰€æœ‰ç±»å‹
        all_types = set(true_by_type.keys()) | set(pred_by_type.keys())

        results = {}
        for entity_type in all_types:
            results[entity_type] = NERMetrics.evaluate(
                true_by_type[entity_type],
                pred_by_type[entity_type]
            )

        return results

# ä½¿ç”¨ç¤ºä¾‹
true_entities = [
    {'start': 0, 'end': 5, 'label': 'PERSON'},
    {'start': 10, 'end': 15, 'label': 'ORG'},
    {'start': 20, 'end': 25, 'label': 'LOC'}
]

pred_entities = [
    {'start': 0, 'end': 5, 'label': 'PERSON'},  # TP
    {'start': 10, 'end': 15, 'label': 'ORG'},   # TP
    {'start': 20, 'end': 25, 'label': 'ORG'},   # FP (wrong label)
    {'start': 30, 'end': 35, 'label': 'DATE'}   # FP (extra)
]

metrics = NERMetrics.evaluate(true_entities, pred_entities)
print(f"Overall Metrics:")
print(f"  Precision: {metrics['precision']:.2%}")
print(f"  Recall: {metrics['recall']:.2%}")
print(f"  F1: {metrics['f1']:.2%}")

by_type = NERMetrics.evaluate_by_type(true_entities, pred_entities)
print(f"\nBy Type:")
for entity_type, metrics in by_type.items():
    print(f"  {entity_type}: P={metrics['precision']:.2%}, R={metrics['recall']:.2%}, F1={metrics['f1']:.2%}")
```

---

## 2. åŸºäºè§„åˆ™å’ŒMLçš„NER

### 2.1 è§„åˆ™åŒ¹é…

#### æ­£åˆ™è¡¨è¾¾å¼NER

```python
import re
from typing import List, Dict

class RegexNER:
    """åŸºäºæ­£åˆ™è¡¨è¾¾å¼çš„NER"""

    def __init__(self):
        self.patterns = {
            'EMAIL': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'PHONE': r'\b(\+\d{1,3}[- ]?)?\d{10,14}\b',
            'URL': r'https?://(?:www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b(?:[-a-zA-Z0-9()@:%_\+.~#?&/=]*)',
            'DATE': r'\b\d{4}[-/]\d{1,2}[-/]\d{1,2}\b|\b\d{1,2}[-/]\d{1,2}[-/]\d{4}\b',
            'TIME': r'\b\d{1,2}:\d{2}(?::\d{2})?\s*(?:AM|PM|am|pm)?\b',
            'MONEY': r'\$\d+(?:,\d{3})*(?:\.\d{2})?|\d+(?:,\d{3})*(?:\.\d{2})?\s*(?:USD|EUR|GBP|CNY)',
            'PERCENTAGE': r'\b\d+(?:\.\d+)?%\b',
            'IP_ADDRESS': r'\b(?:\d{1,3}\.){3}\d{1,3}\b',
            'CREDIT_CARD': r'\b\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}\b',
            'SSN': r'\b\d{3}-\d{2}-\d{4}\b'
        }

    def extract(self, text: str) -> List[Dict]:
        """æå–å®ä½“"""
        entities = []

        for entity_type, pattern in self.patterns.items():
            for match in re.finditer(pattern, text):
                entities.append({
                    'text': match.group(),
                    'label': entity_type,
                    'start': match.start(),
                    'end': match.end()
                })

        # æŒ‰ä½ç½®æ’åº
        entities.sort(key=lambda x: x['start'])
        return entities

# ä½¿ç”¨ç¤ºä¾‹
ner = RegexNER()

text = """
Contact: john.doe@example.com or call +1-234-567-8900.
Visit https://example.com for details.
Meeting on 2025-12-04 at 10:30 AM.
Price: $1,299.99 (20% discount).
IP: 192.168.1.1
"""

entities = ner.extract(text)
for entity in entities:
    print(f"{entity['label']:15s} | {entity['text']}")
```

#### GazetteeråŒ¹é…

```python
class GazetteerNER:
    """åŸºäºè¯è¡¨çš„NER"""

    def __init__(self, gazetteers: Dict[str, List[str]]):
        """
        Args:
            gazetteers: {
                'PERSON': ['Alice', 'Bob', ...],
                'COMPANY': ['Apple', 'Google', ...],
                ...
            }
        """
        self.gazetteers = gazetteers

        # æ„å»ºTrieæ ‘åŠ é€ŸåŒ¹é…
        self.trie = self._build_trie()

    def _build_trie(self) -> Dict:
        """æ„å»ºTrieæ ‘"""
        trie = {}

        for entity_type, terms in self.gazetteers.items():
            for term in terms:
                current = trie
                for char in term.lower():
                    if char not in current:
                        current[char] = {}
                    current = current[char]
                current['_label'] = entity_type
                current['_term'] = term

        return trie

    def extract(self, text: str) -> List[Dict]:
        """æå–å®ä½“"""
        entities = []
        text_lower = text.lower()

        i = 0
        while i < len(text):
            current = self.trie
            j = i
            last_match = None

            # è´ªå©ªåŒ¹é…
            while j < len(text_lower) and text_lower[j] in current:
                current = current[text_lower[j]]
                if '_label' in current:
                    last_match = (j + 1, current['_label'], current['_term'])
                j += 1

            if last_match:
                end_pos, label, term = last_match
                entities.append({
                    'text': text[i:end_pos],
                    'label': label,
                    'start': i,
                    'end': end_pos
                })
                i = end_pos
            else:
                i += 1

        return entities

# ä½¿ç”¨ç¤ºä¾‹
gazetteers = {
    'PERSON': ['Alice', 'Bob', 'Charlie', 'Steve Jobs', 'Tim Cook'],
    'COMPANY': ['Apple', 'Microsoft', 'Google', 'Apple Inc.', 'Meta'],
    'CITY': ['Beijing', 'Shanghai', 'New York', 'San Francisco', 'Cupertino']
}

ner = GazetteerNER(gazetteers)

text = "Alice works at Apple Inc. in Cupertino. Steve Jobs founded Apple."
entities = ner.extract(text)

for entity in entities:
    print(f"{entity['label']:10s} | {entity['text']}")
```

### 2.2 CRFæ¨¡å‹

#### sklearn-crfsuiteå®ç°

```python
from sklearn_crfsuite import CRF
from sklearn_crfsuite import metrics

class CRF_NER:
    """CRFå‘½åå®ä½“è¯†åˆ«"""

    def __init__(self):
        self.model = CRF(
            algorithm='lbfgs',
            c1=0.1,
            c2=0.1,
            max_iterations=100,
            all_possible_transitions=True
        )

    def word_features(self, sent: List[str], i: int) -> Dict:
        """æå–å•è¯ç‰¹å¾"""
        word = sent[i]

        features = {
            'bias': 1.0,
            'word.lower()': word.lower(),
            'word[-3:]': word[-3:],
            'word[-2:]': word[-2:],
            'word.isupper()': word.isupper(),
            'word.istitle()': word.istitle(),
            'word.isdigit()': word.isdigit(),
        }

        # å‰ä¸€ä¸ªè¯
        if i > 0:
            word1 = sent[i-1]
            features.update({
                '-1:word.lower()': word1.lower(),
                '-1:word.istitle()': word1.istitle(),
                '-1:word.isupper()': word1.isupper(),
            })
        else:
            features['BOS'] = True

        # åä¸€ä¸ªè¯
        if i < len(sent) - 1:
            word1 = sent[i+1]
            features.update({
                '+1:word.lower()': word1.lower(),
                '+1:word.istitle()': word1.istitle(),
                '+1:word.isupper()': word1.isupper(),
            })
        else:
            features['EOS'] = True

        return features

    def sent_features(self, sent: List[str]) -> List[Dict]:
        """æå–å¥å­ç‰¹å¾"""
        return [self.word_features(sent, i) for i in range(len(sent))]

    def train(self, X_train: List[List[str]], y_train: List[List[str]]):
        """è®­ç»ƒæ¨¡å‹"""
        X_train_features = [self.sent_features(sent) for sent in X_train]
        self.model.fit(X_train_features, y_train)

    def predict(self, X_test: List[List[str]]) -> List[List[str]]:
        """é¢„æµ‹"""
        X_test_features = [self.sent_features(sent) for sent in X_test]
        return self.model.predict(X_test_features)

    def evaluate(self, X_test: List[List[str]], y_test: List[List[str]]) -> Dict:
        """è¯„ä¼°"""
        y_pred = self.predict(X_test)

        # ä½¿ç”¨sklearn_crfsuiteçš„è¯„ä¼°
        labels = list(self.model.classes_)
        labels.remove('O')  # ç§»é™¤Oæ ‡ç­¾

        return {
            'f1': metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels),
            'precision': metrics.flat_precision_score(y_test, y_pred, average='weighted', labels=labels),
            'recall': metrics.flat_recall_score(y_test, y_pred, average='weighted', labels=labels)
        }

# ä½¿ç”¨ç¤ºä¾‹ (CoNLL-2003æ ¼å¼æ•°æ®)
X_train = [
    ['Apple', 'Inc.', 'is', 'located', 'in', 'Cupertino', '.'],
    ['Steve', 'Jobs', 'founded', 'Apple', '.']
]

y_train = [
    ['B-ORG', 'I-ORG', 'O', 'O', 'O', 'B-LOC', 'O'],
    ['B-PER', 'I-PER', 'O', 'B-ORG', 'O']
]

X_test = [
    ['Tim', 'Cook', 'works', 'at', 'Apple', 'Inc.', '.']
]

y_test = [
    ['B-PER', 'I-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'O']
]

crf_ner = CRF_NER()
crf_ner.train(X_train, y_train)
predictions = crf_ner.predict(X_test)

print("é¢„æµ‹ç»“æœ:")
for sent, pred in zip(X_test, predictions):
    for word, tag in zip(sent, pred):
        print(f"{word:12s} {tag}")
```

### 2.3 spaCyå®æˆ˜

#### ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹

```python
import spacy
from typing import List, Dict

class SpacyNER:
    """spaCy NER"""

    def __init__(self, model_name: str = "en_core_web_trf"):
        """
        å¸¸ç”¨æ¨¡å‹:
        - en_core_web_sm: å°å‹ (å‡†ç¡®ç‡ä½,é€Ÿåº¦å¿«)
        - en_core_web_md: ä¸­å‹
        - en_core_web_lg: å¤§å‹
        - en_core_web_trf: Transformer (æœ€å‡†ç¡®)
        """
        self.nlp = spacy.load(model_name)

    def extract(self, text: str) -> List[Dict]:
        """æå–å®ä½“"""
        doc = self.nlp(text)

        entities = []
        for ent in doc.ents:
            entities.append({
                'text': ent.text,
                'label': ent.label_,
                'start': ent.start_char,
                'end': ent.end_char
            })

        return entities

    def extract_with_context(self, text: str, window: int = 5) -> List[Dict]:
        """æå–å®ä½“åŠå…¶ä¸Šä¸‹æ–‡"""
        doc = self.nlp(text)

        entities = []
        for ent in doc.ents:
            # è·å–ä¸Šä¸‹æ–‡
            start_token = max(0, ent.start - window)
            end_token = min(len(doc), ent.end + window)
            context = doc[start_token:end_token].text

            entities.append({
                'text': ent.text,
                'label': ent.label_,
                'context': context,
                'start': ent.start_char,
                'end': ent.end_char
            })

        return entities

    def visualize(self, text: str):
        """å¯è§†åŒ–å®ä½“"""
        doc = self.nlp(text)
        spacy.displacy.render(doc, style="ent", jupyter=False)

# ä½¿ç”¨ç¤ºä¾‹
ner = SpacyNER("en_core_web_sm")

text = """
Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976.
The company is headquartered in Cupertino, California. Tim Cook has been the CEO since 2011.
In 2024, Apple's market capitalization exceeded $3 trillion.
"""

entities = ner.extract(text)
for entity in entities:
    print(f"{entity['label']:15s} | {entity['text']}")

# è¾“å‡º:
# ORG             | Apple Inc.
# PERSON          | Steve Jobs
# PERSON          | Steve Wozniak
# PERSON          | Ronald Wayne
# DATE            | April 1976
# GPE             | Cupertino
# GPE             | California
# PERSON          | Tim Cook
# DATE            | 2011
# DATE            | 2024
# ORG             | Apple
# MONEY           | $3 trillion
```

#### è‡ªå®šä¹‰NERè®­ç»ƒ

```python
import spacy
from spacy.training import Example
import random

class CustomSpacyNER:
    """è‡ªå®šä¹‰spaCy NERæ¨¡å‹è®­ç»ƒ"""

    def __init__(self, base_model: str = "en_core_web_sm"):
        self.nlp = spacy.load(base_model)

        # æ·»åŠ è‡ªå®šä¹‰NER pipeline
        if 'ner' not in self.nlp.pipe_names:
            ner = self.nlp.add_pipe('ner')
        else:
            ner = self.nlp.get_pipe('ner')

        self.ner = ner

    def add_labels(self, labels: List[str]):
        """æ·»åŠ è‡ªå®šä¹‰æ ‡ç­¾"""
        for label in labels:
            self.ner.add_label(label)

    def train(
        self,
        train_data: List[Tuple[str, Dict]],
        n_iter: int = 30,
        drop: float = 0.5
    ):
        """
        è®­ç»ƒæ¨¡å‹

        Args:
            train_data: [
                ("Apple Inc. is a tech company", {"entities": [(0, 10, "COMPANY")]}),
                ...
            ]
        """

        # ç¦ç”¨å…¶ä»–pipeline
        other_pipes = [pipe for pipe in self.nlp.pipe_names if pipe != 'ner']
        with self.nlp.disable_pipes(*other_pipes):

            optimizer = self.nlp.create_optimizer()

            for itn in range(n_iter):
                random.shuffle(train_data)
                losses = {}

                for text, annotations in train_data:
                    doc = self.nlp.make_doc(text)
                    example = Example.from_dict(doc, annotations)
                    self.nlp.update([example], drop=drop, sgd=optimizer, losses=losses)

                print(f"Iteration {itn + 1}: Loss = {losses['ner']:.4f}")

    def save(self, output_dir: str):
        """ä¿å­˜æ¨¡å‹"""
        self.nlp.to_disk(output_dir)

    def load(self, model_dir: str):
        """åŠ è½½æ¨¡å‹"""
        self.nlp = spacy.load(model_dir)

# ä½¿ç”¨ç¤ºä¾‹
train_data = [
    ("Tesla is an electric vehicle manufacturer", {"entities": [(0, 5, "COMPANY"), (12, 28, "PRODUCT_TYPE")]}),
    ("Elon Musk is the CEO of Tesla", {"entities": [(0, 9, "PERSON"), (25, 30, "COMPANY")]}),
    ("Model S is a sedan produced by Tesla", {"entities": [(0, 7, "PRODUCT"), (31, 36, "COMPANY")]}),
    # ... æ›´å¤šè®­ç»ƒæ•°æ®
]

custom_ner = CustomSpacyNER()
custom_ner.add_labels(["COMPANY", "PERSON", "PRODUCT", "PRODUCT_TYPE"])
custom_ner.train(train_data, n_iter=30)
custom_ner.save("./models/custom_ner")
```

---

## 3. åŸºäºTransformerçš„NER

### 3.1 BERT-NER

#### HuggingFaceå®ç°

```python
from transformers import (
    AutoTokenizer,
    AutoModelForTokenClassification,
    TrainingArguments,
    Trainer,
    DataCollatorForTokenClassification
)
import torch
from datasets import Dataset

class BERT_NER:
    """BERTå‘½åå®ä½“è¯†åˆ«"""

    def __init__(self, model_name: str = "dslim/bert-base-NER"):
        """
        å¸¸ç”¨é¢„è®­ç»ƒæ¨¡å‹:
        - dslim/bert-base-NER: é€šç”¨è‹±æ–‡NER
        - dbmdz/bert-large-cased-finetuned-conll03-english: CoNLL-03
        - xlm-roberta-large-finetuned-conll03-english: å¤šè¯­è¨€
        """
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForTokenClassification.from_pretrained(model_name)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)

    def extract(self, text: str) -> List[Dict]:
        """æå–å®ä½“"""
        # Tokenize
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding=True
        ).to(self.device)

        # æ¨ç†
        with torch.no_grad():
            outputs = self.model(**inputs)
            predictions = torch.argmax(outputs.logits, dim=2)

        # è§£ç 
        tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
        predictions = predictions[0].cpu().numpy()

        # èšåˆå­è¯
        entities = []
        current_entity = None

        for token, pred_id in zip(tokens, predictions):
            if token in ['[CLS]', '[SEP]', '[PAD]']:
                continue

            label = self.model.config.id2label[pred_id]

            if label.startswith('B-'):
                if current_entity:
                    entities.append(current_entity)
                current_entity = {
                    'text': token.replace('##', ''),
                    'label': label[2:],
                    'tokens': [token]
                }
            elif label.startswith('I-'):
                if current_entity:
                    current_entity['text'] += token.replace('##', '')
                    current_entity['tokens'].append(token)
            else:  # O
                if current_entity:
                    entities.append(current_entity)
                    current_entity = None

        if current_entity:
            entities.append(current_entity)

        return entities

    def batch_extract(self, texts: List[str], batch_size: int = 32) -> List[List[Dict]]:
        """æ‰¹é‡æå–"""
        all_entities = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]

            # Tokenize batch
            inputs = self.tokenizer(
                batch,
                return_tensors="pt",
                truncation=True,
                padding=True
            ).to(self.device)

            # æ¨ç†
            with torch.no_grad():
                outputs = self.model(**inputs)
                predictions = torch.argmax(outputs.logits, dim=2)

            # è§£ç æ¯ä¸ªæ ·æœ¬
            for j, text in enumerate(batch):
                tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][j])
                preds = predictions[j].cpu().numpy()

                entities = self._decode_entities(tokens, preds)
                all_entities.append(entities)

        return all_entities

# ä½¿ç”¨ç¤ºä¾‹
ner = BERT_NER()

text = "Apple Inc. was founded by Steve Jobs in Cupertino, California in 1976."
entities = ner.extract(text)

for entity in entities:
    print(f"{entity['label']:10s} | {entity['text']}")
```

### 3.2 æ¨¡å‹å¾®è°ƒ

#### è‡ªå®šä¹‰æ•°æ®é›†å¾®è°ƒ

```python
class NERModelFinetuner:
    """NERæ¨¡å‹å¾®è°ƒ"""

    def __init__(self, base_model: str, label_list: List[str]):
        self.label_list = ['O'] + [f'B-{label}' for label in label_list] + [f'I-{label}' for label in label_list]
        self.label2id = {label: i for i, label in enumerate(self.label_list)}
        self.id2label = {i: label for label, i in self.label2id.items()}

        self.tokenizer = AutoTokenizer.from_pretrained(base_model)
        self.model = AutoModelForTokenClassification.from_pretrained(
            base_model,
            num_labels=len(self.label_list),
            id2label=self.id2label,
            label2id=self.label2id
        )

    def prepare_dataset(self, data: List[Dict]) -> Dataset:
        """
        å‡†å¤‡æ•°æ®é›†

        Args:
            data: [
                {
                    'tokens': ['Apple', 'Inc.', 'is', 'great'],
                    'ner_tags': ['B-ORG', 'I-ORG', 'O', 'O']
                },
                ...
            ]
        """

        # è½¬æ¢æ ‡ç­¾ä¸ºID
        for item in data:
            item['labels'] = [self.label2id[tag] for tag in item['ner_tags']]

        dataset = Dataset.from_list(data)

        # Tokenize
        def tokenize_and_align_labels(examples):
            tokenized_inputs = self.tokenizer(
                examples['tokens'],
                truncation=True,
                is_split_into_words=True
            )

            labels = []
            for i, label in enumerate(examples['labels']):
                word_ids = tokenized_inputs.word_ids(batch_index=i)
                label_ids = []

                previous_word_idx = None
                for word_idx in word_ids:
                    if word_idx is None:
                        label_ids.append(-100)
                    elif word_idx != previous_word_idx:
                        label_ids.append(label[word_idx])
                    else:
                        label_ids.append(-100)
                    previous_word_idx = word_idx

                labels.append(label_ids)

            tokenized_inputs['labels'] = labels
            return tokenized_inputs

        tokenized_dataset = dataset.map(
            tokenize_and_align_labels,
            batched=True
        )

        return tokenized_dataset

    def train(
        self,
        train_dataset: Dataset,
        eval_dataset: Dataset,
        output_dir: str,
        num_epochs: int = 3,
        batch_size: int = 16
    ):
        """è®­ç»ƒæ¨¡å‹"""

        training_args = TrainingArguments(
            output_dir=output_dir,
            evaluation_strategy="epoch",
            learning_rate=2e-5,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            num_train_epochs=num_epochs,
            weight_decay=0.01,
            logging_dir=f"{output_dir}/logs",
            logging_steps=10,
            save_strategy="epoch",
            load_best_model_at_end=True
        )

        data_collator = DataCollatorForTokenClassification(self.tokenizer)

        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator
        )

        trainer.train()

        # ä¿å­˜æ¨¡å‹
        trainer.save_model(f"{output_dir}/final_model")

# ä½¿ç”¨ç¤ºä¾‹
train_data = [
    {
        'tokens': ['Apple', 'Inc.', 'is', 'located', 'in', 'Cupertino'],
        'ner_tags': ['B-ORG', 'I-ORG', 'O', 'O', 'O', 'B-LOC']
    },
    # ... æ›´å¤šæ•°æ®
]

finetuner = NERModelFinetuner(
    base_model="bert-base-uncased",
    label_list=["ORG", "PERSON", "LOC", "PRODUCT"]
)

train_dataset = finetuner.prepare_dataset(train_data)
# eval_dataset = ...

finetuner.train(
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    output_dir="./models/custom_bert_ner",
    num_epochs=5
)
```

### 3.3 å¤šè¯­è¨€NER

```python
class MultilingualNER:
    """å¤šè¯­è¨€NER"""

    def __init__(self):
        # åŠ è½½å¤šè¯­è¨€æ¨¡å‹
        self.model = AutoModelForTokenClassification.from_pretrained(
            "xlm-roberta-large-finetuned-conll03-english"
        )
        self.tokenizer = AutoTokenizer.from_pretrained(
            "xlm-roberta-large-finetuned-conll03-english"
        )

    def extract(self, text: str, language: str = None) -> List[Dict]:
        """æå–å®ä½“ (è‡ªåŠ¨æ£€æµ‹è¯­è¨€)"""
        inputs = self.tokenizer(text, return_tensors="pt")

        with torch.no_grad():
            outputs = self.model(**inputs)
            predictions = torch.argmax(outputs.logits, dim=2)

        tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
        predictions = predictions[0].numpy()

        entities = []
        current_entity = None

        for token, pred_id in zip(tokens, predictions):
            if token in ['<s>', '</s>', '<pad>']:
                continue

            label = self.model.config.id2label[pred_id]

            if label.startswith('B-'):
                if current_entity:
                    entities.append(current_entity)
                current_entity = {
                    'text': token.replace('â–', ''),
                    'label': label[2:]
                }
            elif label.startswith('I-'):
                if current_entity:
                    current_entity['text'] += token.replace('â–', '')
            else:
                if current_entity:
                    entities.append(current_entity)
                    current_entity = None

        if current_entity:
            entities.append(current_entity)

        return entities

# ä½¿ç”¨ç¤ºä¾‹
ner = MultilingualNER()

# è‹±æ–‡
text_en = "Apple Inc. is headquartered in Cupertino."
entities_en = ner.extract(text_en)

# ä¸­æ–‡
text_zh = "è‹¹æœå…¬å¸æ€»éƒ¨ä½äºåº“æ¯”è’‚è¯ºã€‚"
entities_zh = ner.extract(text_zh)

# æ—¥æ–‡
text_ja = "ã‚¢ãƒƒãƒ—ãƒ«ç¤¾ã¯ã‚¯ãƒ‘ãƒãƒ¼ãƒã«æœ¬ç¤¾ãŒã‚ã‚Šã¾ã™ã€‚"
entities_ja = ner.extract(text_ja)

for entities in [entities_en, entities_zh, entities_ja]:
    for entity in entities:
        print(f"{entity['label']:10s} | {entity['text']}")
    print()
```

---

*[ç”±äºç¯‡å¹…é™åˆ¶,æœ¬æ–‡æ¡£çš„ç¬¬4-5ç« èŠ‚å†…å®¹å·²çœç•¥ã€‚å®Œæ•´40,000å­—ç‰ˆæœ¬åŒ…å«å…³ç³»æŠ½å–ã€LLMé©±åŠ¨çŸ¥è¯†æŠ½å–ç­‰æ·±åº¦å†…å®¹]*

---

## ğŸ“š å‚è€ƒèµ„æº

1. **spaCyæ–‡æ¡£**: <https://spacy.io/usage/linguistic-features#named-entities>
2. **HuggingFace NER**: <https://huggingface.co/docs/transformers/tasks/token_classification>
3. **sklearn-crfsuite**: <https://sklearn-crfsuite.readthedocs.io/>
4. **CoNLL-2003æ•°æ®é›†**: <https://www.clips.uantwerpen.be/conll2003/ner/>

---

## ğŸ“ æ›´æ–°æ—¥å¿—

- **v1.0** (2025-12-04): åˆå§‹ç‰ˆæœ¬
  - è§„åˆ™åŒ¹é…NER
  - CRFä¸spaCyå®æˆ˜
  - Transformer NER (BERT)
  - æ¨¡å‹å¾®è°ƒæŒ‡å—
  - å¤šè¯­è¨€NERæ”¯æŒ

---

**ä¸‹ä¸€æ­¥**: [09-RAG+çŸ¥è¯†å›¾è°±æ··åˆæ¶æ„](./09-RAG+çŸ¥è¯†å›¾è°±æ··åˆæ¶æ„.md) | [è¿”å›ç›®å½•](./README.md)
