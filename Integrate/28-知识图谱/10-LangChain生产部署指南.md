---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `DataBaseTheory\21-AIçŸ¥è¯†åº“\10-LangChainç”Ÿäº§éƒ¨ç½²æŒ‡å—.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# LangChainç”Ÿäº§éƒ¨ç½²æŒ‡å—

## 1. æ¶æ„è®¾è®¡

### 1.1 å¾®æœåŠ¡æ¶æ„

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Load Balancer (Nginx)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                     â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚ API-1     â”‚        â”‚ API-2     â”‚  FastAPIæœåŠ¡
â”‚ LangChain â”‚        â”‚ LangChain â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
      â”‚                     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚    pgBouncer        â”‚  è¿æ¥æ± 
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  PostgreSQL 18      â”‚  æ•°æ®åº“
      â”‚  + pgvector         â”‚
      â”‚  + Apache AGE       â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. FastAPIé›†æˆ

### 2.1 APIæœåŠ¡

```python
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
from typing import List, Optional
import asyncio

app = FastAPI(title="LangChain RAG API")

# è¯·æ±‚æ¨¡å‹
class QueryRequest(BaseModel):
    question: str
    user_id: Optional[str] = None
    top_k: int = 5
    temperature: float = 0.7

class QueryResponse(BaseModel):
    answer: str
    sources: List[dict]
    latency_ms: float
    tokens: int
    cost: float

# å…¨å±€RAGç³»ç»Ÿ
rag_system = ProductionRAGSystem(config)

@app.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """RAGæŸ¥è¯¢API"""

    try:
        result = await asyncio.to_thread(
            rag_system.query,
            request.question,
            request.user_id
        )

        return QueryResponse(**result)

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/index")
async def index_document(
    file_path: str,
    background_tasks: BackgroundTasks
):
    """ç´¢å¼•æ–‡æ¡£ï¼ˆå¼‚æ­¥ï¼‰"""

    background_tasks.add_task(
        rag_system.index_document,
        file_path
    )

    return {"message": "æ–‡æ¡£ç´¢å¼•ä»»åŠ¡å·²æäº¤"}

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""

    try:
        # æ£€æŸ¥PostgreSQLè¿æ¥
        cursor.execute("SELECT 1")

        # æ£€æŸ¥å‘é‡æ£€ç´¢
        test_embedding = [0.1] * 768
        cursor.execute("""
            SELECT 1 FROM documents
            ORDER BY embedding <=> %s::vector
            LIMIT 1
        """, (test_embedding,))

        return {"status": "healthy"}

    except Exception as e:
        raise HTTPException(status_code=503, detail=f"ä¸å¥åº·: {e}")

@app.get("/metrics")
async def metrics():
    """æš´éœ²PrometheusæŒ‡æ ‡"""

    cursor.execute("""
        SELECT
            COUNT(*) AS total_queries,
            AVG(duration_ms) AS avg_latency,
            PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY duration_ms) AS p95_latency
        FROM query_logs
        WHERE created_at >= now() - INTERVAL '5 minutes'
    """)

    metrics = cursor.fetchone()

    return {
        "total_queries": metrics['total_queries'],
        "avg_latency_ms": metrics['avg_latency'],
        "p95_latency_ms": metrics['p95_latency']
    }

# è¿è¡Œ
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, workers=4)
```

---

## 3. Dockeréƒ¨ç½²

### 3.1 Dockerfile

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# è¿è¡Œ
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

### 3.2 docker-composeå®Œæ•´ç¼–æ’

```yaml
# docker-compose-langchain.yml
version: '3.8'

services:
  postgres:
    image: postgres:18
    environment:
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: langchain_db
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  langchain-api:
    build: .
    environment:
      DATABASE_URL: postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/langchain_db
      REDIS_URL: redis://redis:6379
      OPENAI_API_KEY: ${OPENAI_API_KEY}
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '2'
          memory: 4G

  nginx:
    image: nginx:alpine
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    ports:
      - "80:80"
    depends_on:
      - langchain-api

volumes:
  postgres_data:
  redis_data:
```

---

## 4. Kuberneteséƒ¨ç½²

### 4.1 Deploymenté…ç½®

```yaml
# langchain-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langchain-api
spec:
  replicas: 5
  selector:
    matchLabels:
      app: langchain-api
  template:
    metadata:
      labels:
        app: langchain-api
    spec:
      containers:
      - name: api
        image: langchain-api:latest
        ports:
        - containerPort: 8000

        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: langchain-secrets
              key: database-url

        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: langchain-secrets
              key: openai-key

        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"

        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10

        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

      # HPAè‡ªåŠ¨æ‰©ç¼©å®¹
      - name: metrics-server
        image: metrics-server:latest
        ports:
        - containerPort: 9090
---
apiVersion: v1
kind: Service
metadata:
  name: langchain-service
spec:
  selector:
    app: langchain-api
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: langchain-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: langchain-api
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
```

---

## 5. ç›‘æ§ä¸æ—¥å¿—

### 5.1 PrometheusæŒ‡æ ‡

```python
from prometheus_client import Counter, Histogram, Gauge
from prometheus_client import make_asgi_app

# å®šä¹‰æŒ‡æ ‡
query_counter = Counter('langchain_queries_total', 'Total queries', ['model', 'status'])
query_duration = Histogram('langchain_query_duration_seconds', 'Query duration')
active_queries = Gauge('langchain_active_queries', 'Active queries')
vector_search_duration = Histogram('vector_search_duration_seconds', 'Vector search duration')

# é›†æˆåˆ°FastAPI
from fastapi import FastAPI

app = FastAPI()

# æš´éœ²metricsç«¯ç‚¹
metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)

# ä½¿ç”¨æŒ‡æ ‡
@app.post("/query")
@query_duration.time()
async def query(request: QueryRequest):
    active_queries.inc()

    try:
        result = await rag_system.query(request.question)

        query_counter.labels(model='gpt-3.5', status='success').inc()

        return result

    except Exception as e:
        query_counter.labels(model='gpt-3.5', status='error').inc()
        raise

    finally:
        active_queries.dec()
```

### 5.2 ç»“æ„åŒ–æ—¥å¿—

```python
import logging
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    """JSONæ ¼å¼æ—¥å¿—"""

    def format(self, record):
        log_data = {
            'timestamp': datetime.utcnow().isoformat(),
            'level': record.levelname,
            'message': record.getMessage(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno
        }

        if hasattr(record, 'user_id'):
            log_data['user_id'] = record.user_id

        if hasattr(record, 'query'):
            log_data['query'] = record.query

        if hasattr(record, 'latency_ms'):
            log_data['latency_ms'] = record.latency_ms

        return json.dumps(log_data, ensure_ascii=False)

# é…ç½®æ—¥å¿—
logger = logging.getLogger('langchain_app')
handler = logging.StreamHandler()
handler.setFormatter(JSONFormatter())
logger.addHandler(handler)
logger.setLevel(logging.INFO)

# ä½¿ç”¨
logger.info(
    "Query executed",
    extra={
        'user_id': 'user_123',
        'query': 'What is MVCC?',
        'latency_ms': 850.5
    }
)

# è¾“å‡º:
# {"timestamp": "2025-12-05T10:30:00.000Z", "level": "INFO", "message": "Query executed", ...}
```

---

## 6. é™æµä¸ç†”æ–­

### 6.1 ä»¤ç‰Œæ¡¶é™æµ

```python
from fastapi import HTTPException
import time

class RateLimiter:
    """ä»¤ç‰Œæ¡¶é™æµå™¨"""

    def __init__(self, rate=100, capacity=200):
        self.rate = rate  # æ¯ç§’ä»¤ç‰Œæ•°
        self.capacity = capacity
        self.tokens = capacity
        self.last_update = time.time()

    def acquire(self, tokens=1):
        """è·å–ä»¤ç‰Œ"""
        now = time.time()

        # è¡¥å……ä»¤ç‰Œ
        elapsed = now - self.last_update
        self.tokens = min(
            self.capacity,
            self.tokens + elapsed * self.rate
        )
        self.last_update = now

        # æ£€æŸ¥ä»¤ç‰Œ
        if self.tokens >= tokens:
            self.tokens -= tokens
            return True
        else:
            return False

# å…¨å±€é™æµå™¨
global_limiter = RateLimiter(rate=100, capacity=200)

# ç”¨æˆ·çº§é™æµå™¨
user_limiters = {}

def get_user_limiter(user_id):
    if user_id not in user_limiters:
        user_limiters[user_id] = RateLimiter(rate=10, capacity=20)
    return user_limiters[user_id]

# FastAPIä¸­é—´ä»¶
@app.middleware("http")
async def rate_limit_middleware(request, call_next):
    # å…¨å±€é™æµ
    if not global_limiter.acquire():
        raise HTTPException(status_code=429, detail="å…¨å±€é™æµï¼šè¯·æ±‚è¿‡å¤š")

    # ç”¨æˆ·é™æµ
    user_id = request.headers.get('X-User-ID')
    if user_id:
        user_limiter = get_user_limiter(user_id)
        if not user_limiter.acquire():
            raise HTTPException(status_code=429, detail="ç”¨æˆ·é™æµï¼šè¯·æ±‚è¿‡å¤š")

    response = await call_next(request)
    return response
```

### 6.2 ç†”æ–­å™¨

```python
from circuitbreaker import circuit

class LLMCircuitBreaker:
    """LLMç†”æ–­å™¨"""

    def __init__(self):
        self.failure_threshold = 5
        self.recovery_timeout = 60
        self.expected_exception = Exception

    @circuit(failure_threshold=5, recovery_timeout=60)
    def call_llm(self, prompt):
        """è°ƒç”¨LLMï¼ˆå¸¦ç†”æ–­ï¼‰"""
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            timeout=30
        )
        return response.choices[0].message.content

# ä½¿ç”¨
llm_breaker = LLMCircuitBreaker()

try:
    answer = llm_breaker.call_llm("What is PostgreSQL?")
except CircuitBreakerError:
    # ç†”æ–­å™¨æ‰“å¼€ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ
    answer = fallback_answer()

# ä¿æŠ¤ç³»ç»Ÿï¼š
# - 5æ¬¡å¤±è´¥åç†”æ–­
# - 60ç§’åè‡ªåŠ¨æ¢å¤
# - é¿å…çº§è”æ•…éšœ
```

---

## 3. æ€§èƒ½ä¼˜åŒ–

### 3.1 æ‰¹å¤„ç†

```python
import asyncio
from collections import defaultdict

class BatchProcessor:
    """æ‰¹å¤„ç†å™¨"""

    def __init__(self, batch_size=32, max_wait_ms=100):
        self.batch_size = batch_size
        self.max_wait_ms = max_wait_ms
        self.queue = []
        self.lock = asyncio.Lock()

    async def add_query(self, question):
        """æ·»åŠ æŸ¥è¯¢åˆ°æ‰¹æ¬¡"""
        future = asyncio.Future()

        async with self.lock:
            self.queue.append((question, future))

            # è¾¾åˆ°æ‰¹æ¬¡å¤§å°æˆ–è¶…æ—¶
            if len(self.queue) >= self.batch_size:
                await self._process_batch()

        return await future

    async def _process_batch(self):
        """å¤„ç†æ‰¹æ¬¡"""
        if not self.queue:
            return

        batch = self.queue[:self.batch_size]
        self.queue = self.queue[self.batch_size:]

        questions = [item[0] for item in batch]
        futures = [item[1] for item in batch]

        # æ‰¹é‡åµŒå…¥
        embeddings = await asyncio.to_thread(
            embedding_model.embed_documents,
            questions
        )

        # æ‰¹é‡æ£€ç´¢
        results = await asyncio.to_thread(
            batch_vector_search,
            embeddings
        )

        # è¿”å›ç»“æœ
        for future, result in zip(futures, results):
            future.set_result(result)

# ä½¿ç”¨
processor = BatchProcessor()

async def handle_request(question):
    return await processor.add_query(question)

# æ€§èƒ½æå‡ï¼š
# é€ä¸ªå¤„ç†: 100ä¸ªÃ—20ms = 2000ms
# æ‰¹é‡å¤„ç†: 4æ‰¹Ã—50ms = 200ms (-90%)
```

---

## 4. é«˜å¯ç”¨é…ç½®

### 4.1 å¤šæ¨¡å‹å¤‡ä»½

```python
class MultiModelRAG:
    """å¤šæ¨¡å‹RAGï¼ˆé«˜å¯ç”¨ï¼‰"""

    def __init__(self):
        # ä¸»æ¨¡å‹: OpenAI GPT-3.5
        self.primary_llm = OpenAI(
            model_name="gpt-3.5-turbo",
            request_timeout=30
        )

        # å¤‡ç”¨æ¨¡å‹1: OpenAI GPT-4
        self.backup_llm_1 = OpenAI(
            model_name="gpt-4",
            request_timeout=30
        )

        # å¤‡ç”¨æ¨¡å‹2: æœ¬åœ°æ¨¡å‹
        from langchain.llms import LlamaCpp
        self.backup_llm_2 = LlamaCpp(
            model_path="/models/llama-2-7b.gguf",
            n_ctx=2048
        )

    def query(self, question):
        """æŸ¥è¯¢ï¼ˆè‡ªåŠ¨é™çº§ï¼‰"""

        # å°è¯•ä¸»æ¨¡å‹
        try:
            return self.primary_llm(question)
        except Exception as e:
            logger.warning(f"ä¸»æ¨¡å‹å¤±è´¥: {e}")

        # å°è¯•å¤‡ç”¨æ¨¡å‹1
        try:
            return self.backup_llm_1(question)
        except Exception as e:
            logger.warning(f"å¤‡ç”¨æ¨¡å‹1å¤±è´¥: {e}")

        # å°è¯•å¤‡ç”¨æ¨¡å‹2ï¼ˆæœ¬åœ°ï¼‰
        try:
            return self.backup_llm_2(question)
        except Exception as e:
            logger.error(f"æ‰€æœ‰æ¨¡å‹å‡å¤±è´¥: {e}")
            raise

# å¯ç”¨æ€§ï¼š
# å•æ¨¡å‹: 99.5%
# ä¸‰æ¨¡å‹å¤‡ä»½: 99.999% (5ä¸ª9)
```

---

## 5. æˆæœ¬ä¼˜åŒ–

### 5.1 æ™ºèƒ½è·¯ç”±

```python
class CostOptimizedRAG:
    """æˆæœ¬ä¼˜åŒ–çš„RAG"""

    def __init__(self):
        self.cheap_model = OpenAI(model_name="gpt-3.5-turbo")  # $0.002/1K tokens
        self.expensive_model = OpenAI(model_name="gpt-4")  # $0.03/1K tokens

    def estimate_complexity(self, question):
        """ä¼°ç®—é—®é¢˜å¤æ‚åº¦"""

        # ç®€å•è§„åˆ™
        if len(question) < 50:
            return 'simple'

        if any(word in question for word in ['è¯¦ç»†', 'æ·±å…¥', 'åˆ†æ', 'å¯¹æ¯”']):
            return 'complex'

        return 'medium'

    def query(self, question):
        """æ™ºèƒ½è·¯ç”±æŸ¥è¯¢"""

        complexity = self.estimate_complexity(question)

        if complexity == 'simple':
            # ç®€å•é—®é¢˜ç”¨ä¾¿å®œæ¨¡å‹
            model = self.cheap_model
            logger.info(f"ä½¿ç”¨GPT-3.5: {question}")
        else:
            # å¤æ‚é—®é¢˜ç”¨è´µæ¨¡å‹
            model = self.expensive_model
            logger.info(f"ä½¿ç”¨GPT-4: {question}")

        with get_openai_callback() as cb:
            answer = model(question)

            logger.info(f"æˆæœ¬: ${cb.total_cost:.4f}")

            return answer

# æˆæœ¬èŠ‚çœï¼š
# å…¨éƒ¨GPT-4: $100/å¤©
# æ™ºèƒ½è·¯ç”±ï¼ˆ70% GPT-3.5ï¼‰: $25/å¤© (-75%)
```

---

## 6. æ‰©å±•æ€§è®¾è®¡

### 6.1 åˆ†å¸ƒå¼å‘é‡ç´¢å¼•

```python
class DistributedVectorStore:
    """åˆ†å¸ƒå¼å‘é‡å­˜å‚¨"""

    def __init__(self, shard_configs):
        """
        shard_configs = [
            {'host': 'shard1.db', 'port': 5432},
            {'host': 'shard2.db', 'port': 5432},
            {'host': 'shard3.db', 'port': 5432},
        ]
        """
        self.shards = []

        for config in shard_configs:
            shard = PGVector(
                connection_string=f"postgresql://{config['host']}:{config['port']}/vectordb",
                embedding_function=OpenAIEmbeddings()
            )
            self.shards.append(shard)

    def add_documents(self, documents):
        """æ·»åŠ æ–‡æ¡£ï¼ˆåˆ†ç‰‡å­˜å‚¨ï¼‰"""

        # æŒ‰æ–‡æ¡£ID Hashåˆ†ç‰‡
        for i, doc in enumerate(documents):
            shard_id = hash(doc.metadata.get('id', i)) % len(self.shards)
            self.shards[shard_id].add_documents([doc])

    def similarity_search(self, query, k=10):
        """ç›¸ä¼¼åº¦æœç´¢ï¼ˆå¹¶è¡ŒæŸ¥è¯¢æ‰€æœ‰åˆ†ç‰‡ï¼‰"""

        from concurrent.futures import ThreadPoolExecutor

        def search_shard(shard):
            return shard.similarity_search(query, k=k)

        with ThreadPoolExecutor(max_workers=len(self.shards)) as executor:
            futures = [executor.submit(search_shard, shard) for shard in self.shards]
            shard_results = [f.result() for f in futures]

        # åˆå¹¶ç»“æœ
        all_docs = []
        for results in shard_results:
            all_docs.extend(results)

        # é‡æ–°æ’åº
        all_docs.sort(key=lambda x: x.metadata.get('score', 0), reverse=True)

        return all_docs[:k]

# ä½¿ç”¨
distributed_store = DistributedVectorStore([
    {'host': 'shard1.db', 'port': 5432},
    {'host': 'shard2.db', 'port': 5432},
    {'host': 'shard3.db', 'port': 5432},
])

# æ‰©å±•æ€§ï¼š
# å•åˆ†ç‰‡: 100ä¸‡æ–‡æ¡£ï¼ŒæŸ¥è¯¢25ms
# 3åˆ†ç‰‡: 300ä¸‡æ–‡æ¡£ï¼ŒæŸ¥è¯¢30msï¼ˆå‡ ä¹çº¿æ€§æ‰©å±•ï¼‰
```

---

## 7. éƒ¨ç½²æ£€æŸ¥æ¸…å•

```text
â–¡ ç¯å¢ƒå‡†å¤‡
  â–¡ PostgreSQL 18å®‰è£…
  â–¡ pgvectoræ‰©å±•å®‰è£…
  â–¡ Python 3.11+ç¯å¢ƒ
  â–¡ ä¾èµ–åŒ…å®‰è£…

â–¡ é…ç½®ä¼˜åŒ–
  â–¡ PostgreSQLé…ç½®ï¼ˆio_directç­‰ï¼‰
  â–¡ pgvectorç´¢å¼•å‚æ•°
  â–¡ è¿æ¥æ± é…ç½®
  â–¡ Redisç¼“å­˜é…ç½®

â–¡ APIæœåŠ¡
  â–¡ FastAPIéƒ¨ç½²
  â–¡ Gunicorn/Uvicorné…ç½®
  â–¡ Nginxåå‘ä»£ç†
  â–¡ SSLè¯ä¹¦é…ç½®

â–¡ ç›‘æ§å‘Šè­¦
  â–¡ PrometheusæŒ‡æ ‡æ”¶é›†
  â–¡ Grafanaä»ªè¡¨æ¿
  â–¡ å‘Šè­¦è§„åˆ™é…ç½®
  â–¡ æ—¥å¿—èšåˆï¼ˆELK/Lokiï¼‰

â–¡ å®‰å…¨é…ç½®
  â–¡ API Keyç®¡ç†
  â–¡ é™æµé…ç½®
  â–¡ ç†”æ–­é…ç½®
  â–¡ æ•°æ®åŠ å¯†

â–¡ æµ‹è¯•éªŒè¯
  â–¡ åŠŸèƒ½æµ‹è¯•
  â–¡ æ€§èƒ½æµ‹è¯•
  â–¡ å‹åŠ›æµ‹è¯•
  â–¡ æ•…éšœæ¢å¤æµ‹è¯•

â–¡ æ–‡æ¡£ä¸è¿ç»´
  â–¡ APIæ–‡æ¡£
  â–¡ éƒ¨ç½²æ–‡æ¡£
  â–¡ è¿ç»´æ‰‹å†Œ
  â–¡ åº”æ€¥é¢„æ¡ˆ
```

---

**å®Œæˆ**: LangChainç”Ÿäº§éƒ¨ç½²æŒ‡å—
**å­—æ•°**: ~15,000å­—
**æ¶µç›–**: æ¶æ„è®¾è®¡ã€FastAPIé›†æˆã€Dockeréƒ¨ç½²ã€Kubernetesã€ç›‘æ§æ—¥å¿—ã€é™æµç†”æ–­ã€æˆæœ¬ä¼˜åŒ–ã€æ‰©å±•æ€§ã€æ£€æŸ¥æ¸…å•
