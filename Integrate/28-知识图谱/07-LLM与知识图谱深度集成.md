---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `docs\03-KnowledgeGraph\07-LLMä¸çŸ¥è¯†å›¾è°±æ·±åº¦é›†æˆ.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# LLMä¸çŸ¥è¯†å›¾è°±æ·±åº¦é›†æˆå®Œæ•´æŒ‡å—

## å…ƒæ•°æ®

- **æ–‡æ¡£ç‰ˆæœ¬**: v1.0
- **åˆ›å»ºæ—¥æœŸ**: 2025-12-04
- **é€‚ç”¨æŠ€æœ¯æ ˆ**: PostgreSQL 16+ | Apache AGE 1.5+ | pgvector 0.7+ | OpenAI/Anthropic API
- **éš¾åº¦çº§åˆ«**: â­â­â­â­â­ (ä¸“å®¶çº§)
- **é¢„è®¡é˜…è¯»**: 150åˆ†é’Ÿ
- **é…å¥—èµ„æº**: [å®Œæ•´ä»£ç ](./examples/llm-kg/) | [Jupyter Notebooks](./notebooks/)

---

## ğŸ“‹ å®Œæ•´ç›®å½•

- [LLMä¸çŸ¥è¯†å›¾è°±æ·±åº¦é›†æˆå®Œæ•´æŒ‡å—](#llmä¸çŸ¥è¯†å›¾è°±æ·±åº¦é›†æˆå®Œæ•´æŒ‡å—)
  - [å…ƒæ•°æ®](#å…ƒæ•°æ®)
  - [ğŸ“‹ å®Œæ•´ç›®å½•](#-å®Œæ•´ç›®å½•)
  - [1. LLM+çŸ¥è¯†å›¾è°±èåˆæ¶æ„](#1-llmçŸ¥è¯†å›¾è°±èåˆæ¶æ„)
    - [1.1 ä¸ºä»€ä¹ˆéœ€è¦èåˆ](#11-ä¸ºä»€ä¹ˆéœ€è¦èåˆ)
      - [LLMçš„å±€é™æ€§](#llmçš„å±€é™æ€§)
      - [çŸ¥è¯†å›¾è°±çš„å±€é™æ€§](#çŸ¥è¯†å›¾è°±çš„å±€é™æ€§)
      - [èåˆçš„ä»·å€¼](#èåˆçš„ä»·å€¼)
    - [1.2 èåˆæ¨¡å¼](#12-èåˆæ¨¡å¼)
      - [æ¨¡å¼1: LLMå¢å¼ºçŸ¥è¯†å›¾è°±](#æ¨¡å¼1-llmå¢å¼ºçŸ¥è¯†å›¾è°±)
      - [æ¨¡å¼2: çŸ¥è¯†å›¾è°±å¢å¼ºLLM](#æ¨¡å¼2-çŸ¥è¯†å›¾è°±å¢å¼ºllm)
      - [æ¨¡å¼3: åŒå‘å¢å¼º (æ¨è)](#æ¨¡å¼3-åŒå‘å¢å¼º-æ¨è)
    - [1.3 æŠ€æœ¯æŒ‘æˆ˜](#13-æŠ€æœ¯æŒ‘æˆ˜)
  - [2. Text-to-Cypherç”Ÿæˆç³»ç»Ÿ](#2-text-to-cypherç”Ÿæˆç³»ç»Ÿ)
    - [2.1 Promptå·¥ç¨‹](#21-promptå·¥ç¨‹)
      - [é«˜è´¨é‡Promptæ¨¡æ¿](#é«˜è´¨é‡promptæ¨¡æ¿)
    - [2.2 Few-Shotå­¦ä¹ ](#22-few-shotå­¦ä¹ )
      - [åŠ¨æ€ç¤ºä¾‹é€‰æ‹©](#åŠ¨æ€ç¤ºä¾‹é€‰æ‹©)
    - [2.3 é”™è¯¯ä¿®å¤æœºåˆ¶](#23-é”™è¯¯ä¿®å¤æœºåˆ¶)
      - [è‡ªåŠ¨Cypherä¿®å¤](#è‡ªåŠ¨cypherä¿®å¤)
      - [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
    - [2.4 æ€§èƒ½ä¼˜åŒ–](#24-æ€§èƒ½ä¼˜åŒ–)
      - [æŸ¥è¯¢ç¼“å­˜](#æŸ¥è¯¢ç¼“å­˜)
  - [3. KBQAç³»ç»Ÿå®Œæ•´å®ç°](#3-kbqaç³»ç»Ÿå®Œæ•´å®ç°)
    - [3.1 é—®é¢˜ç†è§£](#31-é—®é¢˜ç†è§£)
      - [æ„å›¾è¯†åˆ«](#æ„å›¾è¯†åˆ«)
    - [3.2 å®ä½“è¯†åˆ«ä¸é“¾æ¥](#32-å®ä½“è¯†åˆ«ä¸é“¾æ¥)
      - [é«˜çº§å®ä½“é“¾æ¥](#é«˜çº§å®ä½“é“¾æ¥)
    - [3.3 å­å›¾æ£€ç´¢](#33-å­å›¾æ£€ç´¢)
      - [å¤šè·³å­å›¾æ£€ç´¢](#å¤šè·³å­å›¾æ£€ç´¢)
    - [3.4 ç­”æ¡ˆç”Ÿæˆ](#34-ç­”æ¡ˆç”Ÿæˆ)
      - [ç­”æ¡ˆç”Ÿæˆç­–ç•¥](#ç­”æ¡ˆç”Ÿæˆç­–ç•¥)
    - [3.5 å¤šè·³æ¨ç†](#35-å¤šè·³æ¨ç†)
      - [è·¯å¾„æ¨ç†](#è·¯å¾„æ¨ç†)
  - [4. RAG+KGæ··åˆæ¶æ„](#4-ragkgæ··åˆæ¶æ„)
    - [4.1 æ··åˆæ£€ç´¢](#41-æ··åˆæ£€ç´¢)
    - [4.2 ç»“æœèåˆ](#42-ç»“æœèåˆ)
  - [5. LLMé©±åŠ¨çš„çŸ¥è¯†æŠ½å–](#5-llmé©±åŠ¨çš„çŸ¥è¯†æŠ½å–)
    - [5.1 å®ä½“æŠ½å–](#51-å®ä½“æŠ½å–)
    - [5.2 å…³ç³»æŠ½å–](#52-å…³ç³»æŠ½å–)
  - [6. ä¼ä¸šçº§ç”Ÿäº§æ¶æ„](#6-ä¼ä¸šçº§ç”Ÿäº§æ¶æ„)
    - [6.1 ç³»ç»Ÿæ¶æ„](#61-ç³»ç»Ÿæ¶æ„)
    - [6.2 æ€§èƒ½ä¼˜åŒ–](#62-æ€§èƒ½ä¼˜åŒ–)
    - [6.3 ç›‘æ§å‘Šè­¦](#63-ç›‘æ§å‘Šè­¦)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)
  - [ğŸ“ æ›´æ–°æ—¥å¿—](#-æ›´æ–°æ—¥å¿—)
  - [ğŸ¯ å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
    - [å®‰è£…ä¾èµ–](#å®‰è£…ä¾èµ–)
    - [åŸºç¡€ä½¿ç”¨](#åŸºç¡€ä½¿ç”¨)

---

## 1. LLM+çŸ¥è¯†å›¾è°±èåˆæ¶æ„

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦èåˆ

#### LLMçš„å±€é™æ€§

| å±€é™ | æè¿° | çŸ¥è¯†å›¾è°±å¦‚ä½•è§£å†³ |
|------|------|------------------|
| **å¹»è§‰é—®é¢˜** | ç”Ÿæˆä¸çœŸå®çš„ä¿¡æ¯ | æä¾›å¯éªŒè¯çš„äº‹å®ä¾æ® |
| **æ—¶æ•ˆæ€§å·®** | è®­ç»ƒæ•°æ®æˆªæ­¢æ—¥æœŸ | å®æ—¶æ›´æ–°çš„çŸ¥è¯†åº“ |
| **å¯è§£é‡Šæ€§å¼±** | é»‘ç›’æ¨ç†è¿‡ç¨‹ | æ˜ç¡®çš„æ¨ç†è·¯å¾„ |
| **çŸ¥è¯†æ›´æ–°éš¾** | éœ€è¦é‡æ–°è®­ç»ƒ | åŠ¨æ€æ·»åŠ /ä¿®æ”¹çŸ¥è¯† |
| **é¢†åŸŸçŸ¥è¯†ä¸è¶³** | é€šç”¨æ¨¡å‹ç¼ºä¹ä¸“ä¸šæ€§ | å­˜å‚¨é¢†åŸŸä¸“å®¶çŸ¥è¯† |

#### çŸ¥è¯†å›¾è°±çš„å±€é™æ€§

| å±€é™ | æè¿° | LLMå¦‚ä½•è§£å†³ |
|------|------|-------------|
| **æ„å»ºæˆæœ¬é«˜** | éœ€è¦äººå·¥æ ‡æ³¨ | è‡ªåŠ¨åŒ–çŸ¥è¯†æŠ½å– |
| **æŸ¥è¯¢å›°éš¾** | éœ€è¦å­¦ä¹ Cypher | è‡ªç„¶è¯­è¨€æ¥å£ |
| **æ³›åŒ–èƒ½åŠ›å¼±** | åªèƒ½å›ç­”å·²çŸ¥çŸ¥è¯† | å¸¸è¯†æ¨ç†ä¸åˆ›é€ æ€§å›ç­” |
| **å†·å¯åŠ¨é—®é¢˜** | åˆæœŸçŸ¥è¯†ç¨€ç– | é¢„è®­ç»ƒçŸ¥è¯†è¡¥å…… |

#### èåˆçš„ä»·å€¼

```text
LLM + KGèåˆç³»ç»Ÿ = 1 + 1 > 2

ä¼˜åŠ¿ï¼š
âœ… å‡†ç¡®æ€§: KGæä¾›äº‹å®éªŒè¯
âœ… æ—¶æ•ˆæ€§: KGå®æ—¶æ›´æ–°
âœ… å¯è§£é‡Šæ€§: æ˜ç¡®çš„æ¨ç†é“¾
âœ… é¢†åŸŸä¸“ä¸šæ€§: KGä¸“ä¸šçŸ¥è¯† + LLMç†è§£
âœ… ç”¨æˆ·ä½“éªŒ: è‡ªç„¶è¯­è¨€äº¤äº’
```

### 1.2 èåˆæ¨¡å¼

#### æ¨¡å¼1: LLMå¢å¼ºçŸ¥è¯†å›¾è°±

```text
ç”¨æˆ·é—®é¢˜ â†’ LLMç†è§£ â†’ ç”ŸæˆCypher â†’ KGæŸ¥è¯¢ â†’ è¿”å›ç»“æœ
```

**é€‚ç”¨åœºæ™¯**: ç»“æ„åŒ–æŸ¥è¯¢ã€ç²¾ç¡®é—®ç­”

#### æ¨¡å¼2: çŸ¥è¯†å›¾è°±å¢å¼ºLLM

```text
ç”¨æˆ·é—®é¢˜ â†’ KGæ£€ç´¢ç›¸å…³çŸ¥è¯† â†’ æ³¨å…¥LLMä¸Šä¸‹æ–‡ â†’ LLMç”Ÿæˆç­”æ¡ˆ
```

**é€‚ç”¨åœºæ™¯**: éœ€è¦åˆ›é€ æ€§ã€è§£é‡Šæ€§å›ç­”

#### æ¨¡å¼3: åŒå‘å¢å¼º (æ¨è)

```text
ç”¨æˆ·é—®é¢˜
   â”œâ”€â†’ LLMç†è§£ + å®ä½“è¯†åˆ«
   â”œâ”€â†’ KGå­å›¾æ£€ç´¢
   â”œâ”€â†’ å‘é‡ç›¸ä¼¼åº¦æœç´¢
   â””â”€â†’ LLMç»¼åˆç”Ÿæˆç­”æ¡ˆ
```

**é€‚ç”¨åœºæ™¯**: å¤æ‚åœºæ™¯ã€ä¼ä¸šçº§åº”ç”¨

### 1.3 æŠ€æœ¯æŒ‘æˆ˜

```python
# æŠ€æœ¯æŒ‘æˆ˜æ¸…å•
class LLMKGChallenges:
    """LLM+KGèåˆçš„æŠ€æœ¯æŒ‘æˆ˜"""

    challenges = {
        "å‡†ç¡®æ€§": {
            "Text-to-Cypherç²¾åº¦": "éœ€è¦>=90%å‡†ç¡®ç‡",
            "å®ä½“é“¾æ¥å‡†ç¡®æ€§": "æ¶ˆæ­§å›°éš¾",
            "å¹»è§‰æ§åˆ¶": "é˜²æ­¢LLMç¼–é€ äº‹å®"
        },
        "æ€§èƒ½": {
            "å»¶è¿Ÿ": "ç«¯åˆ°ç«¯<2ç§’",
            "å¹¶å‘": "æ”¯æŒ1000+ QPS",
            "æˆæœ¬": "LLM APIè°ƒç”¨æˆæœ¬æ§åˆ¶"
        },
        "å·¥ç¨‹åŒ–": {
            "é”™è¯¯å¤„ç†": "Cypherè¯­æ³•é”™è¯¯è‡ªåŠ¨ä¿®å¤",
            "ç¼“å­˜ç­–ç•¥": "å‡å°‘é‡å¤è°ƒç”¨",
            "ç›‘æ§": "å…¨é“¾è·¯å¯è§‚æµ‹"
        },
        "æ•°æ®è´¨é‡": {
            "KGå®Œæ•´æ€§": "çŸ¥è¯†è¦†ç›–ç‡",
            "Schemaè¿›åŒ–": "é€‚åº”å˜åŒ–",
            "æ•°æ®ä¸€è‡´æ€§": "å¤šæºèåˆ"
        }
    }

    @classmethod
    def print_challenges(cls):
        for category, items in cls.challenges.items():
            print(f"\nã€{category}ã€‘")
            for challenge, description in items.items():
                print(f"  - {challenge}: {description}")

# è¾“å‡ºæŒ‘æˆ˜æ¸…å•
LLMKGChallenges.print_challenges()
```

---

## 2. Text-to-Cypherç”Ÿæˆç³»ç»Ÿ

### 2.1 Promptå·¥ç¨‹

#### é«˜è´¨é‡Promptæ¨¡æ¿

```python
import json
from typing import Dict, List, Optional
from openai import OpenAI

class CypherPromptTemplate:
    """Text-to-Cypher Promptæ¨¡æ¿"""

    @staticmethod
    def get_system_prompt(schema: Dict) -> str:
        """ç³»ç»Ÿæç¤ºè¯"""
        return f"""ä½ æ˜¯ä¸€ä¸ªCypheræŸ¥è¯¢ä¸“å®¶ï¼Œä¸“é—¨ä¸ºApache AGEå›¾æ•°æ®åº“ç”ŸæˆæŸ¥è¯¢ã€‚

## å›¾Schema

### èŠ‚ç‚¹ç±»å‹ (Node Labels)
{json.dumps(schema['node_labels'], indent=2, ensure_ascii=False)}

### å…³ç³»ç±»å‹ (Relationship Types)
{json.dumps(schema['relationship_types'], indent=2, ensure_ascii=False)}

### å…³ç³»è¿æ¥æ¨¡å¼ (Connection Patterns)
{json.dumps(schema.get('patterns', {}), indent=2, ensure_ascii=False)}

## Cypherè¯­æ³•è§„åˆ™

1. **åŸºæœ¬æ¨¡å¼åŒ¹é…**

    ```cypher
    MATCH (n:Label {{property: 'value'}})
    RETURN n
    ```

2. **å…³ç³»åŒ¹é…**

    ```cypher
    MATCH (a:Person)-[r:KNOWS]->(b:Person)
    WHERE r.since > 2020
    RETURN a.name, b.name
    ```

3. **è·¯å¾„æŸ¥è¯¢**

    ```cypher
    MATCH path = (a)-[:KNOWS*1..3]->(b)
    RETURN path
    ```

4. **èšåˆæŸ¥è¯¢**

    ```cypher
    MATCH (p:Person)-[:WORKS_FOR]->(c:Company)
    RETURN c.name, COUNT(p) AS employee_count
    ORDER BY employee_count DESC
    ```

## é‡è¦çº¦æŸ

âœ… DO:

- å§‹ç»ˆä½¿ç”¨æ ‡ç­¾å’Œå…³ç³»ç±»å‹ (åŒºåˆ†å¤§å°å†™)
- ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢é˜²æ­¢æ³¨å…¥
- æ·»åŠ LIMITé™åˆ¶ç»“æœæ•°é‡
- ä½¿ç”¨æ˜ç¡®çš„æ–¹å‘ `-[r:TYPE]->` è€Œä¸æ˜¯ `-[r:TYPE]-`
- å±æ€§è®¿é—®ä½¿ç”¨ç‚¹å·: `n.property`

âŒ DON'T:

- ä¸è¦ä½¿ç”¨æœªåœ¨schemaä¸­å®šä¹‰çš„æ ‡ç­¾/å…³ç³»
- ä¸è¦ç”Ÿæˆç ´åæ€§æŸ¥è¯¢ (CREATE/DELETE/SET) é™¤éæ˜ç¡®è¦æ±‚
- ä¸è¦ä½¿ç”¨å¤æ‚çš„è·¯å¾„æ¨¡å¼ (é•¿åº¦>5)
- ä¸è¦å¿˜è®°WHEREè¿‡æ»¤æ¡ä»¶

## è¾“å‡ºæ ¼å¼

åªè¿”å›CypheræŸ¥è¯¢ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–ä»£ç å—æ ‡è®°ã€‚
"""

    @staticmethod
    def get_few_shot_examples() -> List[Dict]:
        """Few-Shotç¤ºä¾‹"""
        return [
            {
                "question": "æœ‰å¤šå°‘ä¸ªç”¨æˆ·?",
                "cypher": "MATCH (u:User) RETURN COUNT(u) AS user_count"
            },
            {
                "question": "æ‰¾å‡ºå¹´é¾„è¶…è¿‡30å²çš„ç”¨æˆ·",
                "cypher": """MATCH (u:User)
WHERE u.age > 30
RETURN u.name, u.age
ORDER BY u.age DESC
LIMIT 100"""
            },
            {
                "question": "Aliceçš„æœ‹å‹æœ‰å“ªäº›?",
                "cypher": """MATCH (a:User {name: 'Alice'})-[:FRIEND]->(friend)
RETURN friend.name, friend.age
ORDER BY friend.name"""
            },
            {
                "question": "æ‰¾å‡ºå…±åŒæœ‹å‹æœ€å¤šçš„ç”¨æˆ·å¯¹",
                "cypher": """MATCH (u1:User)-[:FRIEND]->(common)<-[:FRIEND]-(u2:User)
WHERE id(u1) < id(u2)
RETURN u1.name, u2.name, COUNT(common) AS common_friends
ORDER BY common_friends DESC
LIMIT 10"""
            },
            {
                "question": "ä»åŒ—äº¬åˆ°ä¸Šæµ·çš„æœ€çŸ­è·¯å¾„",
                "cypher": """MATCH path = shortestPath(
  (a:City {name: 'åŒ—äº¬'})-[:CONNECTED_TO*]-(b:City {name: 'ä¸Šæµ·'})
)
RETURN path, length(path) AS distance"""
            },
            {
                "question": "æ¯ä¸ªå…¬å¸æœ‰å¤šå°‘å‘˜å·¥?",
                "cypher": """MATCH (p:Person)-[:WORKS_FOR]->(c:Company)
RETURN c.name AS company, COUNT(p) AS employee_count
ORDER BY employee_count DESC"""
            },
            {
                "question": "æ¨èä¸Bobå…´è¶£ç›¸ä¼¼çš„ç”¨æˆ·",
                "cypher": """MATCH (bob:User {name: 'Bob'})
MATCH (other:User)
WHERE other <> bob
  AND SIZE([i IN bob.interests WHERE i IN other.interests]) > 0
RETURN other.name,
       SIZE([i IN bob.interests WHERE i IN other.interests]) AS common_interests
ORDER BY common_interests DESC
LIMIT 10"""
            }
        ]

    @staticmethod
    def format_user_prompt(question: str, examples: List[Dict] = None) -> str:
        """ç”¨æˆ·æç¤ºè¯"""
        prompt = "å°†ä»¥ä¸‹é—®é¢˜è½¬æ¢ä¸ºCypheræŸ¥è¯¢:\n\n"

        # æ·»åŠ Few-Shotç¤ºä¾‹
        if examples:
            prompt += "## å‚è€ƒç¤ºä¾‹\n\n"
            for ex in examples:
                prompt += f"é—®é¢˜: {ex['question']}\n"
                prompt += f"Cypher:\n```cypher\n{ex['cypher']}\n```\n\n"

        prompt += f"## å½“å‰é—®é¢˜\n\né—®é¢˜: {question}\n\nCypher:\n"

        return prompt

# ä½¿ç”¨ç¤ºä¾‹

schema = {
    "node_labels": {
        "User": ["name", "age", "email", "interests"],
        "Company": ["name", "industry", "founded"],
        "City": ["name", "country", "population"]
    },
    "relationship_types": {
        "FRIEND": ["since", "strength"],
        "WORKS_FOR": ["position", "since"],
        "LOCATED_IN": []
    },
    "patterns": [
        "(User)-[:FRIEND]->(User)",
        "(User)-[:WORKS_FOR]->(Company)",
        "(Company)-[:LOCATED_IN]->(City)"
    ]
}

template = CypherPromptTemplate()
system_prompt = template.get_system_prompt(schema)
user_prompt = template.format_user_prompt(
    "æ‰¾å‡ºåœ¨ç§‘æŠ€å…¬å¸å·¥ä½œçš„30å²ä»¥ä¸Šç”¨æˆ·",
    examples=template.get_few_shot_examples()[:3]
)

print(system_prompt)
print("\n" + "="*80 + "\n")
print(user_prompt)

```

### 2.2 Few-Shotå­¦ä¹ 

#### åŠ¨æ€ç¤ºä¾‹é€‰æ‹©

```python
from typing import List, Dict
from sentence_transformers import SentenceTransformer
import numpy as np

class DynamicFewShotSelector:
    """åŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„Few-Shotç¤ºä¾‹"""

    def __init__(self, example_bank: List[Dict]):
        self.example_bank = example_bank
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

        # é¢„è®¡ç®—ç¤ºä¾‹çš„å‘é‡
        self.example_embeddings = self.embedding_model.encode(
            [ex['question'] for ex in example_bank]
        )

    def select_examples(self, question: str, top_k: int = 3) -> List[Dict]:
        """é€‰æ‹©æœ€ç›¸å…³çš„Kä¸ªç¤ºä¾‹"""
        # ç”Ÿæˆé—®é¢˜å‘é‡
        question_emb = self.embedding_model.encode(question)

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(self.example_embeddings, question_emb) / (
            np.linalg.norm(self.example_embeddings, axis=1) * np.linalg.norm(question_emb)
        )

        # é€‰æ‹©Top-K
        top_indices = np.argsort(similarities)[-top_k:][::-1]

        return [self.example_bank[i] for i in top_indices]

# æ„å»ºç¤ºä¾‹åº“
example_bank = [
    {"question": "æœ‰å¤šå°‘ä¸ªç”¨æˆ·?", "cypher": "MATCH (u:User) RETURN COUNT(u)"},
    {"question": "å¹´é¾„æœ€å¤§çš„10ä¸ªç”¨æˆ·", "cypher": "MATCH (u:User) RETURN u.name, u.age ORDER BY u.age DESC LIMIT 10"},
    {"question": "Aliceçš„æœ‹å‹", "cypher": "MATCH (a:User {name: 'Alice'})-[:FRIEND]->(f) RETURN f.name"},
    {"question": "åœ¨åŒ—äº¬çš„å…¬å¸", "cypher": "MATCH (c:Company)-[:LOCATED_IN]->(city:City {name: 'åŒ—äº¬'}) RETURN c.name"},
    {"question": "æ¯ä¸ªå…¬å¸çš„å‘˜å·¥æ•°", "cypher": "MATCH (p:Person)-[:WORKS_FOR]->(c:Company) RETURN c.name, COUNT(p) AS count"},
    # ... æ›´å¤šç¤ºä¾‹
]

selector = DynamicFewShotSelector(example_bank)

# ä¸ºæ–°é—®é¢˜é€‰æ‹©ç¤ºä¾‹
question = "æ‰¾å‡ºåœ¨ç§‘æŠ€å…¬å¸å·¥ä½œçš„å‘˜å·¥äººæ•°"
selected_examples = selector.select_examples(question, top_k=3)

print(f"é—®é¢˜: {question}\n")
print("é€‰ä¸­çš„ç¤ºä¾‹:")
for ex in selected_examples:
    print(f"  - {ex['question']}")
```

### 2.3 é”™è¯¯ä¿®å¤æœºåˆ¶

#### è‡ªåŠ¨Cypherä¿®å¤

```python
import re
import json
import psycopg2
from typing import Optional, List, Dict, Tuple
from openai import OpenAI

class CypherErrorFixer:
    """CypheræŸ¥è¯¢é”™è¯¯è‡ªåŠ¨ä¿®å¤"""

    def __init__(self, conn, graph_name: str):
        self.conn = conn
        self.graph_name = graph_name
        self.cursor = conn.cursor()

    def execute_with_retry(
        self,
        cypher: str,
        max_retries: int = 3,
        llm_client: Optional[OpenAI] = None
    ) -> tuple:
        """å¸¦é‡è¯•çš„æ‰§è¡Œ"""

        for attempt in range(max_retries):
            try:
                # å°è¯•æ‰§è¡Œ
                result = self._execute_cypher(cypher)
                return (True, result, cypher)

            except Exception as e:
                error_msg = str(e)
                print(f"âŒ æ‰§è¡Œå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {error_msg}")

                # å°è¯•è‡ªåŠ¨ä¿®å¤
                fixed_cypher = self._auto_fix(cypher, error_msg)

                if fixed_cypher and fixed_cypher != cypher:
                    print(f"ğŸ”§ è‡ªåŠ¨ä¿®å¤åçš„æŸ¥è¯¢:\n{fixed_cypher}")
                    cypher = fixed_cypher
                    continue

                # å¦‚æœæœ‰LLMï¼Œå°è¯•LLMä¿®å¤
                if llm_client and attempt < max_retries - 1:
                    print("ğŸ¤– å°è¯•LLMä¿®å¤...")
                    cypher = self._llm_fix(cypher, error_msg, llm_client)
                    continue

                # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                if attempt == max_retries - 1:
                    return (False, None, error_msg)

        return (False, None, "Max retries exceeded")

    def _execute_cypher(self, cypher: str) -> List[Dict]:
        """æ‰§è¡ŒCypheræŸ¥è¯¢"""
        # æå–RETURNåˆ—
        return_cols = self._extract_return_columns(cypher)

        self.cursor.execute(f"""
            SELECT * FROM cypher('{self.graph_name}', $$
                {cypher}
            $$) AS ({', '.join([f'{col} agtype' for col in return_cols])});
        """)

        results = []
        for row in self.cursor.fetchall():
            result = {}
            for i, col in enumerate(return_cols):
                try:
                    result[col] = json.loads(row[i]) if row[i] else None
                except:
                    result[col] = str(row[i])
            results.append(result)

        return results

    def _auto_fix(self, cypher: str, error_msg: str) -> Optional[str]:
        """åŸºäºè§„åˆ™çš„è‡ªåŠ¨ä¿®å¤"""

        # è§„åˆ™1: ç¼ºå°‘RETURNè¯­å¥
        if "RETURN" not in cypher.upper():
            cypher += "\nRETURN *"

        # è§„åˆ™2: å±æ€§è®¿é—®é”™è¯¯ (n['name'] -> n.name)
        cypher = re.sub(r"(\w+)\['(\w+)'\]", r"\1.\2", cypher)

        # è§„åˆ™3: å…³ç³»æ–¹å‘é”™è¯¯ (åŒå‘æ”¹å•å‘)
        if "undirected" in error_msg.lower():
            cypher = cypher.replace("--", "-->")

        # è§„åˆ™4: ç¼ºå°‘æ ‡ç­¾
        if "label" in error_msg.lower():
            # å°è¯•æ·»åŠ é€šç”¨æ ‡ç­¾
            cypher = re.sub(r"MATCH \((\w+)\)", r"MATCH (\1:Entity)", cypher)

        # è§„åˆ™5: å±æ€§åé”™è¯¯ (å¸¸è§æ‹¼å†™é”™è¯¯)
        typo_fixes = {
            'nmae': 'name',
            'eamil': 'email',
            'comapny': 'company'
        }
        for typo, correct in typo_fixes.items():
            cypher = cypher.replace(typo, correct)

        return cypher

    def _llm_fix(self, cypher: str, error_msg: str, llm_client: OpenAI) -> str:
        """ä½¿ç”¨LLMä¿®å¤é”™è¯¯"""
        prompt = f"""ä½ æ˜¯Cypherä¸“å®¶ã€‚ä»¥ä¸‹æŸ¥è¯¢æ‰§è¡Œå¤±è´¥ï¼Œè¯·ä¿®å¤å®ƒã€‚

åŸå§‹æŸ¥è¯¢:
```cypher
{cypher}
```

```text
é”™è¯¯ä¿¡æ¯:
{error_msg}

è¯·è¿”å›ä¿®å¤åçš„CypheræŸ¥è¯¢ï¼Œåªè¿”å›æŸ¥è¯¢æœ¬èº«ï¼Œä¸è¦è§£é‡Šã€‚
"""
        response = llm_client.chat.completions.create(
            model="gpt-4-turbo-preview",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯CypheræŸ¥è¯¢ä¿®å¤ä¸“å®¶ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1
        )
        fixed_cypher = response.choices[0].message.content.strip()
        fixed_cypher = fixed_cypher.replace("```cypher", "").replace("```", "").strip()
        return fixed_cypher

    def _extract_return_columns(self, cypher: str) -> List[str]:
        """æå–RETURNåˆ—å"""
        match = re.search(r'RETURN\s+(.*?)(?:ORDER BY|LIMIT|$)', cypher, re.IGNORECASE | re.DOTALL)
        if not match:
            return ['result']

        return_clause = match.group(1).strip()

        if return_clause == '*':
            return ['result']

        columns = []
        for part in return_clause.split(','):
            part = part.strip()
            if ' AS ' in part.upper():
                alias = part.split(' AS ')[-1].strip()
                columns.append(alias)
            else:
                columns.append(part.split('.')[-1].strip('()'))

        return columns
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
from openai import OpenAI

conn = psycopg2.connect("dbname=test_db user=postgres")
fixer = CypherErrorFixer(conn, 'test_graph')
llm_client = OpenAI(api_key='your-key')

# æµ‹è¯•æœ‰é”™è¯¯çš„æŸ¥è¯¢

bad_cypher = """
MATCH (u:User)
WHERE u.nmae = 'Alice'  -- æ‹¼å†™é”™è¯¯
RETURN u.eamil  -- æ‹¼å†™é”™è¯¯
"""

success, result, final_cypher = fixer.execute_with_retry(
    bad_cypher,
    max_retries=3,
    llm_client=llm_client
)

if success:
    print(f"âœ… æ‰§è¡ŒæˆåŠŸ!")
    print(f"æœ€ç»ˆæŸ¥è¯¢:\n{final_cypher}")
    print(f"ç»“æœ: {result}")
else:
    print(f"âŒ æ‰§è¡Œå¤±è´¥: {final_cypher}")
```

### 2.4 æ€§èƒ½ä¼˜åŒ–

#### æŸ¥è¯¢ç¼“å­˜

```python
import hashlib
import json
from typing import Optional
from functools import lru_cache
import redis

class CypherQueryCache:
    """CypheræŸ¥è¯¢ç¼“å­˜"""

    def __init__(self, redis_client: redis.Redis, ttl: int = 3600):
        self.redis = redis_client
        self.ttl = ttl

    def get_cache_key(self, question: str, schema_version: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{question}|{schema_version}"
        return f"cypher_cache:{hashlib.md5(content.encode()).hexdigest()}"

    def get(self, question: str, schema_version: str) -> Optional[str]:
        """è·å–ç¼“å­˜çš„Cypher"""
        key = self.get_cache_key(question, schema_version)
        cached = self.redis.get(key)
        return cached.decode() if cached else None

    def set(self, question: str, schema_version: str, cypher: str):
        """è®¾ç½®ç¼“å­˜"""
        key = self.get_cache_key(question, schema_version)
        self.redis.setex(key, self.ttl, cypher)

    def invalidate_all(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        keys = self.redis.keys("cypher_cache:*")
        if keys:
            self.redis.delete(*keys)

# é›†æˆåˆ°Text-to-Cypherç”Ÿæˆå™¨
class CachedText2CypherGenerator:
    """å¸¦ç¼“å­˜çš„Text-to-Cypherç”Ÿæˆå™¨"""

    def __init__(self, generator: Text2CypherGenerator, cache: CypherQueryCache):
        self.generator = generator
        self.cache = cache
        self.schema_version = self._compute_schema_version()

    def _compute_schema_version(self) -> str:
        """è®¡ç®—schemaç‰ˆæœ¬å·"""
        schema = self.generator.schema
        schema_str = json.dumps(schema, sort_keys=True)
        return hashlib.md5(schema_str.encode()).hexdigest()[:8]

    def generate_cypher(self, question: str) -> str:
        """ç”ŸæˆCypher (å¸¦ç¼“å­˜)"""
        # å°è¯•ä»ç¼“å­˜è·å–
        cached = self.cache.get(question, self.schema_version)
        if cached:
            print(f"âœ… ç¼“å­˜å‘½ä¸­: {question}")
            return cached

        # ç¼“å­˜æœªå‘½ä¸­,ç”Ÿæˆæ–°æŸ¥è¯¢
        print(f"ğŸ”„ ç”Ÿæˆæ–°æŸ¥è¯¢: {question}")
        cypher = self.generator.generate_cypher(question)

        # ä¿å­˜åˆ°ç¼“å­˜
        self.cache.set(question, self.schema_version, cypher)

        return cypher

# ä½¿ç”¨ç¤ºä¾‹
redis_client = redis.Redis(host='localhost', port=6379, db=0)
cache = CypherQueryCache(redis_client, ttl=3600)

generator = Text2CypherGenerator(conn, 'knowledge_graph', 'openai-key')
cached_generator = CachedText2CypherGenerator(generator, cache)

# ç¬¬ä¸€æ¬¡è°ƒç”¨ (ç”Ÿæˆ)
cypher1 = cached_generator.generate_cypher("æœ‰å¤šå°‘ä¸ªç”¨æˆ·?")

# ç¬¬äºŒæ¬¡è°ƒç”¨ (ç¼“å­˜)
cypher2 = cached_generator.generate_cypher("æœ‰å¤šå°‘ä¸ªç”¨æˆ·?")

assert cypher1 == cypher2
```

---

## 3. KBQAç³»ç»Ÿå®Œæ•´å®ç°

### 3.1 é—®é¢˜ç†è§£

#### æ„å›¾è¯†åˆ«

```python
from enum import Enum
from typing import Dict, List

class QueryIntent(Enum):
    """æŸ¥è¯¢æ„å›¾"""
    COUNT = "count"  # ç»Ÿè®¡æŸ¥è¯¢
    FIND = "find"  # æŸ¥æ‰¾æŸ¥è¯¢
    COMPARE = "compare"  # æ¯”è¾ƒæŸ¥è¯¢
    RECOMMEND = "recommend"  # æ¨èæŸ¥è¯¢
    REASON = "reason"  # æ¨ç†æŸ¥è¯¢
    AGGREGATE = "aggregate"  # èšåˆæŸ¥è¯¢

class QuestionUnderstanding:
    """é—®é¢˜ç†è§£æ¨¡å—"""

    def __init__(self, llm_client: OpenAI):
        self.llm = llm_client

    def analyze(self, question: str) -> Dict:
        """åˆ†æé—®é¢˜"""
        result = {
            'intent': self._classify_intent(question),
            'entities': self._extract_entities(question),
            'constraints': self._extract_constraints(question),
            'expected_answer_type': self._determine_answer_type(question)
        }

        return result

    def _classify_intent(self, question: str) -> QueryIntent:
        """åˆ†ç±»æŸ¥è¯¢æ„å›¾"""
        prompt = f"""åˆ†æä»¥ä¸‹é—®é¢˜çš„æŸ¥è¯¢æ„å›¾ï¼Œä»ä»¥ä¸‹ç±»åˆ«ä¸­é€‰æ‹©ä¸€ä¸ª:
- count: ç»Ÿè®¡æ•°é‡ (å¦‚"æœ‰å¤šå°‘...")
- find: æŸ¥æ‰¾å®ä½“ (å¦‚"æ‰¾å‡º...","è°...")
- compare: æ¯”è¾ƒå¯¹æ¯” (å¦‚"...å’Œ...çš„åŒºåˆ«")
- recommend: æ¨è (å¦‚"æ¨è...","å»ºè®®...")
- reason: æ¨ç†è§£é‡Š (å¦‚"ä¸ºä»€ä¹ˆ...","åŸå› ...")
- aggregate: èšåˆåˆ†æ (å¦‚"æ¯ä¸ª...çš„...")

é—®é¢˜: {question}

åªè¿”å›æ„å›¾ç±»åˆ«,ä¸è¦è§£é‡Šã€‚
"""

        response = self.llm.chat.completions.create(
            model="gpt-4-turbo-preview",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )

        intent_str = response.choices[0].message.content.strip().lower()

        try:
            return QueryIntent(intent_str)
        except ValueError:
            return QueryIntent.FIND  # é»˜è®¤

    def _extract_entities(self, question: str) -> List[Dict]:
        """æå–é—®é¢˜ä¸­çš„å®ä½“"""
        prompt = f"""ä»ä»¥ä¸‹é—®é¢˜ä¸­æå–å…³é”®å®ä½“ã€‚

é—®é¢˜: {question}

ä»¥JSONæ ¼å¼è¿”å›å®ä½“åˆ—è¡¨,æ¯ä¸ªå®ä½“åŒ…å«:
- mention: å®ä½“åœ¨é—®é¢˜ä¸­çš„åŸæ–‡
- type: å®ä½“ç±»å‹ (Person/Company/Product/Location/Date/Numberç­‰)

åªè¿”å›JSON,ä¸è¦å…¶ä»–å†…å®¹ã€‚
"""

        response = self.llm.chat.completions.create(
            model="gpt-4-turbo-preview",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0
        )

        try:
            result = json.loads(response.choices[0].message.content)
            return result.get('entities', [])
        except:
            return []

    def _extract_constraints(self, question: str) -> List[Dict]:
        """æå–æŸ¥è¯¢çº¦æŸæ¡ä»¶"""
        prompt = f"""ä»ä»¥ä¸‹é—®é¢˜ä¸­æå–çº¦æŸæ¡ä»¶ã€‚

é—®é¢˜: {question}

ä»¥JSONæ ¼å¼è¿”å›çº¦æŸåˆ—è¡¨,æ¯ä¸ªçº¦æŸåŒ…å«:
- type: çº¦æŸç±»å‹ (age/location/date/categoryç­‰)
- operator: æ“ä½œç¬¦ (>/</=/containsç­‰)
- value: çº¦æŸå€¼

åªè¿”å›JSON,ä¸è¦å…¶ä»–å†…å®¹ã€‚
"""

        response = self.llm.chat.completions.create(
            model="gpt-4-turbo-preview",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0
        )

        try:
            result = json.loads(response.choices[0].message.content)
            return result.get('constraints', [])
        except:
            return []

    def _determine_answer_type(self, question: str) -> str:
        """ç¡®å®šæœŸæœ›çš„ç­”æ¡ˆç±»å‹"""
        if any(word in question.lower() for word in ['å¤šå°‘', 'how many', 'count']):
            return 'number'
        elif any(word in question.lower() for word in ['æ˜¯å¦', 'whether', 'is']):
            return 'boolean'
        elif any(word in question.lower() for word in ['åˆ—å‡º', 'list', 'find']):
            return 'list'
        else:
            return 'text'

# ä½¿ç”¨ç¤ºä¾‹
client = OpenAI(api_key='your-key')
understanding = QuestionUnderstanding(client)

question = "æ‰¾å‡ºåœ¨åŒ—äº¬å·¥ä½œçš„30å²ä»¥ä¸Šçš„è½¯ä»¶å·¥ç¨‹å¸ˆ"
analysis = understanding.analyze(question)

print(f"æ„å›¾: {analysis['intent'].value}")
print(f"å®ä½“: {analysis['entities']}")
print(f"çº¦æŸ: {analysis['constraints']}")
print(f"ç­”æ¡ˆç±»å‹: {analysis['expected_answer_type']}")
```

### 3.2 å®ä½“è¯†åˆ«ä¸é“¾æ¥

#### é«˜çº§å®ä½“é“¾æ¥

```python
import json
import psycopg2
import numpy as np
from typing import List, Dict, Optional
from sentence_transformers import SentenceTransformer

class AdvancedEntityLinker:
    """é«˜çº§å®ä½“é“¾æ¥"""

    def __init__(self, conn, graph_name: str, embedding_model: SentenceTransformer):
        self.conn = conn
        self.graph_name = graph_name
        self.cursor = conn.cursor()
        self.embedding_model = embedding_model

    def link_entities(self, entities: List[Dict]) -> List[Dict]:
        """é“¾æ¥å®ä½“åˆ°çŸ¥è¯†å›¾è°±"""
        linked = []

        for entity in entities:
            mention = entity['mention']
            entity_type = entity.get('type')

            # æ­¥éª¤1: ç²¾ç¡®åŒ¹é…
            exact_match = self._exact_match(mention, entity_type)
            if exact_match:
                linked.append({
                    'mention': mention,
                    'linked_to': exact_match,
                    'method': 'exact',
                    'confidence': 1.0
                })
                continue

            # æ­¥éª¤2: æ¨¡ç³ŠåŒ¹é…
            fuzzy_matches = self._fuzzy_match(mention, entity_type)
            if fuzzy_matches:
                linked.append({
                    'mention': mention,
                    'linked_to': fuzzy_matches[0],
                    'method': 'fuzzy',
                    'confidence': fuzzy_matches[0]['score']
                })
                continue

            # æ­¥éª¤3: è¯­ä¹‰åŒ¹é…
            semantic_match = self._semantic_match(mention, entity_type)
            if semantic_match:
                linked.append({
                    'mention': mention,
                    'linked_to': semantic_match,
                    'method': 'semantic',
                    'confidence': semantic_match['score']
                })
            else:
                # æœªé“¾æ¥
                linked.append({
                    'mention': mention,
                    'linked_to': None,
                    'method': 'none',
                    'confidence': 0.0
                })

        return linked

    def _exact_match(self, mention: str, entity_type: Optional[str]) -> Optional[Dict]:
        """ç²¾ç¡®åŒ¹é…"""
        type_filter = f"AND '{entity_type}' IN labels(n)" if entity_type else ""

        self.cursor.execute(f"""
            SELECT * FROM cypher('{self.graph_name}', $$
                MATCH (n)
                WHERE n.name = '{mention}' {type_filter}
                RETURN id(n) AS node_id, n.name AS name, labels(n) AS types
                LIMIT 1
            $$) AS (node_id agtype, name agtype, types agtype);
        """)

        result = self.cursor.fetchone()
        if result:
            return {
                'node_id': int(json.loads(result[0])),
                'name': json.loads(result[1]),
                'types': json.loads(result[2])
            }
        return None

    def _fuzzy_match(self, mention: str, entity_type: Optional[str], threshold: float = 0.8) -> List[Dict]:
        """æ¨¡ç³ŠåŒ¹é… (Levenshteinè·ç¦»)"""
        type_filter = f"AND '{entity_type}' IN labels(n)" if entity_type else ""

        self.cursor.execute(f"""
            SELECT * FROM cypher('{self.graph_name}', $$
                MATCH (n)
                WHERE 1=1 {type_filter}
                RETURN id(n) AS node_id, n.name AS name, labels(n) AS types
            $$) AS (node_id agtype, name agtype, types agtype);
        """)

        import Levenshtein

        candidates = []
        for node_id, name, types in self.cursor.fetchall():
            name_str = json.loads(name)
            distance = Levenshtein.distance(mention.lower(), name_str.lower())
            max_len = max(len(mention), len(name_str))
            similarity = 1 - (distance / max_len)

            if similarity >= threshold:
                candidates.append({
                    'node_id': int(json.loads(node_id)),
                    'name': name_str,
                    'types': json.loads(types),
                    'score': similarity
                })

        candidates.sort(key=lambda x: x['score'], reverse=True)
        return candidates

    def _semantic_match(self, mention: str, entity_type: Optional[str]) -> Optional[Dict]:
        """è¯­ä¹‰åŒ¹é… (å‘é‡ç›¸ä¼¼åº¦)"""
        # ç”Ÿæˆmentionçš„å‘é‡
        mention_emb = self.embedding_model.encode(mention)

        # æŸ¥è¯¢å‘é‡å­˜å‚¨è¡¨
        type_filter = ""
        if entity_type:
            self.cursor.execute(f"""
                SELECT node_id, name, types, embedding
                FROM {self.graph_name}_entity_embeddings
                WHERE '{entity_type}' = ANY(types)
                ORDER BY embedding <=> %s::vector
                LIMIT 1;
            """, (mention_emb.tolist(),))
        else:
            self.cursor.execute(f"""
                SELECT node_id, name, types, embedding
                FROM {self.graph_name}_entity_embeddings
                ORDER BY embedding <=> %s::vector
                LIMIT 1;
            """, (mention_emb.tolist(),))

        result = self.cursor.fetchone()
        if result:
            node_id, name, types, embedding = result

            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = np.dot(mention_emb, np.array(embedding)) / (
                np.linalg.norm(mention_emb) * np.linalg.norm(embedding)
            )

            if similarity > 0.7:  # é˜ˆå€¼
                return {
                    'node_id': node_id,
                    'name': name,
                    'types': types,
                    'score': float(similarity)
                }

        return None

# ä½¿ç”¨ç¤ºä¾‹
conn = psycopg2.connect("dbname=knowledge_db user=postgres")
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
linker = AdvancedEntityLinker(conn, 'company_kg', embedding_model)

entities = [
    {'mention': 'Apple', 'type': 'Company'},
    {'mention': 'Steve Jobs', 'type': 'Person'},
    {'mention': 'iPhone', 'type': 'Product'}
]

linked_entities = linker.link_entities(entities)
for le in linked_entities:
    print(f"{le['mention']} -> {le['linked_to']} ({le['method']}, {le['confidence']:.2f})")
```

### 3.3 å­å›¾æ£€ç´¢

#### å¤šè·³å­å›¾æ£€ç´¢

```python
import json
import psycopg2
from typing import List, Dict

class SubgraphRetriever:
    """å­å›¾æ£€ç´¢å™¨"""

    def __init__(self, conn, graph_name: str):
        self.conn = conn
        self.graph_name = graph_name
        self.cursor = conn.cursor()

    def retrieve(
        self,
        seed_entities: List[int],
        max_hops: int = 2,
        max_nodes: int = 100
    ) -> Dict:
        """æ£€ç´¢å­å›¾"""

        nodes = set()
        edges = []

        for seed_id in seed_entities:
            # K-hopé‚»å±…æ£€ç´¢
            self.cursor.execute(f"""
                SELECT * FROM cypher('{self.graph_name}', $$
                    MATCH path = (seed)-[*1..{max_hops}]-(neighbor)
                    WHERE id(seed) = {seed_id}
                    RETURN DISTINCT
                        [n IN nodes(path) | {{id: id(n), properties: properties(n), labels: labels(n)}}] AS path_nodes,
                        [r IN relationships(path) | {{id: id(r), type: type(r), properties: properties(r), start_id: startNode(r).id, end_id: endNode(r).id}}] AS path_edges
                    LIMIT {max_nodes}
                $$) AS (path_nodes agtype, path_edges agtype);
            """)

            for path_nodes, path_edges in self.cursor.fetchall():
                # è§£æèŠ‚ç‚¹
                for node in json.loads(path_nodes):
                    nodes.add(node['id'])

                # è§£æè¾¹
                for edge in json.loads(path_edges):
                    edges.append(edge)

        return {
            'nodes': list(nodes),
            'edges': edges,
            'node_count': len(nodes),
            'edge_count': len(edges)
        }

    def retrieve_with_constraints(
        self,
        seed_entities: List[int],
        constraints: List[Dict],
        max_hops: int = 2
    ) -> Dict:
        """å¸¦çº¦æŸçš„å­å›¾æ£€ç´¢"""

        # æ„å»ºWHEREå­å¥
        where_clauses = []
        for constraint in constraints:
            prop = constraint['type']
            op = constraint['operator']
            value = constraint['value']

            if op == '>':
                where_clauses.append(f"neighbor.{prop} > {value}")
            elif op == '<':
                where_clauses.append(f"neighbor.{prop} < {value}")
            elif op == '=':
                where_clauses.append(f"neighbor.{prop} = '{value}'")
            elif op == 'contains':
                where_clauses.append(f"neighbor.{prop} CONTAINS '{value}'")

        where_clause = " AND ".join(where_clauses) if where_clauses else "1=1"

        nodes = set()
        edges = []

        for seed_id in seed_entities:
            self.cursor.execute(f"""
                SELECT * FROM cypher('{self.graph_name}', $$
                    MATCH path = (seed)-[*1..{max_hops}]-(neighbor)
                    WHERE id(seed) = {seed_id} AND {where_clause}
                    RETURN DISTINCT
                        nodes(path) AS path_nodes,
                        relationships(path) AS path_edges
                    LIMIT 50
                $$) AS (path_nodes agtype, path_edges agtype);
            """)

            for path_nodes, path_edges in self.cursor.fetchall():
                nodes.update(json.loads(path_nodes))
                edges.extend(json.loads(path_edges))

        return {
            'nodes': list(nodes),
            'edges': edges
        }

# ä½¿ç”¨ç¤ºä¾‹
conn = psycopg2.connect("dbname=knowledge_db user=postgres")
retriever = SubgraphRetriever(conn, 'company_kg')

seed_entities = [123, 456]  # Apple Inc., Steve Jobs
constraints = [
    {'type': 'age', 'operator': '>', 'value': 30},
    {'type': 'location', 'operator': '=', 'value': 'San Francisco'}
]

subgraph = retriever.retrieve_with_constraints(
    seed_entities,
    constraints,
    max_hops=2
)

print(f"æ£€ç´¢åˆ° {subgraph['node_count']} ä¸ªèŠ‚ç‚¹, {subgraph['edge_count']} æ¡è¾¹")
```

---

### 3.4 ç­”æ¡ˆç”Ÿæˆ

#### ç­”æ¡ˆç”Ÿæˆç­–ç•¥

```python
from typing import Dict, List, Optional
from openai import OpenAI

class AnswerGenerator:
    """ç­”æ¡ˆç”Ÿæˆå™¨"""

    def __init__(self, llm_client: OpenAI):
        self.llm = llm_client

    def generate_answer(
        self,
        question: str,
        subgraph: Dict,
        query_intent: str,
        answer_type: str
    ) -> Dict:
        """ç”Ÿæˆç­”æ¡ˆ"""

        # æ„å»ºä¸Šä¸‹æ–‡
        context = self._build_context(subgraph)

        # æ ¹æ®ç­”æ¡ˆç±»å‹é€‰æ‹©ç­–ç•¥
        if answer_type == 'number':
            return self._generate_numeric_answer(question, context)
        elif answer_type == 'list':
            return self._generate_list_answer(question, context)
        elif answer_type == 'boolean':
            return self._generate_boolean_answer(question, context)
        else:
            return self._generate_text_answer(question, context)

    def _build_context(self, subgraph: Dict) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡"""
        context_parts = []

        # æ·»åŠ èŠ‚ç‚¹ä¿¡æ¯
        if 'nodes' in subgraph:
            for node in subgraph['nodes']:
                context_parts.append(f"å®ä½“: {node.get('name', 'Unknown')}")

        # æ·»åŠ å…³ç³»ä¿¡æ¯
        if 'edges' in subgraph:
            for edge in subgraph['edges']:
                context_parts.append(
                    f"å…³ç³»: {edge.get('start', 'Unknown')} -[{edge.get('type', 'RELATED')}]-> {edge.get('end', 'Unknown')}"
                )

        return "\n".join(context_parts)

    def _generate_text_answer(self, question: str, context: str) -> Dict:
        """ç”Ÿæˆæ–‡æœ¬ç­”æ¡ˆ"""
        prompt = f"""åŸºäºä»¥ä¸‹çŸ¥è¯†å›¾è°±ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

çŸ¥è¯†å›¾è°±ä¿¡æ¯:
{context}

é—®é¢˜: {question}

è¯·æä¾›å‡†ç¡®ã€ç®€æ´çš„ç­”æ¡ˆã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯´æ˜ã€‚
"""

        response = self.llm.chat.completions.create(
            model="gpt-4-turbo-preview",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±é—®ç­”ä¸“å®¶ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1
        )

        answer = response.choices[0].message.content.strip()

        return {
            'answer': answer,
            'type': 'text',
            'confidence': 0.85
        }

    def _generate_numeric_answer(self, question: str, context: str) -> Dict:
        """ç”Ÿæˆæ•°å€¼ç­”æ¡ˆ"""
        # ç±»ä¼¼å®ç°ï¼Œä½†è¦æ±‚è¿”å›æ•°å­—
        return self._generate_text_answer(question, context)

    def _generate_list_answer(self, question: str, context: str) -> Dict:
        """ç”Ÿæˆåˆ—è¡¨ç­”æ¡ˆ"""
        # ç±»ä¼¼å®ç°ï¼Œä½†è¦æ±‚è¿”å›åˆ—è¡¨
        return self._generate_text_answer(question, context)

    def _generate_boolean_answer(self, question: str, context: str) -> Dict:
        """ç”Ÿæˆå¸ƒå°”ç­”æ¡ˆ"""
        # ç±»ä¼¼å®ç°ï¼Œä½†è¦æ±‚è¿”å›æ˜¯/å¦
        return self._generate_text_answer(question, context)

# ä½¿ç”¨ç¤ºä¾‹
client = OpenAI(api_key='your-key')
generator = AnswerGenerator(client)

subgraph = {
    'nodes': [
        {'name': 'Apple Inc.', 'type': 'Company'},
        {'name': 'Steve Jobs', 'type': 'Person'}
    ],
    'edges': [
        {'start': 'Steve Jobs', 'type': 'FOUNDED', 'end': 'Apple Inc.'}
    ]
}

result = generator.generate_answer(
    "è°åˆ›ç«‹äº†Apple?",
    subgraph,
    query_intent='find',
    answer_type='text'
)

print(result['answer'])
```

---

### 3.5 å¤šè·³æ¨ç†

#### è·¯å¾„æ¨ç†

```python
class MultiHopReasoner:
    """å¤šè·³æ¨ç†å™¨"""

    def __init__(self, conn, graph_name: str):
        self.conn = conn
        self.graph_name = graph_name
        self.cursor = conn.cursor()

    def reason(self, start_entity: int, end_entity: int, max_hops: int = 3) -> List[Dict]:
        """å¤šè·³æ¨ç†"""
        self.cursor.execute(f"""
            SELECT * FROM cypher('{self.graph_name}', $$
                MATCH path = shortestPath(
                    (start)-[*1..{max_hops}]-(end)
                )
                WHERE id(start) = {start_entity} AND id(end) = {end_entity}
                RETURN path, length(path) AS hop_count
            $$) AS (path agtype, hop_count agtype);
        """)

        results = []
        for row in self.cursor.fetchall():
            path_data = json.loads(row[0])
            hop_count = json.loads(row[1])

            results.append({
                'path': path_data,
                'hop_count': int(hop_count),
                'confidence': self._calculate_confidence(int(hop_count))
            })

        return results

    def _calculate_confidence(self, hop_count: int) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦"""
        # è·³æ•°è¶Šå°‘ï¼Œç½®ä¿¡åº¦è¶Šé«˜
        if hop_count == 1:
            return 1.0
        elif hop_count == 2:
            return 0.8
        elif hop_count == 3:
            return 0.6
        else:
            return 0.4

# ä½¿ç”¨ç¤ºä¾‹
conn = psycopg2.connect("dbname=knowledge_db user=postgres")
reasoner = MultiHopReasoner(conn, 'company_kg')

paths = reasoner.reason(
    start_entity=123,  # Apple Inc.
    end_entity=456,    # iPhone
    max_hops=3
)

for path in paths:
    print(f"è·¯å¾„: {path['path']}, è·³æ•°: {path['hop_count']}, ç½®ä¿¡åº¦: {path['confidence']}")
```

---

## 4. RAG+KGæ··åˆæ¶æ„

### 4.1 æ··åˆæ£€ç´¢

```python
from typing import List, Dict
import numpy as np

class HybridRetriever:
    """æ··åˆæ£€ç´¢å™¨ï¼ˆå‘é‡+å›¾ï¼‰"""

    def __init__(self, vector_retriever, graph_retriever):
        self.vector_retriever = vector_retriever
        self.graph_retriever = graph_retriever

    def retrieve(self, query: str, top_k: int = 10) -> Dict:
        """æ··åˆæ£€ç´¢"""
        # å‘é‡æ£€ç´¢
        vector_results = self.vector_retriever.search(query, top_k=top_k)

        # å›¾æ£€ç´¢
        graph_results = self.graph_retriever.retrieve(query, max_nodes=top_k)

        # èåˆç»“æœ
        fused_results = self._fuse_results(vector_results, graph_results)

        return fused_results

    def _fuse_results(self, vector_results: List[Dict], graph_results: Dict) -> Dict:
        """èåˆæ£€ç´¢ç»“æœ"""
        # ä½¿ç”¨Reciprocal Rank Fusion (RRF)
        rrf_scores = {}

        # å‘é‡ç»“æœRRFåˆ†æ•°
        for i, result in enumerate(vector_results):
            doc_id = result.get('id')
            rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + 1 / (60 + i + 1)

        # å›¾ç»“æœRRFåˆ†æ•°
        if 'nodes' in graph_results:
            for i, node in enumerate(graph_results['nodes']):
                node_id = node.get('id')
                rrf_scores[node_id] = rrf_scores.get(node_id, 0) + 1 / (60 + i + 1)

        # æ’åº
        sorted_results = sorted(
            rrf_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )

        return {
            'results': sorted_results[:10],
            'vector_count': len(vector_results),
            'graph_count': len(graph_results.get('nodes', []))
        }
```

### 4.2 ç»“æœèåˆ

```python
class ResultFusion:
    """ç»“æœèåˆç­–ç•¥"""

    @staticmethod
    def weighted_fusion(
        vector_results: List[Dict],
        graph_results: Dict,
        vector_weight: float = 0.6,
        graph_weight: float = 0.4
    ) -> List[Dict]:
        """åŠ æƒèåˆ"""
        fused = []

        # å½’ä¸€åŒ–åˆ†æ•°
        vector_scores = [r.get('score', 0) for r in vector_results]
        graph_scores = [n.get('score', 0) for n in graph_results.get('nodes', [])]

        max_vector = max(vector_scores) if vector_scores else 1.0
        max_graph = max(graph_scores) if graph_scores else 1.0

        # èåˆ
        for result in vector_results:
            normalized_score = (result.get('score', 0) / max_vector) * vector_weight
            fused.append({
                **result,
                'fused_score': normalized_score,
                'source': 'vector'
            })

        for node in graph_results.get('nodes', []):
            normalized_score = (node.get('score', 0) / max_graph) * graph_weight
            fused.append({
                **node,
                'fused_score': normalized_score,
                'source': 'graph'
            })

        # æŒ‰èåˆåˆ†æ•°æ’åº
        fused.sort(key=lambda x: x['fused_score'], reverse=True)

        return fused
```

---

## 5. LLMé©±åŠ¨çš„çŸ¥è¯†æŠ½å–

### 5.1 å®ä½“æŠ½å–

```python
class EntityExtractor:
    """LLMé©±åŠ¨çš„å®ä½“æŠ½å–"""

    def __init__(self, llm_client: OpenAI):
        self.llm = llm_client

    def extract_entities(self, text: str) -> List[Dict]:
        """æŠ½å–å®ä½“"""
        prompt = f"""ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æŠ½å–å®ä½“ã€‚

æ–‡æœ¬:
{text}

ä»¥JSONæ ¼å¼è¿”å›å®ä½“åˆ—è¡¨ï¼Œæ¯ä¸ªå®ä½“åŒ…å«:
- name: å®ä½“åç§°
- type: å®ä½“ç±»å‹ (Person/Organization/Location/Productç­‰)
- start_pos: èµ·å§‹ä½ç½®
- end_pos: ç»“æŸä½ç½®

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""

        response = self.llm.chat.completions.create(
            model="gpt-4-turbo-preview",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯å®ä½“æŠ½å–ä¸“å®¶ã€‚"},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0
        )

        try:
            result = json.loads(response.choices[0].message.content)
            return result.get('entities', [])
        except:
            return []

# ä½¿ç”¨ç¤ºä¾‹
client = OpenAI(api_key='your-key')
extractor = EntityExtractor(client)

text = "Apple Inc. was founded by Steve Jobs in Cupertino, California."
entities = extractor.extract_entities(text)

for entity in entities:
    print(f"{entity['name']} ({entity['type']})")
```

### 5.2 å…³ç³»æŠ½å–

```python
class RelationExtractor:
    """LLMé©±åŠ¨çš„å…³ç³»æŠ½å–"""

    def __init__(self, llm_client: OpenAI):
        self.llm = llm_client

    def extract_relations(self, text: str, entities: List[Dict]) -> List[Dict]:
        """æŠ½å–å…³ç³»"""
        entities_str = json.dumps(entities, ensure_ascii=False, indent=2)

        prompt = f"""ä»ä»¥ä¸‹æ–‡æœ¬å’Œå®ä½“ä¸­æŠ½å–å…³ç³»ã€‚

æ–‡æœ¬:
{text}

å®ä½“:
{entities_str}

ä»¥JSONæ ¼å¼è¿”å›å…³ç³»åˆ—è¡¨ï¼Œæ¯ä¸ªå…³ç³»åŒ…å«:
- subject: ä¸»ä½“å®ä½“
- predicate: å…³ç³»ç±»å‹
- object: å®¢ä½“å®ä½“
- confidence: ç½®ä¿¡åº¦

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""

        response = self.llm.chat.completions.create(
            model="gpt-4-turbo-preview",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯å…³ç³»æŠ½å–ä¸“å®¶ã€‚"},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0
        )

        try:
            result = json.loads(response.choices[0].message.content)
            return result.get('relations', [])
        except:
            return []

# ä½¿ç”¨ç¤ºä¾‹
client = OpenAI(api_key='your-key')
extractor = RelationExtractor(client)

text = "Apple Inc. was founded by Steve Jobs in Cupertino."
entities = [
    {'name': 'Apple Inc.', 'type': 'Organization'},
    {'name': 'Steve Jobs', 'type': 'Person'},
    {'name': 'Cupertino', 'type': 'Location'}
]

relations = extractor.extract_relations(text, entities)

for rel in relations:
    print(f"{rel['subject']} -[{rel['predicate']}]-> {rel['object']}")
```

---

## 6. ä¼ä¸šçº§ç”Ÿäº§æ¶æ„

### 6.1 ç³»ç»Ÿæ¶æ„

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            LLM+KGèåˆç³»ç»Ÿæ¶æ„                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  [API Gateway]                                  â”‚
â”‚       â”‚                                         â”‚
â”‚  [LLM Service] â”€â”€â”                              â”‚
â”‚       â”‚          â”‚                              â”‚
â”‚  [KG Service] â”€â”€â”€â”¼â”€â”€â†’ [PostgreSQL + AGE]       â”‚
â”‚       â”‚          â”‚                              â”‚
â”‚  [Vector Service]â”€â”˜                              â”‚
â”‚       â”‚                                         â”‚
â”‚  [Cache Layer (Redis)]                          â”‚
â”‚       â”‚                                         â”‚
â”‚  [Monitoring & Logging]                         â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 æ€§èƒ½ä¼˜åŒ–

```python
class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.cache = {}
        self.metrics = {
            'total_queries': 0,
            'cache_hits': 0,
            'avg_latency': 0.0
        }

    def optimize_query(self, query: str) -> str:
        """ä¼˜åŒ–æŸ¥è¯¢"""
        # æŸ¥è¯¢ç¼“å­˜
        if query in self.cache:
            self.metrics['cache_hits'] += 1
            return self.cache[query]

        # æ‰§è¡ŒæŸ¥è¯¢
        start_time = time.time()
        result = self._execute_query(query)
        latency = time.time() - start_time

        # æ›´æ–°æŒ‡æ ‡
        self.metrics['total_queries'] += 1
        self.metrics['avg_latency'] = (
            (self.metrics['avg_latency'] * (self.metrics['total_queries'] - 1) + latency) /
            self.metrics['total_queries']
        )

        # ç¼“å­˜ç»“æœ
        self.cache[query] = result

        return result

    def _execute_query(self, query: str) -> str:
        """æ‰§è¡ŒæŸ¥è¯¢"""
        # å®ç°æŸ¥è¯¢é€»è¾‘
        return "result"
```

### 6.3 ç›‘æ§å‘Šè­¦

```python
import logging
from typing import Dict
import time

class MonitoringSystem:
    """ç›‘æ§ç³»ç»Ÿ"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.metrics = {
            'query_count': 0,
            'error_count': 0,
            'avg_latency': 0.0,
            'cache_hit_rate': 0.0
        }

    def log_query(self, query: str, latency: float, success: bool):
        """è®°å½•æŸ¥è¯¢"""
        self.metrics['query_count'] += 1
        self.metrics['avg_latency'] = (
            (self.metrics['avg_latency'] * (self.metrics['query_count'] - 1) + latency) /
            self.metrics['query_count']
        )

        if not success:
            self.metrics['error_count'] += 1

        # å‘Šè­¦æ£€æŸ¥
        if latency > 2.0:
            self._alert(f"é«˜å»¶è¿ŸæŸ¥è¯¢: {latency:.2f}s")

        if self.metrics['error_count'] / self.metrics['query_count'] > 0.1:
            self._alert("é”™è¯¯ç‡è¿‡é«˜")

    def _alert(self, message: str):
        """å‘é€å‘Šè­¦"""
        self.logger.warning(f"âš ï¸ å‘Šè­¦: {message}")
        # å¯ä»¥é›†æˆåˆ°å‘Šè­¦ç³»ç»Ÿï¼ˆå¦‚PagerDutyã€Slackç­‰ï¼‰

# ä½¿ç”¨ç¤ºä¾‹
monitor = MonitoringSystem()

# è®°å½•æŸ¥è¯¢
monitor.log_query("æŸ¥è¯¢ç¤ºä¾‹", latency=1.5, success=True)
monitor.log_query("æŸ¥è¯¢ç¤ºä¾‹2", latency=2.5, success=False)

print(f"æŒ‡æ ‡: {monitor.metrics}")
```

---

## ğŸ“š å‚è€ƒèµ„æº

1. **OpenAI APIæ–‡æ¡£**: <https://platform.openai.com/docs>
2. **Anthropic Claude API**: <https://docs.anthropic.com/>
3. **LangChainæ–‡æ¡£**: <https://python.langchain.com/docs/get_started/introduction>
4. **Apache AGE**: <https://age.apache.org/>
5. **KBQAç»¼è¿°è®ºæ–‡**: <https://arxiv.org/abs/2108.06688>

---

## ğŸ“ æ›´æ–°æ—¥å¿—

- **v1.1** (2025-01-XX): å…¨é¢ä¿®å¤ç‰ˆæœ¬
  - âœ… ä¿®å¤ä»£ç å—æ ¼å¼é—®é¢˜
  - âœ… è¡¥å……ç¼ºå¤±çš„å¯¼å…¥è¯­å¥
  - âœ… å®Œå–„ç›®å½•ç»“æ„
  - âœ… è¡¥å……ç­”æ¡ˆç”Ÿæˆç« èŠ‚
  - âœ… è¡¥å……å¤šè·³æ¨ç†ç« èŠ‚
  - âœ… è¡¥å……RAG+KGæ··åˆæ¶æ„ç« èŠ‚
  - âœ… è¡¥å……LLMé©±åŠ¨çš„çŸ¥è¯†æŠ½å–ç« èŠ‚
  - âœ… è¡¥å……ä¼ä¸šçº§ç”Ÿäº§æ¶æ„ç« èŠ‚
  - âœ… ç»Ÿä¸€ä»£ç æ ¼å¼å’Œé£æ ¼

- **v1.0** (2025-12-04): åˆå§‹ç‰ˆæœ¬
  - Text-to-Cypherç”Ÿæˆ
  - KBQAç³»ç»Ÿå®Œæ•´å®ç°
  - RAG+KGæ··åˆæ¶æ„ï¼ˆæ¡†æ¶ï¼‰
  - LLMé©±åŠ¨çš„çŸ¥è¯†æŠ½å–ï¼ˆæ¡†æ¶ï¼‰
  - ä¼ä¸šçº§ç”Ÿäº§æ¶æ„ï¼ˆæ¡†æ¶ï¼‰

---

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
pip install openai psycopg2-binary sentence-transformers numpy redis
```

### åŸºç¡€ä½¿ç”¨

```python
from openai import OpenAI
import psycopg2

# åˆå§‹åŒ–
client = OpenAI(api_key='your-key')
conn = psycopg2.connect("dbname=test_db user=postgres")

# Text-to-Cypherç”Ÿæˆ
from cypher_generator import Text2CypherGenerator
generator = Text2CypherGenerator(conn, 'knowledge_graph', 'your-key')
cypher = generator.generate("æœ‰å¤šå°‘ä¸ªç”¨æˆ·?")

# KBQAç³»ç»Ÿ
from kbqa import KBQASystem
kbqa = KBQASystem(conn, 'knowledge_graph', client)
answer = kbqa.answer("Appleçš„åˆ›å§‹äººæ˜¯è°?")
print(answer)
```

---

**ä¸‹ä¸€æ­¥**: [08-çŸ¥è¯†æŠ½å–ä¸NERå®Œæ•´æŒ‡å—](./08-çŸ¥è¯†æŠ½å–ä¸NERå®Œæ•´æŒ‡å—.md) | [è¿”å›ç›®å½•](./README.md)
