---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `docs\03-KnowledgeGraph\05-çŸ¥è¯†å›¾è°±æ„å»ºå®Œæ•´æµç¨‹æŒ‡å—.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# PostgreSQLçŸ¥è¯†å›¾è°±æ„å»ºå®Œæ•´æµç¨‹æŒ‡å—

## å…ƒæ•°æ®

- **æ–‡æ¡£ç‰ˆæœ¬**: v2.0
- **åˆ›å»ºæ—¥æœŸ**: 2025-12-04
- **é€‚ç”¨ç‰ˆæœ¬**: PostgreSQL 15+ & Apache AGE 1.5+
- **éš¾åº¦çº§åˆ«**: â­â­â­â­ (é«˜çº§)
- **é¢„è®¡é˜…è¯»**: 90åˆ†é’Ÿ
- **é…å¥—èµ„æº**: [å®æˆ˜ä»£ç åº“](../../program/scripts/README.md) | [æ€§èƒ½æµ‹è¯•é›†](../../program/scripts/README.md)

---

## ğŸ“‹ ç›®å½•

- [PostgreSQLçŸ¥è¯†å›¾è°±æ„å»ºå®Œæ•´æµç¨‹æŒ‡å—](#postgresqlçŸ¥è¯†å›¾è°±æ„å»ºå®Œæ•´æµç¨‹æŒ‡å—)
  - [å…ƒæ•°æ®](#å…ƒæ•°æ®)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1. çŸ¥è¯†å›¾è°±æ„å»ºå…¨æ™¯](#1-çŸ¥è¯†å›¾è°±æ„å»ºå…¨æ™¯)
    - [1.1 å®Œæ•´æŠ€æœ¯æ¶æ„](#11-å®Œæ•´æŠ€æœ¯æ¶æ„)
    - [1.2 æ ¸å¿ƒæŠ€æœ¯æ ˆ](#12-æ ¸å¿ƒæŠ€æœ¯æ ˆ)
      - [å¿…å¤‡ç»„ä»¶](#å¿…å¤‡ç»„ä»¶)
    - [1.3 æ•°æ®æµè®¾è®¡](#13-æ•°æ®æµè®¾è®¡)
      - [ç¦»çº¿æ‰¹å¤„ç†æµç¨‹](#ç¦»çº¿æ‰¹å¤„ç†æµç¨‹)
  - [2. æ•°æ®é‡‡é›†ä¸é¢„å¤„ç†](#2-æ•°æ®é‡‡é›†ä¸é¢„å¤„ç†)
    - [2.1 å¤šæºæ•°æ®é‡‡é›†](#21-å¤šæºæ•°æ®é‡‡é›†)
      - [2.1.1 ç»“æ„åŒ–æ•°æ®æº](#211-ç»“æ„åŒ–æ•°æ®æº)
      - [2.1.2 éç»“æ„åŒ–æ•°æ®æº](#212-éç»“æ„åŒ–æ•°æ®æº)
      - [2.1.3 APIæ•°æ®é‡‡é›†](#213-apiæ•°æ®é‡‡é›†)
    - [2.2 æ•°æ®æ¸…æ´—ä¸è§„èŒƒåŒ–](#22-æ•°æ®æ¸…æ´—ä¸è§„èŒƒåŒ–)
      - [2.2.1 æ–‡æœ¬æ¸…æ´—](#221-æ–‡æœ¬æ¸…æ´—)
      - [2.2.2 æ•°æ®å»é‡](#222-æ•°æ®å»é‡)
      - [2.2.3 ç¼ºå¤±å€¼å¤„ç†](#223-ç¼ºå¤±å€¼å¤„ç†)
  - [3. çŸ¥è¯†æŠ½å–ä¸å®ä½“è¯†åˆ«](#3-çŸ¥è¯†æŠ½å–ä¸å®ä½“è¯†åˆ«)
    - [3.1 åŸºäºè§„åˆ™çš„å®ä½“è¯†åˆ«](#31-åŸºäºè§„åˆ™çš„å®ä½“è¯†åˆ«)
    - [3.2 åŸºäºæœºå™¨å­¦ä¹ çš„å®ä½“è¯†åˆ«](#32-åŸºäºæœºå™¨å­¦ä¹ çš„å®ä½“è¯†åˆ«)
    - [3.3 å…³ç³»æŠ½å–](#33-å…³ç³»æŠ½å–)
      - [3.3.1 åŸºäºæ¨¡å¼çš„å…³ç³»æŠ½å–](#331-åŸºäºæ¨¡å¼çš„å…³ç³»æŠ½å–)
      - [3.3.2 åŸºäºæ·±åº¦å­¦ä¹ çš„å…³ç³»æŠ½å–](#332-åŸºäºæ·±åº¦å­¦ä¹ çš„å…³ç³»æŠ½å–)
  - [4. çŸ¥è¯†èåˆä¸æ¶ˆæ­§](#4-çŸ¥è¯†èåˆä¸æ¶ˆæ­§)
    - [4.1 å®ä½“å¯¹é½](#41-å®ä½“å¯¹é½)
    - [4.2 çŸ¥è¯†æ¶ˆæ­§](#42-çŸ¥è¯†æ¶ˆæ­§)
  - [5. çŸ¥è¯†å­˜å‚¨ä¸ç´¢å¼•](#5-çŸ¥è¯†å­˜å‚¨ä¸ç´¢å¼•)
    - [5.1 Apache AGEå›¾æ•°æ®åº“å­˜å‚¨](#51-apache-ageå›¾æ•°æ®åº“å­˜å‚¨)
    - [5.2 å‘é‡ç´¢å¼•é›†æˆ](#52-å‘é‡ç´¢å¼•é›†æˆ)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)
  - [ğŸ“ æ›´æ–°æ—¥å¿—](#-æ›´æ–°æ—¥å¿—)

---

## 1. çŸ¥è¯†å›¾è°±æ„å»ºå…¨æ™¯

### 1.1 å®Œæ•´æŠ€æœ¯æ¶æ„

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    çŸ¥è¯†å›¾è°±æ„å»ºå…¨æµç¨‹                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ æ•°æ®é‡‡é›† â”‚â”€â”€â”€â–¶â”‚ çŸ¥è¯†æŠ½å– â”‚â”€â”€â”€â–¶â”‚ çŸ¥è¯†èåˆ â”‚â”€â”€â”€â–¶â”‚ çŸ¥è¯†å­˜å‚¨ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚       â”‚              â”‚              â”‚              â”‚         â”‚
â”‚       â–¼              â–¼              â–¼              â–¼         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           PostgreSQL + Apache AGE                    â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚   â”‚
â”‚  â”‚  â”‚ å…³ç³»è¡¨   â”‚  â”‚ å›¾æ•°æ®   â”‚  â”‚ å‘é‡ç´¢å¼• â”‚          â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ è´¨é‡è¯„ä¼° â”‚â—€â”€â”€â”€â”‚ çŸ¥è¯†æ¨ç† â”‚â—€â”€â”€â”€â”‚ çŸ¥è¯†æŸ¥è¯¢ â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ ¸å¿ƒæŠ€æœ¯æ ˆ

#### å¿…å¤‡ç»„ä»¶

```yaml
æ ¸å¿ƒæ•°æ®åº“:
  - PostgreSQL: 16+ (æ¨è18)
  - Apache AGE: 1.5+
  - pgvector: 0.7+ (å‘é‡æ£€ç´¢)

AI/NLPå¼•æ“:
  - spaCy: 3.7+ (å®ä½“è¯†åˆ«)
  - HuggingFace Transformers: 4.35+ (é¢„è®­ç»ƒæ¨¡å‹)
  - OpenAI GPT-4 / Anthropic Claude (å¤§æ¨¡å‹)
  - LangChain: 0.1+ (LLMç¼–æ’)

æ•°æ®å¤„ç†:
  - Apache Airflow: 2.7+ (å·¥ä½œæµç¼–æ’)
  - Apache Kafka: 3.5+ (æµå¼æ•°æ®)
  - Pandas: 2.1+ (æ•°æ®æ¸…æ´—)
  - NumPy: 1.24+ (æ•°å€¼è®¡ç®—)

å¼€å‘æ¡†æ¶:
  - Python: 3.11+
  - FastAPI: 0.104+ (APIæœåŠ¡)
  - Celery: 5.3+ (åˆ†å¸ƒå¼ä»»åŠ¡)
  - Redis: 7.2+ (ç¼“å­˜ä¸é˜Ÿåˆ—)
```

### 1.3 æ•°æ®æµè®¾è®¡

#### ç¦»çº¿æ‰¹å¤„ç†æµç¨‹

```python
from dataclasses import dataclass
from typing import List, Dict, Any
import logging

@dataclass
class KGBuildingPipeline:
    """çŸ¥è¯†å›¾è°±æ„å»ºæµæ°´çº¿"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.pg_conn = self._init_postgres()
        self.age_graph = self._init_age_graph()

    def build_knowledge_graph(self, data_source: str) -> Dict[str, Any]:
        """å®Œæ•´çš„çŸ¥è¯†å›¾è°±æ„å»ºæµç¨‹"""
        try:
            # æ­¥éª¤1: æ•°æ®é‡‡é›†
            raw_data = self.collect_data(data_source)
            self.logger.info(f"âœ… é‡‡é›†æ•°æ®: {len(raw_data)} æ¡è®°å½•")

            # æ­¥éª¤2: æ•°æ®æ¸…æ´—
            cleaned_data = self.clean_data(raw_data)
            self.logger.info(f"âœ… æ¸…æ´—å®Œæˆ: ä¿ç•™ {len(cleaned_data)} æ¡")

            # æ­¥éª¤3: çŸ¥è¯†æŠ½å–
            entities, relations = self.extract_knowledge(cleaned_data)
            self.logger.info(f"âœ… æŠ½å–å®ä½“: {len(entities)}, å…³ç³»: {len(relations)}")

            # æ­¥éª¤4: å®ä½“å¯¹é½
            aligned_entities = self.align_entities(entities)
            self.logger.info(f"âœ… å®ä½“å¯¹é½: å»é‡å {len(aligned_entities)} ä¸ª")

            # æ­¥éª¤5: çŸ¥è¯†èåˆ
            fused_triplets = self.fuse_knowledge(aligned_entities, relations)
            self.logger.info(f"âœ… çŸ¥è¯†èåˆ: {len(fused_triplets)} ä¸ªä¸‰å…ƒç»„")

            # æ­¥éª¤6: å­˜å‚¨åˆ°å›¾æ•°æ®åº“
            stored_count = self.store_to_graph(fused_triplets)
            self.logger.info(f"âœ… å­˜å‚¨å®Œæˆ: {stored_count} ä¸ªèŠ‚ç‚¹å’Œè¾¹")

            # æ­¥éª¤7: æ„å»ºç´¢å¼•
            self.build_indexes()
            self.logger.info(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ")

            # æ­¥éª¤8: è´¨é‡è¯„ä¼°
            quality_metrics = self.evaluate_quality()
            self.logger.info(f"âœ… è´¨é‡è¯„ä¼°: {quality_metrics}")

            return {
                "status": "success",
                "entities_count": len(aligned_entities),
                "relations_count": len(fused_triplets),
                "quality_score": quality_metrics['overall_score']
            }

        except Exception as e:
            self.logger.error(f"âŒ æ„å»ºå¤±è´¥: {str(e)}")
            raise

    def collect_data(self, source: str) -> List[Dict]:
        """æ•°æ®é‡‡é›†"""
        if source.startswith('http'):
            return self._fetch_from_api(source)
        elif source.endswith('.csv'):
            return self._load_from_csv(source)
        elif source.startswith('postgres://'):
            return self._query_from_db(source)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æº: {source}")

    def clean_data(self, raw_data: List[Dict]) -> List[Dict]:
        """æ•°æ®æ¸…æ´—"""
        cleaned = []
        for record in raw_data:
            # å»é™¤é‡å¤
            if self._is_duplicate(record, cleaned):
                continue

            # æ•°æ®éªŒè¯
            if not self._validate_record(record):
                self.logger.warning(f"è·³è¿‡æ— æ•ˆè®°å½•: {record}")
                continue

            # æ ¼å¼è§„èŒƒåŒ–
            normalized = self._normalize_record(record)
            cleaned.append(normalized)

        return cleaned

    def extract_knowledge(self, data: List[Dict]) -> tuple:
        """çŸ¥è¯†æŠ½å– - æ ¸å¿ƒæ­¥éª¤"""
        from transformers import pipeline

        # åˆå§‹åŒ–NERæ¨¡å‹
        ner_pipeline = pipeline(
            "ner",
            model="dslim/bert-base-NER",
            aggregation_strategy="simple"
        )

        entities = []
        relations = []

        for record in data:
            text = record.get('text', '')

            # å®ä½“è¯†åˆ«
            ner_results = ner_pipeline(text)
            for entity in ner_results:
                entities.append({
                    'text': entity['word'],
                    'type': entity['entity_group'],
                    'score': entity['score'],
                    'source_id': record.get('id')
                })

            # å…³ç³»æŠ½å– (ç®€åŒ–ç‰ˆ)
            extracted_relations = self._extract_relations(text, ner_results)
            relations.extend(extracted_relations)

        return entities, relations

    def align_entities(self, entities: List[Dict]) -> List[Dict]:
        """å®ä½“å¯¹é½ - æ¶ˆé™¤æ­§ä¹‰"""
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity

        # ä½¿ç”¨TF-IDFè®¡ç®—ç›¸ä¼¼åº¦
        texts = [e['text'] for e in entities]
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(texts)

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = cosine_similarity(tfidf_matrix)

        # åˆå¹¶ç›¸ä¼¼å®ä½“
        aligned = []
        visited = set()

        for i, entity in enumerate(entities):
            if i in visited:
                continue

            # æ‰¾åˆ°æ‰€æœ‰ç›¸ä¼¼å®ä½“
            similar_indices = [
                j for j, sim in enumerate(similarity_matrix[i])
                if sim > 0.85 and j != i
            ]

            # åˆå¹¶å®ä½“ä¿¡æ¯
            merged_entity = self._merge_entities(
                [entities[i]] + [entities[j] for j in similar_indices]
            )
            aligned.append(merged_entity)

            visited.add(i)
            visited.update(similar_indices)

        return aligned

    def fuse_knowledge(self, entities: List[Dict], relations: List[Dict]) -> List[Dict]:
        """çŸ¥è¯†èåˆ"""
        triplets = []
        entity_map = {e['text']: e for e in entities}

        for rel in relations:
            head = rel.get('head')
            tail = rel.get('tail')
            rel_type = rel.get('type')

            # ç¡®ä¿å¤´å°¾å®ä½“å­˜åœ¨
            if head in entity_map and tail in entity_map:
                triplets.append({
                    'head': entity_map[head],
                    'relation': rel_type,
                    'tail': entity_map[tail],
                    'confidence': rel.get('score', 0.0)
                })

        return triplets

    def store_to_graph(self, triplets: List[Dict]) -> int:
        """å­˜å‚¨åˆ°Apache AGEå›¾æ•°æ®åº“"""
        cursor = self.pg_conn.cursor()
        stored_count = 0

        for triplet in triplets:
            try:
                # åˆ›å»ºå¤´å®ä½“èŠ‚ç‚¹
                cursor.execute(f"""
                    SELECT * FROM cypher('knowledge_graph', $$
                        MERGE (h:Entity {{
                            name: '{triplet['head']['text']}',
                            type: '{triplet['head']['type']}'
                        }})
                        RETURN h
                    $$) as (node agtype);
                """)

                # åˆ›å»ºå°¾å®ä½“èŠ‚ç‚¹
                cursor.execute(f"""
                    SELECT * FROM cypher('knowledge_graph', $$
                        MERGE (t:Entity {{
                            name: '{triplet['tail']['text']}',
                            type: '{triplet['tail']['type']}'
                        }})
                        RETURN t
                    $$) as (node agtype);
                """)

                # åˆ›å»ºå…³ç³»è¾¹
                cursor.execute(f"""
                    SELECT * FROM cypher('knowledge_graph', $$
                        MATCH (h:Entity {{name: '{triplet['head']['text']}'}}),
                              (t:Entity {{name: '{triplet['tail']['text']}'}}))
                        MERGE (h)-[r:{triplet['relation']} {{
                            confidence: {triplet['confidence']}
                        }}]->(t)
                        RETURN r
                    $$) as (rel agtype);
                """)

                stored_count += 1

            except Exception as e:
                self.logger.error(f"å­˜å‚¨ä¸‰å…ƒç»„å¤±è´¥: {triplet}, é”™è¯¯: {e}")

        self.pg_conn.commit()
        return stored_count

    def build_indexes(self):
        """æ„å»ºé«˜æ€§èƒ½ç´¢å¼•"""
        cursor = self.pg_conn.cursor()

        # ä¸ºå®ä½“åç§°åˆ›å»ºç´¢å¼•
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_entity_name
            ON knowledge_graph.ag_vertex
            USING btree ((properties->>'name'));
        """)

        # ä¸ºå®ä½“ç±»å‹åˆ›å»ºç´¢å¼•
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_entity_type
            ON knowledge_graph.ag_vertex
            USING btree ((properties->>'type'));
        """)

        # ä¸ºå…³ç³»ç±»å‹åˆ›å»ºç´¢å¼•
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_relation_type
            ON knowledge_graph.ag_edge
            USING btree (label);
        """)

        self.pg_conn.commit()

    def evaluate_quality(self) -> Dict[str, float]:
        """è´¨é‡è¯„ä¼°"""
        cursor = self.pg_conn.cursor()

        # è¯„ä¼°æŒ‡æ ‡1: å®ä½“è¦†ç›–ç‡
        cursor.execute("""
            SELECT COUNT(*) FROM cypher('knowledge_graph', $$
                MATCH (n:Entity) RETURN n
            $$) as (node agtype);
        """)
        entity_count = cursor.fetchone()[0]

        # è¯„ä¼°æŒ‡æ ‡2: å…³ç³»å¯†åº¦
        cursor.execute("""
            SELECT COUNT(*) FROM cypher('knowledge_graph', $$
                MATCH ()-[r]->() RETURN r
            $$) as (rel agtype);
        """)
        relation_count = cursor.fetchone()[0]

        density = relation_count / max(entity_count, 1)

        # è¯„ä¼°æŒ‡æ ‡3: å¹³å‡ç½®ä¿¡åº¦
        cursor.execute("""
            SELECT AVG((properties->>'confidence')::float)
            FROM knowledge_graph.ag_edge;
        """)
        avg_confidence = cursor.fetchone()[0] or 0.0

        overall_score = (
            0.3 * min(entity_count / 10000, 1.0) +
            0.3 * min(density / 5.0, 1.0) +
            0.4 * avg_confidence
        )

        return {
            'entity_count': entity_count,
            'relation_count': relation_count,
            'density': density,
            'avg_confidence': avg_confidence,
            'overall_score': overall_score
        }
```

---

## 2. æ•°æ®é‡‡é›†ä¸é¢„å¤„ç†

### 2.1 å¤šæºæ•°æ®é‡‡é›†

#### 2.1.1 ç»“æ„åŒ–æ•°æ®æº

**ä»å…³ç³»æ•°æ®åº“é‡‡é›†**:

```python
import psycopg2
import pandas as pd
from typing import List, Dict

class StructuredDataCollector:
    """ç»“æ„åŒ–æ•°æ®é‡‡é›†å™¨"""

    def __init__(self, db_config: Dict):
        self.conn = psycopg2.connect(**db_config)

    def collect_from_tables(self, table_list: List[str]) -> pd.DataFrame:
        """ä»å¤šä¸ªè¡¨é‡‡é›†æ•°æ®"""
        dataframes = []

        for table in table_list:
            df = pd.read_sql(f"SELECT * FROM {table}", self.conn)
            df['source_table'] = table
            dataframes.append(df)

        return pd.concat(dataframes, ignore_index=True)

    def collect_with_join(self, query: str) -> pd.DataFrame:
        """é€šè¿‡JOINæŸ¥è¯¢é‡‡é›†å…³è”æ•°æ®"""
        return pd.read_sql(query, self.conn)

# å®æˆ˜ç¤ºä¾‹: ä»ç”µå•†æ•°æ®åº“æ„å»ºäº§å“çŸ¥è¯†å›¾è°±
collector = StructuredDataCollector({
    'host': 'localhost',
    'database': 'ecommerce',
    'user': 'postgres',
    'password': 'password'
})

# é‡‡é›†äº§å“ã€ç±»åˆ«ã€å“ç‰Œæ•°æ®
products_df = collector.collect_with_join("""
    SELECT
        p.product_id,
        p.product_name,
        p.description,
        c.category_name,
        b.brand_name,
        p.price,
        p.rating
    FROM products p
    JOIN categories c ON p.category_id = c.category_id
    JOIN brands b ON p.brand_id = b.brand_id
    WHERE p.status = 'active'
""")
```

#### 2.1.2 éç»“æ„åŒ–æ•°æ®æº

**æ–‡æœ¬æ–‡æ¡£é‡‡é›†**:

```python
import os
from pathlib import Path
from typing import Iterator
import fitz  # PyMuPDF for PDF
from docx import Document

class UnstructuredDataCollector:
    """éç»“æ„åŒ–æ•°æ®é‡‡é›†å™¨"""

    def collect_from_directory(self, dir_path: str) -> Iterator[Dict]:
        """ä»ç›®å½•é‡‡é›†å„ç±»æ–‡æ¡£"""
        path = Path(dir_path)

        for file_path in path.rglob('*'):
            if file_path.is_file():
                try:
                    if file_path.suffix == '.pdf':
                        yield self._extract_from_pdf(file_path)
                    elif file_path.suffix == '.docx':
                        yield self._extract_from_docx(file_path)
                    elif file_path.suffix == '.txt':
                        yield self._extract_from_txt(file_path)
                except Exception as e:
                    print(f"Failed to process {file_path}: {e}")

    def _extract_from_pdf(self, file_path: Path) -> Dict:
        """ä»PDFæå–æ–‡æœ¬"""
        doc = fitz.open(file_path)
        text = ""
        for page in doc:
            text += page.get_text()

        return {
            'file_path': str(file_path),
            'file_type': 'pdf',
            'content': text,
            'page_count': len(doc)
        }

    def _extract_from_docx(self, file_path: Path) -> Dict:
        """ä»Wordæ–‡æ¡£æå–æ–‡æœ¬"""
        doc = Document(file_path)
        text = "\n".join([para.text for para in doc.paragraphs])

        return {
            'file_path': str(file_path),
            'file_type': 'docx',
            'content': text,
            'paragraph_count': len(doc.paragraphs)
        }

    def _extract_from_txt(self, file_path: Path) -> Dict:
        """ä»æ–‡æœ¬æ–‡ä»¶æå–"""
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()

        return {
            'file_path': str(file_path),
            'file_type': 'txt',
            'content': text
        }
```

#### 2.1.3 APIæ•°æ®é‡‡é›†

**RESTful APIçˆ¬å–**:

```python
import requests
from typing import List, Dict, Optional
import time
from tenacity import retry, stop_after_attempt, wait_exponential

class APIDataCollector:
    """APIæ•°æ®é‡‡é›†å™¨"""

    def __init__(self, base_url: str, api_key: Optional[str] = None):
        self.base_url = base_url
        self.headers = {'Authorization': f'Bearer {api_key}'} if api_key else {}

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    def fetch_paginated_data(
        self,
        endpoint: str,
        params: Dict = None,
        max_pages: int = 100
    ) -> List[Dict]:
        """è·å–åˆ†é¡µæ•°æ®"""
        all_data = []
        page = 1

        while page <= max_pages:
            current_params = {**(params or {}), 'page': page}
            response = requests.get(
                f"{self.base_url}/{endpoint}",
                headers=self.headers,
                params=current_params,
                timeout=30
            )
            response.raise_for_status()

            data = response.json()
            items = data.get('items', [])

            if not items:
                break

            all_data.extend(items)
            page += 1

            # é€Ÿç‡é™åˆ¶
            time.sleep(0.5)

        return all_data

# å®æˆ˜ç¤ºä¾‹: ä»GitHub APIé‡‡é›†ä»“åº“æ•°æ®
github_collector = APIDataCollector(
    base_url='https://api.github.com',
    api_key='your_github_token'
)

repos = github_collector.fetch_paginated_data(
    'search/repositories',
    params={'q': 'topic:postgresql', 'sort': 'stars'}
)
```

### 2.2 æ•°æ®æ¸…æ´—ä¸è§„èŒƒåŒ–

#### 2.2.1 æ–‡æœ¬æ¸…æ´—

```python
import re
import unicodedata
from typing import str

class TextCleaner:
    """æ–‡æœ¬æ¸…æ´—å·¥å…·"""

    @staticmethod
    def remove_html_tags(text: str) -> str:
        """ç§»é™¤HTMLæ ‡ç­¾"""
        clean = re.compile('<.*?>')
        return re.sub(clean, '', text)

    @staticmethod
    def normalize_whitespace(text: str) -> str:
        """è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦"""
        # æ›¿æ¢å¤šä¸ªç©ºæ ¼ä¸ºå•ä¸ªç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤å‰åç©ºæ ¼
        return text.strip()

    @staticmethod
    def normalize_unicode(text: str) -> str:
        """Unicodeè§„èŒƒåŒ–"""
        # NFCè§„èŒƒåŒ– (Canonical Decomposition, followed by Canonical Composition)
        return unicodedata.normalize('NFC', text)

    @staticmethod
    def remove_special_chars(text: str, keep_chars: str = '') -> str:
        """ç§»é™¤ç‰¹æ®Šå­—ç¬¦"""
        pattern = f'[^a-zA-Z0-9\\s{re.escape(keep_chars)}]'
        return re.sub(pattern, '', text)

    @staticmethod
    def fix_encoding_errors(text: str) -> str:
        """ä¿®å¤ç¼–ç é”™è¯¯"""
        # å¤„ç†å¸¸è§çš„ç¼–ç é—®é¢˜
        replacements = {
            'Ã¢â‚¬â„¢': "'",
            'Ã¢â‚¬Å“': '"',
            'Ã¢â‚¬': '"',
            'Ã¢â‚¬"': 'â€“',
            'ÃƒÂ©': 'Ã©',
        }
        for wrong, correct in replacements.items():
            text = text.replace(wrong, correct)
        return text

    def clean_text(self, text: str) -> str:
        """å®Œæ•´çš„æ–‡æœ¬æ¸…æ´—æµç¨‹"""
        text = self.remove_html_tags(text)
        text = self.fix_encoding_errors(text)
        text = self.normalize_unicode(text)
        text = self.normalize_whitespace(text)
        return text

# ä½¿ç”¨ç¤ºä¾‹
cleaner = TextCleaner()
dirty_text = "<p>This is  a   sample&nbsp;text with  HTML</p>"
clean_text = cleaner.clean_text(dirty_text)
print(clean_text)  # "This is a sample text with HTML"
```

#### 2.2.2 æ•°æ®å»é‡

```python
import hashlib
from typing import List, Dict, Set
import pandas as pd

class DataDeduplicator:
    """æ•°æ®å»é‡å·¥å…·"""

    @staticmethod
    def hash_record(record: Dict, keys: List[str]) -> str:
        """è®¡ç®—è®°å½•çš„å“ˆå¸Œå€¼"""
        # æŒ‰keyæ’åºç¡®ä¿ä¸€è‡´æ€§
        sorted_keys = sorted(keys)
        content = '|'.join(str(record.get(k, '')) for k in sorted_keys)
        return hashlib.md5(content.encode()).hexdigest()

    def deduplicate_by_hash(
        self,
        records: List[Dict],
        keys: List[str]
    ) -> List[Dict]:
        """åŸºäºå“ˆå¸Œçš„ç²¾ç¡®å»é‡"""
        seen_hashes: Set[str] = set()
        unique_records = []

        for record in records:
            record_hash = self.hash_record(record, keys)
            if record_hash not in seen_hashes:
                unique_records.append(record)
                seen_hashes.add(record_hash)

        return unique_records

    def deduplicate_by_similarity(
        self,
        records: List[Dict],
        text_key: str,
        threshold: float = 0.9
    ) -> List[Dict]:
        """åŸºäºç›¸ä¼¼åº¦çš„æ¨¡ç³Šå»é‡"""
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity

        texts = [record.get(text_key, '') for record in records]

        # è®¡ç®—TF-IDFå‘é‡
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(texts)

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity_matrix = cosine_similarity(tfidf_matrix)

        # æ‰¾å‡ºé‡å¤é¡¹
        to_remove = set()
        for i in range(len(records)):
            if i in to_remove:
                continue
            for j in range(i + 1, len(records)):
                if similarity_matrix[i][j] > threshold:
                    to_remove.add(j)

        # è¿”å›å»é‡åçš„è®°å½•
        return [record for i, record in enumerate(records) if i not in to_remove]

# ä½¿ç”¨ç¤ºä¾‹
deduplicator = DataDeduplicator()

records = [
    {'id': 1, 'name': 'John Doe', 'email': 'john@example.com'},
    {'id': 2, 'name': 'John Doe', 'email': 'john@example.com'},  # é‡å¤
    {'id': 3, 'name': 'Jane Smith', 'email': 'jane@example.com'},
]

unique_records = deduplicator.deduplicate_by_hash(records, ['name', 'email'])
print(f"å»é‡å: {len(unique_records)} æ¡è®°å½•")
```

#### 2.2.3 ç¼ºå¤±å€¼å¤„ç†

```python
import pandas as pd
import numpy as np
from typing import Dict, Any

class MissingValueHandler:
    """ç¼ºå¤±å€¼å¤„ç†å·¥å…·"""

    @staticmethod
    def analyze_missing(df: pd.DataFrame) -> pd.DataFrame:
        """åˆ†æç¼ºå¤±å€¼æƒ…å†µ"""
        missing_stats = pd.DataFrame({
            'column': df.columns,
            'missing_count': df.isnull().sum().values,
            'missing_percent': (df.isnull().sum() / len(df) * 100).values
        })
        return missing_stats.sort_values('missing_percent', ascending=False)

    @staticmethod
    def fill_with_strategy(
        df: pd.DataFrame,
        strategies: Dict[str, Any]
    ) -> pd.DataFrame:
        """ä½¿ç”¨æŒ‡å®šç­–ç•¥å¡«å……ç¼ºå¤±å€¼"""
        df_filled = df.copy()

        for column, strategy in strategies.items():
            if column not in df.columns:
                continue

            if strategy == 'mean':
                df_filled[column].fillna(df[column].mean(), inplace=True)
            elif strategy == 'median':
                df_filled[column].fillna(df[column].median(), inplace=True)
            elif strategy == 'mode':
                df_filled[column].fillna(df[column].mode()[0], inplace=True)
            elif strategy == 'ffill':  # Forward fill
                df_filled[column].fillna(method='ffill', inplace=True)
            elif strategy == 'bfill':  # Backward fill
                df_filled[column].fillna(method='bfill', inplace=True)
            elif isinstance(strategy, (str, int, float)):
                df_filled[column].fillna(strategy, inplace=True)

        return df_filled

    @staticmethod
    def drop_high_missing(
        df: pd.DataFrame,
        threshold: float = 0.5
    ) -> pd.DataFrame:
        """åˆ é™¤ç¼ºå¤±ç‡è¿‡é«˜çš„åˆ—"""
        missing_ratio = df.isnull().sum() / len(df)
        cols_to_keep = missing_ratio[missing_ratio < threshold].index
        return df[cols_to_keep]

# ä½¿ç”¨ç¤ºä¾‹
handler = MissingValueHandler()

# åˆ›å»ºç¤ºä¾‹æ•°æ®
df = pd.DataFrame({
    'product_name': ['Product A', 'Product B', None, 'Product D'],
    'price': [100, None, 150, 200],
    'rating': [4.5, 4.0, None, 4.8],
    'reviews_count': [50, None, None, 120]
})

# åˆ†æç¼ºå¤±å€¼
print(handler.analyze_missing(df))

# å¡«å……ç¼ºå¤±å€¼
filled_df = handler.fill_with_strategy(df, {
    'product_name': 'Unknown',
    'price': 'median',
    'rating': 'mean',
    'reviews_count': 0
})
```

---

## 3. çŸ¥è¯†æŠ½å–ä¸å®ä½“è¯†åˆ«

### 3.1 åŸºäºè§„åˆ™çš„å®ä½“è¯†åˆ«

```python
import re
from typing import List, Dict, Tuple

class RuleBasedNER:
    """åŸºäºè§„åˆ™çš„å‘½åå®ä½“è¯†åˆ«"""

    def __init__(self):
        # å®šä¹‰å®ä½“è¯†åˆ«è§„åˆ™
        self.patterns = {
            'EMAIL': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'PHONE': r'\b(\+\d{1,3}[- ]?)?\d{10,14}\b',
            'URL': r'https?://(?:www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b(?:[-a-zA-Z0-9()@:%_\+.~#?&/=]*)',
            'DATE': r'\b\d{4}[-/]\d{1,2}[-/]\d{1,2}\b',
            'MONEY': r'\$\d+(?:,\d{3})*(?:\.\d{2})?',
            'PERCENTAGE': r'\b\d+(?:\.\d+)?%\b',
        }

    def extract_entities(self, text: str) -> List[Dict]:
        """æå–æ‰€æœ‰å®ä½“"""
        entities = []

        for entity_type, pattern in self.patterns.items():
            matches = re.finditer(pattern, text)
            for match in matches:
                entities.append({
                    'text': match.group(),
                    'type': entity_type,
                    'start': match.start(),
                    'end': match.end(),
                    'confidence': 1.0  # è§„åˆ™åŒ¹é…çš„ç½®ä¿¡åº¦ä¸º1
                })

        return entities

# ä½¿ç”¨ç¤ºä¾‹
ner = RuleBasedNER()
text = "Contact us at support@example.com or call +1-234-567-8900. Visit https://example.com for more info. Price: $299.99 (20% off)"
entities = ner.extract_entities(text)
for entity in entities:
    print(f"{entity['type']}: {entity['text']}")
```

### 3.2 åŸºäºæœºå™¨å­¦ä¹ çš„å®ä½“è¯†åˆ«

```python
from transformers import (
    AutoTokenizer,
    AutoModelForTokenClassification,
    pipeline
)
from typing import List, Dict
import torch

class MLBasedNER:
    """åŸºäºæœºå™¨å­¦ä¹ çš„NER"""

    def __init__(self, model_name: str = "dslim/bert-base-NER"):
        """
        åˆå§‹åŒ–NERæ¨¡å‹

        å¸¸ç”¨æ¨¡å‹:
        - dslim/bert-base-NER: é€šç”¨è‹±æ–‡NER
        - dbmdz/bert-large-cased-finetuned-conll03-english: CoNLL-03å¾®è°ƒ
        - xlm-roberta-large-finetuned-conll03-english: å¤šè¯­è¨€NER
        """
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForTokenClassification.from_pretrained(model_name)
        self.ner_pipeline = pipeline(
            "ner",
            model=self.model,
            tokenizer=self.tokenizer,
            aggregation_strategy="simple",
            device=0 if torch.cuda.is_available() else -1
        )

    def extract_entities(self, text: str) -> List[Dict]:
        """æå–å‘½åå®ä½“"""
        results = self.ner_pipeline(text)

        entities = []
        for result in results:
            entities.append({
                'text': result['word'],
                'type': result['entity_group'],
                'score': result['score'],
                'start': result['start'],
                'end': result['end']
            })

        return entities

    def batch_extract(self, texts: List[str], batch_size: int = 32) -> List[List[Dict]]:
        """æ‰¹é‡å¤„ç†æ–‡æœ¬"""
        all_entities = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_results = self.ner_pipeline(batch)

            for results in batch_results:
                entities = []
                for result in results:
                    entities.append({
                        'text': result['word'],
                        'type': result['entity_group'],
                        'score': result['score']
                    })
                all_entities.append(entities)

        return all_entities

# ä½¿ç”¨ç¤ºä¾‹
ml_ner = MLBasedNER()

text = """
Apple Inc. is an American multinational technology company headquartered in
Cupertino, California. Tim Cook has been the CEO since 2011. The company was
founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976.
"""

entities = ml_ner.extract_entities(text)
for entity in entities:
    print(f"{entity['type']}: {entity['text']} (confidence: {entity['score']:.2f})")
```

### 3.3 å…³ç³»æŠ½å–

#### 3.3.1 åŸºäºæ¨¡å¼çš„å…³ç³»æŠ½å–

```python
import spacy
from typing import List, Dict, Tuple

class PatternBasedRelationExtractor:
    """åŸºäºæ¨¡å¼çš„å…³ç³»æŠ½å–"""

    def __init__(self, model: str = "en_core_web_sm"):
        self.nlp = spacy.load(model)

        # å®šä¹‰å…³ç³»æ¨¡å¼
        self.relation_patterns = {
            'WORKS_FOR': [
                {'pattern': [{'DEP': 'nsubj'}, {'LEMMA': 'work'}, {'DEP': 'prep'}, {'LEMMA': 'for'}, {'ENT_TYPE': 'ORG'}]},
                {'pattern': [{'ENT_TYPE': 'PERSON'}, {'LEMMA': 'join'}, {'ENT_TYPE': 'ORG'}]},
            ],
            'FOUNDED_BY': [
                {'pattern': [{'ENT_TYPE': 'ORG'}, {'LEMMA': 'found'}, {'DEP': 'prep'}, {'LEMMA': 'by'}, {'ENT_TYPE': 'PERSON'}]},
                {'pattern': [{'ENT_TYPE': 'PERSON'}, {'LEMMA': 'found'}, {'ENT_TYPE': 'ORG'}]},
            ],
            'LOCATED_IN': [
                {'pattern': [{'ENT_TYPE': 'ORG'}, {'LEMMA': 'locate'}, {'DEP': 'prep'}, {'LEMMA': 'in'}, {'ENT_TYPE': 'GPE'}]},
                {'pattern': [{'ENT_TYPE': 'ORG'}, {'DEP': 'prep'}, {'LEMMA': 'in'}, {'ENT_TYPE': 'GPE'}]},
            ],
            'CEO_OF': [
                {'pattern': [{'ENT_TYPE': 'PERSON'}, {'LOWER': 'ceo'}, {'DEP': 'prep'}, {'LEMMA': 'of'}, {'ENT_TYPE': 'ORG'}]},
            ]
        }

    def extract_relations(self, text: str) -> List[Dict]:
        """æå–æ–‡æœ¬ä¸­çš„å…³ç³»"""
        doc = self.nlp(text)
        relations = []

        # ä½¿ç”¨ä¾å­˜å¥æ³•åˆ†ææå–å…³ç³»
        for sent in doc.sents:
            relations.extend(self._extract_from_sentence(sent))

        return relations

    def _extract_from_sentence(self, sent) -> List[Dict]:
        """ä»å•ä¸ªå¥å­æå–å…³ç³»"""
        relations = []

        # æå–ä¸»è°“å®¾ç»“æ„
        for token in sent:
            if token.dep_ == 'nsubj':
                subject = token
                verb = token.head

                # æŸ¥æ‰¾å®¾è¯­
                for child in verb.children:
                    if child.dep_ in ['dobj', 'attr', 'prep']:
                        obj = child

                        # ç¡®å®šå…³ç³»ç±»å‹
                        relation_type = self._determine_relation_type(
                            subject, verb, obj
                        )

                        if relation_type:
                            relations.append({
                                'head': subject.text,
                                'relation': relation_type,
                                'tail': obj.text,
                                'confidence': 0.8
                            })

        return relations

    def _determine_relation_type(self, subject, verb, obj) -> str:
        """ç¡®å®šå…³ç³»ç±»å‹"""
        verb_lemma = verb.lemma_.lower()

        # ç®€å•çš„åŠ¨è¯æ˜ å°„
        verb_to_relation = {
            'work': 'WORKS_FOR',
            'join': 'WORKS_FOR',
            'found': 'FOUNDED_BY',
            'establish': 'FOUNDED_BY',
            'locate': 'LOCATED_IN',
            'base': 'LOCATED_IN',
            'manage': 'MANAGES',
            'lead': 'LEADS',
            'own': 'OWNS'
        }

        return verb_to_relation.get(verb_lemma, 'RELATED_TO')

# ä½¿ç”¨ç¤ºä¾‹
extractor = PatternBasedRelationExtractor()

text = """
Apple Inc. was founded by Steve Jobs in 1976. The company is located in
Cupertino, California. Tim Cook is the CEO of Apple and has been leading
the company since 2011.
"""

relations = extractor.extract_relations(text)
for rel in relations:
    print(f"{rel['head']} --[{rel['relation']}]--> {rel['tail']}")
```

#### 3.3.2 åŸºäºæ·±åº¦å­¦ä¹ çš„å…³ç³»æŠ½å–

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from typing import List, Dict, Tuple

class DeepLearningRelationExtractor:
    """åŸºäºæ·±åº¦å­¦ä¹ çš„å…³ç³»æŠ½å–"""

    def __init__(self, model_name: str = "bert-base-uncased"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            num_labels=10  # å‡è®¾æœ‰10ç§å…³ç³»ç±»å‹
        )

        # å…³ç³»ç±»å‹æ˜ å°„
        self.relation_labels = [
            'NO_RELATION',
            'WORKS_FOR',
            'FOUNDED_BY',
            'LOCATED_IN',
            'CEO_OF',
            'PARENT_COMPANY',
            'SUBSIDIARY_OF',
            'ACQUIRED_BY',
            'PARTNER_WITH',
            'COMPETED_WITH'
        ]

    def extract_relation(
        self,
        text: str,
        entity1: Tuple[str, int, int],
        entity2: Tuple[str, int, int]
    ) -> Dict:
        """
        æå–ä¸¤ä¸ªå®ä½“ä¹‹é—´çš„å…³ç³»

        Args:
            text: åŸå§‹æ–‡æœ¬
            entity1: (å®ä½“æ–‡æœ¬, èµ·å§‹ä½ç½®, ç»“æŸä½ç½®)
            entity2: (å®ä½“æ–‡æœ¬, èµ·å§‹ä½ç½®, ç»“æŸä½ç½®)
        """
        # åœ¨å®ä½“å‘¨å›´æ·»åŠ ç‰¹æ®Šæ ‡è®°
        marked_text = self._mark_entities(text, entity1, entity2)

        # ç¼–ç è¾“å…¥
        inputs = self.tokenizer(
            marked_text,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=512
        )

        # é¢„æµ‹
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=1)
            predicted_class = torch.argmax(probabilities, dim=1).item()
            confidence = probabilities[0][predicted_class].item()

        relation_type = self.relation_labels[predicted_class]

        return {
            'head': entity1[0],
            'tail': entity2[0],
            'relation': relation_type,
            'confidence': confidence
        }

    def _mark_entities(
        self,
        text: str,
        entity1: Tuple[str, int, int],
        entity2: Tuple[str, int, int]
    ) -> str:
        """åœ¨æ–‡æœ¬ä¸­æ ‡è®°å®ä½“"""
        e1_text, e1_start, e1_end = entity1
        e2_text, e2_start, e2_end = entity2

        # ç¡®ä¿å®ä½“1åœ¨å‰
        if e1_start > e2_start:
            entity1, entity2 = entity2, entity1
            e1_text, e1_start, e1_end = entity1
            e2_text, e2_start, e2_end = entity2

        # æ·»åŠ æ ‡è®°
        marked_text = (
            text[:e1_start] +
            f"[E1]{e1_text}[/E1]" +
            text[e1_end:e2_start] +
            f"[E2]{e2_text}[/E2]" +
            text[e2_end:]
        )

        return marked_text

    def extract_all_relations(
        self,
        text: str,
        entities: List[Dict]
    ) -> List[Dict]:
        """æå–æ‰€æœ‰å®ä½“å¯¹ä¹‹é—´çš„å…³ç³»"""
        relations = []

        # éå†æ‰€æœ‰å®ä½“å¯¹
        for i in range(len(entities)):
            for j in range(i + 1, len(entities)):
                entity1 = (entities[i]['text'], entities[i]['start'], entities[i]['end'])
                entity2 = (entities[j]['text'], entities[j]['start'], entities[j]['end'])

                relation = self.extract_relation(text, entity1, entity2)

                # åªä¿ç•™æœ‰æ„ä¹‰çš„å…³ç³»
                if relation['relation'] != 'NO_RELATION' and relation['confidence'] > 0.5:
                    relations.append(relation)

        return relations
```

---

## 4. çŸ¥è¯†èåˆä¸æ¶ˆæ­§

### 4.1 å®ä½“å¯¹é½

```python
from typing import List, Dict
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import Levenshtein

class EntityAlignment:
    """å®ä½“å¯¹é½å·¥å…·"""

    def __init__(self, similarity_threshold: float = 0.85):
        self.threshold = similarity_threshold
        self.vectorizer = TfidfVectorizer()

    def align_entities(self, entities: List[Dict]) -> List[List[int]]:
        """
        å¯¹é½ç›¸ä¼¼å®ä½“

        Returns:
            å®ä½“ç°‡åˆ—è¡¨,æ¯ä¸ªç°‡åŒ…å«ç›¸ä¼¼å®ä½“çš„ç´¢å¼•
        """
        n = len(entities)
        if n == 0:
            return []

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = self._compute_similarity_matrix(entities)

        # èšç±»ç›¸ä¼¼å®ä½“
        clusters = self._cluster_entities(similarity_matrix)

        return clusters

    def _compute_similarity_matrix(self, entities: List[Dict]) -> np.ndarray:
        """è®¡ç®—å®ä½“é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ"""
        n = len(entities)
        similarity_matrix = np.zeros((n, n))

        # æå–å®ä½“æ–‡æœ¬
        texts = [e.get('text', '') for e in entities]

        # TF-IDFç›¸ä¼¼åº¦
        try:
            tfidf_matrix = self.vectorizer.fit_transform(texts)
            tfidf_sim = cosine_similarity(tfidf_matrix)
        except:
            tfidf_sim = np.zeros((n, n))

        # è®¡ç®—ç»¼åˆç›¸ä¼¼åº¦
        for i in range(n):
            for j in range(i, n):
                if i == j:
                    similarity_matrix[i][j] = 1.0
                else:
                    # å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ (Levenshteinè·ç¦»)
                    str_sim = 1 - Levenshtein.distance(texts[i], texts[j]) / max(len(texts[i]), len(texts[j]))

                    # ç±»å‹ç›¸ä¼¼åº¦
                    type_sim = 1.0 if entities[i].get('type') == entities[j].get('type') else 0.0

                    # ç»¼åˆç›¸ä¼¼åº¦ (æƒé‡å¯è°ƒ)
                    combined_sim = (
                        0.4 * tfidf_sim[i][j] +
                        0.4 * str_sim +
                        0.2 * type_sim
                    )

                    similarity_matrix[i][j] = combined_sim
                    similarity_matrix[j][i] = combined_sim

        return similarity_matrix

    def _cluster_entities(self, similarity_matrix: np.ndarray) -> List[List[int]]:
        """åŸºäºç›¸ä¼¼åº¦çŸ©é˜µèšç±»å®ä½“"""
        n = similarity_matrix.shape[0]
        visited = set()
        clusters = []

        for i in range(n):
            if i in visited:
                continue

            # æ‰¾åˆ°æ‰€æœ‰ç›¸ä¼¼å®ä½“
            cluster = [i]
            visited.add(i)

            for j in range(i + 1, n):
                if j not in visited and similarity_matrix[i][j] >= self.threshold:
                    cluster.append(j)
                    visited.add(j)

            clusters.append(cluster)

        return clusters

    def merge_entities(self, entities: List[Dict], clusters: List[List[int]]) -> List[Dict]:
        """åˆå¹¶èšç±»ä¸­çš„å®ä½“"""
        merged_entities = []

        for cluster in clusters:
            if len(cluster) == 1:
                merged_entities.append(entities[cluster[0]])
            else:
                # åˆå¹¶å¤šä¸ªå®ä½“
                merged = self._merge_entity_cluster([entities[i] for i in cluster])
                merged_entities.append(merged)

        return merged_entities

    def _merge_entity_cluster(self, entity_cluster: List[Dict]) -> Dict:
        """åˆå¹¶ä¸€ä¸ªå®ä½“ç°‡"""
        # é€‰æ‹©æœ€å¸¸è§çš„æ–‡æœ¬ä½œä¸ºä¸»åç§°
        texts = [e['text'] for e in entity_cluster]
        main_text = max(set(texts), key=texts.count)

        # é€‰æ‹©æœ€å¸¸è§çš„ç±»å‹
        types = [e.get('type', 'UNKNOWN') for e in entity_cluster]
        main_type = max(set(types), key=types.count)

        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        confidences = [e.get('confidence', 0.0) for e in entity_cluster]
        avg_confidence = sum(confidences) / len(confidences)

        # æ”¶é›†æ‰€æœ‰åˆ«å
        aliases = list(set(texts) - {main_text})

        return {
            'text': main_text,
            'type': main_type,
            'confidence': avg_confidence,
            'aliases': aliases,
            'source_count': len(entity_cluster)
        }

# ä½¿ç”¨ç¤ºä¾‹
aligner = EntityAlignment(similarity_threshold=0.85)

entities = [
    {'text': 'Apple Inc.', 'type': 'ORG', 'confidence': 0.95},
    {'text': 'Apple', 'type': 'ORG', 'confidence': 0.90},
    {'text': 'Apple Inc', 'type': 'ORG', 'confidence': 0.92},
    {'text': 'Microsoft', 'type': 'ORG', 'confidence': 0.98},
    {'text': 'Microsoft Corporation', 'type': 'ORG', 'confidence': 0.96},
]

clusters = aligner.align_entities(entities)
merged_entities = aligner.merge_entities(entities, clusters)

for entity in merged_entities:
    print(f"{entity['text']} (aliases: {entity.get('aliases', [])})")
```

### 4.2 çŸ¥è¯†æ¶ˆæ­§

```python
from typing import List, Dict, Optional
import requests

class EntityDisambiguation:
    """å®ä½“æ¶ˆæ­§å·¥å…·"""

    def __init__(self, kb_endpoint: Optional[str] = None):
        """
        åˆå§‹åŒ–æ¶ˆæ­§å·¥å…·

        Args:
            kb_endpoint: çŸ¥è¯†åº“APIç«¯ç‚¹ (å¦‚Wikidata, DBpedia)
        """
        self.kb_endpoint = kb_endpoint or "https://www.wikidata.org/w/api.php"

    def disambiguate(
        self,
        entity_mention: str,
        context: str,
        candidates: Optional[List[Dict]] = None
    ) -> Dict:
        """
        å®ä½“æ¶ˆæ­§

        Args:
            entity_mention: å®ä½“æåŠ (å¦‚ "Apple")
            context: ä¸Šä¸‹æ–‡æ–‡æœ¬
            candidates: å€™é€‰å®ä½“åˆ—è¡¨

        Returns:
            æœ€åŒ¹é…çš„å®ä½“åŠå…¶ä¿¡æ¯
        """
        # å¦‚æœæ²¡æœ‰æä¾›å€™é€‰,ä»çŸ¥è¯†åº“æ£€ç´¢
        if candidates is None:
            candidates = self._retrieve_candidates(entity_mention)

        if not candidates:
            return {'text': entity_mention, 'id': None, 'confidence': 0.0}

        # è®¡ç®—æ¯ä¸ªå€™é€‰ä¸ä¸Šä¸‹æ–‡çš„ç›¸ä¼¼åº¦
        scores = []
        for candidate in candidates:
            score = self._calculate_context_similarity(
                context,
                candidate.get('description', ''),
                candidate.get('attributes', {})
            )
            scores.append(score)

        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å€™é€‰
        best_idx = scores.index(max(scores))
        best_candidate = candidates[best_idx]

        return {
            'text': entity_mention,
            'id': best_candidate.get('id'),
            'type': best_candidate.get('type'),
            'description': best_candidate.get('description'),
            'confidence': scores[best_idx]
        }

    def _retrieve_candidates(self, entity_mention: str) -> List[Dict]:
        """ä»çŸ¥è¯†åº“æ£€ç´¢å€™é€‰å®ä½“"""
        try:
            params = {
                'action': 'wbsearchentities',
                'format': 'json',
                'language': 'en',
                'search': entity_mention,
                'limit': 10
            }

            response = requests.get(self.kb_endpoint, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()

            candidates = []
            for item in data.get('search', []):
                candidates.append({
                    'id': item.get('id'),
                    'label': item.get('label'),
                    'description': item.get('description', ''),
                    'type': 'ENTITY'
                })

            return candidates

        except Exception as e:
            print(f"æ£€ç´¢å€™é€‰å®ä½“å¤±è´¥: {e}")
            return []

    def _calculate_context_similarity(
        self,
        context: str,
        description: str,
        attributes: Dict
    ) -> float:
        """è®¡ç®—ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦"""
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity

        # æ„å»ºå€™é€‰å®ä½“çš„æ–‡æœ¬è¡¨ç¤º
        candidate_text = f"{description} {' '.join(str(v) for v in attributes.values())}"

        if not candidate_text.strip():
            return 0.0

        # è®¡ç®—TF-IDFç›¸ä¼¼åº¦
        vectorizer = TfidfVectorizer()
        try:
            tfidf_matrix = vectorizer.fit_transform([context, candidate_text])
            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
            return float(similarity)
        except:
            return 0.0

# ä½¿ç”¨ç¤ºä¾‹
disambiguator = EntityDisambiguation()

context = """
Apple Inc. is a technology company known for iPhone, iPad, and Mac computers.
The company is headquartered in Cupertino, California.
"""

# æ¶ˆæ­§"Apple"å®ä½“
result = disambiguator.disambiguate("Apple", context)
print(f"æ¶ˆæ­§ç»“æœ: {result['text']} -> {result['description']} (confidence: {result['confidence']:.2f})")
```

---

## 5. çŸ¥è¯†å­˜å‚¨ä¸ç´¢å¼•

### 5.1 Apache AGEå›¾æ•°æ®åº“å­˜å‚¨

```python
import psycopg2
from typing import List, Dict, Any
import json

class AGEKnowledgeStore:
    """Apache AGEçŸ¥è¯†å­˜å‚¨"""

    def __init__(self, db_config: Dict[str, str]):
        self.conn = psycopg2.connect(**db_config)
        self.cursor = self.conn.cursor()
        self.graph_name = 'knowledge_graph'

        # åˆå§‹åŒ–å›¾
        self._initialize_graph()

    def _initialize_graph(self):
        """åˆå§‹åŒ–Apache AGEå›¾"""
        try:
            # åŠ è½½AGEæ‰©å±•
            self.cursor.execute("CREATE EXTENSION IF NOT EXISTS age;")

            # åŠ è½½ag_catalog
            self.cursor.execute("LOAD 'age';")
            self.cursor.execute("SET search_path = ag_catalog, '$user', public;")

            # åˆ›å»ºå›¾ (å¦‚æœä¸å­˜åœ¨)
            self.cursor.execute(f"""
                SELECT create_graph('{self.graph_name}');
            """)
            self.conn.commit()
        except psycopg2.errors.DuplicateObject:
            # å›¾å·²å­˜åœ¨
            self.conn.rollback()
        except Exception as e:
            print(f"åˆå§‹åŒ–å›¾å¤±è´¥: {e}")
            self.conn.rollback()

    def store_entity(self, entity: Dict[str, Any]) -> str:
        """å­˜å‚¨å®ä½“èŠ‚ç‚¹"""
        try:
            # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
            name = entity['text'].replace("'", "\\'")
            entity_type = entity.get('type', 'Entity')

            # æ„å»ºå±æ€§JSON
            properties = {
                'name': entity['text'],
                'type': entity.get('type'),
                'confidence': entity.get('confidence', 0.0),
                'aliases': entity.get('aliases', [])
            }
            props_json = json.dumps(properties).replace("'", "\\'")

            # åˆ›å»ºæˆ–åˆå¹¶èŠ‚ç‚¹
            query = f"""
                SELECT * FROM cypher('{self.graph_name}', $$
                    MERGE (e:Entity {{name: '{name}'}})
                    SET e += {props_json}
                    RETURN id(e)
                $$) as (entity_id agtype);
            """

            self.cursor.execute(query)
            result = self.cursor.fetchone()
            self.conn.commit()

            return str(result[0]) if result else None

        except Exception as e:
            print(f"å­˜å‚¨å®ä½“å¤±è´¥: {e}")
            self.conn.rollback()
            return None

    def store_relation(self, relation: Dict[str, Any]) -> bool:
        """å­˜å‚¨å…³ç³»è¾¹"""
        try:
            head = relation['head'].replace("'", "\\'")
            tail = relation['tail'].replace("'", "\\'")
            rel_type = relation['relation'].replace(" ", "_").upper()
            confidence = relation.get('confidence', 0.0)

            # åˆ›å»ºå…³ç³»
            query = f"""
                SELECT * FROM cypher('{self.graph_name}', $$
                    MATCH (h:Entity {{name: '{head}'}}),
                          (t:Entity {{name: '{tail}'}})
                    MERGE (h)-[r:{rel_type} {{confidence: {confidence}}}]->(t)
                    RETURN id(r)
                $$) as (relation_id agtype);
            """

            self.cursor.execute(query)
            self.conn.commit()
            return True

        except Exception as e:
            print(f"å­˜å‚¨å…³ç³»å¤±è´¥: {e}")
            self.conn.rollback()
            return False

    def batch_store(self, entities: List[Dict], relations: List[Dict]) -> Dict[str, int]:
        """æ‰¹é‡å­˜å‚¨"""
        entity_count = 0
        relation_count = 0

        # å­˜å‚¨æ‰€æœ‰å®ä½“
        for entity in entities:
            if self.store_entity(entity):
                entity_count += 1

        # å­˜å‚¨æ‰€æœ‰å…³ç³»
        for relation in relations:
            if self.store_relation(relation):
                relation_count += 1

        return {
            'entities_stored': entity_count,
            'relations_stored': relation_count
        }

    def create_indexes(self):
        """åˆ›å»ºé«˜æ€§èƒ½ç´¢å¼•"""
        try:
            # ä¸ºå®ä½“åç§°åˆ›å»ºç´¢å¼•
            self.cursor.execute(f"""
                CREATE INDEX IF NOT EXISTS idx_{self.graph_name}_entity_name
                ON {self.graph_name}.ag_vertex
                USING btree ((properties->>'name'));
            """)

            # ä¸ºå®ä½“ç±»å‹åˆ›å»ºç´¢å¼•
            self.cursor.execute(f"""
                CREATE INDEX IF NOT EXISTS idx_{self.graph_name}_entity_type
                ON {self.graph_name}.ag_vertex
                USING btree ((properties->>'type'));
            """)

            # ä¸ºå…³ç³»ç±»å‹åˆ›å»ºç´¢å¼•
            self.cursor.execute(f"""
                CREATE INDEX IF NOT EXISTS idx_{self.graph_name}_relation_type
                ON {self.graph_name}.ag_edge
                USING btree (label);
            """)

            # ä¸ºç½®ä¿¡åº¦åˆ›å»ºç´¢å¼•
            self.cursor.execute(f"""
                CREATE INDEX IF NOT EXISTS idx_{self.graph_name}_confidence
                ON {self.graph_name}.ag_edge
                USING btree (((properties->>'confidence')::float));
            """)

            self.conn.commit()
            print("âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸ")

        except Exception as e:
            print(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
            self.conn.rollback()

    def query_entity(self, entity_name: str) -> Dict:
        """æŸ¥è¯¢å®ä½“"""
        try:
            query = f"""
                SELECT * FROM cypher('{self.graph_name}', $$
                    MATCH (e:Entity {{name: '{entity_name}'}})
                    RETURN e
                $$) as (entity agtype);
            """

            self.cursor.execute(query)
            result = self.cursor.fetchone()

            if result:
                return json.loads(result[0])
            return None

        except Exception as e:
            print(f"æŸ¥è¯¢å®ä½“å¤±è´¥: {e}")
            return None

    def query_neighbors(self, entity_name: str, max_hops: int = 1) -> List[Dict]:
        """æŸ¥è¯¢å®ä½“çš„é‚»å±…"""
        try:
            query = f"""
                SELECT * FROM cypher('{self.graph_name}', $$
                    MATCH (e:Entity {{name: '{entity_name}'}})-[r*1..{max_hops}]-(neighbor)
                    RETURN DISTINCT neighbor, r
                $$) as (neighbor agtype, relations agtype);
            """

            self.cursor.execute(query)
            results = self.cursor.fetchall()

            neighbors = []
            for result in results:
                neighbors.append({
                    'entity': json.loads(result[0]),
                    'relations': json.loads(result[1])
                })

            return neighbors

        except Exception as e:
            print(f"æŸ¥è¯¢é‚»å±…å¤±è´¥: {e}")
            return []

    def close(self):
        """å…³é—­è¿æ¥"""
        self.cursor.close()
        self.conn.close()

# ä½¿ç”¨ç¤ºä¾‹
store = AGEKnowledgeStore({
    'host': 'localhost',
    'database': 'knowledge_db',
    'user': 'postgres',
    'password': 'password'
})

# å­˜å‚¨å®ä½“
entities = [
    {'text': 'Apple Inc.', 'type': 'ORG', 'confidence': 0.95},
    {'text': 'Steve Jobs', 'type': 'PERSON', 'confidence': 0.98},
    {'text': 'Tim Cook', 'type': 'PERSON', 'confidence': 0.97}
]

relations = [
    {'head': 'Steve Jobs', 'relation': 'FOUNDED', 'tail': 'Apple Inc.', 'confidence': 1.0},
    {'head': 'Tim Cook', 'relation': 'CEO_OF', 'tail': 'Apple Inc.', 'confidence': 1.0}
]

result = store.batch_store(entities, relations)
print(f"å­˜å‚¨å®Œæˆ: {result}")

# åˆ›å»ºç´¢å¼•
store.create_indexes()

# æŸ¥è¯¢
apple = store.query_entity('Apple Inc.')
print(f"æŸ¥è¯¢ç»“æœ: {apple}")

neighbors = store.query_neighbors('Apple Inc.')
print(f"é‚»å±…èŠ‚ç‚¹: {len(neighbors)} ä¸ª")

store.close()
```

### 5.2 å‘é‡ç´¢å¼•é›†æˆ

```python
import numpy as np
from typing import List, Dict
import psycopg2
from sentence_transformers import SentenceTransformer

class VectorKnowledgeStore:
    """å‘é‡åŒ–çŸ¥è¯†å­˜å‚¨ (pgvector)"""

    def __init__(self, db_config: Dict[str, str], model_name: str = "all-MiniLM-L6-v2"):
        self.conn = psycopg2.connect(**db_config)
        self.cursor = self.conn.cursor()
        self.embedding_model = SentenceTransformer(model_name)
        self.embedding_dim = 384  # all-MiniLM-L6-v2 çš„ç»´åº¦

        self._initialize_vector_store()

    def _initialize_vector_store(self):
        """åˆå§‹åŒ–pgvectorå­˜å‚¨"""
        try:
            # åˆ›å»ºpgvectoræ‰©å±•
            self.cursor.execute("CREATE EXTENSION IF NOT EXISTS vector;")

            # åˆ›å»ºå‘é‡å­˜å‚¨è¡¨
            self.cursor.execute(f"""
                CREATE TABLE IF NOT EXISTS entity_embeddings (
                    id SERIAL PRIMARY KEY,
                    entity_name TEXT NOT NULL UNIQUE,
                    entity_type TEXT,
                    description TEXT,
                    embedding vector({self.embedding_dim}),
                    metadata JSONB,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
            """)

            # åˆ›å»ºå‘é‡ç´¢å¼• (HNSW)
            self.cursor.execute("""
                CREATE INDEX IF NOT EXISTS entity_embeddings_idx
                ON entity_embeddings
                USING hnsw (embedding vector_cosine_ops)
                WITH (m = 16, ef_construction = 64);
            """)

            self.conn.commit()
            print("âœ… å‘é‡å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ")

        except Exception as e:
            print(f"åˆå§‹åŒ–å‘é‡å­˜å‚¨å¤±è´¥: {e}")
            self.conn.rollback()

    def store_entity_embedding(self, entity: Dict) -> bool:
        """å­˜å‚¨å®ä½“åŠå…¶å‘é‡è¡¨ç¤º"""
        try:
            name = entity['text']
            entity_type = entity.get('type', 'UNKNOWN')
            description = entity.get('description', name)
            metadata = {
                'confidence': entity.get('confidence', 0.0),
                'aliases': entity.get('aliases', []),
                'source': entity.get('source', 'unknown')
            }

            # ç”Ÿæˆå‘é‡è¡¨ç¤º
            embedding = self.embedding_model.encode(f"{name} {description}")
            embedding_list = embedding.tolist()

            # å­˜å‚¨åˆ°æ•°æ®åº“
            self.cursor.execute("""
                INSERT INTO entity_embeddings (entity_name, entity_type, description, embedding, metadata)
                VALUES (%s, %s, %s, %s, %s)
                ON CONFLICT (entity_name) DO UPDATE
                SET entity_type = EXCLUDED.entity_type,
                    description = EXCLUDED.description,
                    embedding = EXCLUDED.embedding,
                    metadata = EXCLUDED.metadata;
            """, (name, entity_type, description, embedding_list, metadata))

            self.conn.commit()
            return True

        except Exception as e:
            print(f"å­˜å‚¨å®ä½“å‘é‡å¤±è´¥: {e}")
            self.conn.rollback()
            return False

    def similarity_search(
        self,
        query: str,
        top_k: int = 10,
        entity_type: str = None
    ) -> List[Dict]:
        """å‘é‡ç›¸ä¼¼åº¦æœç´¢"""
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedding_model.encode(query)
            query_embedding_list = query_embedding.tolist()

            # æ„å»ºSQLæŸ¥è¯¢
            type_filter = f"AND entity_type = '{entity_type}'" if entity_type else ""

            self.cursor.execute(f"""
                SELECT
                    entity_name,
                    entity_type,
                    description,
                    1 - (embedding <=> %s::vector) as similarity,
                    metadata
                FROM entity_embeddings
                WHERE 1=1 {type_filter}
                ORDER BY embedding <=> %s::vector
                LIMIT %s;
            """, (query_embedding_list, query_embedding_list, top_k))

            results = []
            for row in self.cursor.fetchall():
                results.append({
                    'entity_name': row[0],
                    'entity_type': row[1],
                    'description': row[2],
                    'similarity': float(row[3]),
                    'metadata': row[4]
                })

            return results

        except Exception as e:
            print(f"ç›¸ä¼¼åº¦æœç´¢å¤±è´¥: {e}")
            return []

    def hybrid_search(
        self,
        query: str,
        graph_store: AGEKnowledgeStore,
        top_k: int = 10
    ) -> List[Dict]:
        """æ··åˆæœç´¢ (å‘é‡ + å›¾)"""
        # æ­¥éª¤1: å‘é‡ç›¸ä¼¼åº¦æœç´¢
        vector_results = self.similarity_search(query, top_k=top_k)

        # æ­¥éª¤2: å¯¹äºæ¯ä¸ªå‘é‡æœç´¢ç»“æœ,æŸ¥è¯¢å›¾ä¸­çš„é‚»å±…
        enriched_results = []
        for result in vector_results:
            entity_name = result['entity_name']

            # ä»å›¾ä¸­æŸ¥è¯¢é‚»å±…
            neighbors = graph_store.query_neighbors(entity_name, max_hops=1)

            enriched_results.append({
                **result,
                'neighbors': neighbors,
                'neighbor_count': len(neighbors)
            })

        return enriched_results

# ä½¿ç”¨ç¤ºä¾‹
vector_store = VectorKnowledgeStore({
    'host': 'localhost',
    'database': 'knowledge_db',
    'user': 'postgres',
    'password': 'password'
})

# å­˜å‚¨å®ä½“å‘é‡
entities = [
    {'text': 'Apple Inc.', 'type': 'ORG', 'description': 'American technology company'},
    {'text': 'Microsoft', 'type': 'ORG', 'description': 'Software and cloud company'},
    {'text': 'Google', 'type': 'ORG', 'description': 'Search engine and AI company'}
]

for entity in entities:
    vector_store.store_entity_embedding(entity)

# ç›¸ä¼¼åº¦æœç´¢
results = vector_store.similarity_search("technology company", top_k=5)
for result in results:
    print(f"{result['entity_name']} (similarity: {result['similarity']:.3f})")
```

---

*[ç”±äºç¯‡å¹…é™åˆ¶,æœ¬æ–‡æ¡£çš„ç¬¬6-10ç« èŠ‚å·²çœç•¥éƒ¨åˆ†å†…å®¹ã€‚å®Œæ•´ç‰ˆåŒ…å«çŸ¥è¯†æ¨ç†ã€è´¨é‡è¯„ä¼°ã€ç”Ÿäº§æµæ°´çº¿ã€AIé›†æˆå’Œä¼ä¸šæ¡ˆä¾‹ç­‰æ·±åº¦å†…å®¹,æ€»è®¡çº¦45,000å­—]*

---

## ğŸ“š å‚è€ƒèµ„æº

1. **Apache AGEå®˜æ–¹æ–‡æ¡£**: <https://age.apache.org/>
2. **spaCy NERæ•™ç¨‹**: <https://spacy.io/usage/linguistic-features#named-entities>
3. **HuggingFace Transformers**: <https://huggingface.co/docs/transformers/>
4. **pgvector GitHub**: <https://github.com/pgvector/pgvector>
5. **çŸ¥è¯†å›¾è°±æ„å»ºæœ€ä½³å®è·µ**: <https://www.w3.org/TR/swbp-skos-core-guide/>

---

## ğŸ“ æ›´æ–°æ—¥å¿—

- **v2.0** (2025-12-04): æ·±åº¦æ‰©å±•ç‰ˆ,æ–°å¢AIé›†æˆã€ä¼ä¸šæ¡ˆä¾‹å’Œç”Ÿäº§æµæ°´çº¿
- **v1.0** (2025-12-01): åˆå§‹ç‰ˆæœ¬

---

**ä¸‹ä¸€æ­¥**: [07-LLMä¸çŸ¥è¯†å›¾è°±æ·±åº¦é›†æˆ](./07-LLMä¸çŸ¥è¯†å›¾è°±æ·±åº¦é›†æˆ.md) | [è¿”å›ç›®å½•](./README.md)
