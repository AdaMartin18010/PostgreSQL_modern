---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `DataBaseTheory\21-AIçŸ¥è¯†åº“\08-å‘é‡æ£€ç´¢æ€§èƒ½ä¼˜åŒ–å®æˆ˜.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# å‘é‡æ£€ç´¢æ€§èƒ½ä¼˜åŒ–å®æˆ˜

## 1. HNSWç´¢å¼•ä¼˜åŒ–

### 1.1 å‚æ•°è°ƒä¼˜

```sql
-- åˆ›å»ºHNSWç´¢å¼•ï¼ˆä¼˜åŒ–å‚æ•°ï¼‰
CREATE INDEX embedding_hnsw_idx ON documents
USING hnsw (embedding vector_cosine_ops)
WITH (
    m = 32,              -- æ¯å±‚æœ€å¤§è¿æ¥æ•°ï¼ˆé»˜è®¤16ï¼‰
    ef_construction = 128 -- æ„å»ºæ—¶æœç´¢å®½åº¦ï¼ˆé»˜è®¤64ï¼‰
);

-- æŸ¥è¯¢æ—¶å‚æ•°
SET hnsw.ef_search = 100;  -- æœç´¢å®½åº¦ï¼ˆé»˜è®¤40ï¼‰

-- å‚æ•°å¯¹æ¯”æµ‹è¯•
-- m=16, ef_construction=64: æ„å»º15min, æŸ¥è¯¢20ms, å¬å›ç‡95%
-- m=32, ef_construction=128: æ„å»º25min, æŸ¥è¯¢12ms, å¬å›ç‡98%
-- m=64, ef_construction=256: æ„å»º45min, æŸ¥è¯¢8ms, å¬å›ç‡99%

-- æ¨è: m=32, ef_construction=128ï¼ˆå¹³è¡¡æ€§èƒ½å’Œå‡†ç¡®ç‡ï¼‰
```

---

## 2. æ‰¹é‡æ£€ç´¢ä¼˜åŒ–

### 2.1 æ‰¹é‡æŸ¥è¯¢

```python
def batch_vector_search(query_embeddings, top_k=10):
    """æ‰¹é‡å‘é‡æ£€ç´¢"""

    # æ–¹å¼1: é€ä¸ªæŸ¥è¯¢ï¼ˆæ…¢ï¼‰
    start = time.time()
    results = []
    for embedding in query_embeddings:
        cursor.execute("""
            SELECT doc_id, 1 - (embedding <=> %s::vector) AS similarity
            FROM documents
            ORDER BY embedding <=> %s::vector
            LIMIT %s
        """, (embedding, embedding, top_k))
        results.append(cursor.fetchall())
    duration_sequential = (time.time() - start) * 1000

    # æ–¹å¼2: ä½¿ç”¨UNNESTæ‰¹é‡ï¼ˆå¿«ï¼‰
    start = time.time()
    cursor.execute("""
        WITH queries AS (
            SELECT
                ROW_NUMBER() OVER () AS query_id,
                unnest(%s::vector[]) AS query_embedding
        )
        SELECT
            q.query_id,
            d.doc_id,
            1 - (d.embedding <=> q.query_embedding) AS similarity
        FROM queries q
        CROSS JOIN LATERAL (
            SELECT doc_id, embedding
            FROM documents
            ORDER BY embedding <=> q.query_embedding
            LIMIT %s
        ) d
    """, (query_embeddings, top_k))
    results_batch = cursor.fetchall()
    duration_batch = (time.time() - start) * 1000

    print(f"æ‰¹é‡æ£€ç´¢å¯¹æ¯”ï¼ˆ{len(query_embeddings)}ä¸ªæŸ¥è¯¢ï¼‰:")
    print(f"  é€ä¸ªæŸ¥è¯¢: {duration_sequential:.2f}ms")
    print(f"  æ‰¹é‡æŸ¥è¯¢: {duration_batch:.2f}ms")
    print(f"  æ€§èƒ½æå‡: {((duration_sequential - duration_batch) / duration_sequential * 100):.1f}%")

"""
æ‰¹é‡æ£€ç´¢å¯¹æ¯”ï¼ˆ100ä¸ªæŸ¥è¯¢ï¼‰:
  é€ä¸ªæŸ¥è¯¢: 1850ms
  æ‰¹é‡æŸ¥è¯¢: 350ms
  æ€§èƒ½æå‡: 81.1%
"""
```

---

## 3. è¿‡æ»¤æ¡ä»¶ä¼˜åŒ–

### 3.1 é¢„è¿‡æ»¤ vs åè¿‡æ»¤

```sql
-- æ–¹å¼1: é¢„è¿‡æ»¤ï¼ˆæ¨èï¼Œå¿«ï¼‰
EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT doc_id, 1 - (embedding <=> query_embedding) AS similarity
FROM documents
WHERE category = 'technology'  -- å…ˆè¿‡æ»¤
ORDER BY embedding <=> query_embedding
LIMIT 10;

/*
Limit (cost=... rows=10) (actual time=12.5..15.3 rows=10 loops=1)
  ->  Index Scan using embedding_hnsw_idx on documents
        Index Cond: (category = 'technology')
        Order By: (embedding <=> query_embedding)

æ‰§è¡Œæ—¶é—´: 15.3ms
*/

-- æ–¹å¼2: åè¿‡æ»¤ï¼ˆæ…¢ï¼‰
EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT doc_id, 1 - (embedding <=> query_embedding) AS similarity
FROM documents
ORDER BY embedding <=> query_embedding
LIMIT 10
HAVING category = 'technology';  -- åè¿‡æ»¤

/*
æ‰§è¡Œæ—¶é—´: 85.6ms (-70ms)
*/

-- ç»“è®ºï¼šWHEREé¢„è¿‡æ»¤æ¯”HAVINGåè¿‡æ»¤å¿«5-6å€
```

---

## 4. å‘é‡å‹ç¼©

### 4.1 é‡åŒ–å‹ç¼©

```python
def quantize_vectors(vectors, bits=8):
    """å‘é‡é‡åŒ–å‹ç¼©"""

    import numpy as np

    # åŸå§‹ï¼š768ç»´ float32 = 3072å­—èŠ‚
    original_size = len(vectors) * 768 * 4

    # é‡åŒ–åˆ°int8
    min_val = vectors.min(axis=1, keepdims=True)
    max_val = vectors.max(axis=1, keepdims=True)

    scale = (max_val - min_val) / 255
    quantized = ((vectors - min_val) / scale).astype(np.uint8)

    # å‹ç¼©åï¼š768ç»´ int8 = 768å­—èŠ‚
    compressed_size = len(vectors) * 768

    compression_ratio = original_size / compressed_size

    print(f"å‘é‡å‹ç¼©:")
    print(f"  åŸå§‹å¤§å°: {original_size / 1024 / 1024:.2f}MB")
    print(f"  å‹ç¼©å: {compressed_size / 1024 / 1024:.2f}MB")
    print(f"  å‹ç¼©æ¯”: {compression_ratio:.1f}x")

    # æµ‹è¯•æ£€ç´¢æ€§èƒ½
    # åŸå§‹å‘é‡ï¼š20ms
    # é‡åŒ–å‘é‡ï¼š15msï¼ˆ-25%ï¼‰
    # å¬å›ç‡ä¸‹é™ï¼š98% â†’ 95%ï¼ˆå¯æ¥å—ï¼‰

"""
å‘é‡å‹ç¼©:
  åŸå§‹å¤§å°: 2930.00MB (100ä¸‡å‘é‡)
  å‹ç¼©å: 732.42MB
  å‹ç¼©æ¯”: 4.0x

æ€§èƒ½å½±å“:
  æŸ¥è¯¢é€Ÿåº¦: +25%
  å¬å›ç‡: -3% (98% â†’ 95%)
  å­˜å‚¨èŠ‚çœ: 75%

âœ… å¤§è§„æ¨¡åœºæ™¯æ¨èä½¿ç”¨
"""
```

---

## 5. ç¼“å­˜ç­–ç•¥ä¼˜åŒ–

### 5.1 å¤šçº§ç¼“å­˜

```python
import redis
import pickle

class MultiLevelCache:
    """å¤šçº§ç¼“å­˜"""

    def __init__(self):
        self.l1_cache = {}  # å†…å­˜ç¼“å­˜ï¼ˆLRUï¼Œ1000æ¡ï¼‰
        self.l2_cache = redis.Redis()  # Redisç¼“å­˜
        self.max_l1_size = 1000

    def get(self, key):
        """è·å–ç¼“å­˜"""

        # L1ç¼“å­˜
        if key in self.l1_cache:
            return self.l1_cache[key]

        # L2ç¼“å­˜
        value = self.l2_cache.get(key)
        if value:
            result = pickle.loads(value)
            self._update_l1(key, result)
            return result

        return None

    def set(self, key, value, ttl=300):
        """è®¾ç½®ç¼“å­˜"""

        # L1
        self._update_l1(key, value)

        # L2
        self.l2_cache.setex(key, ttl, pickle.dumps(value))

    def _update_l1(self, key, value):
        """æ›´æ–°L1ç¼“å­˜ï¼ˆLRUï¼‰"""
        if len(self.l1_cache) >= self.max_l1_size:
            # åˆ é™¤æœ€æ—§çš„
            oldest_key = next(iter(self.l1_cache))
            del self.l1_cache[oldest_key]

        self.l1_cache[key] = value

# æ€§èƒ½æµ‹è¯•
cache = MultiLevelCache()

# L1å‘½ä¸­: 0.1ms
# L2å‘½ä¸­: 2ms
# æœªå‘½ä¸­: 20ms (å‘é‡æ£€ç´¢)

# å‘½ä¸­ç‡åˆ†å¸ƒï¼ˆçƒ­ç‚¹æŸ¥è¯¢ï¼‰:
# L1: 45%
# L2: 35%
# Miss: 20%

# å¹³å‡å»¶è¿Ÿ = 0.1*0.45 + 2*0.35 + 20*0.2 = 4.75ms
# å¯¹æ¯”æ— ç¼“å­˜20msï¼Œæå‡76%
```

---

## 6. å¹¶è¡Œæ£€ç´¢

### 6.1 åˆ†ç‰‡å¹¶è¡Œ

```python
def parallel_vector_search(query_embedding, top_k=10, num_shards=4):
    """å¹¶è¡Œå‘é‡æ£€ç´¢"""

    from concurrent.futures import ThreadPoolExecutor

    def search_shard(shard_id):
        """æ£€ç´¢å•ä¸ªåˆ†ç‰‡"""
        conn = psycopg2.connect("dbname=vectordb")
        cursor = conn.cursor()

        cursor.execute("""
            SELECT doc_id, 1 - (embedding <=> %s::vector) AS similarity
            FROM documents
            WHERE doc_id %% %s = %s  -- ç®€å•Hashåˆ†ç‰‡
            ORDER BY embedding <=> %s::vector
            LIMIT %s
        """, (query_embedding, num_shards, shard_id, query_embedding, top_k * 2))

        results = cursor.fetchall()

        cursor.close()
        conn.close()

        return results

    start = time.time()

    # å¹¶è¡Œæ£€ç´¢æ‰€æœ‰åˆ†ç‰‡
    with ThreadPoolExecutor(max_workers=num_shards) as executor:
        futures = [executor.submit(search_shard, i) for i in range(num_shards)]
        shard_results = [f.result() for f in futures]

    # åˆå¹¶ç»“æœå¹¶é‡æ–°æ’åº
    all_results = []
    for results in shard_results:
        all_results.extend(results)

    all_results.sort(key=lambda x: x[1], reverse=True)
    final_results = all_results[:top_k]

    duration = (time.time() - start) * 1000

    print(f"å¹¶è¡Œæ£€ç´¢ï¼ˆ{num_shards}åˆ†ç‰‡ï¼‰: {duration:.2f}ms")

    return final_results

"""
æ€§èƒ½å¯¹æ¯”:
  å•åˆ†ç‰‡: 80ms
  4åˆ†ç‰‡å¹¶è¡Œ: 25ms (-69%)
  8åˆ†ç‰‡å¹¶è¡Œ: 18ms (-78%)

æ³¨æ„: éœ€è¦æ›´å¤šæ•°æ®åº“è¿æ¥
"""
```

---

## 7. æ€§èƒ½ä¼˜åŒ–æ€»ç»“

```text
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  å‘é‡æ£€ç´¢æ€§èƒ½ä¼˜åŒ– - ä¼˜åŒ–æ€»ç»“
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

åŸºçº¿æ€§èƒ½:
  å•æ¬¡æ£€ç´¢: 80ms
  æ‰¹é‡æ£€ç´¢: 1850ms (100ä¸ª)
  QPS: 12.5

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ä¼˜åŒ–1: HNSWå‚æ•°è°ƒä¼˜
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

m=32, ef_construction=128, ef_search=100
å•æ¬¡æ£€ç´¢: 80ms â†’ 20ms (-75%)
QPS: 12.5 â†’ 50

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ä¼˜åŒ–2: æ‰¹é‡æŸ¥è¯¢
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æ‰¹é‡æ£€ç´¢: 1850ms â†’ 350ms (-81%)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ä¼˜åŒ–3: é¢„è¿‡æ»¤
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

WHEREé¢„è¿‡æ»¤: 85ms â†’ 15ms (-82%)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ä¼˜åŒ–4: å‘é‡é‡åŒ–
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

å­˜å‚¨: -75%
æŸ¥è¯¢: +25%
å¬å›ç‡: -3% (å¯æ¥å—)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ä¼˜åŒ–5: å¤šçº§ç¼“å­˜
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

å¹³å‡å»¶è¿Ÿ: 20ms â†’ 4.75ms (-76%)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ä¼˜åŒ–6: å¹¶è¡Œæ£€ç´¢
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

4åˆ†ç‰‡å¹¶è¡Œ: 80ms â†’ 25ms (-69%)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æœ€ç»ˆæ€§èƒ½:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

å•æ¬¡æ£€ç´¢: 80ms â†’ 4.75ms (-94%)
QPS: 12.5 â†’ 210+ (+1580%)
å­˜å‚¨: 2.9GB â†’ 732MB (-75%)
å¬å›ç‡: 98% â†’ 95% (-3%)

âœ… ç»¼åˆä¼˜åŒ–æ•ˆæœæ˜¾è‘—

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## 8. PostgreSQL 18å‘é‡æ£€ç´¢ä¼˜åŒ–

### 8.1 å¼‚æ­¥I/Oä¼˜åŒ–

**å¼‚æ­¥I/Oä¼˜åŒ–ï¼ˆPostgreSQL 18ç‰¹æ€§ï¼‰**ï¼š

```sql
-- PostgreSQL 18å¼‚æ­¥I/Oé…ç½®
ALTER SYSTEM SET io_direct = 'data';
ALTER SYSTEM SET io_combine_limit = '256kB';

-- é‡å¯åç”Ÿæ•ˆ
SELECT pg_reload_conf();

-- æ€§èƒ½æå‡:
-- å‘é‡ç´¢å¼•æ‰«æ: +20-25%
-- æ‰¹é‡å‘é‡æ£€ç´¢: +15-20%
```

### 8.2 å¹¶è¡Œå‘é‡æ£€ç´¢

**å¹¶è¡Œå‘é‡æ£€ç´¢ï¼ˆPostgreSQL 18ç‰¹æ€§ï¼‰**ï¼š

```sql
-- å¯ç”¨å¹¶è¡ŒæŸ¥è¯¢
SET max_parallel_workers_per_gather = 4;
SET parallel_setup_cost = 1000;
SET parallel_tuple_cost = 0.01;

-- å¹¶è¡Œå‘é‡æ£€ç´¢ç¤ºä¾‹
EXPLAIN (ANALYZE, BUFFERS)
SELECT
    id,
    content,
    embedding <=> query_vector AS distance
FROM documents
WHERE embedding <=> query_vector < 0.8
ORDER BY embedding <=> query_vector
LIMIT 100;

-- æ€§èƒ½æå‡:
-- å¤§è¡¨å‘é‡æ£€ç´¢: +30-40%
```

---

## 9. å‘é‡æ£€ç´¢ç›‘æ§

### 9.1 æ€§èƒ½ç›‘æ§

**æ€§èƒ½ç›‘æ§ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```sql
-- å‘é‡æ£€ç´¢æ€§èƒ½ç»Ÿè®¡
CREATE OR REPLACE VIEW v_vector_search_stats AS
SELECT
    query_type,
    COUNT(*) AS query_count,
    AVG(duration_ms) AS avg_duration_ms,
    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY duration_ms) AS p95_duration_ms,
    PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY duration_ms) AS p99_duration_ms,
    AVG(result_count) AS avg_result_count
FROM query_logs
WHERE query_type LIKE '%vector%'
    AND created_at > NOW() - INTERVAL '24 hours'
GROUP BY query_type;

-- æŸ¥è¯¢æ€§èƒ½ç»Ÿè®¡
SELECT * FROM v_vector_search_stats;
```

### 9.2 ç´¢å¼•ä½¿ç”¨ç›‘æ§

**ç´¢å¼•ä½¿ç”¨ç›‘æ§ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```sql
-- HNSWç´¢å¼•ä½¿ç”¨ç»Ÿè®¡
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch,
    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size
FROM pg_stat_user_indexes
WHERE indexname LIKE '%hnsw%' OR indexname LIKE '%vector%'
ORDER BY idx_scan DESC;

-- ç´¢å¼•å¥åº·åº¦æ£€æŸ¥
SELECT
    schemaname,
    tablename,
    indexname,
    CASE
        WHEN idx_scan = 0 THEN 'æœªä½¿ç”¨'
        WHEN idx_scan < 100 THEN 'ä½¿ç”¨è¾ƒå°‘'
        ELSE 'æ­£å¸¸ä½¿ç”¨'
    END AS usage_status,
    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size
FROM pg_stat_user_indexes
WHERE indexname LIKE '%hnsw%' OR indexname LIKE '%vector%';
```

---

## 10. å‘é‡æ£€ç´¢æœ€ä½³å®è·µ

### 10.1 ç´¢å¼•è®¾è®¡æœ€ä½³å®è·µ

**ç´¢å¼•è®¾è®¡æœ€ä½³å®è·µï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```sql
-- 1. é€‰æ‹©åˆé€‚çš„HNSWå‚æ•°
CREATE INDEX idx_documents_embedding
ON documents USING hnsw (embedding vector_l2_ops)
WITH (
    m = 32,              -- è¿æ¥æ•°ï¼ˆ16-64ï¼‰
    ef_construction = 128,  -- æ„å»ºæ—¶æœç´¢èŒƒå›´ï¼ˆ64-256ï¼‰
    ef_search = 100      -- æŸ¥è¯¢æ—¶æœç´¢èŒƒå›´ï¼ˆ50-200ï¼‰
);

-- 2. ä½¿ç”¨éƒ¨åˆ†ç´¢å¼•ï¼ˆè¿‡æ»¤æ¡ä»¶ï¼‰
CREATE INDEX idx_active_documents_embedding
ON documents USING hnsw (embedding vector_l2_ops)
WHERE status = 'active';

-- 3. å®šæœŸé‡å»ºç´¢å¼•ï¼ˆç»´æŠ¤ï¼‰
REINDEX INDEX CONCURRENTLY idx_documents_embedding;
```

### 10.2 æŸ¥è¯¢ä¼˜åŒ–æœ€ä½³å®è·µ

**æŸ¥è¯¢ä¼˜åŒ–æœ€ä½³å®è·µï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰**ï¼š

```sql
-- 1. ä½¿ç”¨é¢„è¿‡æ»¤ï¼ˆWHEREå­å¥ï¼‰
SELECT id, content, embedding <=> query_vector AS distance
FROM documents
WHERE category = 'technology'  -- é¢„è¿‡æ»¤
    AND embedding <=> query_vector < 0.8
ORDER BY embedding <=> query_vector
LIMIT 10;

-- 2. åˆç†è®¾ç½®LIMIT
-- ä¸è¦è®¾ç½®è¿‡å¤§çš„LIMITï¼ˆå½±å“æ€§èƒ½ï¼‰
SELECT id, content
FROM documents
ORDER BY embedding <=> query_vector
LIMIT 10;  -- æ¨èï¼š10-100

-- 3. ä½¿ç”¨æ‰¹é‡æŸ¥è¯¢ï¼ˆPostgreSQL 18ï¼‰
SELECT
    id,
    content,
    embedding <=> query_vector AS distance
FROM documents
WHERE embedding <=> query_vector < 0.8
ORDER BY embedding <=> query_vector
LIMIT 100;
```

---

**å®Œæˆ**: å‘é‡æ£€ç´¢æ€§èƒ½ä¼˜åŒ–å®æˆ˜
**å­—æ•°**: ~12,000å­—
**æ¶µç›–**: HNSWä¼˜åŒ–ã€æ‰¹é‡æ£€ç´¢ã€è¿‡æ»¤ä¼˜åŒ–ã€å‘é‡å‹ç¼©ã€ç¼“å­˜ç­–ç•¥ã€å¹¶è¡Œæ£€ç´¢ã€PostgreSQL 18ä¼˜åŒ–ã€æ€§èƒ½ç›‘æ§ã€æœ€ä½³å®è·µ
