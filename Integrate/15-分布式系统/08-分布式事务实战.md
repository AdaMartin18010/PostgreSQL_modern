---

> **ğŸ“‹ æ–‡æ¡£æ¥æº**: `docs\04-Distributed\08-åˆ†å¸ƒå¼äº‹åŠ¡å®æˆ˜.md`
> **ğŸ“… å¤åˆ¶æ—¥æœŸ**: 2025-12-22
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºå¤åˆ¶ç‰ˆæœ¬ï¼ŒåŸæ–‡ä»¶ä¿æŒä¸å˜

---

# PostgreSQLåˆ†å¸ƒå¼äº‹åŠ¡å®æˆ˜

## 1. ä¸¤é˜¶æ®µæäº¤ï¼ˆ2PCï¼‰

### 1.1 åŸºç¡€å®ç°

```sql
-- å‡†å¤‡é˜¶æ®µ
-- èŠ‚ç‚¹A
BEGIN;
UPDATE accounts SET balance = balance - 100 WHERE account_id = 1;
PREPARE TRANSACTION 'tx_001_node_a';

-- èŠ‚ç‚¹B
BEGIN;
UPDATE accounts SET balance = balance + 100 WHERE account_id = 2;
PREPARE TRANSACTION 'tx_001_node_b';

-- æäº¤é˜¶æ®µ
-- åè°ƒè€…ç¡®è®¤æ‰€æœ‰èŠ‚ç‚¹å‡†å¤‡æˆåŠŸå
COMMIT PREPARED 'tx_001_node_a';  -- èŠ‚ç‚¹A
COMMIT PREPARED 'tx_001_node_b';  -- èŠ‚ç‚¹B

-- æˆ–å›æ»š
ROLLBACK PREPARED 'tx_001_node_a';
ROLLBACK PREPARED 'tx_001_node_b';
```

### 1.2 Pythonå®ç°

```python
import psycopg2
import uuid

class TwoPhaseCommitCoordinator:
    """2PCåè°ƒå™¨"""

    def __init__(self, connections: dict):
        self.connections = connections  # {'node_a': conn_a, 'node_b': conn_b}

    def distributed_transaction(self, operations: dict):
        """
        æ‰§è¡Œåˆ†å¸ƒå¼äº‹åŠ¡

        Args:
            operations: {'node_a': 'UPDATE ...', 'node_b': 'UPDATE ...'}
        """

        tx_id = str(uuid.uuid4())
        prepared_nodes = []

        try:
            # é˜¶æ®µ1: å‡†å¤‡
            for node_name, sql in operations.items():
                conn = self.connections[node_name]
                cursor = conn.cursor()

                cursor.execute("BEGIN;")
                cursor.execute(sql)
                cursor.execute(f"PREPARE TRANSACTION '{tx_id}_{node_name}';")

                prepared_nodes.append(node_name)
                print(f"âœ“ {node_name} å‡†å¤‡å®Œæˆ")

            # é˜¶æ®µ2: æäº¤
            for node_name in prepared_nodes:
                conn = self.connections[node_name]
                cursor = conn.cursor()
                cursor.execute(f"COMMIT PREPARED '{tx_id}_{node_name}';")
                print(f"âœ“ {node_name} æäº¤å®Œæˆ")

            return True

        except Exception as e:
            # å›æ»šæ‰€æœ‰å·²å‡†å¤‡çš„èŠ‚ç‚¹
            print(f"é”™è¯¯: {e}, å›æ»šäº‹åŠ¡")

            for node_name in prepared_nodes:
                try:
                    conn = self.connections[node_name]
                    cursor = conn.cursor()
                    cursor.execute(f"ROLLBACK PREPARED '{tx_id}_{node_name}';")
                    print(f"âœ“ {node_name} å›æ»šå®Œæˆ")
                except Exception as rollback_error:
                    print(f"âœ— {node_name} å›æ»šå¤±è´¥: {rollback_error}")

            return False

# ä½¿ç”¨
coordinator = TwoPhaseCommitCoordinator({
    'node_a': psycopg2.connect("host=node-a ..."),
    'node_b': psycopg2.connect("host=node-b ...")
})

success = coordinator.distributed_transaction({
    'node_a': "UPDATE accounts SET balance = balance - 100 WHERE account_id = 1",
    'node_b': "UPDATE accounts SET balance = balance + 100 WHERE account_id = 2"
})
```

### 1.3 ç›‘æ§preparedäº‹åŠ¡

```sql
-- æŸ¥çœ‹preparedäº‹åŠ¡
SELECT
    gid,
    prepared,
    owner,
    database,
    transaction AS xid
FROM pg_prepared_xacts;

-- æ¸…ç†åƒµå°¸preparedäº‹åŠ¡
-- å¦‚æœåè°ƒè€…å´©æºƒï¼Œéœ€è¦æ‰‹åŠ¨å¤„ç†
ROLLBACK PREPARED 'orphan_tx_id';
```

---

## 2. Sagaæ¨¡å¼

### 2.1 è¡¥å¿äº‹åŠ¡

```sql
-- è®¢å•å¤„ç†Saga
-- æ­¥éª¤1: åˆ›å»ºè®¢å•
INSERT INTO orders (user_id, amount) VALUES (123, 999.99) RETURNING order_id;
-- è¡¥å¿: DELETE FROM orders WHERE order_id = ?

-- æ­¥éª¤2: æ‰£å‡åº“å­˜
UPDATE inventory SET stock = stock - 1 WHERE product_id = 456;
-- è¡¥å¿: UPDATE inventory SET stock = stock + 1 WHERE product_id = 456

-- æ­¥éª¤3: æ‰£æ¬¾
UPDATE accounts SET balance = balance - 999.99 WHERE user_id = 123;
-- è¡¥å¿: UPDATE accounts SET balance = balance + 999.99 WHERE user_id = 123

-- å¦‚æœä»»ä¸€æ­¥éª¤å¤±è´¥ï¼Œæ‰§è¡Œæ‰€æœ‰å·²å®Œæˆæ­¥éª¤çš„è¡¥å¿æ“ä½œ
```

### 2.2 Sagaåè°ƒå™¨

```python
class SagaCoordinator:
    """Sagaåè°ƒå™¨"""

    def __init__(self, conn):
        self.conn = conn
        self.steps = []

    def add_step(self, forward_func, compensate_func):
        """æ·»åŠ Sagaæ­¥éª¤"""
        self.steps.append({
            'forward': forward_func,
            'compensate': compensate_func
        })

    def execute(self):
        """æ‰§è¡ŒSaga"""

        completed_steps = []

        try:
            # æ­£å‘æ‰§è¡Œ
            for i, step in enumerate(self.steps):
                print(f"æ‰§è¡Œæ­¥éª¤ {i+1}/{len(self.steps)}")
                step['forward'](self.conn)
                completed_steps.append(step)

            return True

        except Exception as e:
            print(f"æ­¥éª¤å¤±è´¥: {e}")
            print(f"å¼€å§‹è¡¥å¿...")

            # åå‘è¡¥å¿
            for step in reversed(completed_steps):
                try:
                    step['compensate'](self.conn)
                    print(f"âœ“ è¡¥å¿æˆåŠŸ")
                except Exception as comp_error:
                    print(f"âœ— è¡¥å¿å¤±è´¥: {comp_error}")

            return False

# ä½¿ç”¨
saga = SagaCoordinator(conn)

# å®šä¹‰æ­¥éª¤
saga.add_step(
    forward=lambda c: c.cursor().execute("INSERT INTO orders ..."),
    compensate=lambda c: c.cursor().execute("DELETE FROM orders WHERE order_id = ?")
)

saga.add_step(
    forward=lambda c: c.cursor().execute("UPDATE inventory SET stock = stock - 1 ..."),
    compensate=lambda c: c.cursor().execute("UPDATE inventory SET stock = stock + 1 ...")
)

# æ‰§è¡Œ
success = saga.execute()
```

---

## 3. æœ€ç»ˆä¸€è‡´æ€§

### 3.1 å¼‚æ­¥æ¶ˆæ¯é˜Ÿåˆ—

```sql
-- å¤–å‘äº‹ä»¶è¡¨
CREATE TABLE outbox_events (
    event_id BIGSERIAL PRIMARY KEY,
    aggregate_id BIGINT,
    event_type VARCHAR(100),
    event_data JSONB,
    status VARCHAR(20) DEFAULT 'pending',
    created_at TIMESTAMPTZ DEFAULT now(),
    processed_at TIMESTAMPTZ
);

CREATE INDEX ON outbox_events (status, created_at) WHERE status = 'pending';

-- æœ¬åœ°äº‹åŠ¡+å‘å¸ƒäº‹ä»¶
BEGIN;

-- ä¸šåŠ¡æ“ä½œ
INSERT INTO orders (user_id, amount) VALUES (123, 999.99) RETURNING order_id;

-- å‘å¸ƒäº‹ä»¶
INSERT INTO outbox_events (aggregate_id, event_type, event_data)
VALUES (currval('orders_order_id_seq'), 'OrderCreated', '{"amount": 999.99}');

COMMIT;

-- åå°Workerå‘é€äº‹ä»¶åˆ°æ¶ˆæ¯é˜Ÿåˆ—
SELECT * FROM outbox_events WHERE status = 'pending' ORDER BY created_at FOR UPDATE SKIP LOCKED LIMIT 100;
-- å‘é€åˆ°Kafka/RabbitMQ
-- UPDATE outbox_events SET status = 'sent', processed_at = now() WHERE event_id = ?
```

---

## 4. FDWï¼ˆå¤–éƒ¨æ•°æ®åŒ…è£…å™¨ï¼‰

### 4.1 postgres_fdw

```sql
-- å®‰è£…
CREATE EXTENSION postgres_fdw;

-- åˆ›å»ºå¤–éƒ¨æœåŠ¡å™¨
CREATE SERVER remote_db
FOREIGN DATA WRAPPER postgres_fdw
OPTIONS (host 'remote-host', port '5432', dbname 'remotedb');

-- åˆ›å»ºç”¨æˆ·æ˜ å°„
CREATE USER MAPPING FOR postgres
SERVER remote_db
OPTIONS (user 'remote_user', password 'password');

-- å¯¼å…¥å¤–éƒ¨è¡¨
IMPORT FOREIGN SCHEMA public
FROM SERVER remote_db
INTO public;

-- æˆ–å•ç‹¬åˆ›å»º
CREATE FOREIGN TABLE remote_users (
    user_id INT,
    username VARCHAR(100),
    email VARCHAR(255)
)
SERVER remote_db
OPTIONS (schema_name 'public', table_name 'users');

-- è·¨åº“æŸ¥è¯¢
SELECT
    local.order_id,
    remote.username
FROM orders local
JOIN remote_users remote ON local.user_id = remote.user_id;

-- è·¨åº“å†™å…¥
INSERT INTO remote_users (username, email) VALUES ('alice', 'alice@example.com');
```

### 4.2 æ€§èƒ½ä¼˜åŒ–

```sql
-- å¯ç”¨è¿œç¨‹èšåˆä¸‹æ¨
ALTER SERVER remote_db OPTIONS (ADD extensions 'postgres_fdw');

-- å¯ç”¨è¿œç¨‹æ’åº
ALTER SERVER remote_db OPTIONS (ADD use_remote_estimate 'true');

-- æ‰¹é‡fetch
ALTER FOREIGN TABLE remote_users OPTIONS (ADD fetch_size '10000');
```

---

## 5. å¹‚ç­‰æ€§è®¾è®¡

### 5.1 å¹‚ç­‰æ“ä½œ

```sql
-- ä½¿ç”¨å”¯ä¸€çº¦æŸä¿è¯å¹‚ç­‰
CREATE TABLE payments (
    payment_id BIGSERIAL PRIMARY KEY,
    idempotency_key VARCHAR(100) UNIQUE NOT NULL,
    order_id BIGINT,
    amount NUMERIC,
    status VARCHAR(20),
    created_at TIMESTAMPTZ DEFAULT now()
);

-- å¹‚ç­‰æ’å…¥
INSERT INTO payments (idempotency_key, order_id, amount, status)
VALUES ('pay_abc123', 789, 99.99, 'success')
ON CONFLICT (idempotency_key) DO NOTHING;

-- å®¢æˆ·ç«¯é‡è¯•å®‰å…¨
-- ç›¸åŒidempotency_keyå¤šæ¬¡è°ƒç”¨ï¼Œåªä¼šæ’å…¥ä¸€æ¬¡
```

---

## 6. åˆ†å¸ƒå¼é”

### 6.1 PostgreSQL advisory lock

```sql
-- è·å–é”
SELECT pg_try_advisory_lock(12345);  -- trueè¡¨ç¤ºè·å–æˆåŠŸ

-- ä¸šåŠ¡é€»è¾‘
UPDATE limited_resource SET ...;

-- é‡Šæ”¾é”
SELECT pg_advisory_unlock(12345);

-- ä¼šè¯çº§é”ï¼ˆè¿æ¥æ–­å¼€è‡ªåŠ¨é‡Šæ”¾ï¼‰
SELECT pg_advisory_lock(12345);  -- é˜»å¡ç›´åˆ°è·å–

-- äº‹åŠ¡çº§é”
SELECT pg_try_advisory_xact_lock(12345);  -- äº‹åŠ¡ç»“æŸè‡ªåŠ¨é‡Šæ”¾
```

### 6.2 åˆ†å¸ƒå¼é”å®ç°

```python
class DistributedLock:
    """åŸºäºPostgreSQLçš„åˆ†å¸ƒå¼é”"""

    def __init__(self, conn, lock_id: int, timeout: int = 30):
        self.conn = conn
        self.lock_id = lock_id
        self.timeout = timeout
        self.acquired = False

    def __enter__(self):
        cursor = self.conn.cursor()

        # å°è¯•è·å–é”ï¼ˆå¸¦è¶…æ—¶ï¼‰
        cursor.execute(f"SET lock_timeout = '{self.timeout}s';")
        cursor.execute("SELECT pg_advisory_lock(%s);", (self.lock_id,))

        self.acquired = True
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.acquired:
            cursor = self.conn.cursor()
            cursor.execute("SELECT pg_advisory_unlock(%s);", (self.lock_id,))

# ä½¿ç”¨
with DistributedLock(conn, 12345):
    # ä¸´ç•ŒåŒºä»£ç 
    cursor.execute("UPDATE global_counter SET value = value + 1;")
    conn.commit()
```

---

## 7. å®¹é‡è§„åˆ’

### 7.1 æ•°æ®å¢é•¿é¢„æµ‹

```sql
-- æŸ¥è¯¢å†å²å¢é•¿è¶‹åŠ¿
WITH daily_growth AS (
    SELECT
        DATE(created_at) AS date,
        COUNT(*) AS new_records,
        SUM(COUNT(*)) OVER (ORDER BY DATE(created_at)) AS cumulative
    FROM large_table
    WHERE created_at > CURRENT_DATE - INTERVAL '90 days'
    GROUP BY DATE(created_at)
),
growth_stats AS (
    SELECT
        AVG(new_records) AS avg_daily,
        STDDEV(new_records) AS stddev_daily,
        MAX(new_records) AS max_daily
    FROM daily_growth
)
SELECT
    avg_daily,
    avg_daily * 30 AS projected_monthly,
    avg_daily * 365 AS projected_yearly,
    max_daily AS peak_daily
FROM growth_stats;

-- å­˜å‚¨é¢„æµ‹
WITH current_size AS (
    SELECT pg_total_relation_size('large_table') AS bytes
),
growth AS (
    SELECT 125000 AS daily_rows  -- ä»ä¸Šé¢æŸ¥è¯¢è·å–
)
SELECT
    pg_size_pretty(bytes) AS current_size,
    pg_size_pretty(bytes + (daily_rows * 30 * avg_row_size)) AS size_1_month,
    pg_size_pretty(bytes + (daily_rows * 365 * avg_row_size)) AS size_1_year
FROM current_size, growth,
     (SELECT pg_relation_size('large_table') / NULLIF(COUNT(*), 0) AS avg_row_size
      FROM large_table) row_size;
```

---

## 8. è·¨æ•°æ®åº“æŸ¥è¯¢

### 8.1 dblink

```sql
CREATE EXTENSION dblink;

-- è·¨åº“æŸ¥è¯¢
SELECT * FROM dblink(
    'host=remote-server port=5432 dbname=remotedb user=postgres password=xxx',
    'SELECT user_id, username FROM users'
) AS remote_users(user_id INT, username TEXT);

-- è·¨åº“JOIN
SELECT
    local.order_id,
    remote.username
FROM orders local
JOIN dblink(
    'host=remote-server ...',
    'SELECT user_id, username FROM users'
) AS remote(user_id INT, username TEXT)
ON local.user_id = remote.user_id;

-- è·¨åº“å†™å…¥
SELECT * FROM dblink_exec(
    'host=remote-server ...',
    'INSERT INTO users (username) VALUES (''alice'')'
);
```

---

## 9. åˆ†ç‰‡ç­–ç•¥

### 9.1 åº”ç”¨å±‚åˆ†ç‰‡

```python
class ShardRouter:
    """åˆ†ç‰‡è·¯ç”±å™¨"""

    def __init__(self, shard_connections: list):
        self.shards = shard_connections
        self.shard_count = len(shard_connections)

    def get_shard(self, user_id: int):
        """æ ¹æ®user_idè·¯ç”±åˆ°åˆ†ç‰‡"""
        shard_id = user_id % self.shard_count
        return self.shards[shard_id]

    def execute(self, user_id: int, sql: str, params=None):
        """åœ¨æ­£ç¡®çš„åˆ†ç‰‡æ‰§è¡Œ"""
        conn = self.get_shard(user_id)
        cursor = conn.cursor()
        cursor.execute(sql, params)
        return cursor.fetchall()

    def broadcast(self, sql: str):
        """å¹¿æ’­åˆ°æ‰€æœ‰åˆ†ç‰‡"""
        results = []
        for conn in self.shards:
            cursor = conn.cursor()
            cursor.execute(sql)
            results.extend(cursor.fetchall())
        return results

# ä½¿ç”¨
router = ShardRouter([
    psycopg2.connect("host=shard-0 ..."),
    psycopg2.connect("host=shard-1 ..."),
    psycopg2.connect("host=shard-2 ..."),
    psycopg2.connect("host=shard-3 ...")
])

# å•åˆ†ç‰‡æŸ¥è¯¢
result = router.execute(123, "SELECT * FROM users WHERE user_id = %s", (123,))

# å¹¿æ’­æŸ¥è¯¢ï¼ˆèšåˆï¼‰
results = router.broadcast("SELECT COUNT(*) FROM users")
total = sum(r[0] for r in results)
```

---

## 10. æœ€ç»ˆä¸€è‡´æ€§éªŒè¯

### 10.1 æ•°æ®å¯¹è´¦

```sql
-- å¯¹è´¦ä»»åŠ¡è¡¨
CREATE TABLE reconciliation_tasks (
    task_id BIGSERIAL PRIMARY KEY,
    source_system VARCHAR(50),
    target_system VARCHAR(50),
    entity_type VARCHAR(50),
    check_date DATE,
    status VARCHAR(20) DEFAULT 'pending',
    inconsistencies JSONB,
    created_at TIMESTAMPTZ DEFAULT now()
);

-- å¯¹è´¦å‡½æ•°
CREATE OR REPLACE FUNCTION reconcile_accounts(check_date DATE)
RETURNS TABLE (
    account_id BIGINT,
    source_balance NUMERIC,
    computed_balance NUMERIC,
    difference NUMERIC
) AS $$
BEGIN
    RETURN QUERY
    WITH computed AS (
        SELECT
            account_id,
            COALESCE(SUM(
                CASE
                    WHEN transaction_type = 'credit' THEN amount
                    WHEN transaction_type = 'debit' THEN -amount
                END
            ), 0) AS balance
        FROM transactions
        WHERE DATE(created_at) <= check_date
        GROUP BY account_id
    )
    SELECT
        a.account_id,
        a.balance AS source_balance,
        c.balance AS computed_balance,
        a.balance - c.balance AS difference
    FROM accounts a
    LEFT JOIN computed c ON a.account_id = c.account_id
    WHERE ABS(a.balance - COALESCE(c.balance, 0)) > 0.01;
END;
$$ LANGUAGE plpgsql;

-- æ‰§è¡Œå¯¹è´¦
SELECT * FROM reconcile_accounts(CURRENT_DATE);
```

---

**å®Œæˆ**: PostgreSQLåˆ†å¸ƒå¼äº‹åŠ¡å®æˆ˜
**å­—æ•°**: ~10,000å­—
**æ¶µç›–**: 2PCã€Sagaã€æœ€ç»ˆä¸€è‡´æ€§ã€FDWã€åˆ†ç‰‡ã€åˆ†å¸ƒå¼é”ã€å¯¹è´¦
