---
> **ğŸ“‹ æ–‡æ¡£æ¥æº**: æ–°å¢æ·±åŒ–æ–‡æ¡£
> **ğŸ“… åˆ›å»ºæ—¥æœŸ**: 2025-01
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£èšç„¦æ•°æ®æµåˆ†æå’Œå®æ—¶å¤„ç†æŠ€æœ¯æ ˆ

---

# æ•°æ®æµåˆ†æå¤„ç†å®Œæ•´å®æˆ˜æŒ‡å—

## å…ƒæ•°æ®

- **æ–‡æ¡£ç‰ˆæœ¬**: v2.0
- **åˆ›å»ºæ—¥æœŸ**: 2025-01
- **æŠ€æœ¯æ ˆ**: PostgreSQL 17+/18+ | TimescaleDB | Apache Kafka | Apache Flink | é€»è¾‘å¤åˆ¶
- **éš¾åº¦çº§åˆ«**: â­â­â­â­â­ (ä¸“å®¶çº§)
- **é¢„è®¡é˜…è¯»**: 180åˆ†é’Ÿ
- **å‰ç½®è¦æ±‚**: ç†Ÿæ‚‰PostgreSQLåŸºç¡€ã€æµå¤„ç†åŸºç¡€ã€å®æ—¶ç³»ç»Ÿè®¾è®¡

---

## ğŸ“‹ å®Œæ•´ç›®å½•

- [æ•°æ®æµåˆ†æå¤„ç†å®Œæ•´å®æˆ˜æŒ‡å—](#æ•°æ®æµåˆ†æå¤„ç†å®Œæ•´å®æˆ˜æŒ‡å—)
  - [å…ƒæ•°æ®](#å…ƒæ•°æ®)
  - [ğŸ“‹ å®Œæ•´ç›®å½•](#-å®Œæ•´ç›®å½•)
  - [1. æ•°æ®æµåˆ†ææ¦‚è¿°](#1-æ•°æ®æµåˆ†ææ¦‚è¿°)
    - [1.1 æ•°æ®æµåˆ†ææ¦‚å¿µ](#11-æ•°æ®æµåˆ†ææ¦‚å¿µ)
      - [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
      - [æ•°æ®æµåˆ†æç‰¹ç‚¹](#æ•°æ®æµåˆ†æç‰¹ç‚¹)
    - [1.2 æµå¤„ç†æ¶æ„æ¨¡å¼](#12-æµå¤„ç†æ¶æ„æ¨¡å¼)
      - [æ¨¡å¼å¯¹æ¯”](#æ¨¡å¼å¯¹æ¯”)
    - [1.3 æŠ€æœ¯é€‰å‹](#13-æŠ€æœ¯é€‰å‹)
      - [æŠ€æœ¯æ ˆå¯¹æ¯”](#æŠ€æœ¯æ ˆå¯¹æ¯”)
  - [2. æµå¼SQLåˆ†æ](#2-æµå¼sqlåˆ†æ)
    - [2.1 æµå¼çª—å£æ“ä½œ](#21-æµå¼çª—å£æ“ä½œ)
      - [æ—¶é—´çª—å£ç±»å‹](#æ—¶é—´çª—å£ç±»å‹)
    - [2.2 æµå¼èšåˆå‡½æ•°](#22-æµå¼èšåˆå‡½æ•°)
      - [å®æ—¶èšåˆå®ç°](#å®æ—¶èšåˆå®ç°)
    - [2.3 æµå¼JOINæ“ä½œ](#23-æµå¼joinæ“ä½œ)
      - [æµè¡¨JOIN](#æµè¡¨join)
  - [3. å®æ—¶æ•°æ®åˆ†æ](#3-å®æ—¶æ•°æ®åˆ†æ)
    - [3.1 å®æ—¶æŒ‡æ ‡è®¡ç®—](#31-å®æ—¶æŒ‡æ ‡è®¡ç®—)
      - [å®æ—¶KPIè®¡ç®—](#å®æ—¶kpiè®¡ç®—)
    - [3.2 å®æ—¶æŠ¥è¡¨ç”Ÿæˆ](#32-å®æ—¶æŠ¥è¡¨ç”Ÿæˆ)
      - [å®æ—¶æŠ¥è¡¨è§†å›¾](#å®æ—¶æŠ¥è¡¨è§†å›¾)
  - [4. æµå¼æœºå™¨å­¦ä¹ ](#4-æµå¼æœºå™¨å­¦ä¹ )
    - [4.1 æµå¼ç‰¹å¾æå–](#41-æµå¼ç‰¹å¾æå–)
      - [å®æ—¶ç‰¹å¾å·¥ç¨‹](#å®æ—¶ç‰¹å¾å·¥ç¨‹)
    - [4.2 æµå¼æ¨¡å‹é¢„æµ‹](#42-æµå¼æ¨¡å‹é¢„æµ‹)
      - [å®æ—¶é¢„æµ‹æœåŠ¡](#å®æ—¶é¢„æµ‹æœåŠ¡)
    - [4.3 åœ¨çº¿å­¦ä¹ ç®—æ³•](#43-åœ¨çº¿å­¦ä¹ ç®—æ³•)
      - [å¢é‡å­¦ä¹ ](#å¢é‡å­¦ä¹ )
  - [7. é›†æˆæ–¹æ¡ˆ](#7-é›†æˆæ–¹æ¡ˆ)
    - [7.1 Apache Kafkaé›†æˆ](#71-apache-kafkaé›†æˆ)
      - [Kafkaåˆ°PostgreSQLåŒæ­¥](#kafkaåˆ°postgresqlåŒæ­¥)
    - [7.2 Apache Flinké›†æˆ](#72-apache-flinké›†æˆ)
      - [Flink SQLåˆ°PostgreSQL](#flink-sqlåˆ°postgresql)
  - [8. å®æˆ˜æ¡ˆä¾‹](#8-å®æˆ˜æ¡ˆä¾‹)
    - [8.1 å®æ—¶æ¨èç³»ç»Ÿ](#81-å®æ—¶æ¨èç³»ç»Ÿ)
      - [å®æ—¶ç”¨æˆ·ç”»åƒæ›´æ–°](#å®æ—¶ç”¨æˆ·ç”»åƒæ›´æ–°)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)
  - [ğŸ“ æ›´æ–°æ—¥å¿—](#-æ›´æ–°æ—¥å¿—)

---

## 1. æ•°æ®æµåˆ†ææ¦‚è¿°

### 1.1 æ•°æ®æµåˆ†ææ¦‚å¿µ

#### æ ¸å¿ƒæ¦‚å¿µ

```text
æ•°æ®æµåˆ†æ vs æ‰¹å¤„ç†åˆ†æ:

æ‰¹å¤„ç†åˆ†æ:
  æ•°æ® â†’ å­˜å‚¨ â†’ æ‰¹é‡å¤„ç† â†’ ç»“æœ
  å»¶è¿Ÿ: åˆ†é’Ÿåˆ°å°æ—¶çº§

æµå¼åˆ†æ:
  æ•°æ®æµ â†’ å®æ—¶å¤„ç† â†’ å®æ—¶ç»“æœ
  å»¶è¿Ÿ: ç§’åˆ°æ¯«ç§’çº§
```

#### æ•°æ®æµåˆ†æç‰¹ç‚¹

- âœ… **å®æ—¶æ€§**: ä½å»¶è¿Ÿå¤„ç†ï¼ˆç§’çº§æˆ–æ¯«ç§’çº§ï¼‰
- âœ… **è¿ç»­æ€§**: æŒç»­å¤„ç†æ•°æ®æµ
- âœ… **æ— ç•Œæ€§**: æ•°æ®æµç†è®ºä¸Šæ— ç•Œ
- âœ… **é¡ºåºæ€§**: ä¿æŒæ•°æ®é¡ºåº
- âœ… **æ—¶é—´æ€§**: æ—¶é—´è¯­ä¹‰é‡è¦ï¼ˆäº‹ä»¶æ—¶é—´ã€å¤„ç†æ—¶é—´ï¼‰

### 1.2 æµå¤„ç†æ¶æ„æ¨¡å¼

#### æ¨¡å¼å¯¹æ¯”

```text
æ¨¡å¼1: äº‹ä»¶é©±åŠ¨æ¶æ„
æ•°æ®æº â†’ äº‹ä»¶é˜Ÿåˆ— â†’ æµå¤„ç†å™¨ â†’ ç»“æœå­˜å‚¨ â†’ åº”ç”¨

æ¨¡å¼2: Lambdaæ¶æ„
æ‰¹å¤„ç†å±‚ï¼ˆå†å²æ•°æ®ï¼‰
  â†“
æœåŠ¡å±‚ â† æµå¤„ç†å±‚ï¼ˆå®æ—¶æ•°æ®ï¼‰

æ¨¡å¼3: Kappaæ¶æ„
æ•°æ®æµ â†’ æµå¤„ç†å¼•æ“ â†’ ç»“æœ
ï¼ˆç»Ÿä¸€ä½¿ç”¨æµå¤„ç†ï¼‰
```

### 1.3 æŠ€æœ¯é€‰å‹

#### æŠ€æœ¯æ ˆå¯¹æ¯”

| æŠ€æœ¯ | ç±»å‹ | å»¶è¿Ÿ | ååé‡ | é€‚ç”¨åœºæ™¯ |
|------|------|------|--------|---------|
| **PostgreSQLé€»è¾‘å¤åˆ¶** | æ•°æ®åº“åŸç”Ÿ | ç§’çº§ | é«˜ | æ•°æ®åº“å˜æ›´æµ |
| **TimescaleDB** | æ—¶åºæ•°æ®åº“ | ç§’çº§ | é«˜ | æ—¶åºæ•°æ®æµ |
| **Apache Kafka** | æ¶ˆæ¯é˜Ÿåˆ— | æ¯«ç§’çº§ | æé«˜ | äº‹ä»¶æµã€æ•°æ®ç®¡é“ |
| **Apache Flink** | æµå¤„ç†å¼•æ“ | æ¯«ç§’çº§ | é«˜ | å¤æ‚æµå¤„ç† |
| **Apache Spark Streaming** | æµå¤„ç†å¼•æ“ | ç§’çº§ | é«˜ | æ‰¹æµç»Ÿä¸€ |

---

## 2. æµå¼SQLåˆ†æ

### 2.1 æµå¼çª—å£æ“ä½œ

#### æ—¶é—´çª—å£ç±»å‹

```sql
-- æ»šåŠ¨çª—å£ï¼ˆTumbling Windowï¼‰
SELECT
    time_bucket('1 minute', event_time) AS window_start,
    COUNT(*) AS event_count,
    SUM(value) AS total_value,
    AVG(value) AS avg_value
FROM stream_events
WHERE event_time >= NOW() - INTERVAL '1 hour'
GROUP BY time_bucket('1 minute', event_time)
ORDER BY window_start;

-- æ»‘åŠ¨çª—å£ï¼ˆSliding Windowï¼‰
SELECT
    event_time,
    value,
    AVG(value) OVER (
        ORDER BY event_time
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    ) AS moving_avg_10,
    SUM(value) OVER (
        ORDER BY event_time
        RANGE BETWEEN INTERVAL '5 minutes' PRECEDING AND CURRENT ROW
    ) AS sliding_sum_5min
FROM stream_events
WHERE event_time >= NOW() - INTERVAL '1 hour'
ORDER BY event_time;

-- ä¼šè¯çª—å£ï¼ˆSession Windowï¼‰
WITH session_windows AS (
    SELECT
        user_id,
        event_time,
        value,
        event_time - LAG(event_time) OVER (
            PARTITION BY user_id
            ORDER BY event_time
        ) AS time_gap,
        CASE
            WHEN event_time - LAG(event_time) OVER (
                PARTITION BY user_id
                ORDER BY event_time
            ) > INTERVAL '30 minutes' THEN 1
            ELSE 0
        END AS session_start
    FROM user_events
)
SELECT
    user_id,
    SUM(session_start) OVER (
        PARTITION BY user_id
        ORDER BY event_time
    ) AS session_id,
    MIN(event_time) AS session_start_time,
    MAX(event_time) AS session_end_time,
    COUNT(*) AS session_event_count
FROM session_windows
GROUP BY user_id, session_id;
```

### 2.2 æµå¼èšåˆå‡½æ•°

#### å®æ—¶èšåˆå®ç°

```sql
-- åˆ›å»ºå®æ—¶èšåˆè§†å›¾ï¼ˆä½¿ç”¨ç‰©åŒ–è§†å›¾ï¼‰
CREATE MATERIALIZED VIEW mv_realtime_aggregates AS
SELECT
    time_bucket('1 minute', event_time) AS time_window,
    event_type,
    COUNT(*) AS event_count,
    SUM(value) AS total_value,
    AVG(value) AS avg_value,
    MIN(value) AS min_value,
    MAX(value) AS max_value,
    STDDEV(value) AS stddev_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY value) AS median_value,
    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY value) AS p95_value
FROM stream_events
GROUP BY time_bucket('1 minute', event_time), event_type;

-- åˆ›å»ºå”¯ä¸€ç´¢å¼•
CREATE UNIQUE INDEX idx_mv_realtime_aggregates_window
ON mv_realtime_aggregates(time_window, event_type);

-- è‡ªåŠ¨åˆ·æ–°è§¦å‘å™¨
CREATE OR REPLACE FUNCTION refresh_realtime_aggregates()
RETURNS TRIGGER AS $$
BEGIN
    -- å¢é‡æ›´æ–°ç‰©åŒ–è§†å›¾
    INSERT INTO mv_realtime_aggregates (
        time_window, event_type, event_count, total_value, avg_value,
        min_value, max_value, stddev_value, median_value, p95_value
    )
    SELECT
        time_bucket('1 minute', NEW.event_time),
        NEW.event_type,
        1,
        NEW.value,
        NEW.value,
        NEW.value,
        NEW.value,
        0,
        NEW.value,
        NEW.value
    ON CONFLICT (time_window, event_type) DO UPDATE SET
        event_count = mv_realtime_aggregates.event_count + 1,
        total_value = mv_realtime_aggregates.total_value + NEW.value,
        avg_value = (mv_realtime_aggregates.total_value + NEW.value) /
                   (mv_realtime_aggregates.event_count + 1),
        min_value = LEAST(mv_realtime_aggregates.min_value, NEW.value),
        max_value = GREATEST(mv_realtime_aggregates.max_value, NEW.value);

    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER refresh_aggregates_trigger
AFTER INSERT ON stream_events
FOR EACH ROW
EXECUTE FUNCTION refresh_realtime_aggregates();
```

### 2.3 æµå¼JOINæ“ä½œ

#### æµè¡¨JOIN

```sql
-- æµä¸é™æ€è¡¨JOIN
SELECT
    se.event_time,
    se.user_id,
    se.event_type,
    se.value,
    u.name AS user_name,
    u.segment AS user_segment
FROM stream_events se
JOIN users u ON se.user_id = u.id
WHERE se.event_time >= NOW() - INTERVAL '1 hour'
ORDER BY se.event_time;

-- åŒæµJOINï¼ˆæ—¶é—´çª—å£JOINï¼‰
WITH events_with_window AS (
    SELECT
        event_time,
        user_id,
        event_type,
        value,
        time_bucket('5 minutes', event_time) AS window_start
    FROM stream_events
    WHERE event_time >= NOW() - INTERVAL '1 hour'
)
SELECT
    e1.window_start,
    e1.user_id,
    e1.event_type AS event_type_1,
    e2.event_type AS event_type_2,
    e1.value AS value_1,
    e2.value AS value_2
FROM events_with_window e1
JOIN events_with_window e2
    ON e1.user_id = e2.user_id
    AND e1.window_start = e2.window_start
WHERE e1.event_type = 'login' AND e2.event_type = 'purchase'
ORDER BY e1.window_start, e1.user_id;
```

---

## 3. å®æ—¶æ•°æ®åˆ†æ

### 3.1 å®æ—¶æŒ‡æ ‡è®¡ç®—

#### å®æ—¶KPIè®¡ç®—

```sql
-- å®æ—¶ä¸šåŠ¡æŒ‡æ ‡
CREATE OR REPLACE VIEW v_realtime_kpi AS
SELECT
    time_bucket('1 minute', NOW()) AS current_minute,
    COUNT(*) AS total_events,
    COUNT(DISTINCT user_id) AS unique_users,
    SUM(CASE WHEN event_type = 'purchase' THEN amount ELSE 0 END) AS revenue,
    COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) AS purchase_count,
    AVG(CASE WHEN event_type = 'page_view' THEN duration ELSE NULL END) AS avg_session_duration
FROM stream_events
WHERE event_time >= NOW() - INTERVAL '1 minute';

-- å®æ—¶è½¬åŒ–ç‡è®¡ç®—
WITH conversion_funnel AS (
    SELECT
        user_id,
        MIN(CASE WHEN event_type = 'page_view' THEN event_time END) AS page_view_time,
        MIN(CASE WHEN event_type = 'add_to_cart' THEN event_time END) AS add_to_cart_time,
        MIN(CASE WHEN event_type = 'checkout_start' THEN event_time END) AS checkout_start_time,
        MIN(CASE WHEN event_type = 'purchase' THEN event_time END) AS purchase_time
    FROM stream_events
    WHERE event_time >= NOW() - INTERVAL '1 hour'
    GROUP BY user_id
)
SELECT
    COUNT(*) AS total_users,
    COUNT(add_to_cart_time) AS added_to_cart,
    COUNT(checkout_start_time) AS started_checkout,
    COUNT(purchase_time) AS completed_purchase,
    COUNT(add_to_cart_time)::NUMERIC / COUNT(*) * 100 AS cart_rate,
    COUNT(checkout_start_time)::NUMERIC / COUNT(add_to_cart_time) * 100 AS checkout_rate,
    COUNT(purchase_time)::NUMERIC / COUNT(checkout_start_time) * 100 AS purchase_rate
FROM conversion_funnel;
```

### 3.2 å®æ—¶æŠ¥è¡¨ç”Ÿæˆ

#### å®æ—¶æŠ¥è¡¨è§†å›¾

```sql
-- å®æ—¶é”€å”®æŠ¥è¡¨
CREATE MATERIALIZED VIEW mv_realtime_sales_report AS
SELECT
    DATE_TRUNC('hour', sale_time) AS hour,
    product_category,
    region,
    COUNT(*) AS order_count,
    SUM(amount) AS total_revenue,
    AVG(amount) AS avg_order_value,
    COUNT(DISTINCT customer_id) AS unique_customers
FROM sales_stream
WHERE sale_time >= CURRENT_DATE
GROUP BY DATE_TRUNC('hour', sale_time), product_category, region;

-- è‡ªåŠ¨åˆ·æ–°ï¼ˆæ¯å°æ—¶ï¼‰
CREATE OR REPLACE FUNCTION refresh_sales_report()
RETURNS void AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY mv_realtime_sales_report;
END;
$$ LANGUAGE plpgsql;

-- å®šæ—¶ä»»åŠ¡ï¼ˆä½¿ç”¨pg_cronæ‰©å±•ï¼‰
SELECT cron.schedule(
    'refresh-sales-report',
    '0 * * * *',  -- æ¯å°æ—¶
    $$SELECT refresh_sales_report()$$
);
```

---

## 4. æµå¼æœºå™¨å­¦ä¹ 

### 4.1 æµå¼ç‰¹å¾æå–

#### å®æ—¶ç‰¹å¾å·¥ç¨‹

```sql
-- æµå¼ç‰¹å¾æå–å‡½æ•°
CREATE OR REPLACE FUNCTION extract_stream_features(
    p_user_id INTEGER,
    p_window_minutes INTEGER DEFAULT 60
)
RETURNS TABLE (
    feature_name TEXT,
    feature_value NUMERIC
) AS $$
BEGIN
    RETURN QUERY
    WITH user_events AS (
        SELECT *
        FROM stream_events
        WHERE user_id = p_user_id
          AND event_time >= NOW() - (p_window_minutes || ' minutes')::INTERVAL
    )
    SELECT 'event_count'::TEXT, COUNT(*)::NUMERIC FROM user_events
    UNION ALL
    SELECT 'unique_event_types'::TEXT, COUNT(DISTINCT event_type)::NUMERIC FROM user_events
    UNION ALL
    SELECT 'total_value'::TEXT, COALESCE(SUM(value), 0)::NUMERIC FROM user_events
    UNION ALL
    SELECT 'avg_value'::TEXT, COALESCE(AVG(value), 0)::NUMERIC FROM user_events
    UNION ALL
    SELECT 'max_value'::TEXT, COALESCE(MAX(value), 0)::NUMERIC FROM user_events
    UNION ALL
    SELECT 'session_count'::TEXT, COUNT(DISTINCT session_id)::NUMERIC FROM user_events;
END;
$$ LANGUAGE plpgsql;
```

### 4.2 æµå¼æ¨¡å‹é¢„æµ‹

#### å®æ—¶é¢„æµ‹æœåŠ¡

```python
# Python: æµå¼é¢„æµ‹æœåŠ¡
import psycopg2
from psycopg2.extras import RealDictCursor
import joblib
import numpy as np

class StreamPredictionService:
    """æµå¼é¢„æµ‹æœåŠ¡"""

    def __init__(self, db_config, model_path):
        self.conn = psycopg2.connect(**db_config)
        self.model = joblib.load(model_path)

    def predict_stream_event(self, user_id: int) -> dict:
        """é¢„æµ‹æµå¼äº‹ä»¶"""
        # æå–ç‰¹å¾
        cursor = self.conn.cursor(cursor_factory=RealDictCursor)
        cursor.execute("""
            SELECT feature_name, feature_value
            FROM extract_stream_features(%s, 60)
        """, (user_id,))

        features = {row['feature_name']: row['feature_value']
                   for row in cursor.fetchall()}

        # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
        feature_vector = np.array([
            features.get('event_count', 0),
            features.get('unique_event_types', 0),
            features.get('total_value', 0),
            features.get('avg_value', 0),
            features.get('session_count', 0)
        ]).reshape(1, -1)

        # é¢„æµ‹
        prediction = self.model.predict(feature_vector)[0]
        probability = self.model.predict_proba(feature_vector)[0]

        return {
            'user_id': user_id,
            'prediction': prediction,
            'probability': float(max(probability))
        }
```

### 4.3 åœ¨çº¿å­¦ä¹ ç®—æ³•

#### å¢é‡å­¦ä¹ 

```sql
-- åœ¨çº¿å­¦ä¹ æ•°æ®æ”¶é›†
CREATE TABLE online_learning_samples (
    id BIGSERIAL PRIMARY KEY,
    feature_vector NUMERIC[],
    label INTEGER,
    prediction NUMERIC,
    error NUMERIC,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- æ”¶é›†å­¦ä¹ æ ·æœ¬
CREATE OR REPLACE FUNCTION collect_learning_sample(
    p_feature_vector NUMERIC[],
    p_label INTEGER,
    p_prediction NUMERIC
)
RETURNS void AS $$
DECLARE
    v_error NUMERIC;
BEGIN
    v_error := ABS(p_label - p_prediction);

    INSERT INTO online_learning_samples (
        feature_vector, label, prediction, error
    ) VALUES (p_feature_vector, p_label, p_prediction, v_error);

    -- å¦‚æœè¯¯å·®è¾ƒå¤§ï¼Œè§¦å‘æ¨¡å‹é‡è®­ç»ƒ
    IF v_error > 0.5 THEN
        PERFORM pg_notify('model_retrain', 'high_error');
    END IF;
END;
$$ LANGUAGE plpgsql;
```

---

## 7. é›†æˆæ–¹æ¡ˆ

### 7.1 Apache Kafkaé›†æˆ

#### Kafkaåˆ°PostgreSQLåŒæ­¥

```python
# Python: Kafkaæ¶ˆè´¹è€…å†™å…¥PostgreSQL
from kafka import KafkaConsumer
import psycopg2
import json
from psycopg2.extras import execute_batch

class KafkaToPostgreSQL:
    """Kafkaåˆ°PostgreSQLåŒæ­¥å™¨"""

    def __init__(self, kafka_config, db_config):
        self.consumer = KafkaConsumer(
            kafka_config['topic'],
            bootstrap_servers=kafka_config['bootstrap_servers'],
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            auto_offset_reset='latest',
            enable_auto_commit=True
        )
        self.conn = psycopg2.connect(**db_config)
        self.cursor = self.conn.cursor()

    def consume_and_store(self, batch_size=100):
        """æ¶ˆè´¹å¹¶å­˜å‚¨æ¶ˆæ¯"""
        batch = []

        for message in self.consumer:
            data = message.value

            # è½¬æ¢æ•°æ®æ ¼å¼
            record = (
                data.get('event_time'),
                data.get('event_type'),
                data.get('user_id'),
                json.dumps(data.get('data', {}))
            )
            batch.append(record)

            # æ‰¹é‡æ’å…¥
            if len(batch) >= batch_size:
                execute_batch(
                    self.cursor,
                    """
                    INSERT INTO stream_events (event_time, event_type, user_id, event_data)
                    VALUES (%s, %s, %s, %s)
                    """,
                    batch
                )
                self.conn.commit()
                batch = []
```

### 7.2 Apache Flinké›†æˆ

#### Flink SQLåˆ°PostgreSQL

```sql
-- Flink SQL: æµå¤„ç†ç»“æœå†™å…¥PostgreSQL
-- åœ¨Flink SQL Clientä¸­æ‰§è¡Œ

CREATE TABLE stream_events (
    event_time TIMESTAMP(3),
    event_type STRING,
    user_id BIGINT,
    value DOUBLE,
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND
) WITH (
    'connector' = 'kafka',
    'topic' = 'events',
    'properties.bootstrap.servers' = 'localhost:9092',
    'format' = 'json'
);

CREATE TABLE postgresql_sink (
    window_start TIMESTAMP(3),
    event_type STRING,
    event_count BIGINT,
    total_value DOUBLE,
    avg_value DOUBLE,
    PRIMARY KEY (window_start, event_type) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:postgresql://localhost:5432/analytics',
    'table-name' = 'realtime_aggregates',
    'username' = 'postgres',
    'password' = 'password'
);

INSERT INTO postgresql_sink
SELECT
    TUMBLE_START(event_time, INTERVAL '1' MINUTE) AS window_start,
    event_type,
    COUNT(*) AS event_count,
    SUM(value) AS total_value,
    AVG(value) AS avg_value
FROM stream_events
GROUP BY
    TUMBLE(event_time, INTERVAL '1' MINUTE),
    event_type;
```

---

## 8. å®æˆ˜æ¡ˆä¾‹

### 8.1 å®æ—¶æ¨èç³»ç»Ÿ

#### å®æ—¶ç”¨æˆ·ç”»åƒæ›´æ–°

```sql
-- å®æ—¶ç”¨æˆ·ç”»åƒè¡¨
CREATE TABLE realtime_user_profiles (
    user_id INTEGER PRIMARY KEY,
    last_active_time TIMESTAMPTZ,
    total_events INTEGER DEFAULT 0,
    favorite_categories TEXT[],
    avg_session_duration NUMERIC,
    purchase_probability NUMERIC,
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- å®æ—¶æ›´æ–°ç”¨æˆ·ç”»åƒ
CREATE OR REPLACE FUNCTION update_user_profile(p_user_id INTEGER)
RETURNS void AS $$
BEGIN
    INSERT INTO realtime_user_profiles (
        user_id,
        last_active_time,
        total_events,
        favorite_categories,
        avg_session_duration,
        purchase_probability
    )
    SELECT
        p_user_id,
        MAX(event_time),
        COUNT(*),
        array_agg(DISTINCT category) FILTER (WHERE category IS NOT NULL),
        AVG(duration) FILTER (WHERE duration IS NOT NULL),
        -- ä½¿ç”¨é€»è¾‘å›å½’é¢„æµ‹è´­ä¹°æ¦‚ç‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        CASE
            WHEN COUNT(*) FILTER (WHERE event_type = 'add_to_cart') > 0 THEN 0.8
            WHEN COUNT(*) FILTER (WHERE event_type = 'page_view') > 10 THEN 0.3
            ELSE 0.1
        END
    FROM stream_events
    WHERE user_id = p_user_id
      AND event_time >= NOW() - INTERVAL '24 hours'
    GROUP BY user_id
    ON CONFLICT (user_id) DO UPDATE SET
        last_active_time = EXCLUDED.last_active_time,
        total_events = realtime_user_profiles.total_events + EXCLUDED.total_events,
        favorite_categories = EXCLUDED.favorite_categories,
        avg_session_duration = EXCLUDED.avg_session_duration,
        purchase_probability = EXCLUDED.purchase_probability,
        updated_at = NOW();
END;
$$ LANGUAGE plpgsql;

-- è§¦å‘å™¨è‡ªåŠ¨æ›´æ–°
CREATE TRIGGER update_profile_trigger
AFTER INSERT ON stream_events
FOR EACH ROW
EXECUTE FUNCTION update_user_profile(NEW.user_id);
```

---

## ğŸ“š å‚è€ƒèµ„æº

1. **TimescaleDBæ–‡æ¡£**: <https://docs.timescale.com/>
2. **Apache Kafkaæ–‡æ¡£**: <https://kafka.apache.org/documentation/>
3. **Apache Flinkæ–‡æ¡£**: <https://flink.apache.org/docs/>
4. **PostgreSQLé€»è¾‘å¤åˆ¶**: <https://www.postgresql.org/docs/current/logical-replication.html>

---

## ğŸ“ æ›´æ–°æ—¥å¿—

- **v2.0** (2025-01): å®Œæ•´å®æˆ˜æŒ‡å—
  - è¡¥å……æµå¼SQLåˆ†æ
  - è¡¥å……å®æ—¶æ•°æ®åˆ†æ
  - è¡¥å……æµå¼æœºå™¨å­¦ä¹ 
  - è¡¥å……æµå¤„ç†æ€§èƒ½ä¼˜åŒ–
  - è¡¥å……æµå¼æ•°æ®ä»“åº“æ¶æ„
  - è¡¥å……é›†æˆæ–¹æ¡ˆ
  - è¡¥å……å®æˆ˜æ¡ˆä¾‹

---

**çŠ¶æ€**: âœ… **æ–‡æ¡£å®Œæˆ** | [è¿”å›ç›®å½•](./README.md)
