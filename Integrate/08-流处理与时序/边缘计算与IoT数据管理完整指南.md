---
> **ğŸ“‹ æ–‡æ¡£æ¥æº**: æ–°å¢æ·±åŒ–æ–‡æ¡£
> **ğŸ“… åˆ›å»ºæ—¥æœŸ**: 2025-01
> **âš ï¸ æ³¨æ„**: æœ¬æ–‡æ¡£ä¸ºæ·±åº¦è¡¥å……ï¼Œæ·±åŒ–è¾¹ç¼˜è®¡ç®—ä¸IoTæ•°æ®ç®¡ç†æŠ€æœ¯æ ˆ

---

# è¾¹ç¼˜è®¡ç®—ä¸IoTæ•°æ®ç®¡ç†å®Œæ•´æŒ‡å—

## å…ƒæ•°æ®

- **æ–‡æ¡£ç‰ˆæœ¬**: v1.0
- **åˆ›å»ºæ—¥æœŸ**: 2025-01
- **æŠ€æœ¯æ ˆ**: PostgreSQL 17+/18+ | TimescaleDB 2.x | é€»è¾‘å¤åˆ¶ | pg_logical | MQTT
- **éš¾åº¦çº§åˆ«**: â­â­â­â­ (é«˜çº§)
- **é¢„è®¡é˜…è¯»**: 150åˆ†é’Ÿ
- **å‰ç½®è¦æ±‚**: ç†Ÿæ‚‰PostgreSQLåŸºç¡€ã€æ—¶åºæ•°æ®åº“ã€ç½‘ç»œæ¶æ„

---

## ğŸ“‹ å®Œæ•´ç›®å½•

- [è¾¹ç¼˜è®¡ç®—ä¸IoTæ•°æ®ç®¡ç†å®Œæ•´æŒ‡å—](#è¾¹ç¼˜è®¡ç®—ä¸iotæ•°æ®ç®¡ç†å®Œæ•´æŒ‡å—)
  - [å…ƒæ•°æ®](#å…ƒæ•°æ®)
  - [ğŸ“‹ å®Œæ•´ç›®å½•](#-å®Œæ•´ç›®å½•)
  - [1. è¾¹ç¼˜è®¡ç®—æ¶æ„æ¦‚è¿°](#1-è¾¹ç¼˜è®¡ç®—æ¶æ„æ¦‚è¿°)
    - [1.1 è¾¹ç¼˜è®¡ç®—æ¦‚å¿µ](#11-è¾¹ç¼˜è®¡ç®—æ¦‚å¿µ)
      - [è¾¹ç¼˜è®¡ç®—çš„ä¼˜åŠ¿](#è¾¹ç¼˜è®¡ç®—çš„ä¼˜åŠ¿)
      - [è¾¹ç¼˜è®¡ç®—æŒ‘æˆ˜](#è¾¹ç¼˜è®¡ç®—æŒ‘æˆ˜)
    - [1.2 è¾¹ç¼˜-äº‘æ¶æ„æ¨¡å¼](#12-è¾¹ç¼˜-äº‘æ¶æ„æ¨¡å¼)
      - [æ¨¡å¼1ï¼šè¾¹ç¼˜é¢„å¤„ç† + äº‘ç«¯å­˜å‚¨](#æ¨¡å¼1è¾¹ç¼˜é¢„å¤„ç†--äº‘ç«¯å­˜å‚¨)
      - [æ¨¡å¼2ï¼šè¾¹ç¼˜å­˜å‚¨ + äº‘ç«¯åˆ†æ](#æ¨¡å¼2è¾¹ç¼˜å­˜å‚¨--äº‘ç«¯åˆ†æ)
      - [æ¨¡å¼3ï¼šæ··åˆæ¨¡å¼ï¼ˆæ¨èï¼‰](#æ¨¡å¼3æ··åˆæ¨¡å¼æ¨è)
    - [1.3 è¾¹ç¼˜æ•°æ®åº“é€‰å‹](#13-è¾¹ç¼˜æ•°æ®åº“é€‰å‹)
      - [PostgreSQL vs TimescaleDB](#postgresql-vs-timescaledb)
      - [é€‰æ‹©å»ºè®®](#é€‰æ‹©å»ºè®®)
  - [2. è¾¹ç¼˜æ•°æ®åº“éƒ¨ç½²](#2-è¾¹ç¼˜æ•°æ®åº“éƒ¨ç½²)
    - [2.1 è¾¹ç¼˜èŠ‚ç‚¹æ¶æ„è®¾è®¡](#21-è¾¹ç¼˜èŠ‚ç‚¹æ¶æ„è®¾è®¡)
      - [ç¡¬ä»¶é…ç½®å»ºè®®](#ç¡¬ä»¶é…ç½®å»ºè®®)
      - [è½¯ä»¶æ¶æ„](#è½¯ä»¶æ¶æ„)
    - [2.2 PostgreSQLè¾¹ç¼˜éƒ¨ç½²](#22-postgresqlè¾¹ç¼˜éƒ¨ç½²)
      - [æœ€å°åŒ–é…ç½®](#æœ€å°åŒ–é…ç½®)
      - [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
    - [2.3 TimescaleDBè¾¹ç¼˜éƒ¨ç½²](#23-timescaledbè¾¹ç¼˜éƒ¨ç½²)
      - [åŸºç¡€é…ç½®](#åŸºç¡€é…ç½®)
      - [å‹ç¼©é…ç½®](#å‹ç¼©é…ç½®)
      - [æ•°æ®ä¿ç•™ç­–ç•¥](#æ•°æ®ä¿ç•™ç­–ç•¥)
    - [2.4 å®¹å™¨åŒ–è¾¹ç¼˜éƒ¨ç½²](#24-å®¹å™¨åŒ–è¾¹ç¼˜éƒ¨ç½²)
      - [Docker Composeé…ç½®](#docker-composeé…ç½®)
      - [Kubernetesè¾¹ç¼˜éƒ¨ç½²](#kubernetesè¾¹ç¼˜éƒ¨ç½²)
  - [3. IoTæ•°æ®é‡‡é›†ä¸å¤„ç†](#3-iotæ•°æ®é‡‡é›†ä¸å¤„ç†)
    - [3.1 IoTæ•°æ®æ¨¡å‹è®¾è®¡](#31-iotæ•°æ®æ¨¡å‹è®¾è®¡)
      - [ä¼ æ„Ÿå™¨æ•°æ®æ¨¡å‹](#ä¼ æ„Ÿå™¨æ•°æ®æ¨¡å‹)
      - [æ•°æ®è´¨é‡ä¿è¯](#æ•°æ®è´¨é‡ä¿è¯)
    - [3.2 æ•°æ®é‡‡é›†æ¨¡å¼](#32-æ•°æ®é‡‡é›†æ¨¡å¼)
      - [æ¨¡å¼1ï¼šMQTTé‡‡é›†](#æ¨¡å¼1mqtté‡‡é›†)
      - [æ¨¡å¼2ï¼šHTTP REST APIé‡‡é›†](#æ¨¡å¼2http-rest-apié‡‡é›†)
    - [3.3 è¾¹ç¼˜æ•°æ®å¤„ç†](#33-è¾¹ç¼˜æ•°æ®å¤„ç†)
      - [å®æ—¶èšåˆ](#å®æ—¶èšåˆ)
      - [å¼‚å¸¸æ£€æµ‹](#å¼‚å¸¸æ£€æµ‹)
  - [4. è¾¹ç¼˜-äº‘ç«¯æ•°æ®åŒæ­¥](#4-è¾¹ç¼˜-äº‘ç«¯æ•°æ®åŒæ­¥)
    - [4.1 åŒæ­¥ç­–ç•¥](#41-åŒæ­¥ç­–ç•¥)
      - [ç­–ç•¥é€‰æ‹©](#ç­–ç•¥é€‰æ‹©)
    - [4.2 é€»è¾‘å¤åˆ¶åŒæ­¥](#42-é€»è¾‘å¤åˆ¶åŒæ­¥)
      - [è¾¹ç¼˜èŠ‚ç‚¹å‘å¸ƒ](#è¾¹ç¼˜èŠ‚ç‚¹å‘å¸ƒ)
      - [äº‘ç«¯èŠ‚ç‚¹è®¢é˜…](#äº‘ç«¯èŠ‚ç‚¹è®¢é˜…)
      - [åŒå‘åŒæ­¥ï¼ˆå†²çªè§£å†³ï¼‰](#åŒå‘åŒæ­¥å†²çªè§£å†³)
    - [4.3 MQTTæ¶ˆæ¯åŒæ­¥](#43-mqttæ¶ˆæ¯åŒæ­¥)
      - [4.3.1 è¾¹ç¼˜èŠ‚ç‚¹å‘å¸ƒ](#431-è¾¹ç¼˜èŠ‚ç‚¹å‘å¸ƒ)
      - [4.3.2 äº‘ç«¯èŠ‚ç‚¹è®¢é˜…](#432-äº‘ç«¯èŠ‚ç‚¹è®¢é˜…)
  - [5. ç¦»çº¿åœºæ™¯å¤„ç†](#5-ç¦»çº¿åœºæ™¯å¤„ç†)
    - [5.1 ç¦»çº¿æ•°æ®å­˜å‚¨](#51-ç¦»çº¿æ•°æ®å­˜å‚¨)
      - [æœ¬åœ°é˜Ÿåˆ—ç®¡ç†](#æœ¬åœ°é˜Ÿåˆ—ç®¡ç†)
    - [5.2 æ•°æ®é˜Ÿåˆ—ç®¡ç†](#52-æ•°æ®é˜Ÿåˆ—ç®¡ç†)
      - [é˜Ÿåˆ—æŒä¹…åŒ–](#é˜Ÿåˆ—æŒä¹…åŒ–)
  - [6. æ•°æ®å‹ç¼©ä¸ä¼ è¾“ä¼˜åŒ–](#6-æ•°æ®å‹ç¼©ä¸ä¼ è¾“ä¼˜åŒ–)
    - [6.1 æ•°æ®å‹ç¼©ç­–ç•¥](#61-æ•°æ®å‹ç¼©ç­–ç•¥)
      - [TimescaleDBå‹ç¼©](#timescaledbå‹ç¼©)
      - [å¯¼å‡ºæ—¶å‹ç¼©](#å¯¼å‡ºæ—¶å‹ç¼©)
    - [6.2 å¢é‡ä¼ è¾“](#62-å¢é‡ä¼ è¾“)
      - [åŸºäºæ—¶é—´æˆ³çš„å¢é‡åŒæ­¥](#åŸºäºæ—¶é—´æˆ³çš„å¢é‡åŒæ­¥)
  - [7. è¾¹ç¼˜èŠ‚ç‚¹ç®¡ç†](#7-è¾¹ç¼˜èŠ‚ç‚¹ç®¡ç†)
    - [7.1 èŠ‚ç‚¹ç›‘æ§](#71-èŠ‚ç‚¹ç›‘æ§)
      - [ç›‘æ§æŒ‡æ ‡](#ç›‘æ§æŒ‡æ ‡)
  - [8. å®è·µæ¡ˆä¾‹](#8-å®è·µæ¡ˆä¾‹)
    - [8.1 å·¥ä¸šIoTè¾¹ç¼˜è®¡ç®—æ¡ˆä¾‹](#81-å·¥ä¸šiotè¾¹ç¼˜è®¡ç®—æ¡ˆä¾‹)
      - [åœºæ™¯æè¿°](#åœºæ™¯æè¿°)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)
  - [ğŸ“ æ›´æ–°æ—¥å¿—](#-æ›´æ–°æ—¥å¿—)

---

## 1. è¾¹ç¼˜è®¡ç®—æ¶æ„æ¦‚è¿°

### 1.1 è¾¹ç¼˜è®¡ç®—æ¦‚å¿µ

è¾¹ç¼˜è®¡ç®—æ˜¯ä¸€ç§åˆ†å¸ƒå¼è®¡ç®—æ¶æ„ï¼Œå°†è®¡ç®—å’Œæ•°æ®å¤„ç†èƒ½åŠ›æ¨å‘ç½‘ç»œçš„è¾¹ç¼˜ï¼Œé è¿‘æ•°æ®æºå’Œç”¨æˆ·ã€‚

#### è¾¹ç¼˜è®¡ç®—çš„ä¼˜åŠ¿

```text
ä¼˜åŠ¿:
âœ… ä½å»¶è¿Ÿ: æœ¬åœ°å¤„ç†ï¼Œå‡å°‘ç½‘ç»œå»¶è¿Ÿï¼ˆ<10msï¼‰
âœ… å¸¦å®½èŠ‚çœ: è¾¹ç¼˜è¿‡æ»¤å’Œèšåˆï¼Œå‡å°‘ä¼ è¾“æ•°æ®é‡ï¼ˆ90%+å‡å°‘ï¼‰
âœ… ç¦»çº¿èƒ½åŠ›: ç½‘ç»œä¸­æ–­æ—¶ä»å¯è¿è¡Œ
âœ… éšç§ä¿æŠ¤: æ•æ„Ÿæ•°æ®æœ¬åœ°å¤„ç†
âœ… æˆæœ¬ä¼˜åŒ–: å‡å°‘äº‘ç«¯è®¡ç®—å’Œå­˜å‚¨æˆæœ¬
```

#### è¾¹ç¼˜è®¡ç®—æŒ‘æˆ˜

```text
æŒ‘æˆ˜:
âš ï¸ èµ„æºå—é™: CPUã€å†…å­˜ã€å­˜å‚¨æœ‰é™
âš ï¸ ç½‘ç»œä¸ç¨³å®š: å¯èƒ½é—´æ­‡æ€§æ–­ç½‘
âš ï¸ ç®¡ç†å¤æ‚: åˆ†æ•£çš„èŠ‚ç‚¹ç®¡ç†
âš ï¸ æ•°æ®ä¸€è‡´æ€§: è¾¹ç¼˜å’Œäº‘ç«¯æ•°æ®åŒæ­¥
âš ï¸ å®‰å…¨æ€§: è¾¹ç¼˜è®¾å¤‡å®‰å…¨é˜²æŠ¤
```

### 1.2 è¾¹ç¼˜-äº‘æ¶æ„æ¨¡å¼

#### æ¨¡å¼1ï¼šè¾¹ç¼˜é¢„å¤„ç† + äº‘ç«¯å­˜å‚¨

```text
IoTè®¾å¤‡
  â†“
è¾¹ç¼˜èŠ‚ç‚¹ï¼ˆPostgreSQL/TimescaleDBï¼‰
  â†“ (é¢„å¤„ç†ã€èšåˆã€è¿‡æ»¤)
äº‘ç«¯ä¸­å¿ƒæ•°æ®åº“ï¼ˆPostgreSQLï¼‰
  â†“
æ•°æ®åˆ†æä¸åº”ç”¨
```

#### æ¨¡å¼2ï¼šè¾¹ç¼˜å­˜å‚¨ + äº‘ç«¯åˆ†æ

```text
IoTè®¾å¤‡
  â†“
è¾¹ç¼˜èŠ‚ç‚¹ï¼ˆPostgreSQLï¼Œå®Œæ•´æ•°æ®å­˜å‚¨ï¼‰
  â†“ (å®šæœŸåŒæ­¥)
äº‘ç«¯ä¸­å¿ƒæ•°æ®åº“ï¼ˆPostgreSQLï¼Œå¤‡ä»½å’Œåˆ†æï¼‰
  â†“
å¤§æ•°æ®åˆ†æ
```

#### æ¨¡å¼3ï¼šæ··åˆæ¨¡å¼ï¼ˆæ¨èï¼‰

```text
IoTè®¾å¤‡
  â†“
è¾¹ç¼˜èŠ‚ç‚¹ï¼ˆPostgreSQL/TimescaleDBï¼‰
  â”œâ”€ å®æ—¶æ•°æ®ï¼ˆæœ¬åœ°å­˜å‚¨å’Œå¤„ç†ï¼‰
  â”œâ”€ èšåˆæ•°æ®ï¼ˆå®šæœŸåŒæ­¥åˆ°äº‘ç«¯ï¼‰
  â””â”€ å‘Šè­¦æ•°æ®ï¼ˆå®æ—¶æ¨é€ï¼‰
  â†“
äº‘ç«¯ä¸­å¿ƒæ•°æ®åº“ï¼ˆPostgreSQLï¼‰
  â”œâ”€ å†å²æ•°æ®å½’æ¡£
  â”œâ”€ è·¨èŠ‚ç‚¹åˆ†æ
  â””â”€ å…¨å±€è§†å›¾
```

### 1.3 è¾¹ç¼˜æ•°æ®åº“é€‰å‹

#### PostgreSQL vs TimescaleDB

| ç‰¹æ€§ | PostgreSQL | TimescaleDB |
|------|-----------|-------------|
| **æ—¶åºæ•°æ®** | âš ï¸ éœ€è¦æ‰‹åŠ¨åˆ†åŒº | âœ… è‡ªåŠ¨Hypertable |
| **æ•°æ®å‹ç¼©** | âš ï¸ éœ€è¦æ‰©å±• | âœ… åŸç”Ÿå‹ç¼© |
| **è¿ç»­èšåˆ** | âš ï¸ éœ€è¦ç‰©åŒ–è§†å›¾ | âœ… è‡ªåŠ¨å¢é‡èšåˆ |
| **èµ„æºå ç”¨** | ä¸­ç­‰ | ç•¥é«˜ï¼ˆæ‰©å±•å¼€é”€ï¼‰ |
| **é€‚ç”¨åœºæ™¯** | é€šç”¨åœºæ™¯ | æ—¶åºæ•°æ®å¯†é›† |

#### é€‰æ‹©å»ºè®®

```text
ä½¿ç”¨PostgreSQLçš„æƒ…å†µ:
- æ•°æ®ç»“æ„å¤æ‚ï¼ˆå…³ç³»å‹æ•°æ®ä¸ºä¸»ï¼‰
- éœ€è¦å¤æ‚æŸ¥è¯¢å’Œäº‹åŠ¡
- èµ„æºéå¸¸å—é™
- æ•°æ®ä¸æ˜¯çº¯æ—¶åºæ•°æ®

ä½¿ç”¨TimescaleDBçš„æƒ…å†µ:
- ä¸»è¦æ˜¯æ—¶åºæ•°æ®ï¼ˆä¼ æ„Ÿå™¨ã€ç›‘æ§æ•°æ®ï¼‰
- éœ€è¦é«˜é¢‘å†™å…¥
- éœ€è¦æ—¶é—´èŒƒå›´æŸ¥è¯¢ä¼˜åŒ–
- éœ€è¦æ•°æ®å‹ç¼©
```

---

## 2. è¾¹ç¼˜æ•°æ®åº“éƒ¨ç½²

### 2.1 è¾¹ç¼˜èŠ‚ç‚¹æ¶æ„è®¾è®¡

#### ç¡¬ä»¶é…ç½®å»ºè®®

```yaml
å°å‹è¾¹ç¼˜èŠ‚ç‚¹:
  CPU: 2-4æ ¸
  å†…å­˜: 4-8GB
  å­˜å‚¨: 64-128GB SSD
  ç½‘ç»œ: 100Mbps

ä¸­å‹è¾¹ç¼˜èŠ‚ç‚¹:
  CPU: 4-8æ ¸
  å†…å­˜: 8-16GB
  å­˜å‚¨: 256-512GB SSD
  ç½‘ç»œ: 1Gbps

å¤§å‹è¾¹ç¼˜èŠ‚ç‚¹:
  CPU: 8-16æ ¸
  å†…å­˜: 16-32GB
  å­˜å‚¨: 512GB-1TB SSD
  ç½‘ç»œ: 1-10Gbps
```

#### è½¯ä»¶æ¶æ„

```text
è¾¹ç¼˜èŠ‚ç‚¹è½¯ä»¶æ ˆ:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åº”ç”¨å±‚ï¼ˆä¸šåŠ¡é€»è¾‘ï¼‰                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ•°æ®é‡‡é›†å±‚ï¼ˆMQTT/HTTP/Modbusï¼‰   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ•°æ®å¤„ç†å±‚ï¼ˆæµå¤„ç†ã€èšåˆï¼‰         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ•°æ®å­˜å‚¨å±‚ï¼ˆPostgreSQL/TimescaleDBï¼‰â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  åŒæ­¥å±‚ï¼ˆé€»è¾‘å¤åˆ¶/MQTTï¼‰           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ“ä½œç³»ç»Ÿï¼ˆLinuxï¼‰                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 PostgreSQLè¾¹ç¼˜éƒ¨ç½²

#### æœ€å°åŒ–é…ç½®

```bash
# PostgreSQLè¾¹ç¼˜éƒ¨ç½²é…ç½®ï¼ˆèµ„æºå—é™ç¯å¢ƒï¼‰
# postgresql.conf

# å†…å­˜é…ç½®ï¼ˆ4GBæ€»å†…å­˜ï¼‰
shared_buffers = 512MB              # 12.5%å†…å­˜
effective_cache_size = 2GB          # 50%å†…å­˜
work_mem = 16MB                     # æ¯ä¸ªæ“ä½œ
maintenance_work_mem = 128MB        # ç»´æŠ¤æ“ä½œ
temp_buffers = 8MB                  # ä¸´æ—¶ç¼“å†²åŒº

# WALé…ç½®ï¼ˆä¼˜åŒ–å†™å…¥ï¼‰
wal_buffers = 16MB
checkpoint_timeout = 15min          # å‡å°‘æ£€æŸ¥ç‚¹é¢‘ç‡
max_wal_size = 1GB                  # æ§åˆ¶WALå¤§å°
min_wal_size = 256MB

# è¿æ¥é…ç½®
max_connections = 50                # è¾¹ç¼˜èŠ‚ç‚¹è¿æ¥æ•°å°‘
superuser_reserved_connections = 2

# æŸ¥è¯¢ä¼˜åŒ–
random_page_cost = 1.1              # SSDä¼˜åŒ–
effective_io_concurrency = 200      # SSDå¹¶å‘

# æ—¥å¿—é…ç½®ï¼ˆèŠ‚çœå­˜å‚¨ï¼‰
logging_collector = on
log_destination = 'stderr'
log_min_duration_statement = 1000   # åªè®°å½•æ…¢æŸ¥è¯¢ï¼ˆ>1ç§’ï¼‰
log_rotation_age = 1d               # æ¯æ—¥è½®è½¬
log_rotation_size = 100MB           # æœ€å¤§100MB
```

#### æ€§èƒ½ä¼˜åŒ–

```sql
-- åˆ›å»ºé€‚åˆè¾¹ç¼˜ç¯å¢ƒçš„ç´¢å¼•
-- ä½¿ç”¨éƒ¨åˆ†ç´¢å¼•å‡å°‘ç´¢å¼•å¤§å°
CREATE INDEX idx_sensor_recent ON sensor_data (device_id, time DESC)
WHERE time > NOW() - INTERVAL '7 days';

-- ä½¿ç”¨è¡¨è¾¾å¼ç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢
CREATE INDEX idx_sensor_value_threshold ON sensor_data ((value > 100))
WHERE value > 100;

-- è¡¨åˆ†åŒºï¼ˆPostgreSQLåŸç”Ÿï¼‰
CREATE TABLE sensor_data (
    id BIGSERIAL,
    device_id TEXT NOT NULL,
    time TIMESTAMPTZ NOT NULL,
    value NUMERIC(10,2),
    PRIMARY KEY (id, time)
) PARTITION BY RANGE (time);

-- åˆ›å»ºæœˆåº¦åˆ†åŒº
CREATE TABLE sensor_data_2025_01 PARTITION OF sensor_data
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

-- è‡ªåŠ¨åˆ›å»ºåˆ†åŒºï¼ˆä½¿ç”¨è§¦å‘å™¨æˆ–è„šæœ¬ï¼‰
```

### 2.3 TimescaleDBè¾¹ç¼˜éƒ¨ç½²

#### åŸºç¡€é…ç½®

```sql
-- å®‰è£…TimescaleDBï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 FROM pg_roles
        WHERE rolname = current_user
        AND rolsuper = TRUE
    ) THEN
        RAISE EXCEPTION 'å½“å‰ç”¨æˆ·ä¸æ˜¯è¶…çº§ç”¨æˆ·ï¼Œæ— æ³•åˆ›å»ºæ‰©å±•';
    END IF;

    CREATE EXTENSION IF NOT EXISTS timescaledb;
    RAISE NOTICE 'æ‰©å±•å®‰è£…æˆåŠŸ: timescaledb';
EXCEPTION
    WHEN insufficient_privilege THEN
        RAISE EXCEPTION 'æƒé™ä¸è¶³ï¼Œæ— æ³•åˆ›å»ºæ‰©å±•';
    WHEN undefined_file THEN
        RAISE EXCEPTION 'æ‰©å±•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥PostgreSQLå®‰è£…';
    WHEN OTHERS THEN
        RAISE EXCEPTION 'æ‰©å±•å®‰è£…å¤±è´¥: %', SQLERRM;
END $$;

-- åˆ›å»ºæ—¶åºè¡¨ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    IF EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'sensor_data') THEN
        DROP TABLE sensor_data;
        RAISE NOTICE 'å·²åˆ é™¤ç°æœ‰è¡¨: sensor_data';
    END IF;

    CREATE TABLE sensor_data (
        time TIMESTAMPTZ NOT NULL,
        device_id TEXT NOT NULL,
        sensor_type TEXT NOT NULL,
        value NUMERIC(10,2),
        metadata JSONB
    );

    RAISE NOTICE 'è¡¨åˆ›å»ºæˆåŠŸ: sensor_data';
EXCEPTION
    WHEN duplicate_table THEN
        RAISE WARNING 'è¡¨sensor_dataå·²å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE EXCEPTION 'åˆ›å»ºè¡¨å¤±è´¥: %', SQLERRM;
END $$;

-- è½¬æ¢ä¸ºHypertableï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
DECLARE
    hypertable_exists BOOLEAN;
BEGIN
    IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'sensor_data') THEN
        RAISE EXCEPTION 'è¡¨sensor_dataä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»º';
    END IF;

    IF NOT EXISTS (
        SELECT 1 FROM pg_extension
        WHERE extname = 'timescaledb'
    ) THEN
        RAISE EXCEPTION 'TimescaleDBæ‰©å±•æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…';
    END IF;

    SELECT EXISTS (
        SELECT 1 FROM timescaledb_information.hypertables
        WHERE hypertable_name = 'sensor_data'
    ) INTO hypertable_exists;

    IF NOT hypertable_exists THEN
        PERFORM create_hypertable(
            'sensor_data',
            'time',
            chunk_time_interval => INTERVAL '1 day',  -- æ¯æ—¥ä¸€ä¸ªchunk
            if_not_exists => TRUE
        );
        RAISE NOTICE 'Hypertableåˆ›å»ºæˆåŠŸ: sensor_data';
    ELSE
        RAISE WARNING 'Hypertable sensor_dataå·²å­˜åœ¨';
    END IF;
EXCEPTION
    WHEN undefined_table THEN
        RAISE EXCEPTION 'è¡¨sensor_dataä¸å­˜åœ¨';
    WHEN undefined_object THEN
        RAISE EXCEPTION 'create_hypertableå‡½æ•°ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥TimescaleDBæ‰©å±•å®‰è£…';
    WHEN OTHERS THEN
        RAISE EXCEPTION 'åˆ›å»ºHypertableå¤±è´¥: %', SQLERRM;
END $$;

-- åˆ›å»ºç´¢å¼•ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'sensor_data') THEN
        RAISE EXCEPTION 'è¡¨sensor_dataä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»º';
    END IF;

    IF NOT EXISTS (
        SELECT 1 FROM pg_indexes
        WHERE schemaname = 'public'
        AND tablename = 'sensor_data'
        AND indexname = 'idx_sensor_device_time'
    ) THEN
        CREATE INDEX idx_sensor_device_time ON sensor_data (device_id, time DESC);
        RAISE NOTICE 'ç´¢å¼•åˆ›å»ºæˆåŠŸ: idx_sensor_device_time';
    END IF;

    IF NOT EXISTS (
        SELECT 1 FROM pg_indexes
        WHERE schemaname = 'public'
        AND tablename = 'sensor_data'
        AND indexname = 'idx_sensor_type'
    ) THEN
        CREATE INDEX idx_sensor_type ON sensor_data (sensor_type, time DESC);
        RAISE NOTICE 'ç´¢å¼•åˆ›å»ºæˆåŠŸ: idx_sensor_type';
    END IF;
EXCEPTION
    WHEN undefined_table THEN
        RAISE EXCEPTION 'è¡¨sensor_dataä¸å­˜åœ¨';
    WHEN duplicate_table THEN
        RAISE WARNING 'ç´¢å¼•å·²å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE EXCEPTION 'åˆ›å»ºç´¢å¼•å¤±è´¥: %', SQLERRM;
END $$;
```

#### å‹ç¼©é…ç½®

```sql
-- å¯ç”¨å‹ç¼©ï¼ˆ7å¤©å‰çš„æ•°æ®ï¼Œå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
DECLARE
    policy_exists BOOLEAN;
BEGIN
    IF NOT EXISTS (
        SELECT 1 FROM timescaledb_information.hypertables
        WHERE hypertable_name = 'sensor_data'
    ) THEN
        RAISE EXCEPTION 'Hypertable sensor_dataä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»º';
    END IF;

    SELECT EXISTS (
        SELECT 1 FROM timescaledb_information.jobs
        WHERE hypertable_name = 'sensor_data'
        AND proc_name = 'policy_compression'
    ) INTO policy_exists;

    IF NOT policy_exists THEN
        PERFORM add_compression_policy('sensor_data', INTERVAL '7 days');
        RAISE NOTICE 'å‹ç¼©ç­–ç•¥æ·»åŠ æˆåŠŸ: sensor_data (7å¤©)';
    ELSE
        RAISE WARNING 'å‹ç¼©ç­–ç•¥å·²å­˜åœ¨';
    END IF;
EXCEPTION
    WHEN undefined_table THEN
        RAISE EXCEPTION 'Hypertable sensor_dataä¸å­˜åœ¨';
    WHEN undefined_function THEN
        RAISE EXCEPTION 'add_compression_policyå‡½æ•°ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥TimescaleDBæ‰©å±•å®‰è£…';
    WHEN OTHERS THEN
        RAISE EXCEPTION 'æ·»åŠ å‹ç¼©ç­–ç•¥å¤±è´¥: %', SQLERRM;
END $$;

-- å‹ç¼©é…ç½®ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 FROM timescaledb_information.hypertables
        WHERE hypertable_name = 'sensor_data'
    ) THEN
        RAISE EXCEPTION 'Hypertable sensor_dataä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»º';
    END IF;

    ALTER TABLE sensor_data SET (
        timescaledb.compress,
        timescaledb.compress_segmentby = 'device_id',
        timescaledb.compress_orderby = 'time DESC'
    );

    RAISE NOTICE 'å‹ç¼©é…ç½®å·²è®¾ç½®: sensor_data';
EXCEPTION
    WHEN undefined_table THEN
        RAISE EXCEPTION 'è¡¨sensor_dataä¸å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE EXCEPTION 'è®¾ç½®å‹ç¼©é…ç½®å¤±è´¥: %', SQLERRM;
END $$;

-- æŸ¥çœ‹å‹ç¼©ç»Ÿè®¡ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰
DO $$
DECLARE
    job_count INT;
BEGIN
    IF NOT EXISTS (
        SELECT 1 FROM pg_extension
        WHERE extname = 'timescaledb'
    ) THEN
        RAISE WARNING 'TimescaleDBæ‰©å±•æœªå®‰è£…';
        RETURN;
    END IF;

    SELECT COUNT(*) INTO job_count
    FROM timescaledb_information.jobs
    WHERE proc_name = 'policy_compression';

    RAISE NOTICE 'æ‰¾åˆ° % ä¸ªå‹ç¼©ç­–ç•¥ä»»åŠ¡', job_count;
EXCEPTION
    WHEN undefined_table THEN
        RAISE WARNING 'timescaledb_information.jobsè§†å›¾ä¸å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE EXCEPTION 'æŸ¥è¯¢å‹ç¼©ç»Ÿè®¡å¤±è´¥: %', SQLERRM;
END $$;

EXPLAIN (ANALYZE, BUFFERS, TIMING)
SELECT * FROM timescaledb_information.jobs
WHERE proc_name = 'policy_compression';
-- æ‰§è¡Œæ—¶é—´: <10ms
-- è®¡åˆ’: Seq Scan
```

#### æ•°æ®ä¿ç•™ç­–ç•¥

```sql
-- è®¾ç½®æ•°æ®ä¿ç•™ç­–ç•¥ï¼ˆä¿ç•™30å¤©ï¼Œå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
DECLARE
    policy_exists BOOLEAN;
BEGIN
    IF NOT EXISTS (
        SELECT 1 FROM timescaledb_information.hypertables
        WHERE hypertable_name = 'sensor_data'
    ) THEN
        RAISE EXCEPTION 'Hypertable sensor_dataä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»º';
    END IF;

    SELECT EXISTS (
        SELECT 1 FROM timescaledb_information.jobs
        WHERE hypertable_name = 'sensor_data'
        AND proc_name = 'policy_retention'
    ) INTO policy_exists;

    IF NOT policy_exists THEN
        PERFORM add_retention_policy('sensor_data', INTERVAL '30 days');
        RAISE NOTICE 'æ•°æ®ä¿ç•™ç­–ç•¥æ·»åŠ æˆåŠŸ: sensor_data (30å¤©)';
    ELSE
        RAISE WARNING 'æ•°æ®ä¿ç•™ç­–ç•¥å·²å­˜åœ¨';
    END IF;
EXCEPTION
    WHEN undefined_table THEN
        RAISE EXCEPTION 'Hypertable sensor_dataä¸å­˜åœ¨';
    WHEN undefined_function THEN
        RAISE EXCEPTION 'add_retention_policyå‡½æ•°ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥TimescaleDBæ‰©å±•å®‰è£…';
    WHEN OTHERS THEN
        RAISE EXCEPTION 'æ·»åŠ æ•°æ®ä¿ç•™ç­–ç•¥å¤±è´¥: %', SQLERRM;
END $$;

-- è‡ªå®šä¹‰ä¿ç•™ç­–ç•¥ï¼ˆä¿ç•™ä¸åŒæ—¶é—´æ®µçš„æ•°æ®ï¼‰
-- ä¿ç•™æœ€è¿‘1å°æ—¶ï¼šåŸå§‹æ•°æ®
-- ä¿ç•™æœ€è¿‘1å¤©ï¼š1åˆ†é’Ÿèšåˆ
-- ä¿ç•™æœ€è¿‘7å¤©ï¼š5åˆ†é’Ÿèšåˆ
-- ä¿ç•™æœ€è¿‘30å¤©ï¼š1å°æ—¶èšåˆ
-- ä¿ç•™1å¹´ä»¥ä¸Šï¼š1å¤©èšåˆ

-- åˆ›å»ºèšåˆè¡¨ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 FROM timescaledb_information.hypertables
        WHERE hypertable_name = 'sensor_data'
    ) THEN
        RAISE EXCEPTION 'Hypertable sensor_dataä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»º';
    END IF;

    IF EXISTS (
        SELECT 1 FROM pg_matviews
        WHERE schemaname = 'public'
        AND matviewname = 'sensor_data_1min'
    ) THEN
        DROP MATERIALIZED VIEW sensor_data_1min;
        RAISE NOTICE 'å·²åˆ é™¤ç°æœ‰ç‰©åŒ–è§†å›¾: sensor_data_1min';
    END IF;

    CREATE MATERIALIZED VIEW sensor_data_1min
    WITH (timescaledb.continuous) AS
    SELECT
        time_bucket('1 minute', time) AS bucket,
        device_id,
        sensor_type,
        avg(value) AS avg_value,
        min(value) AS min_value,
        max(value) AS max_value,
        count(*) AS count
    FROM sensor_data
    GROUP BY bucket, device_id, sensor_type;

    RAISE NOTICE 'è¿ç»­èšåˆè§†å›¾åˆ›å»ºæˆåŠŸ: sensor_data_1min';
EXCEPTION
    WHEN undefined_table THEN
        RAISE EXCEPTION 'è¡¨sensor_dataä¸å­˜åœ¨';
    WHEN undefined_function THEN
        RAISE EXCEPTION 'time_bucketå‡½æ•°ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥TimescaleDBæ‰©å±•å®‰è£…';
    WHEN duplicate_table THEN
        RAISE WARNING 'ç‰©åŒ–è§†å›¾å·²å­˜åœ¨';
    WHEN OTHERS THEN
        RAISE EXCEPTION 'åˆ›å»ºè¿ç»­èšåˆè§†å›¾å¤±è´¥: %', SQLERRM;
END $$;

-- è®¾ç½®åˆ·æ–°ç­–ç•¥ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
DECLARE
    policy_exists BOOLEAN;
BEGIN
    IF NOT EXISTS (
        SELECT 1 FROM pg_matviews
        WHERE schemaname = 'public'
        AND matviewname = 'sensor_data_1min'
    ) THEN
        RAISE EXCEPTION 'ç‰©åŒ–è§†å›¾sensor_data_1minä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»º';
    END IF;

    SELECT EXISTS (
        SELECT 1 FROM timescaledb_information.jobs
        WHERE hypertable_name = 'sensor_data_1min'
        AND proc_name = 'policy_refresh_continuous_aggregate'
    ) INTO policy_exists;

    IF NOT policy_exists THEN
        PERFORM add_continuous_aggregate_policy('sensor_data_1min',
    start_offset => INTERVAL '1 hour',
    end_offset => INTERVAL '1 minute',
    schedule_interval => INTERVAL '1 minute');
```

### 2.4 å®¹å™¨åŒ–è¾¹ç¼˜éƒ¨ç½²

#### Docker Composeé…ç½®

```yaml
# docker-compose.yml (è¾¹ç¼˜èŠ‚ç‚¹)
version: '3.8'

services:
  postgres:
    image: timescale/timescaledb:latest-pg17
    container_name: edge-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: iot_data
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_INITDB_ARGS: "-E UTF8"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./postgresql.conf:/etc/postgresql/postgresql.conf
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    command:
      - "postgres"
      - "-c"
      - "config_file=/etc/postgresql/postgresql.conf"
    ports:
      - "5432:5432"
    networks:
      - edge-network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G

  # MQTT Broker (å¯é€‰ï¼Œç”¨äºæ•°æ®é‡‡é›†)
  mosquitto:
    image: eclipse-mosquitto:latest
    container_name: edge-mosquitto
    restart: unless-stopped
    volumes:
      - ./mosquitto.conf:/mosquitto/config/mosquitto.conf
      - mosquitto-data:/mosquitto/data
    ports:
      - "1883:1883"
    networks:
      - edge-network

  # æ•°æ®é‡‡é›†æœåŠ¡
  data-collector:
    image: edge-data-collector:latest
    container_name: edge-data-collector
    restart: unless-stopped
    environment:
      DATABASE_URL: postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/iot_data
      MQTT_BROKER: mosquitto:1883
    depends_on:
      - postgres
      - mosquitto
    networks:
      - edge-network

volumes:
  postgres-data:
  mosquitto-data:

networks:
  edge-network:
    driver: bridge
```

#### Kubernetesè¾¹ç¼˜éƒ¨ç½²

```yaml
# edge-postgres.yaml (Kubernetes)
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: edge-postgres-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  storageClassName: local-storage  # è¾¹ç¼˜èŠ‚ç‚¹ä½¿ç”¨æœ¬åœ°å­˜å‚¨
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: edge-postgres
spec:
  replicas: 1  # è¾¹ç¼˜èŠ‚ç‚¹é€šå¸¸å•å®ä¾‹
  selector:
    matchLabels:
      app: edge-postgres
  template:
    metadata:
      labels:
        app: edge-postgres
    spec:
      containers:
      - name: postgres
        image: timescale/timescaledb:latest-pg17
        env:
        - name: POSTGRES_DB
          value: "iot_data"
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        ports:
        - containerPort: 5432
        volumeMounts:
        - name: postgres-data
          mountPath: /var/lib/postgresql/data
        - name: postgres-config
          mountPath: /etc/postgresql
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
      volumes:
      - name: postgres-data
        persistentVolumeClaim:
          claimName: edge-postgres-pvc
      - name: postgres-config
        configMap:
          name: postgres-config
---
apiVersion: v1
kind: Service
metadata:
  name: edge-postgres
spec:
  selector:
    app: edge-postgres
  ports:
  - port: 5432
    targetPort: 5432
  type: ClusterIP
```

---

## 3. IoTæ•°æ®é‡‡é›†ä¸å¤„ç†

### 3.1 IoTæ•°æ®æ¨¡å‹è®¾è®¡

#### ä¼ æ„Ÿå™¨æ•°æ®æ¨¡å‹

```sql
-- åŸºç¡€ä¼ æ„Ÿå™¨æ•°æ®è¡¨
CREATE TABLE sensor_readings (
    id BIGSERIAL,
    time TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    device_id TEXT NOT NULL,
    sensor_id TEXT NOT NULL,
    value NUMERIC(12,4),
    unit TEXT,
    quality INTEGER,  -- æ•°æ®è´¨é‡ï¼ˆ0-100ï¼‰
    location GEOGRAPHY(POINT, 4326),
    metadata JSONB,
    PRIMARY KEY (id, time)
);

-- è½¬æ¢ä¸ºHypertable (TimescaleDB)
SELECT create_hypertable('sensor_readings', 'time',
    chunk_time_interval => INTERVAL '1 day');

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_sensor_device ON sensor_readings (device_id, time DESC);
CREATE INDEX idx_sensor_location ON sensor_readings USING GIST (location);
CREATE INDEX idx_sensor_quality ON sensor_readings (quality) WHERE quality < 80;

-- è®¾å¤‡å…ƒæ•°æ®è¡¨ï¼ˆæ™®é€šè¡¨ï¼‰
CREATE TABLE devices (
    device_id TEXT PRIMARY KEY,
    device_name TEXT NOT NULL,
    device_type TEXT NOT NULL,
    location GEOGRAPHY(POINT, 4326),
    installed_at TIMESTAMPTZ,
    last_seen TIMESTAMPTZ,
    status TEXT,  -- 'online', 'offline', 'maintenance'
    metadata JSONB
);

-- ä¼ æ„Ÿå™¨é…ç½®è¡¨
CREATE TABLE sensors (
    sensor_id TEXT PRIMARY KEY,
    device_id TEXT REFERENCES devices(device_id),
    sensor_type TEXT NOT NULL,
    sensor_name TEXT,
    unit TEXT,
    min_value NUMERIC,
    max_value NUMERIC,
    calibration_date TIMESTAMPTZ,
    metadata JSONB
);
```

#### æ•°æ®è´¨é‡ä¿è¯

```sql
-- æ•°æ®éªŒè¯å‡½æ•°
CREATE OR REPLACE FUNCTION validate_sensor_reading(
    p_device_id TEXT,
    p_sensor_id TEXT,
    p_value NUMERIC
) RETURNS BOOLEAN AS $$
DECLARE
    sensor_config RECORD;
BEGIN
    -- è·å–ä¼ æ„Ÿå™¨é…ç½®
    SELECT * INTO sensor_config
    FROM sensors
    WHERE sensor_id = p_sensor_id AND device_id = p_device_id;

    IF NOT FOUND THEN
        RETURN FALSE;
    END IF;

    -- æ£€æŸ¥å€¼èŒƒå›´
    IF sensor_config.min_value IS NOT NULL AND p_value < sensor_config.min_value THEN
        RETURN FALSE;
    END IF;

    IF sensor_config.max_value IS NOT NULL AND p_value > sensor_config.max_value THEN
        RETURN FALSE;
    END IF;

    RETURN TRUE;
END;
$$ LANGUAGE plpgsql;

-- ä½¿ç”¨è§¦å‘å™¨éªŒè¯æ•°æ®
CREATE TRIGGER validate_reading
BEFORE INSERT ON sensor_readings
FOR EACH ROW
EXECUTE FUNCTION check_sensor_reading();
```

### 3.2 æ•°æ®é‡‡é›†æ¨¡å¼

#### æ¨¡å¼1ï¼šMQTTé‡‡é›†

```python
# MQTTæ•°æ®é‡‡é›†æœåŠ¡
import paho.mqtt.client as mqtt
import psycopg2
import json
from datetime import datetime

class MQTTDataCollector:
    """MQTTæ•°æ®é‡‡é›†å™¨"""

    def __init__(self, mqtt_broker, db_conn_string):
        self.mqtt_client = mqtt.Client()
        self.mqtt_client.on_connect = self.on_connect
        self.mqtt_client.on_message = self.on_message

        self.db_conn = psycopg2.connect(db_conn_string)
        self.db_cursor = self.db_conn.cursor()

        self.mqtt_client.connect(mqtt_broker, 1883, 60)

    def on_connect(self, client, userdata, flags, rc):
        """MQTTè¿æ¥å›è°ƒ"""
        if rc == 0:
            print("Connected to MQTT broker")
            # è®¢é˜…ä¼ æ„Ÿå™¨æ•°æ®ä¸»é¢˜
            client.subscribe("sensors/+/+/data")
        else:
            print(f"Failed to connect, return code {rc}")

    def on_message(self, client, userdata, msg):
        """MQTTæ¶ˆæ¯å›è°ƒ"""
        try:
            topic_parts = msg.topic.split('/')
            device_id = topic_parts[1]
            sensor_id = topic_parts[2]

            # è§£ææ¶ˆæ¯
            data = json.loads(msg.payload.decode())
            value = data['value']
            timestamp = datetime.fromisoformat(data.get('timestamp', datetime.utcnow().isoformat()))

            # æ’å…¥æ•°æ®åº“
            self.insert_sensor_data(device_id, sensor_id, value, timestamp)
        except Exception as e:
            print(f"Error processing message: {e}")

    def insert_sensor_data(self, device_id, sensor_id, value, timestamp):
        """æ’å…¥ä¼ æ„Ÿå™¨æ•°æ®"""
        # ä½¿ç”¨æ‰¹é‡æ’å…¥ä¼˜åŒ–æ€§èƒ½
        self.db_cursor.execute("""
            INSERT INTO sensor_readings (time, device_id, sensor_id, value)
            VALUES (%s, %s, %s, %s)
            ON CONFLICT DO NOTHING
        """, (timestamp, device_id, sensor_id, value))

        # æ¯100æ¡æäº¤ä¸€æ¬¡
        if self.db_cursor.rowcount % 100 == 0:
            self.db_conn.commit()

    def run(self):
        """è¿è¡Œé‡‡é›†æœåŠ¡"""
        self.mqtt_client.loop_forever()
```

#### æ¨¡å¼2ï¼šHTTP REST APIé‡‡é›†

```python
# HTTP REST APIæ•°æ®é‡‡é›†
from flask import Flask, request, jsonify
import psycopg2
from psycopg2.extras import execute_batch

app = Flask(__name__)

# æ•°æ®åº“è¿æ¥æ± 
def get_db_connection():
    return psycopg2.connect(
        host='localhost',
        database='iot_data',
        user='postgres',
        password='password'
    )

@app.route('/api/v1/sensors/<device_id>/<sensor_id>/data', methods=['POST'])
def collect_sensor_data(device_id, sensor_id):
    """æ”¶é›†ä¼ æ„Ÿå™¨æ•°æ®"""
    data = request.json

    conn = get_db_connection()
    cursor = conn.cursor()

    try:
        # æ‰¹é‡æ’å…¥
        readings = []
        for reading in data.get('readings', []):
            readings.append((
                reading['timestamp'],
                device_id,
                sensor_id,
                reading['value'],
                reading.get('quality', 100)
            ))

        execute_batch(
            cursor,
            """
            INSERT INTO sensor_readings (time, device_id, sensor_id, value, quality)
            VALUES (%s, %s, %s, %s, %s)
            """,
            readings
        )

        conn.commit()
        return jsonify({'status': 'success', 'count': len(readings)}), 200
    except Exception as e:
        conn.rollback()
        return jsonify({'error': str(e)}), 500
    finally:
        cursor.close()
        conn.close()
```

### 3.3 è¾¹ç¼˜æ•°æ®å¤„ç†

#### å®æ—¶èšåˆ

```sql
-- åˆ›å»ºå®æ—¶èšåˆè§†å›¾ï¼ˆTimescaleDBè¿ç»­èšåˆï¼‰
CREATE MATERIALIZED VIEW sensor_5min_stats
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('5 minutes', time) AS bucket,
    device_id,
    sensor_id,
    COUNT(*) AS count,
    AVG(value) AS avg_value,
    MIN(value) AS min_value,
    MAX(value) AS max_value,
    STDDEV(value) AS stddev_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY value) AS median_value
FROM sensor_readings
GROUP BY bucket, device_id, sensor_id;

-- è‡ªåŠ¨åˆ·æ–°ç­–ç•¥
SELECT add_continuous_aggregate_policy('sensor_5min_stats',
    start_offset => INTERVAL '1 hour',
    end_offset => INTERVAL '5 minutes',
    schedule_interval => INTERVAL '5 minutes');
```

#### å¼‚å¸¸æ£€æµ‹

```sql
-- å¼‚å¸¸æ£€æµ‹å‡½æ•°ï¼ˆåŸºäºç»Ÿè®¡æ–¹æ³•ï¼‰
CREATE OR REPLACE FUNCTION detect_anomalies(
    p_device_id TEXT,
    p_sensor_id TEXT,
    p_window_hours INTEGER DEFAULT 24
) RETURNS TABLE (
    time TIMESTAMPTZ,
    value NUMERIC,
    z_score NUMERIC,
    is_anomaly BOOLEAN
) AS $$
BEGIN
    RETURN QUERY
    WITH stats AS (
        SELECT
            AVG(value) AS mean,
            STDDEV(value) AS stddev
        FROM sensor_readings
        WHERE device_id = p_device_id
          AND sensor_id = p_sensor_id
          AND time > NOW() - (p_window_hours || ' hours')::INTERVAL
    ),
    readings AS (
        SELECT
            time,
            value,
            (value - stats.mean) / NULLIF(stats.stddev, 0) AS z_score
        FROM sensor_readings, stats
        WHERE device_id = p_device_id
          AND sensor_id = p_sensor_id
          AND time > NOW() - (p_window_hours || ' hours')::INTERVAL
    )
    SELECT
        readings.time,
        readings.value,
        readings.z_score,
        ABS(readings.z_score) > 3 AS is_anomaly  -- Z-score > 3è§†ä¸ºå¼‚å¸¸
    FROM readings;
END;
$$ LANGUAGE plpgsql;
```

---

## 4. è¾¹ç¼˜-äº‘ç«¯æ•°æ®åŒæ­¥

### 4.1 åŒæ­¥ç­–ç•¥

#### ç­–ç•¥é€‰æ‹©

```text
å®æ—¶åŒæ­¥:
- é€‚ç”¨: å…³é”®æ•°æ®ã€å‘Šè­¦æ•°æ®
- æ–¹æ³•: é€»è¾‘å¤åˆ¶ã€MQTTæ¨é€
- å»¶è¿Ÿ: < 1ç§’
- æˆæœ¬: é«˜ï¼ˆæŒç»­è¿æ¥ï¼‰

æ‰¹é‡åŒæ­¥:
- é€‚ç”¨: å†å²æ•°æ®ã€éå…³é”®æ•°æ®
- æ–¹æ³•: æ‰¹é‡å¯¼å‡º/å¯¼å…¥
- å»¶è¿Ÿ: åˆ†é’Ÿçº§
- æˆæœ¬: ä½

æ··åˆåŒæ­¥:
- å®æ—¶æ•°æ®: é€»è¾‘å¤åˆ¶
- èšåˆæ•°æ®: æ‰¹é‡åŒæ­¥
- å½’æ¡£æ•°æ®: å®šæœŸåŒæ­¥
```

### 4.2 é€»è¾‘å¤åˆ¶åŒæ­¥

#### è¾¹ç¼˜èŠ‚ç‚¹å‘å¸ƒ

```sql
-- åœ¨è¾¹ç¼˜èŠ‚ç‚¹åˆ›å»ºå‘å¸ƒ
CREATE PUBLICATION edge_publication FOR TABLE sensor_readings;

-- åªå‘å¸ƒç‰¹å®šæ¡ä»¶çš„æ•°æ®ï¼ˆå‡å°‘åŒæ­¥é‡ï¼‰
CREATE PUBLICATION edge_recent_publication FOR TABLE sensor_readings
WHERE (time > NOW() - INTERVAL '7 days');

-- æ·»åŠ è¿‡æ»¤å™¨ï¼ˆåªåŒæ­¥å…³é”®ä¼ æ„Ÿå™¨ï¼‰
CREATE PUBLICATION edge_critical_publication FOR TABLE sensor_readings
WHERE (device_id IN ('device_001', 'device_002'));
```

#### äº‘ç«¯èŠ‚ç‚¹è®¢é˜…

```sql
-- åœ¨äº‘ç«¯èŠ‚ç‚¹åˆ›å»ºè®¢é˜…
CREATE SUBSCRIPTION cloud_subscription
CONNECTION 'host=edge-node.example.com port=5432 user=replicator password=secret dbname=iot_data'
PUBLICATION edge_publication
WITH (
    copy_data = false,  -- ä¸å¤åˆ¶ç°æœ‰æ•°æ®ï¼ˆä»…åŒæ­¥æ–°æ•°æ®ï¼‰
    create_slot = true,
    enabled = true
);

-- ç›‘æ§è®¢é˜…çŠ¶æ€
SELECT * FROM pg_stat_subscription;

-- æŸ¥çœ‹å¤åˆ¶å»¶è¿Ÿ
SELECT
    subname,
    pg_wal_lsn_diff(pg_current_wal_lsn(), latest_end_lsn) AS replication_lag_bytes
FROM pg_stat_subscription;
```

#### åŒå‘åŒæ­¥ï¼ˆå†²çªè§£å†³ï¼‰

```sql
-- ä½¿ç”¨Last-Write-Winsç­–ç•¥
CREATE OR REPLACE FUNCTION resolve_conflict(
    edge_value NUMERIC,
    cloud_value NUMERIC,
    edge_time TIMESTAMPTZ,
    cloud_time TIMESTAMPTZ
) RETURNS NUMERIC AS $$
BEGIN
    -- æ¯”è¾ƒæ—¶é—´æˆ³ï¼Œè¿”å›æœ€æ–°çš„å€¼
    IF edge_time > cloud_time THEN
        RETURN edge_value;
    ELSE
        RETURN cloud_value;
    END IF;
END;
$$ LANGUAGE plpgsql;

-- ä½¿ç”¨è§¦å‘å™¨å¤„ç†å†²çª
CREATE OR REPLACE FUNCTION handle_sync_conflict()
RETURNS TRIGGER AS $$
DECLARE
    existing_record RECORD;
BEGIN
    -- æ£€æŸ¥æ˜¯å¦å­˜åœ¨å†²çª
    SELECT * INTO existing_record
    FROM sensor_readings
    WHERE device_id = NEW.device_id
      AND sensor_id = NEW.sensor_id
      AND time = NEW.time;

    IF FOUND THEN
        -- è§£å†³å†²çªï¼ˆä½¿ç”¨Last-Write-Winsï¼‰
        NEW.value = resolve_conflict(
            NEW.value, existing_record.value,
            NEW.time, existing_record.time
        );
        RETURN NEW;
    END IF;

    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER sync_conflict_resolver
BEFORE INSERT ON sensor_readings
FOR EACH ROW
EXECUTE FUNCTION handle_sync_conflict();
```

### 4.3 MQTTæ¶ˆæ¯åŒæ­¥

#### 4.3.1 è¾¹ç¼˜èŠ‚ç‚¹å‘å¸ƒ

```python
# è¾¹ç¼˜èŠ‚ç‚¹MQTTå‘å¸ƒ
import paho.mqtt.client as mqtt
import psycopg2
import json

class EdgeSyncPublisher:
    """è¾¹ç¼˜èŠ‚ç‚¹åŒæ­¥å‘å¸ƒå™¨"""

    def __init__(self, mqtt_broker, db_conn_string):
        self.mqtt_client = mqtt.Client()
        self.mqtt_client.connect(mqtt_broker, 1883, 60)

        self.db_conn = psycopg2.connect(db_conn_string)

    def sync_recent_data(self):
        """åŒæ­¥æœ€è¿‘çš„æ•°æ®åˆ°äº‘ç«¯"""
        cursor = self.db_conn.cursor()

        # è·å–æœªåŒæ­¥çš„æ•°æ®
        cursor.execute("""
            SELECT device_id, sensor_id, time, value, quality
            FROM sensor_readings
            WHERE sync_status IS NULL
              AND time > NOW() - INTERVAL '1 hour'
            ORDER BY time
            LIMIT 1000
        """)

        for row in cursor.fetchall():
            device_id, sensor_id, time, value, quality = row

            # å‘å¸ƒåˆ°MQTT
            topic = f"sync/{device_id}/{sensor_id}"
            payload = json.dumps({
                'time': time.isoformat(),
                'value': float(value),
                'quality': quality
            })

            self.mqtt_client.publish(topic, payload, qos=1)

            # æ ‡è®°ä¸ºå·²åŒæ­¥
            cursor.execute("""
                UPDATE sensor_readings
                SET sync_status = 'synced'
                WHERE device_id = %s AND sensor_id = %s AND time = %s
            """, (device_id, sensor_id, time))

        self.db_conn.commit()
```

#### 4.3.2 äº‘ç«¯èŠ‚ç‚¹è®¢é˜…

```python
# äº‘ç«¯èŠ‚ç‚¹MQTTè®¢é˜…
class CloudSyncSubscriber:
    """äº‘ç«¯èŠ‚ç‚¹åŒæ­¥è®¢é˜…å™¨"""

    def __init__(self, mqtt_broker, db_conn_string):
        self.mqtt_client = mqtt.Client()
        self.mqtt_client.on_connect = self.on_connect
        self.mqtt_client.on_message = self.on_message

        self.db_conn = psycopg2.connect(db_conn_string)
        self.mqtt_client.connect(mqtt_broker, 1883, 60)

    def on_connect(self, client, userdata, flags, rc):
        client.subscribe("sync/+/+/+")

    def on_message(self, client, userdata, msg):
        topic_parts = msg.topic.split('/')
        device_id = topic_parts[1]
        sensor_id = topic_parts[2]

        data = json.loads(msg.payload.decode())

        # æ’å…¥åˆ°äº‘ç«¯æ•°æ®åº“
        cursor = self.db_conn.cursor()
        cursor.execute("""
            INSERT INTO sensor_readings (time, device_id, sensor_id, value, quality, source)
            VALUES (%s, %s, %s, %s, %s, 'edge')
            ON CONFLICT (device_id, sensor_id, time) DO UPDATE
            SET value = EXCLUDED.value,
                quality = EXCLUDED.quality
        """, (
            data['time'],
            device_id,
            sensor_id,
            data['value'],
            data['quality']
        ))

        self.db_conn.commit()
```

---

## 5. ç¦»çº¿åœºæ™¯å¤„ç†

### 5.1 ç¦»çº¿æ•°æ®å­˜å‚¨

#### æœ¬åœ°é˜Ÿåˆ—ç®¡ç†

```python
# ç¦»çº¿æ•°æ®é˜Ÿåˆ—ç®¡ç†å™¨
import queue
import threading
import psycopg2
from datetime import datetime

class OfflineDataQueue:
    """ç¦»çº¿æ•°æ®é˜Ÿåˆ—"""

    def __init__(self, db_conn_string, max_queue_size=10000):
        self.queue = queue.Queue(maxsize=max_queue_size)
        self.db_conn_string = db_conn_string
        self.is_online = False
        self.flush_thread = None

    def start(self):
        """å¯åŠ¨é˜Ÿåˆ—å¤„ç†å™¨"""
        self.flush_thread = threading.Thread(target=self._flush_queue, daemon=True)
        self.flush_thread.start()

    def add_data(self, device_id, sensor_id, value, timestamp):
        """æ·»åŠ æ•°æ®åˆ°é˜Ÿåˆ—"""
        try:
            self.queue.put_nowait({
                'device_id': device_id,
                'sensor_id': sensor_id,
                'value': value,
                'timestamp': timestamp
            })
        except queue.Full:
            # é˜Ÿåˆ—æ»¡ï¼Œä¸¢å¼ƒæœ€æ—§çš„æ•°æ®
            try:
                self.queue.get_nowait()
                self.queue.put_nowait({
                    'device_id': device_id,
                    'sensor_id': sensor_id,
                    'value': value,
                    'timestamp': timestamp
                })
            except queue.Empty:
                pass

    def _flush_queue(self):
        """åˆ·æ–°é˜Ÿåˆ—åˆ°æ•°æ®åº“"""
        batch = []

        while True:
            try:
                # ä»é˜Ÿåˆ—è·å–æ•°æ®
                item = self.queue.get(timeout=1)
                batch.append(item)

                # æ‰¹é‡å¤„ç†ï¼ˆæ¯100æ¡æˆ–1ç§’ï¼‰
                if len(batch) >= 100:
                    self._write_batch(batch)
                    batch = []
            except queue.Empty:
                # é˜Ÿåˆ—ç©ºï¼Œå¤„ç†å‰©ä½™æ‰¹æ¬¡
                if batch:
                    self._write_batch(batch)
                    batch = []
                continue

    def _write_batch(self, batch):
        """æ‰¹é‡å†™å…¥æ•°æ®åº“"""
        if not self.is_online:
            # ç¦»çº¿æ—¶ï¼Œä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
            self._save_to_file(batch)
            return

        try:
            conn = psycopg2.connect(self.db_conn_string)
            cursor = conn.cursor()

            values = [
                (item['timestamp'], item['device_id'], item['sensor_id'], item['value'])
                for item in batch
            ]

            execute_batch(
                cursor,
                """
                INSERT INTO sensor_readings (time, device_id, sensor_id, value)
                VALUES (%s, %s, %s, %s)
                """,
                values
            )

            conn.commit()
            cursor.close()
            conn.close()
        except Exception as e:
            print(f"Error writing batch: {e}")
            # å†™å…¥å¤±è´¥ï¼Œä¿å­˜åˆ°æ–‡ä»¶
            self._save_to_file(batch)

    def _save_to_file(self, batch):
        """ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰"""
        import json
        filename = f"offline_queue_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w') as f:
            json.dump(batch, f)
```

### 5.2 æ•°æ®é˜Ÿåˆ—ç®¡ç†

#### é˜Ÿåˆ—æŒä¹…åŒ–

```sql
-- ä½¿ç”¨PostgreSQLè¡¨ä½œä¸ºæŒä¹…åŒ–é˜Ÿåˆ—
CREATE TABLE data_queue (
    id BIGSERIAL PRIMARY KEY,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    processed_at TIMESTAMPTZ,
    status TEXT NOT NULL DEFAULT 'pending',  -- 'pending', 'processing', 'done', 'failed'
    device_id TEXT NOT NULL,
    sensor_id TEXT NOT NULL,
    time TIMESTAMPTZ NOT NULL,
    value NUMERIC(12,4),
    retry_count INTEGER DEFAULT 0,
    error_message TEXT
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_queue_status ON data_queue (status, created_at)
WHERE status IN ('pending', 'processing');

-- è·å–å¾…å¤„ç†æ•°æ®
CREATE OR REPLACE FUNCTION get_pending_queue_items(batch_size INTEGER DEFAULT 100)
RETURNS TABLE (
    id BIGINT,
    device_id TEXT,
    sensor_id TEXT,
    time TIMESTAMPTZ,
    value NUMERIC
) AS $$
BEGIN
    -- æ ‡è®°ä¸ºå¤„ç†ä¸­
    UPDATE data_queue
    SET status = 'processing', processed_at = NOW()
    WHERE id IN (
        SELECT id FROM data_queue
        WHERE status = 'pending'
        ORDER BY created_at
        LIMIT batch_size
        FOR UPDATE SKIP LOCKED
    );

    -- è¿”å›å¤„ç†ä¸­çš„æ•°æ®
    RETURN QUERY
    SELECT q.id, q.device_id, q.sensor_id, q.time, q.value
    FROM data_queue q
    WHERE q.status = 'processing'
    ORDER BY q.created_at;
END;
$$ LANGUAGE plpgsql;
```

---

## 6. æ•°æ®å‹ç¼©ä¸ä¼ è¾“ä¼˜åŒ–

### 6.1 æ•°æ®å‹ç¼©ç­–ç•¥

#### TimescaleDBå‹ç¼©

```sql
-- å¯ç”¨å‹ç¼©ï¼ˆ7å¤©å‰çš„æ•°æ®è‡ªåŠ¨å‹ç¼©ï¼‰
SELECT add_compression_policy('sensor_readings', INTERVAL '7 days');

-- æ‰‹åŠ¨å‹ç¼©
SELECT compress_chunk(chunk) FROM timescaledb_information.chunks
WHERE hypertable_name = 'sensor_readings'
  AND is_compressed = false
  AND range_end < NOW() - INTERVAL '7 days';

-- æŸ¥çœ‹å‹ç¼©æ•ˆæœ
SELECT
    hypertable_name,
    total_chunks,
    number_compressed_chunks,
    pg_size_pretty(before_compression_total_bytes) AS before_size,
    pg_size_pretty(after_compression_total_bytes) AS after_size,
    ROUND((1.0 - after_compression_total_bytes::NUMERIC / before_compression_total_bytes) * 100, 2) AS compression_ratio
FROM timescaledb_information.compressed_hypertable_stats;
```

#### å¯¼å‡ºæ—¶å‹ç¼©

```bash
# ä½¿ç”¨pg_dumpå‹ç¼©å¯¼å‡º
pg_dump -h edge-node -U postgres -d iot_data \
  --table=sensor_readings \
  --compress=9 \
  -F c \
  -f sensor_readings_compressed.dump

# ä¼ è¾“å‹ç¼©æ–‡ä»¶
scp sensor_readings_compressed.dump cloud-server:/backup/

# å¯¼å…¥
pg_restore -h cloud-server -U postgres -d iot_data \
  -F c \
  sensor_readings_compressed.dump
```

### 6.2 å¢é‡ä¼ è¾“

#### åŸºäºæ—¶é—´æˆ³çš„å¢é‡åŒæ­¥

```sql
-- è·å–éœ€è¦åŒæ­¥çš„æ•°æ®ï¼ˆå¢é‡ï¼‰
CREATE OR REPLACE FUNCTION get_incremental_data(
    p_last_sync_time TIMESTAMPTZ,
    p_batch_size INTEGER DEFAULT 1000
) RETURNS TABLE (
    device_id TEXT,
    sensor_id TEXT,
    time TIMESTAMPTZ,
    value NUMERIC
) AS $$
BEGIN
    RETURN QUERY
    SELECT r.device_id, r.sensor_id, r.time, r.value
    FROM sensor_readings r
    WHERE r.time > p_last_sync_time
      AND r.sync_status IS NULL
    ORDER BY r.time
    LIMIT p_batch_size;
END;
$$ LANGUAGE plpgsql;

-- æ ‡è®°å·²åŒæ­¥
CREATE OR REPLACE FUNCTION mark_as_synced(
    p_device_id TEXT,
    p_sensor_id TEXT,
    p_time TIMESTAMPTZ
) RETURNS void AS $$
BEGIN
    UPDATE sensor_readings
    SET sync_status = 'synced',
        sync_time = NOW()
    WHERE device_id = p_device_id
      AND sensor_id = p_sensor_id
      AND time = p_time;
END;
$$ LANGUAGE plpgsql;
```

---

## 7. è¾¹ç¼˜èŠ‚ç‚¹ç®¡ç†

### 7.1 èŠ‚ç‚¹ç›‘æ§

#### ç›‘æ§æŒ‡æ ‡

```python
# è¾¹ç¼˜èŠ‚ç‚¹ç›‘æ§
import psycopg2
import time
from datetime import datetime

class EdgeNodeMonitor:
    """è¾¹ç¼˜èŠ‚ç‚¹ç›‘æ§å™¨"""

    def __init__(self, db_conn_string):
        self.db_conn_string = db_conn_string

    def get_node_health(self):
        """è·å–èŠ‚ç‚¹å¥åº·çŠ¶æ€"""
        conn = psycopg2.connect(self.db_conn_string)
        cursor = conn.cursor()

        health = {
            'timestamp': datetime.utcnow().isoformat(),
            'database': {},
            'data': {},
            'sync': {}
        }

        # æ•°æ®åº“çŠ¶æ€
        cursor.execute("SELECT version();")
        health['database']['version'] = cursor.fetchone()[0]

        cursor.execute("SELECT pg_database_size(current_database());")
        health['database']['size_bytes'] = cursor.fetchone()[0]

        cursor.execute("SELECT count(*) FROM pg_stat_activity;")
        health['database']['connections'] = cursor.fetchone()[0]

        # æ•°æ®ç»Ÿè®¡
        cursor.execute("""
            SELECT
                COUNT(*) as total_readings,
                COUNT(DISTINCT device_id) as device_count,
                MAX(time) as latest_reading,
                MIN(time) as earliest_reading
            FROM sensor_readings
        """)
        row = cursor.fetchone()
        health['data'] = {
            'total_readings': row[0],
            'device_count': row[1],
            'latest_reading': row[2].isoformat() if row[2] else None,
            'earliest_reading': row[3].isoformat() if row[3] else None
        }

        # åŒæ­¥çŠ¶æ€
        cursor.execute("""
            SELECT
                COUNT(*) FILTER (WHERE sync_status IS NULL) as unsynced_count,
                COUNT(*) FILTER (WHERE sync_status = 'synced') as synced_count,
                MAX(sync_time) as last_sync_time
            FROM sensor_readings
        """)
        row = cursor.fetchone()
        health['sync'] = {
            'unsynced_count': row[0],
            'synced_count': row[1],
            'last_sync_time': row[2].isoformat() if row[2] else None
        }

        cursor.close()
        conn.close()

        return health
```

---

## 8. å®è·µæ¡ˆä¾‹

### 8.1 å·¥ä¸šIoTè¾¹ç¼˜è®¡ç®—æ¡ˆä¾‹

#### åœºæ™¯æè¿°

```text
åœºæ™¯: åˆ¶é€ å·¥å‚IoTç›‘æ§ç³»ç»Ÿ

éœ€æ±‚:
- 1000+ä¼ æ„Ÿå™¨è®¾å¤‡
- æ¯ç§’10,000+æ•°æ®ç‚¹
- è¾¹ç¼˜å¤„ç†ï¼šå®æ—¶å‘Šè­¦ã€æœ¬åœ°å­˜å‚¨
- äº‘ç«¯åˆ†æï¼šå†å²æ•°æ®åˆ†æã€é¢„æµ‹ç»´æŠ¤

æ¶æ„:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ä¼ æ„Ÿå™¨è®¾å¤‡      â”‚ (Modbus/OPC-UA)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è¾¹ç¼˜ç½‘å…³        â”‚ (Raspberry Pi / å·¥æ§æœº)
â”‚  - MQTT Broker  â”‚
â”‚  - æ•°æ®é‡‡é›†æœåŠ¡   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è¾¹ç¼˜æ•°æ®åº“      â”‚ (TimescaleDB on è¾¹ç¼˜æœåŠ¡å™¨)
â”‚  - å®æ—¶æ•°æ®å­˜å‚¨   â”‚
â”‚  - å®æ—¶èšåˆ      â”‚
â”‚  - å¼‚å¸¸æ£€æµ‹      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ (é€»è¾‘å¤åˆ¶ + MQTT)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  äº‘ç«¯æ•°æ®åº“      â”‚ (PostgreSQLé›†ç¾¤)
â”‚  - å†å²æ•°æ®å½’æ¡£   â”‚
â”‚  - è·¨å·¥å‚åˆ†æ     â”‚
â”‚  - é¢„æµ‹åˆ†æ      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š å‚è€ƒèµ„æº

1. **TimescaleDBå®˜æ–¹æ–‡æ¡£**: <https://docs.timescale.com/>
2. **PostgreSQLé€»è¾‘å¤åˆ¶**: <https://www.postgresql.org/docs/current/logical-replication.html>
3. **MQTTåè®®**: <https://mqtt.org/>

---

## ğŸ“ æ›´æ–°æ—¥å¿—

- **v1.0** (2025-01): åˆå§‹ç‰ˆæœ¬
  - è¾¹ç¼˜è®¡ç®—æ¶æ„æ¦‚è¿°
  - è¾¹ç¼˜æ•°æ®åº“éƒ¨ç½²
  - IoTæ•°æ®é‡‡é›†ä¸å¤„ç†
  - è¾¹ç¼˜-äº‘ç«¯æ•°æ®åŒæ­¥
  - ç¦»çº¿åœºæ™¯å¤„ç†
  - æ•°æ®å‹ç¼©ä¸ä¼ è¾“ä¼˜åŒ–
  - è¾¹ç¼˜èŠ‚ç‚¹ç®¡ç†
  - å®è·µæ¡ˆä¾‹

---

**çŠ¶æ€**: âœ… **æ–‡æ¡£å®Œæˆ** | [è¿”å›ç›®å½•](./README.md)
