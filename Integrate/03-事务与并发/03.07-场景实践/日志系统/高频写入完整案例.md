---

> **📋 文档来源**: `MVCC-ACID-CAP\03-场景实践\日志系统\高频写入完整案例.md`
> **📅 复制日期**: 2025-12-22
> **⚠️ 注意**: 本文档为复制版本，原文件保持不变

---

# 日志系统高频写入完整案例

> **文档编号**: SCENARIO-LOG-001
> **主题**: 日志系统高频写入
> **版本**: PostgreSQL 17 & 18

---

## 📑 目录

- [日志系统高频写入完整案例](#日志系统高频写入完整案例)
  - [📑 目录](#-目录)
  - [📋 概述](#-概述)
  - [🔍 第一部分：业务场景分析](#-第一部分业务场景分析)
    - [1.1 业务需求](#11-业务需求)
      - [核心需求](#核心需求)
      - [性能要求](#性能要求)
      - [存储要求](#存储要求)
    - [1.2 写入场景](#12-写入场景)
      - [高频写入](#高频写入)
      - [批量写入](#批量写入)
      - [表膨胀问题](#表膨胀问题)
  - [🚀 第二部分：数据库设计](#-第二部分数据库设计)
    - [2.1 分区表设计](#21-分区表设计)
      - [按时间分区](#按时间分区)
      - [分区策略](#分区策略)
      - [自动分区管理](#自动分区管理)
    - [2.2 表结构设计](#22-表结构设计)
      - [日志表设计](#日志表设计)
      - [索引设计](#索引设计)
    - [2.3 MVCC优化设计](#23-mvcc优化设计)
      - [fillfactor设置](#fillfactor设置)
      - [VACUUM策略](#vacuum策略)
  - [📊 第三部分：写入实现方案](#-第三部分写入实现方案)
    - [3.1 方案1：批量插入](#31-方案1批量插入)
      - [实现代码](#实现代码)
      - [性能分析](#性能分析)
    - [3.2 方案2：COPY命令](#32-方案2copy命令)
      - [实现代码](#实现代码-1)
      - [性能分析](#性能分析-1)
    - [3.3 方案](#33-方案)
      - [实现代码](#实现代码-2)
      - [性能分析](#性能分析-2)
  - [🔧 第四部分：多语言实现](#-第四部分多语言实现)
    - [4.1 Python实现](#41-python实现)
    - [4.2 Java实现](#42-java实现)
    - [4.3 Go实现](#43-go实现)
  - [📈 第五部分：性能优化](#-第五部分性能优化)
    - [5.1 COPY命令优化](#51-copy命令优化)
    - [5.2 连接池优化](#52-连接池优化)
    - [5.3 批量大小优化](#53-批量大小优化)
  - [🎯 第六部分：归档和清理](#-第六部分归档和清理)
    - [6.1 归档策略](#61-归档策略)
    - [6.2 清理策略](#62-清理策略)
  - [📝 总结](#-总结)
    - [核心方案](#核心方案)
    - [最佳实践](#最佳实践)
    - [性能指标](#性能指标)

---

## 📋 概述

日志系统高频写入是典型的时序数据场景，需要支持高并发写入、高效查询和自动归档。
本文档提供完整的解决方案，涵盖分区表设计、批量写入、性能优化和归档策略。

---

## 🔍 第一部分：业务场景分析

### 1.1 业务需求

#### 核心需求

```sql
-- 业务场景：应用日志写入
-- 要求：
-- 1. 高并发写入（10000+ QPS）
-- 2. 低延迟（<10ms）
-- 3. 数据持久化
-- 4. 按时间查询
-- 5. 自动归档和清理
```

#### 性能要求

- **写入QPS**: 10000+
- **写入延迟**: P99 < 10ms
- **查询性能**: 时间范围查询 < 100ms
- **存储成本**: 优化存储空间

#### 存储要求

- **数据保留**: 30天热数据，1年冷数据
- **存储压缩**: 支持压缩存储
- **自动归档**: 自动归档旧数据

### 1.2 写入场景

#### 高频写入

```sql
-- 场景：10000个应用实例同时写入日志
-- 问题：如何保证写入性能？
```

#### 批量写入

```sql
-- 场景：应用批量收集日志后写入
-- 问题：如何优化批量写入性能？
```

#### 表膨胀问题

```sql
-- 问题：高频写入导致表快速膨胀
-- 解决：分区表 + 定期清理
```

---

## 🚀 第二部分：数据库设计

### 2.1 分区表设计

#### 按时间分区

```sql
-- 日志表（分区表）
CREATE TABLE app_logs (
    id BIGSERIAL,
    app_name VARCHAR(100) NOT NULL,
    level VARCHAR(20) NOT NULL,
    message TEXT NOT NULL,
    metadata JSONB,
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    PRIMARY KEY (id, created_at)
) PARTITION BY RANGE (created_at);

-- 创建分区（按月）
CREATE TABLE app_logs_2024_01 PARTITION OF app_logs
    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE app_logs_2024_02 PARTITION OF app_logs
    FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');

-- ... 其他月份分区
```

#### 分区策略

```sql
-- 分区策略：
-- 1. 按月分区（适合中等规模）
-- 2. 按周分区（适合大规模）
-- 3. 按日分区（适合超大规模）

-- 按日分区示例
CREATE TABLE app_logs_2024_01_01 PARTITION OF app_logs
    FOR VALUES FROM ('2024-01-01') TO ('2024-01-02');
```

#### 自动分区管理

```sql
-- 自动创建分区函数
CREATE OR REPLACE FUNCTION create_partition_if_not_exists(
    table_name TEXT,
    start_date DATE,
    end_date DATE
) RETURNS VOID AS $$
DECLARE
    partition_name TEXT;
BEGIN
    partition_name := table_name || '_' || to_char(start_date, 'YYYY_MM_DD');

    EXECUTE format('
        CREATE TABLE IF NOT EXISTS %I PARTITION OF %I
        FOR VALUES FROM (%L) TO (%L)',
        partition_name, table_name, start_date, end_date
    );
END;
$$ LANGUAGE plpgsql;

-- 自动创建下个月的分区
SELECT create_partition_if_not_exists(
    'app_logs',
    date_trunc('month', NOW() + interval '1 month')::DATE,
    (date_trunc('month', NOW() + interval '2 month')::DATE)
);
```

### 2.2 表结构设计

#### 日志表设计

```sql
-- 日志表（优化设计）
CREATE TABLE app_logs (
    id BIGSERIAL,
    app_name VARCHAR(100) NOT NULL,
    level VARCHAR(20) NOT NULL,
    message TEXT NOT NULL,
    metadata JSONB,
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    PRIMARY KEY (id, created_at)
) PARTITION BY RANGE (created_at);

-- 索引（只在分区上创建）
CREATE INDEX idx_app_logs_app_name ON app_logs(app_name, created_at);
CREATE INDEX idx_app_logs_level ON app_logs(level, created_at);
CREATE INDEX idx_app_logs_created_at ON app_logs(created_at);

-- GIN索引（JSONB查询）
CREATE INDEX idx_app_logs_metadata ON app_logs USING GIN(metadata);
```

#### 索引设计

```sql
-- 分区索引策略：
-- 1. 主键索引：自动创建
-- 2. 查询索引：按需创建
-- 3. 避免过多索引（影响写入性能）

-- 只创建必要的索引
CREATE INDEX idx_app_logs_app_time ON app_logs(app_name, created_at DESC);
```

### 2.3 MVCC优化设计

#### fillfactor设置

```sql
-- 日志表：只插入，不更新
ALTER TABLE app_logs SET (fillfactor = 100);

-- 分区表设置
ALTER TABLE app_logs_2024_01 SET (fillfactor = 100);
```

#### VACUUM策略

```sql
-- 日志表：较少VACUUM（只插入）
ALTER TABLE app_logs SET (
    autovacuum_vacuum_scale_factor = 0.2,
    autovacuum_analyze_scale_factor = 0.1
);

-- 活跃分区：更频繁ANALYZE
ALTER TABLE app_logs_2024_12 SET (
    autovacuum_analyze_scale_factor = 0.05
);
```

---

## 📊 第三部分：写入实现方案

### 3.1 方案1：批量插入

#### 实现代码

```sql
-- 批量插入（推荐）
INSERT INTO app_logs (app_name, level, message, metadata, created_at)
VALUES
    ('app1', 'INFO', 'message1', '{}', NOW()),
    ('app2', 'ERROR', 'message2', '{}', NOW()),
    -- ... 更多记录
ON CONFLICT DO NOTHING;
```

#### 性能分析

```sql
-- 性能特点：
-- 优点：
-- 1. 事务开销小
-- 2. 性能好
--
-- 缺点：
-- 1. 批量大小有限制
--
-- 性能指标：
-- QPS: 5000-10000（批量1000条）
-- P99延迟: 5-10ms
```

### 3.2 方案2：COPY命令

#### 实现代码

```sql
-- COPY命令（性能最优）
COPY app_logs (app_name, level, message, metadata, created_at)
FROM STDIN WITH (FORMAT CSV);
```

#### 性能分析

```sql
-- 性能特点：
-- 优点：
-- 1. 性能最优
-- 2. 支持大批量
--
-- 缺点：
-- 1. 需要文件或流
--
-- 性能指标：
-- QPS: 10000-20000（批量10000条）
-- P99延迟: 2-5ms
```

### 3.3 方案

#### 实现代码

```sql
-- 时间范围查询（分区裁剪）
SELECT * FROM app_logs
WHERE created_at >= '2024-01-01'
  AND created_at < '2024-02-01'
  AND app_name = 'myapp'
ORDER BY created_at DESC
LIMIT 100;
```

#### 性能分析

```sql
-- 性能特点：
-- 优点：
-- 1. 分区裁剪
-- 2. 查询快速
--
-- 性能指标：
-- 查询时间: < 100ms（单分区）
```

---

## 🔧 第四部分：多语言实现

### 4.1 Python实现

```python
import psycopg2
from psycopg2.extras import execute_batch

def batch_insert_logs(logs):
    """批量插入日志"""
    conn = connection_pool.getconn()
    try:
        cur = conn.cursor()

        # 批量插入
        execute_batch(cur, """
            INSERT INTO app_logs (app_name, level, message, metadata, created_at)
            VALUES (%s, %s, %s, %s, %s)
        """, logs, page_size=1000)

        conn.commit()
        return True

    except Exception as e:
        conn.rollback()
        raise
    finally:
        cur.close()
        connection_pool.putconn(conn)

# COPY命令实现
def copy_insert_logs(logs):
    """使用COPY命令插入日志（性能最优）"""
    conn = connection_pool.getconn()
    try:
        cur = conn.cursor()

        # 使用COPY
        from io import StringIO
        f = StringIO()
        for log in logs:
            f.write(f"{log[0]}\t{log[1]}\t{log[2]}\t{log[3]}\t{log[4]}\n")
        f.seek(0)

        cur.copy_from(f, 'app_logs', columns=('app_name', 'level', 'message', 'metadata', 'created_at'))

        conn.commit()
        return True

    except Exception as e:
        conn.rollback()
        raise
    finally:
        cur.close()
        connection_pool.putconn(conn)
```

### 4.2 Java实现

```java
public void batchInsertLogs(List<Log> logs) {
    String sql = "INSERT INTO app_logs (app_name, level, message, metadata, created_at) VALUES (?, ?, ?, ?, ?)";

    try (Connection conn = dataSource.getConnection();
         PreparedStatement stmt = conn.prepareStatement(sql)) {

        conn.setAutoCommit(false);

        for (Log log : logs) {
            stmt.setString(1, log.getAppName());
            stmt.setString(2, log.getLevel());
            stmt.setString(3, log.getMessage());
            stmt.setString(4, log.getMetadata());
            stmt.setTimestamp(5, log.getCreatedAt());
            stmt.addBatch();

            if (stmt.getParameterMetaData().getParameterCount() % 1000 == 0) {
                stmt.executeBatch();
            }
        }

        stmt.executeBatch();
        conn.commit();
    }
}
```

### 4.3 Go实现

```go
func BatchInsertLogs(ctx context.Context, pool *pgxpool.Pool, logs []Log) error {
    batch := &pgx.Batch{}

    for _, log := range logs {
        batch.Queue(
            "INSERT INTO app_logs (app_name, level, message, metadata, created_at) VALUES ($1, $2, $3, $4, $5)",
            log.AppName, log.Level, log.Message, log.Metadata, log.CreatedAt,
        )
    }

    return pool.SendBatch(ctx, batch).Close()
}
```

---

## 📈 第五部分：性能优化

### 5.1 COPY命令优化

```python
# COPY命令优化（性能最优）
def optimized_copy_insert(logs):
    """优化的COPY插入"""
    conn = connection_pool.getconn()
    try:
        cur = conn.cursor()

        # 使用COPY FROM
        with cur.copy("COPY app_logs (app_name, level, message, metadata, created_at) FROM STDIN") as copy:
            for log in logs:
                copy.write_row([log.app_name, log.level, log.message, log.metadata, log.created_at])

        conn.commit()
        return True

    except Exception as e:
        conn.rollback()
        raise
    finally:
        cur.close()
        connection_pool.putconn(conn)
```

### 5.2 连接池优化

```python
# 日志系统连接池配置（高并发）
connection_pool = psycopg2.pool.ThreadedConnectionPool(
    minconn=20,
    maxconn=100,  # 高并发写入
    host="localhost",
    database="logs",
    user="postgres",
    password="password",
)
```

### 5.3 批量大小优化

```python
# 批量大小优化
BATCH_SIZE = 1000  # 推荐批量大小

def insert_logs_optimized(logs):
    """优化的批量插入"""
    for i in range(0, len(logs), BATCH_SIZE):
        batch = logs[i:i+BATCH_SIZE]
        batch_insert_logs(batch)
```

---

## 🎯 第六部分：归档和清理

### 6.1 归档策略

```sql
-- 归档函数
CREATE OR REPLACE FUNCTION archive_old_logs(archive_date DATE)
RETURNS VOID AS $$
DECLARE
    partition_name TEXT;
BEGIN
    -- 归档30天前的数据
    partition_name := 'app_logs_' || to_char(archive_date, 'YYYY_MM');

    -- 导出到文件
    EXECUTE format('COPY (SELECT * FROM %I) TO ''/archive/%s.csv'' CSV',
                   partition_name, partition_name);

    -- 删除分区
    EXECUTE format('DROP TABLE IF EXISTS %I', partition_name);
END;
$$ LANGUAGE plpgsql;

-- 定期归档
SELECT archive_old_logs(NOW() - interval '30 days');
```

### 6.2 清理策略

```sql
-- 清理策略：
-- 1. 30天内：保留在数据库中
-- 2. 30-365天：归档到文件
-- 3. 365天以上：删除

-- 自动清理函数
CREATE OR REPLACE FUNCTION auto_cleanup_logs()
RETURNS VOID AS $$
BEGIN
    -- 归档30天前的数据
    PERFORM archive_old_logs(NOW() - interval '30 days');

    -- 删除365天前的归档文件
    -- （需要外部脚本实现）
END;
$$ LANGUAGE plpgsql;
```

---

## 📝 总结

### 核心方案

1. **推荐方案**：分区表 + COPY命令
   - 性能最优
   - 支持高并发
   - 自动归档

2. **备选方案**：分区表 + 批量插入
   - 实现简单
   - 性能良好

### 最佳实践

1. **分区设计**：按时间分区，自动管理
2. **写入优化**：使用COPY命令或批量插入
3. **索引优化**：只创建必要索引
4. **归档策略**：自动归档旧数据
5. **监控告警**：监控写入性能和存储空间

### 性能指标

- **写入QPS**: 10000-20000（COPY命令）
- **P99延迟**: 2-5ms
- **查询性能**: < 100ms（单分区）
- **存储优化**: 分区表 + 自动归档

PostgreSQL 17/18的MVCC机制在日志系统场景下表现优异，通过分区表设计和批量写入优化，可以实现高性能、高可靠性的日志系统。
