# PostgreSQL å¼ºåŒ–å­¦ä¹ åŸºç¡€ç®—æ³•å®Œæ•´æŒ‡å—

> **åˆ›å»ºæ—¥æœŸ**: 2025å¹´1æœˆ
> **æŠ€æœ¯æ ˆ**: PostgreSQL 17+/18+ | æœºå™¨å­¦ä¹  | å¼ºåŒ–å­¦ä¹  | Q-learning
> **éš¾åº¦çº§åˆ«**: â­â­â­â­â­ (ä¸“å®¶çº§)
> **å‚è€ƒæ ‡å‡†**: Reinforcement Learning (Sutton & Barto), Q-Learning, Policy Gradient

---

## ğŸ“‹ ç›®å½•

- [PostgreSQL å¼ºåŒ–å­¦ä¹ åŸºç¡€ç®—æ³•å®Œæ•´æŒ‡å—](#postgresql-å¼ºåŒ–å­¦ä¹ åŸºç¡€ç®—æ³•å®Œæ•´æŒ‡å—)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [å¼ºåŒ–å­¦ä¹ æ¦‚è¿°](#å¼ºåŒ–å­¦ä¹ æ¦‚è¿°)
    - [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
  - [1. Q-Learning](#1-q-learning)
    - [1.1 Q-LearningåŸç†](#11-q-learningåŸç†)
    - [1.2 Q-Learningå®ç°](#12-q-learningå®ç°)
    - [1.3 Îµ-è´ªå©ªç­–ç•¥](#13-Îµ-è´ªå©ªç­–ç•¥)
  - [2. ç­–ç•¥æ¢¯åº¦](#2-ç­–ç•¥æ¢¯åº¦)
    - [2.1 ç­–ç•¥æ¢¯åº¦åŸç†](#21-ç­–ç•¥æ¢¯åº¦åŸç†)
    - [2.2 REINFORCEç®—æ³•](#22-reinforceç®—æ³•)
  - [3. ä»·å€¼å‡½æ•°](#3-ä»·å€¼å‡½æ•°)
    - [3.1 çŠ¶æ€ä»·å€¼å‡½æ•°](#31-çŠ¶æ€ä»·å€¼å‡½æ•°)
    - [3.2 åŠ¨ä½œä»·å€¼å‡½æ•°](#32-åŠ¨ä½œä»·å€¼å‡½æ•°)
    - [3.3 ä»·å€¼å‡½æ•°ä¼°è®¡](#33-ä»·å€¼å‡½æ•°ä¼°è®¡)
  - [4. å®é™…åº”ç”¨æ¡ˆä¾‹](#4-å®é™…åº”ç”¨æ¡ˆä¾‹)
    - [4.1 æ¨èç³»ç»Ÿ](#41-æ¨èç³»ç»Ÿ)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)
  - [ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®](#-æ€§èƒ½ä¼˜åŒ–å»ºè®®)
  - [ğŸ¯ æœ€ä½³å®è·µ](#-æœ€ä½³å®è·µ)

---

## å¼ºåŒ–å­¦ä¹ æ¦‚è¿°

**å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰**é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œæ˜¯æœºå™¨å­¦ä¹ çš„é‡è¦åˆ†æ”¯ã€‚

### æ ¸å¿ƒæ¦‚å¿µ

**é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰**ï¼š$(S, A, P, R, \gamma)$

- $S$ï¼šçŠ¶æ€ç©ºé—´
- $A$ï¼šåŠ¨ä½œç©ºé—´
- $P$ï¼šçŠ¶æ€è½¬ç§»æ¦‚ç‡
- $R$ï¼šå¥–åŠ±å‡½æ•°
- $\gamma$ï¼šæŠ˜æ‰£å› å­ï¼ˆ0 â‰¤ Î³ â‰¤ 1ï¼‰

**ç›®æ ‡**ï¼šå­¦ä¹ ç­–ç•¥ $\pi: S \rightarrow A$ï¼Œæœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ï¼š
$$G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

---

## 1. Q-Learning

### 1.1 Q-LearningåŸç†

**Q-Learning**å­¦ä¹ åŠ¨ä½œä»·å€¼å‡½æ•° $Q(s,a)$ï¼Œè¡¨ç¤ºåœ¨çŠ¶æ€ $s$ æ‰§è¡ŒåŠ¨ä½œ $a$ çš„æœŸæœ›ç´¯ç§¯å¥–åŠ±ã€‚

**Qå€¼æ›´æ–°è§„åˆ™**ï¼ˆTDå­¦ä¹ ï¼‰ï¼š
$$Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s', a') - Q(s,a)]$$

å…¶ä¸­ï¼š

- $\alpha$ï¼šå­¦ä¹ ç‡ï¼ˆ0 < Î± â‰¤ 1ï¼‰
- $r$ï¼šå³æ—¶å¥–åŠ±
- $\gamma$ï¼šæŠ˜æ‰£å› å­
- $s'$ï¼šä¸‹ä¸€çŠ¶æ€

### 1.2 Q-Learningå®ç°

```sql
-- Q-Learningç¯å¢ƒå‡†å¤‡ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
DO $$
BEGIN
    BEGIN
        IF EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'q_table') THEN
            DROP TABLE q_table, q_learning_transitions CASCADE;
        END IF;

        -- Qå€¼è¡¨
        CREATE TABLE q_table (
            state_id INTEGER NOT NULL,
            action_id INTEGER NOT NULL,
            q_value NUMERIC DEFAULT 0,
            visit_count INTEGER DEFAULT 0,
            last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            PRIMARY KEY (state_id, action_id)
        );

        -- çŠ¶æ€è½¬ç§»è®°å½•
        CREATE TABLE q_learning_transitions (
            transition_id SERIAL PRIMARY KEY,
            state_id INTEGER NOT NULL,
            action_id INTEGER NOT NULL,
            next_state_id INTEGER NOT NULL,
            reward NUMERIC NOT NULL,
            transition_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );

        -- åˆå§‹åŒ–Qè¡¨ï¼ˆç¤ºä¾‹ï¼š4ä¸ªçŠ¶æ€ï¼Œ2ä¸ªåŠ¨ä½œï¼‰
        INSERT INTO q_table (state_id, action_id) VALUES
            (1, 1), (1, 2),
            (2, 1), (2, 2),
            (3, 1), (3, 2),
            (4, 1), (4, 2);

        -- æ’å…¥ç¤ºä¾‹è½¬ç§»è®°å½•
        INSERT INTO q_learning_transitions (state_id, action_id, next_state_id, reward) VALUES
            (1, 1, 2, 10),
            (1, 2, 3, 5),
            (2, 1, 4, 20),
            (2, 2, 1, -5);

        RAISE NOTICE 'Q-Learningç¯å¢ƒåˆ›å»ºæˆåŠŸ';
    EXCEPTION
        WHEN OTHERS THEN
            RAISE EXCEPTION 'åˆ›å»ºè¡¨å¤±è´¥: %', SQLERRM;
    END;
END $$;

-- Qå€¼æ›´æ–°å‡½æ•°
CREATE OR REPLACE FUNCTION update_q_value(
    p_state_id INTEGER,
    p_action_id INTEGER,
    p_reward NUMERIC,
    p_next_state_id INTEGER,
    p_learning_rate NUMERIC DEFAULT 0.1,
    p_discount_factor NUMERIC DEFAULT 0.9
)
RETURNS NUMERIC AS $$
DECLARE
    current_q NUMERIC;
    max_next_q NUMERIC;
    new_q NUMERIC;
BEGIN
    -- è·å–å½“å‰Qå€¼
    SELECT q_value INTO current_q
    FROM q_table
    WHERE state_id = p_state_id AND action_id = p_action_id;

    IF current_q IS NULL THEN
        current_q := 0;
    END IF;

    -- è·å–ä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§Qå€¼
    SELECT COALESCE(MAX(q_value), 0) INTO max_next_q
    FROM q_table
    WHERE state_id = p_next_state_id;

    -- Qå€¼æ›´æ–°
    new_q := current_q + p_learning_rate * (p_reward + p_discount_factor * max_next_q - current_q);

    -- æ›´æ–°Qè¡¨
    UPDATE q_table
    SET
        q_value = new_q,
        visit_count = visit_count + 1,
        last_updated = CURRENT_TIMESTAMP
    WHERE state_id = p_state_id AND action_id = p_action_id;

    RETURN new_q;
END;
$$ LANGUAGE plpgsql;

-- æ‰§è¡ŒQå€¼æ›´æ–°
SELECT
    state_id,
    action_id,
    next_state_id,
    reward,
    update_q_value(state_id, action_id, reward, next_state_id) AS updated_q_value
FROM q_learning_transitions
ORDER BY transition_id;

-- æŸ¥çœ‹æ›´æ–°åçš„Qè¡¨
SELECT
    state_id,
    action_id,
    ROUND(q_value::numeric, 4) AS q_value,
    visit_count,
    last_updated
FROM q_table
ORDER BY state_id, action_id;
```

### 1.3 Îµ-è´ªå©ªç­–ç•¥

**Îµ-è´ªå©ªç­–ç•¥**å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ã€‚

```sql
-- Îµ-è´ªå©ªç­–ç•¥ï¼šé€‰æ‹©åŠ¨ä½œ
CREATE OR REPLACE FUNCTION epsilon_greedy_action(
    p_state_id INTEGER,
    p_epsilon NUMERIC DEFAULT 0.1
)
RETURNS INTEGER AS $$
DECLARE
    best_action INTEGER;
    random_action INTEGER;
    action_choice INTEGER;
BEGIN
    -- ä»¥æ¦‚ç‡(1-Îµ)é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
    IF RANDOM() > p_epsilon THEN
        -- é€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
        SELECT action_id INTO best_action
        FROM q_table
        WHERE state_id = p_state_id
        ORDER BY q_value DESC
        LIMIT 1;

        RETURN best_action;
    ELSE
        -- ä»¥æ¦‚ç‡Îµéšæœºé€‰æ‹©åŠ¨ä½œï¼ˆæ¢ç´¢ï¼‰
        SELECT action_id INTO random_action
        FROM q_table
        WHERE state_id = p_state_id
        ORDER BY RANDOM()
        LIMIT 1;

        RETURN random_action;
    END IF;
END;
$$ LANGUAGE plpgsql;

-- ä½¿ç”¨Îµ-è´ªå©ªç­–ç•¥
SELECT
    state_id,
    epsilon_greedy_action(state_id, 0.1) AS selected_action,
    (SELECT q_value FROM q_table
     WHERE state_id = q.state_id
     AND action_id = epsilon_greedy_action(q.state_id, 0.1)) AS q_value
FROM (SELECT DISTINCT state_id FROM q_table) q;
```

---

## 2. ç­–ç•¥æ¢¯åº¦

### 2.1 ç­–ç•¥æ¢¯åº¦åŸç†

**ç­–ç•¥æ¢¯åº¦æ–¹æ³•**ç›´æ¥ä¼˜åŒ–ç­–ç•¥å‚æ•°ã€‚

**ç­–ç•¥æ¢¯åº¦å®šç†**ï¼š
$$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[\sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot G_t\right]$$

å…¶ä¸­ $G_t$ æ˜¯ç´¯ç§¯å¥–åŠ±ã€‚

### 2.2 REINFORCEç®—æ³•

**REINFORCE**æ˜¯åŸºæœ¬çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ã€‚

```sql
-- ç­–ç•¥è¡¨ï¼ˆç®€åŒ–ï¼šä½¿ç”¨softmaxç­–ç•¥ï¼‰
CREATE TABLE IF NOT EXISTS policy_table (
    state_id INTEGER NOT NULL,
    action_id INTEGER NOT NULL,
    action_probability NUMERIC NOT NULL,
    PRIMARY KEY (state_id, action_id)
);

-- ç­–ç•¥æ¢¯åº¦æ›´æ–°ï¼ˆç®€åŒ–å®ç°ï¼‰
WITH episode_data AS (
    SELECT
        state_id,
        action_id,
        reward,
        -- ç´¯ç§¯å¥–åŠ±ï¼ˆç®€åŒ–ï¼šä½¿ç”¨æŠ˜æ‰£å¥–åŠ±ï¼‰
        SUM(reward * POWER(0.9, ROW_NUMBER() OVER (ORDER BY transition_id DESC) - 1))
        OVER (PARTITION BY state_id ORDER BY transition_id) AS return_value
    FROM q_learning_transitions
),
policy_gradient AS (
    SELECT
        state_id,
        action_id,
        return_value,
        -- ç­–ç•¥æ¢¯åº¦æ›´æ–°ï¼ˆç®€åŒ–ï¼‰
        return_value * 0.01 AS gradient_update
    FROM episode_data
)
SELECT
    state_id,
    action_id,
    ROUND(return_value::numeric, 4) AS return_value,
    ROUND(gradient_update::numeric, 4) AS gradient_update
FROM policy_gradient
ORDER BY state_id, action_id;
```

---

## 3. ä»·å€¼å‡½æ•°

### 3.1 çŠ¶æ€ä»·å€¼å‡½æ•°

**çŠ¶æ€ä»·å€¼å‡½æ•°** $V^\pi(s)$ è¡¨ç¤ºåœ¨ç­–ç•¥ $\pi$ ä¸‹çŠ¶æ€ $s$ çš„æœŸæœ›ç´¯ç§¯å¥–åŠ±ã€‚

$$V^\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]$$

### 3.2 åŠ¨ä½œä»·å€¼å‡½æ•°

**åŠ¨ä½œä»·å€¼å‡½æ•°** $Q^\pi(s,a)$ å·²åœ¨Q-Learningä¸­ä»‹ç»ã€‚

### 3.3 ä»·å€¼å‡½æ•°ä¼°è®¡

```sql
-- ä»·å€¼å‡½æ•°ä¼°è®¡ï¼ˆä½¿ç”¨è’™ç‰¹å¡æ´›æ–¹æ³•ï¼‰
WITH monte_carlo_returns AS (
    SELECT
        state_id,
        action_id,
        -- è’™ç‰¹å¡æ´›å›æŠ¥ï¼ˆç®€åŒ–ï¼šä½¿ç”¨å¹³å‡å¥–åŠ±ï¼‰
        AVG(reward) OVER (
            PARTITION BY state_id, action_id
            ORDER BY transition_time
        ) AS mc_return,
        COUNT(*) OVER (PARTITION BY state_id, action_id) AS sample_count
    FROM q_learning_transitions
)
SELECT
    state_id,
    action_id,
    ROUND(mc_return::numeric, 4) AS estimated_value,
    sample_count,
    CASE
        WHEN sample_count >= 10 THEN 'Reliable estimate'
        ELSE 'Need more samples'
    END AS estimate_quality
FROM monte_carlo_returns
ORDER BY state_id, action_id;
```

---

## 4. PostgreSQL 18 å¹¶è¡Œå¼ºåŒ–å­¦ä¹ å¢å¼º

**PostgreSQL 18** æ˜¾è‘—å¢å¼ºäº†å¹¶è¡Œå¼ºåŒ–å­¦ä¹ è®¡ç®—èƒ½åŠ›ï¼Œæ”¯æŒå¹¶è¡Œæ‰§è¡ŒQ-Learningã€ç­–ç•¥æ¢¯åº¦å’Œä»·å€¼å‡½æ•°ä¼°è®¡ï¼Œå¤§å¹…æå‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ€§èƒ½ã€‚

### 4.1 å¹¶è¡Œå¼ºåŒ–å­¦ä¹ åŸç†

PostgreSQL 18 çš„å¹¶è¡Œå¼ºåŒ–å­¦ä¹ é€šè¿‡ä»¥ä¸‹æ–¹å¼å®ç°ï¼š

1. **å¹¶è¡Œæ‰«æ**ï¼šå¤šä¸ªå·¥ä½œè¿›ç¨‹å¹¶è¡Œæ‰«æçŠ¶æ€è½¬ç§»æ•°æ®
2. **å¹¶è¡ŒQå€¼æ›´æ–°**ï¼šæ¯ä¸ªå·¥ä½œè¿›ç¨‹ç‹¬ç«‹æ›´æ–°Qå€¼
3. **å¹¶è¡Œç­–ç•¥æ¢¯åº¦**ï¼šå¹¶è¡Œæ‰§è¡Œç­–ç•¥æ¢¯åº¦è®¡ç®—
4. **ç»“æœåˆå¹¶**ï¼šä¸»è¿›ç¨‹åˆå¹¶æ‰€æœ‰å·¥ä½œè¿›ç¨‹çš„è®¡ç®—ç»“æœ

### 4.2 å¹¶è¡ŒQ-Learning

```sql
-- PostgreSQL 18 å¹¶è¡ŒQ-Learningï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'q_learning_transitions') THEN
            RAISE WARNING 'è¡¨ q_learning_transitions ä¸å­˜åœ¨ï¼Œæ— æ³•æ‰§è¡Œå¹¶è¡ŒQ-Learning';
            RETURN;
        END IF;
        RAISE NOTICE 'å¼€å§‹æ‰§è¡ŒPostgreSQL 18å¹¶è¡ŒQ-Learning';
    EXCEPTION
        WHEN OTHERS THEN
            RAISE WARNING 'å¹¶è¡ŒQ-Learningå‡†å¤‡å¤±è´¥: %', SQLERRM;
            RAISE;
    END;
END $$;

SET max_parallel_workers_per_gather = 4;
SET parallel_setup_cost = 0;

-- å¹¶è¡ŒQå€¼æ›´æ–°ï¼šTDå­¦ä¹ 
EXPLAIN (ANALYZE, BUFFERS, TIMING, VERBOSE)
WITH q_updates AS (
    SELECT
        t.state_id,
        t.action_id,
        t.reward,
        t.next_state_id,
        COALESCE(q1.q_value, 0) AS current_q,
        COALESCE(MAX(q2.q_value) OVER (PARTITION BY t.next_state_id), 0) AS max_next_q,
        0.1 AS learning_rate,
        0.9 AS discount_factor
    FROM q_learning_transitions t
    LEFT JOIN q_table q1 ON t.state_id = q1.state_id AND t.action_id = q1.action_id
    LEFT JOIN q_table q2 ON t.next_state_id = q2.state_id
)
SELECT
    state_id,
    action_id,
    ROUND((current_q + learning_rate * (reward + discount_factor * max_next_q - current_q))::numeric, 4) AS updated_q_value
FROM q_updates
ORDER BY state_id, action_id;
```

### 4.3 å¹¶è¡Œç­–ç•¥æ¢¯åº¦

```sql
-- PostgreSQL 18 å¹¶è¡Œç­–ç•¥æ¢¯åº¦ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œæ€§èƒ½æµ‹è¯•ï¼‰
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'q_learning_transitions') THEN
            RAISE WARNING 'è¡¨ q_learning_transitions ä¸å­˜åœ¨ï¼Œæ— æ³•æ‰§è¡Œå¹¶è¡Œç­–ç•¥æ¢¯åº¦';
            RETURN;
        END IF;
        RAISE NOTICE 'å¼€å§‹æ‰§è¡ŒPostgreSQL 18å¹¶è¡Œç­–ç•¥æ¢¯åº¦';
    EXCEPTION
        WHEN OTHERS THEN
            RAISE WARNING 'å¹¶è¡Œç­–ç•¥æ¢¯åº¦å‡†å¤‡å¤±è´¥: %', SQLERRM;
            RAISE;
    END;
END $$;

SET max_parallel_workers_per_gather = 4;
SET parallel_setup_cost = 0;

-- å¹¶è¡Œç­–ç•¥æ¢¯åº¦ï¼šREINFORCEç®—æ³•
EXPLAIN (ANALYZE, BUFFERS, TIMING, VERBOSE)
WITH episode_returns AS (
    SELECT
        state_id,
        action_id,
        reward,
        SUM(reward * POWER(0.9, ROW_NUMBER() OVER (PARTITION BY state_id ORDER BY transition_id DESC) - 1))
            OVER (PARTITION BY state_id ORDER BY transition_id) AS return_value
    FROM q_learning_transitions
)
SELECT
    state_id,
    action_id,
    ROUND(return_value::numeric, 4) AS policy_gradient_return,
    ROUND(return_value * 0.01::numeric, 4) AS gradient_update
FROM episode_returns
ORDER BY state_id, action_id;
```

---

## 5. å®é™…åº”ç”¨æ¡ˆä¾‹

### 4.1 æ¨èç³»ç»Ÿ

```sql
-- æ¨èç³»ç»Ÿï¼šä½¿ç”¨Q-Learningä¼˜åŒ–æ¨èç­–ç•¥
WITH recommendation_q_table AS (
    SELECT
        user_id AS state_id,
        item_id AS action_id,
        -- Qå€¼ï¼šç”¨æˆ·å¯¹ç‰©å“çš„åå¥½
        preference_score AS q_value
    FROM user_item_preferences
),
optimal_recommendations AS (
    SELECT
        state_id AS user_id,
        action_id AS item_id,
        q_value AS preference_score,
        ROW_NUMBER() OVER (PARTITION BY state_id ORDER BY q_value DESC) AS rank
    FROM recommendation_q_table
)
SELECT
    user_id,
    item_id,
    ROUND(preference_score::numeric, 4) AS preference_score,
    rank
FROM optimal_recommendations
WHERE rank <= 10  -- Top-10æ¨è
ORDER BY user_id, rank;
```

---

## ğŸ“š å‚è€ƒèµ„æº

1. **Sutton, R.S., Barto, A.G. (2018)**: "Reinforcement Learning: An Introduction", 2nd Edition
2. **SzepesvÃ¡ri, C. (2010)**: "Algorithms for Reinforcement Learning"
3. **Agarwal, R., et al. (2020)**: "Reinforcement Learning: Theory and Algorithms"

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **ç»éªŒå›æ”¾**ï¼šä½¿ç”¨ç»éªŒç¼“å†²åŒºå­˜å‚¨è½¬ç§»
2. **ç›®æ ‡ç½‘ç»œ**ï¼šä½¿ç”¨ç›®æ ‡Qç½‘ç»œç¨³å®šè®­ç»ƒ
3. **å‡½æ•°è¿‘ä¼¼**ï¼šä½¿ç”¨ç¥ç»ç½‘ç»œè¿‘ä¼¼Qå‡½æ•°
4. **å¹¶è¡Œæ¢ç´¢**ï¼šå¹¶è¡Œæ‰§è¡Œå¤šä¸ªepisode

---

## ğŸ¯ æœ€ä½³å®è·µ

1. **è¶…å‚æ•°è°ƒä¼˜**ï¼šè°ƒæ•´å­¦ä¹ ç‡å’ŒæŠ˜æ‰£å› å­
2. **æ¢ç´¢ç­–ç•¥**ï¼šå¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨
3. **æ”¶æ•›åˆ¤æ–­**ï¼šç›‘æ§Qå€¼å˜åŒ–åˆ¤æ–­æ”¶æ•›
4. **æ€§èƒ½è¯„ä¼°**ï¼šä½¿ç”¨å¹³å‡å¥–åŠ±è¯„ä¼°ç­–ç•¥è´¨é‡

---

**æœ€åæ›´æ–°**: 2025å¹´1æœˆ
**æ–‡æ¡£çŠ¶æ€**: âœ… å·²å®Œæˆ
