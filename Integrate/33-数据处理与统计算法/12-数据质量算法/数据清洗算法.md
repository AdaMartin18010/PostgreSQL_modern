# PostgreSQL 数据清洗算法完整指南

> **创建日期**: 2025年1月
> **技术栈**: PostgreSQL 17+/18+ | 数据清洗 | 数据质量
> **难度级别**: ⭐⭐⭐ (中级)

---

## 📋 目录

- [PostgreSQL 数据清洗算法完整指南](#postgresql-数据清洗算法完整指南)
  - [📋 目录](#-目录)
  - [数据清洗概述](#数据清洗概述)
    - [理论基础](#理论基础)
      - [数据质量维度](#数据质量维度)
      - [缺失值机制](#缺失值机制)
      - [异常值理论](#异常值理论)
    - [核心算法](#核心算法)
  - [1. 缺失值处理](#1-缺失值处理)
    - [1.1 缺失值检测原理](#11-缺失值检测原理)
      - [缺失值表示](#缺失值表示)
      - [缺失模式分析](#缺失模式分析)
    - [1.2 缺失值检测实现](#12-缺失值检测实现)
    - [1.3 缺失值填充方法](#13-缺失值填充方法)
      - [填充方法分类](#填充方法分类)
      - [填充方法选择](#填充方法选择)
    - [1.4 缺失值填充实现](#14-缺失值填充实现)
  - [2. 异常值处理](#2-异常值处理)
    - [2.1 异常值检测原理](#21-异常值检测原理)
      - [异常值检测方法](#异常值检测方法)
      - [IQR方法详解](#iqr方法详解)
      - [Z-score方法详解](#z-score方法详解)
    - [2.2 异常值检测实现](#22-异常值检测实现)
  - [3. 重复数据处理](#3-重复数据处理)
    - [3.1 重复数据检测原理](#31-重复数据检测原理)
      - [重复类型](#重复类型)
      - [检测方法](#检测方法)
      - [编辑距离](#编辑距离)
    - [3.2 重复数据检测实现](#32-重复数据检测实现)
  - [4. 数据格式标准化](#4-数据格式标准化)
    - [4.1 格式统一](#41-格式统一)
    - [4.2 数据类型转换](#42-数据类型转换)
  - [5. 实际应用案例](#5-实际应用案例)
    - [5.1 综合数据清洗](#51-综合数据清洗)
    - [5.2 客户数据清洗](#52-客户数据清洗)
    - [5.3 销售数据清洗](#53-销售数据清洗)
    - [5.4 文本数据清洗](#54-文本数据清洗)
    - [5.5 数据质量监控](#55-数据质量监控)
  - [6. 算法性能对比与优化](#6-算法性能对比与优化)
    - [6.1 清洗方法对比](#61-清洗方法对比)
    - [6.2 性能优化建议](#62-性能优化建议)
    - [6.3 常见问题与解决方案](#63-常见问题与解决方案)
  - [7. 最佳实践](#7-最佳实践)
    - [7.1 数据清洗流程](#71-数据清洗流程)
    - [7.2 缺失值处理策略](#72-缺失值处理策略)
    - [7.3 异常值处理策略](#73-异常值处理策略)
    - [7.4 重复数据处理策略](#74-重复数据处理策略)
    - [7.5 SQL实现注意事项](#75-sql实现注意事项)
  - [📚 参考资源](#-参考资源)
    - [学术文献](#学术文献)
    - [在线资源](#在线资源)
    - [相关算法](#相关算法)

---

## 数据清洗概述

**数据清洗（Data Cleaning）**是数据预处理的核心环节，用于识别和修复数据质量问题，提高数据可靠性和可用性。数据清洗是数据科学和数据分析的基础，直接影响后续分析和建模的质量。

### 理论基础

#### 数据质量维度

数据质量可以从以下维度评估：

1. **完整性（Completeness）**：数据是否完整，缺失值比例
2. **准确性（Accuracy）**：数据是否正确，是否符合业务规则
3. **一致性（Consistency）**：数据在不同来源是否一致
4. **时效性（Timeliness）**：数据是否及时更新
5. **有效性（Validity）**：数据是否符合预定义的格式和范围
6. **唯一性（Uniqueness）**：是否存在重复记录

#### 缺失值机制

**缺失值类型**（Little & Rubin分类）：

1. **完全随机缺失（MCAR, Missing Completely At Random）**：
   - 缺失与观测值和未观测值都无关
   - 删除缺失值不会引入偏差

2. **随机缺失（MAR, Missing At Random）**：
   - 缺失与观测值相关，但与未观测值无关
   - 可以使用观测值预测缺失值

3. **非随机缺失（MNAR, Missing Not At Random）**：
   - 缺失与未观测值相关
   - 最难处理，可能引入偏差

#### 异常值理论

**异常值（Outlier）定义**：与数据集中其他观测值显著不同的值。

**异常值类型**：

- **点异常**：单个数据点异常
- **上下文异常**：在特定上下文中异常
- **集体异常**：一组数据点集体异常

### 核心算法

| 算法 | 用途 | 复杂度 | 优点 | 缺点 |
|------|------|--------|------|------|
| **缺失值检测** | 识别空值 | $O(n)$ | 简单快速 | 只能检测显式NULL |
| **IQR异常检测** | 识别离群值 | $O(n \log n)$ | 稳健、不假设分布 | 只适用于单变量 |
| **Z-score异常检测** | 识别离群值 | $O(n)$ | 快速、标准化 | 假设正态分布 |
| **重复检测（哈希）** | 识别重复记录 | $O(n)$ | 快速 | 可能哈希冲突 |
| **重复检测（排序）** | 识别重复记录 | $O(n \log n)$ | 准确 | 需要排序 |
| **编辑距离** | 模糊重复检测 | $O(n^2)$ | 处理拼写错误 | 计算复杂度高 |

---

## 1. 缺失值处理

### 1.1 缺失值检测原理

**缺失值检测**是数据清洗的第一步，需要准确识别数据中的缺失值。

#### 缺失值表示

在PostgreSQL中，缺失值通常表示为：

- **NULL**：显式缺失值
- **空字符串**：可能表示缺失（需要业务判断）
- **特殊值**：如-1、999等（需要业务规则判断）

#### 缺失模式分析

**缺失模式（Missing Pattern）**：

- **单调缺失**：如果变量$X_j$缺失，则$X_{j+1}, ..., X_p$也缺失
- **任意缺失**：缺失值随机分布
- **块缺失**：某些变量组合经常一起缺失

### 1.2 缺失值检测实现

```sql
-- 创建测试数据表（带错误处理）
DO $$
BEGIN
    BEGIN
        IF EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'data_quality_test') THEN
            RAISE WARNING '表 data_quality_test 已存在，先删除';
            DROP TABLE data_quality_test CASCADE;
        END IF;

        CREATE TABLE data_quality_test (
            id SERIAL PRIMARY KEY,
            name VARCHAR(100),
            email VARCHAR(100),
            age INTEGER,
            salary NUMERIC(10, 2),
            created_at TIMESTAMP
        );

        INSERT INTO data_quality_test (name, email, age, salary, created_at) VALUES
            ('John Doe', 'john@example.com', 30, 50000.00, '2024-01-01'),
            ('Jane Smith', NULL, 25, 45000.00, '2024-01-02'),
            (NULL, 'bob@example.com', NULL, 60000.00, '2024-01-03'),
            ('Alice Brown', 'alice@example.com', 35, NULL, '2024-01-04'),
            ('Charlie Wilson', 'charlie@example.com', 28, 55000.00, NULL);

        RAISE NOTICE '表 data_quality_test 创建成功';
    EXCEPTION
        WHEN duplicate_table THEN
            RAISE WARNING '表 data_quality_test 已存在';
        WHEN OTHERS THEN
            RAISE EXCEPTION '创建表失败: %', SQLERRM;
    END;
END $$;

-- 缺失值检测（带错误处理和性能测试）
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'data_quality_test') THEN
            RAISE WARNING '表 data_quality_test 不存在，无法检测缺失值';
            RETURN;
        END IF;
        RAISE NOTICE '开始检测缺失值';
    EXCEPTION
        WHEN OTHERS THEN
            RAISE WARNING '缺失值检测准备失败: %', SQLERRM;
            RAISE;
    END;
END $$;

-- 统计各字段的缺失值
SELECT
    'name' AS column_name,
    COUNT(*) AS total_rows,
    COUNT(name) AS non_null_count,
    COUNT(*) - COUNT(name) AS null_count,
    ROUND(((COUNT(*) - COUNT(name))::numeric / COUNT(*) * 100)::numeric, 2) AS null_percentage
FROM data_quality_test
UNION ALL
SELECT
    'email' AS column_name,
    COUNT(*) AS total_rows,
    COUNT(email) AS non_null_count,
    COUNT(*) - COUNT(email) AS null_count,
    ROUND(((COUNT(*) - COUNT(email))::numeric / COUNT(*) * 100)::numeric, 2) AS null_percentage
FROM data_quality_test
UNION ALL
SELECT
    'age' AS column_name,
    COUNT(*) AS total_rows,
    COUNT(age) AS non_null_count,
    COUNT(*) - COUNT(age) AS null_count,
    ROUND(((COUNT(*) - COUNT(age))::numeric / COUNT(*) * 100)::numeric, 2) AS null_percentage
FROM data_quality_test
UNION ALL
SELECT
    'salary' AS column_name,
    COUNT(*) AS total_rows,
    COUNT(salary) AS non_null_count,
    COUNT(*) - COUNT(salary) AS null_count,
    ROUND(((COUNT(*) - COUNT(salary))::numeric / COUNT(*) * 100)::numeric, 2) AS null_percentage
FROM data_quality_test
ORDER BY null_percentage DESC;

-- 性能测试
EXPLAIN (ANALYZE, BUFFERS, TIMING, VERBOSE)
SELECT
    COUNT(*) - COUNT(name) AS null_count
FROM data_quality_test;
```

### 1.3 缺失值填充方法

**缺失值填充（Imputation）**使用合理值填充缺失数据，有多种方法。

#### 填充方法分类

1. **删除法**：
   - **列表删除**：删除包含缺失值的整行
   - **成对删除**：只删除分析中涉及的缺失值
   - **优点**：简单、无偏差（MCAR时）
   - **缺点**：丢失信息、样本量减少

2. **单值填充**：
   - **均值/中位数/众数填充**：使用统计量填充
   - **前向填充/后向填充**：使用前一个/后一个值填充
   - **常数填充**：使用固定值填充
   - **优点**：简单、快速
   - **缺点**：可能引入偏差、低估方差

3. **插值法**：
   - **线性插值**：使用线性函数插值
   - **样条插值**：使用样条函数插值
   - **优点**：保持数据趋势
   - **缺点**：假设数据连续

4. **模型填充**：
   - **回归填充**：使用回归模型预测
   - **KNN填充**：使用K近邻填充
   - **优点**：考虑变量关系
   - **缺点**：计算复杂、可能过拟合

#### 填充方法选择

| 数据类型 | 推荐方法 | 原因 |
|---------|---------|------|
| **连续数值** | 均值/中位数 | 保持中心趋势 |
| **分类数据** | 众数 | 最常见值 |
| **时间序列** | 前向/后向填充 | 保持时间连续性 |
| **有序数据** | 中位数 | 稳健性 |
| **有相关性** | 回归/KNN | 利用变量关系 |

### 1.4 缺失值填充实现

```sql
-- 缺失值填充（带错误处理和性能测试）
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'data_quality_test') THEN
            RAISE WARNING '表 data_quality_test 不存在，无法填充缺失值';
            RETURN;
        END IF;
        RAISE NOTICE '开始填充缺失值';
    EXCEPTION
        WHEN OTHERS THEN
            RAISE WARNING '缺失值填充准备失败: %', SQLERRM;
            RAISE;
    END;
END $$;

-- 使用均值填充数值字段
WITH filled_data AS (
    SELECT
        id,
        COALESCE(name, 'Unknown') AS name,
        COALESCE(email, 'noemail@example.com') AS email,
        COALESCE(age, (SELECT ROUND(AVG(age)) FROM data_quality_test WHERE age IS NOT NULL)) AS age,
        COALESCE(salary, (SELECT AVG(salary) FROM data_quality_test WHERE salary IS NOT NULL)) AS salary,
        COALESCE(created_at, NOW()) AS created_at
    FROM data_quality_test
)
SELECT
    id,
    name,
    email,
    age,
    ROUND(salary::numeric, 2) AS salary,
    created_at
FROM filled_data
ORDER BY id;

-- 性能测试
EXPLAIN (ANALYZE, BUFFERS, TIMING, VERBOSE)
SELECT
    COALESCE(age, (SELECT AVG(age) FROM data_quality_test WHERE age IS NOT NULL)) AS age
FROM data_quality_test;
```

---

## 2. 异常值处理

### 2.1 异常值检测原理

**异常值检测（Outlier Detection）**使用统计方法识别与数据集中其他观测值显著不同的值。

#### 异常值检测方法

1. **基于统计的方法**：
   - **Z-score方法**：$z = \frac{x - \mu}{\sigma}$，通常$|z| > 3$为异常
   - **IQR方法**：使用四分位距，$x < Q_1 - 1.5 \times IQR$ 或 $x > Q_3 + 1.5 \times IQR$
   - **Grubbs检验**：假设检验方法

2. **基于距离的方法**：
   - **KNN距离**：计算到K近邻的距离
   - **局部异常因子（LOF）**：相对密度方法

3. **基于密度的方法**：
   - **DBSCAN**：基于密度的聚类
   - **Isolation Forest**：隔离森林

#### IQR方法详解

**四分位距（IQR）**：
$$IQR = Q_3 - Q_1$$

**异常值边界**：

- **下界**：$Q_1 - 1.5 \times IQR$
- **上界**：$Q_3 + 1.5 \times IQR$

**极端异常值边界**（可选）：

- **下界**：$Q_1 - 3 \times IQR$
- **上界**：$Q_3 + 3 \times IQR$

#### Z-score方法详解

**Z-score标准化**：
$$z = \frac{x - \mu}{\sigma}$$

**异常值判断**：

- **温和异常值**：$|z| > 2$
- **极端异常值**：$|z| > 3$

**注意**：Z-score方法假设数据近似正态分布。

### 2.2 异常值检测实现

```sql
-- 异常值检测（带错误处理和性能测试）
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'data_quality_test') THEN
            RAISE WARNING '表 data_quality_test 不存在，无法检测异常值';
            RETURN;
        END IF;
        RAISE NOTICE '开始检测异常值';
    EXCEPTION
        WHEN OTHERS THEN
            RAISE WARNING '异常值检测准备失败: %', SQLERRM;
            RAISE;
    END;
END $$;

-- 使用IQR方法检测异常值
WITH salary_stats AS (
    SELECT
        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY salary) AS q1,
        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY salary) AS q3,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY salary) AS median,
        AVG(salary) AS mean,
        STDDEV(salary) AS stddev
    FROM data_quality_test
    WHERE salary IS NOT NULL
),
outlier_detection AS (
    SELECT
        dqt.id,
        dqt.salary,
        ss.mean,
        ss.median,
        ss.q1,
        ss.q3,
        ss.q3 - ss.q1 AS iqr,
        ss.q1 - 1.5 * (ss.q3 - ss.q1) AS lower_bound,
        ss.q3 + 1.5 * (ss.q3 - ss.q1) AS upper_bound,
        CASE
            WHEN dqt.salary < ss.q1 - 1.5 * (ss.q3 - ss.q1) THEN 'Lower Outlier'
            WHEN dqt.salary > ss.q3 + 1.5 * (ss.q3 - ss.q1) THEN 'Upper Outlier'
            ELSE 'Normal'
        END AS outlier_status
    FROM data_quality_test dqt
    CROSS JOIN salary_stats ss
    WHERE dqt.salary IS NOT NULL
)
SELECT
    id,
    ROUND(salary::numeric, 2) AS salary,
    ROUND(mean::numeric, 2) AS mean,
    ROUND(median::numeric, 2) AS median,
    ROUND(lower_bound::numeric, 2) AS lower_bound,
    ROUND(upper_bound::numeric, 2) AS upper_bound,
    outlier_status
FROM outlier_detection
ORDER BY salary DESC;

-- 性能测试
EXPLAIN (ANALYZE, BUFFERS, TIMING, VERBOSE)
SELECT
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY salary) AS q3
FROM data_quality_test
WHERE salary IS NOT NULL;
```

---

## 3. 重复数据处理

### 3.1 重复数据检测原理

**重复数据检测（Duplicate Detection）**识别数据集中重复或近似重复的记录。

#### 重复类型

1. **完全重复**：所有字段完全相同
2. **部分重复**：关键字段相同（如email、身份证号）
3. **近似重复**：由于拼写错误、格式差异导致的重复

#### 检测方法

1. **精确匹配**：
   - **哈希方法**：计算记录哈希值
   - **排序方法**：排序后比较相邻记录
   - **复杂度**：$O(n)$（哈希）或 $O(n \log n)$（排序）

2. **模糊匹配**：
   - **编辑距离（Levenshtein）**：计算字符串相似度
   - **Jaccard相似度**：集合相似度
   - **余弦相似度**：向量相似度
   - **复杂度**：$O(n^2)$ 或更高

#### 编辑距离

**Levenshtein距离**：将一个字符串转换为另一个字符串所需的最少单字符编辑操作数。

**操作**：

- **插入**：插入一个字符
- **删除**：删除一个字符
- **替换**：替换一个字符

**动态规划算法**：

$$
d[i][j] = \begin{cases}
\max(i, j) & \text{if } \min(i, j) = 0 \\
\min\begin{cases}
d[i-1][j] + 1 \\
d[i][j-1] + 1 \\
d[i-1][j-1] + (s1[i] \neq s2[j])
\end{cases} & \text{otherwise}
\end{cases}
$$

### 3.2 重复数据检测实现

```sql
-- 重复数据检测（带错误处理和性能测试）
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'data_quality_test') THEN
            RAISE WARNING '表 data_quality_test 不存在，无法检测重复数据';
            RETURN;
        END IF;
        RAISE NOTICE '开始检测重复数据';
    EXCEPTION
        WHEN OTHERS THEN
            RAISE WARNING '重复数据检测准备失败: %', SQLERRM;
            RAISE;
    END;
END $$;

-- 检测基于email的重复记录
WITH duplicate_emails AS (
    SELECT
        email,
        COUNT(*) AS duplicate_count,
        ARRAY_AGG(id ORDER BY id) AS duplicate_ids
    FROM data_quality_test
    WHERE email IS NOT NULL
    GROUP BY email
    HAVING COUNT(*) > 1
)
SELECT
    email,
    duplicate_count,
    duplicate_ids,
    'Duplicate email addresses found' AS issue_description
FROM duplicate_emails
ORDER BY duplicate_count DESC;

-- 检测完全重复的记录
WITH row_hashes AS (
    SELECT
        id,
        MD5(COALESCE(name, '') || COALESCE(email, '') || COALESCE(age::text, '') || COALESCE(salary::text, '')) AS row_hash
    FROM data_quality_test
),
duplicate_rows AS (
    SELECT
        row_hash,
        COUNT(*) AS duplicate_count,
        ARRAY_AGG(id ORDER BY id) AS duplicate_ids
    FROM row_hashes
    GROUP BY row_hash
    HAVING COUNT(*) > 1
)
SELECT
    row_hash,
    duplicate_count,
    duplicate_ids,
    'Duplicate rows found' AS issue_description
FROM duplicate_rows
ORDER BY duplicate_count DESC;

-- 性能测试
EXPLAIN (ANALYZE, BUFFERS, TIMING, VERBOSE)
SELECT
    email,
    COUNT(*) AS duplicate_count
FROM data_quality_test
WHERE email IS NOT NULL
GROUP BY email
HAVING COUNT(*) > 1;
```

---

## 4. 数据格式标准化

### 4.1 格式统一

**数据格式标准化**统一数据格式，提高数据一致性。

```sql
-- 数据格式标准化：统一电话号码、邮箱、日期格式
WITH raw_data AS (
    SELECT
        id,
        phone,
        email,
        date_string
    FROM customer_data
),
formatted_data AS (
    SELECT
        id,
        -- 电话号码标准化：移除空格、括号、连字符
        REGEXP_REPLACE(REGEXP_REPLACE(REGEXP_REPLACE(phone, '[-\s()]', '', 'g'), '^\+', ''), '^0', '') AS standardized_phone,
        -- 邮箱标准化：转小写、去除空格
        LOWER(TRIM(email)) AS standardized_email,
        -- 日期格式标准化
        CASE
            WHEN date_string ~ '^\d{4}-\d{2}-\d{2}$' THEN date_string::DATE
            WHEN date_string ~ '^\d{2}/\d{2}/\d{4}$' THEN TO_DATE(date_string, 'MM/DD/YYYY')
            WHEN date_string ~ '^\d{2}-\d{2}-\d{4}$' THEN TO_DATE(date_string, 'DD-MM-YYYY')
            ELSE NULL
        END AS standardized_date
    FROM raw_data
)
SELECT
    id,
    standardized_phone,
    standardized_email,
    standardized_date
FROM formatted_data;
```

### 4.2 数据类型转换

**数据类型转换**确保数据类型正确，便于后续处理。

```sql
-- 数据类型转换和验证
WITH type_conversion AS (
    SELECT
        id,
        -- 字符串转数值（处理非数字字符）
        CASE
            WHEN value_string ~ '^-?\d+\.?\d*$' THEN value_string::NUMERIC
            ELSE NULL
        END AS numeric_value,
        -- 字符串转日期
        CASE
            WHEN date_string ~ '^\d{4}-\d{2}-\d{2}$' THEN date_string::DATE
            ELSE NULL
        END AS date_value,
        -- 布尔值标准化
        CASE
            WHEN UPPER(bool_string) IN ('TRUE', '1', 'YES', 'Y') THEN TRUE
            WHEN UPPER(bool_string) IN ('FALSE', '0', 'NO', 'N') THEN FALSE
            ELSE NULL
        END AS bool_value
    FROM raw_data
)
SELECT * FROM type_conversion;
```

## 5. 实际应用案例

### 5.1 综合数据清洗

**综合数据清洗**整合多种清洗方法，全面提升数据质量。

```sql
-- 综合数据清洗示例（带错误处理和性能测试）
DO $$
BEGIN
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'data_quality_test') THEN
            RAISE WARNING '表 data_quality_test 不存在，无法进行综合数据清洗';
            RETURN;
        END IF;
        RAISE NOTICE '开始进行综合数据清洗';
    EXCEPTION
        WHEN OTHERS THEN
            RAISE WARNING '综合数据清洗准备失败: %', SQLERRM;
            RAISE;
    END;
END $$;

-- 综合数据质量报告
WITH quality_metrics AS (
    SELECT
        COUNT(*) AS total_rows,
        COUNT(name) AS name_filled,
        COUNT(email) AS email_filled,
        COUNT(age) AS age_filled,
        COUNT(salary) AS salary_filled,
        COUNT(DISTINCT email) FILTER (WHERE email IS NOT NULL) AS unique_emails,
        COUNT(*) FILTER (WHERE email IS NOT NULL) AS total_emails
    FROM data_quality_test
),
salary_stats AS (
    SELECT
        AVG(salary) AS avg_salary,
        STDDEV(salary) AS stddev_salary,
        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY salary) AS q1,
        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY salary) AS q3
    FROM data_quality_test
    WHERE salary IS NOT NULL
),
outlier_count AS (
    SELECT
        COUNT(*) AS outlier_rows
    FROM data_quality_test dqt
    CROSS JOIN salary_stats ss
    WHERE dqt.salary IS NOT NULL
        AND (dqt.salary < ss.q1 - 1.5 * (ss.q3 - ss.q1) OR dqt.salary > ss.q3 + 1.5 * (ss.q3 - ss.q1))
)
SELECT
    qm.total_rows,
    ROUND((qm.name_filled::numeric / qm.total_rows * 100)::numeric, 2) AS name_completeness_pct,
    ROUND((qm.email_filled::numeric / qm.total_rows * 100)::numeric, 2) AS email_completeness_pct,
    ROUND((qm.age_filled::numeric / qm.total_rows * 100)::numeric, 2) AS age_completeness_pct,
    ROUND((qm.salary_filled::numeric / qm.total_rows * 100)::numeric, 2) AS salary_completeness_pct,
    qm.total_emails - qm.unique_emails AS duplicate_emails,
    oc.outlier_rows,
    CASE
        WHEN (qm.name_filled::numeric / qm.total_rows) < 0.9 THEN 'Low'
        WHEN (qm.email_filled::numeric / qm.total_rows) < 0.9 THEN 'Low'
        WHEN oc.outlier_rows > qm.total_rows * 0.1 THEN 'Low'
        ELSE 'High'
    END AS overall_quality
FROM quality_metrics qm
CROSS JOIN outlier_count oc;

-- 性能测试
EXPLAIN (ANALYZE, BUFFERS, TIMING, VERBOSE)
SELECT
    COUNT(*) AS total_rows,
    COUNT(name) AS name_filled
FROM data_quality_test;
```

### 5.2 客户数据清洗

**场景**：清洗客户数据，统一格式，去除重复，填充缺失值。

```sql
-- 客户数据清洗：综合处理
WITH customer_raw AS (
    SELECT
        customer_id,
        name,
        email,
        phone,
        address,
        registration_date,
        last_purchase_date
    FROM raw_customer_data
),
-- 步骤1：格式标准化
formatted_customers AS (
    SELECT
        customer_id,
        -- 姓名标准化：去除多余空格、首字母大写
        INITCAP(REGEXP_REPLACE(TRIM(name), '\s+', ' ', 'g')) AS standardized_name,
        -- 邮箱标准化：转小写、去除空格
        LOWER(TRIM(email)) AS standardized_email,
        -- 电话标准化：统一格式
        REGEXP_REPLACE(REGEXP_REPLACE(phone, '[-\s()]', '', 'g'), '^\+', '') AS standardized_phone,
        -- 地址标准化：去除多余空格
        REGEXP_REPLACE(TRIM(address), '\s+', ' ', 'g') AS standardized_address,
        registration_date,
        last_purchase_date
    FROM customer_raw
),
-- 步骤2：缺失值填充
filled_customers AS (
    SELECT
        customer_id,
        COALESCE(standardized_name, 'Unknown') AS name,
        COALESCE(standardized_email, 'noemail@unknown.com') AS email,
        COALESCE(standardized_phone, '0000000000') AS phone,
        COALESCE(standardized_address, 'Unknown Address') AS address,
        COALESCE(registration_date, CURRENT_DATE) AS registration_date,
        last_purchase_date
    FROM formatted_customers
),
-- 步骤3：重复检测
duplicate_analysis AS (
    SELECT
        email,
        COUNT(*) AS duplicate_count,
        ARRAY_AGG(customer_id ORDER BY registration_date) AS customer_ids,
        MIN(registration_date) AS earliest_registration
    FROM filled_customers
    WHERE email != 'noemail@unknown.com'
    GROUP BY email
    HAVING COUNT(*) > 1
),
-- 步骤4：异常值检测（注册日期异常）
outlier_dates AS (
    SELECT
        customer_id,
        registration_date,
        CASE
            WHEN registration_date > CURRENT_DATE THEN 'Future date'
            WHEN registration_date < '1900-01-01' THEN 'Too old'
            WHEN registration_date > last_purchase_date THEN 'Registration after purchase'
            ELSE 'Normal'
        END AS date_anomaly
    FROM filled_customers
    WHERE registration_date IS NOT NULL
),
-- 步骤5：清洗后的数据
cleaned_customers AS (
    SELECT
        fc.customer_id,
        fc.name,
        fc.email,
        fc.phone,
        fc.address,
        fc.registration_date,
        fc.last_purchase_date,
        CASE
            WHEN da.email IN (SELECT email FROM duplicate_analysis) THEN 'Duplicate'
            WHEN od.date_anomaly != 'Normal' THEN 'Date anomaly'
            ELSE 'Clean'
        END AS data_quality_status
    FROM filled_customers fc
    LEFT JOIN duplicate_analysis da ON fc.email = da.email
    LEFT JOIN outlier_dates od ON fc.customer_id = od.customer_id
)
SELECT
    customer_id,
    name,
    email,
    phone,
    registration_date,
    data_quality_status
FROM cleaned_customers
ORDER BY customer_id;
```

### 5.3 销售数据清洗

**场景**：清洗销售数据，检测异常订单，统一金额格式。

```sql
-- 销售数据清洗：检测异常订单和格式问题
WITH sales_raw AS (
    SELECT
        order_id,
        customer_id,
        order_date,
        order_amount,
        product_count,
        discount_rate,
        payment_method
    FROM raw_sales_data
    WHERE order_date >= CURRENT_DATE - INTERVAL '90 days'
),
-- 金额异常检测
amount_outliers AS (
    SELECT
        order_id,
        order_amount,
        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY order_amount) AS q1,
        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY order_amount) AS q3,
        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY order_amount) -
        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY order_amount) AS iqr
    FROM sales_raw
    GROUP BY order_id, order_amount
),
-- 数据质量检查
quality_checks AS (
    SELECT
        sr.order_id,
        sr.customer_id,
        sr.order_date,
        sr.order_amount,
        sr.product_count,
        sr.discount_rate,
        sr.payment_method,
        -- 金额异常
        CASE
            WHEN sr.order_amount < ao.q1 - 1.5 * ao.iqr OR
                 sr.order_amount > ao.q3 + 1.5 * ao.iqr THEN 'Amount outlier'
            WHEN sr.order_amount <= 0 THEN 'Invalid amount'
            WHEN sr.order_amount > 1000000 THEN 'Suspiciously large'
            ELSE 'Normal'
        END AS amount_status,
        -- 产品数量异常
        CASE
            WHEN sr.product_count <= 0 THEN 'Invalid count'
            WHEN sr.product_count > 1000 THEN 'Suspiciously large'
            ELSE 'Normal'
        END AS count_status,
        -- 折扣率异常
        CASE
            WHEN sr.discount_rate < 0 OR sr.discount_rate > 1 THEN 'Invalid discount'
            WHEN sr.discount_rate > 0.9 THEN 'Suspicious discount'
            ELSE 'Normal'
        END AS discount_status,
        -- 日期异常
        CASE
            WHEN sr.order_date > CURRENT_DATE THEN 'Future date'
            WHEN sr.order_date < CURRENT_DATE - INTERVAL '10 years' THEN 'Too old'
            ELSE 'Normal'
        END AS date_status
    FROM sales_raw sr
    LEFT JOIN amount_outliers ao ON sr.order_id = ao.order_id
),
-- 综合质量评估
quality_summary AS (
    SELECT
        order_id,
        customer_id,
        order_date,
        order_amount,
        product_count,
        amount_status,
        count_status,
        discount_status,
        date_status,
        CASE
            WHEN amount_status != 'Normal' OR count_status != 'Normal' OR
                 discount_status != 'Normal' OR date_status != 'Normal' THEN 'Needs review'
            ELSE 'Clean'
        END AS overall_status
    FROM quality_checks
)
SELECT
    order_id,
    customer_id,
    order_date,
    ROUND(order_amount::numeric, 2) AS order_amount,
    product_count,
    overall_status,
    ARRAY_REMOVE(ARRAY[
        CASE WHEN amount_status != 'Normal' THEN amount_status END,
        CASE WHEN count_status != 'Normal' THEN count_status END,
        CASE WHEN discount_status != 'Normal' THEN discount_status END,
        CASE WHEN date_status != 'Normal' THEN date_status END
    ], NULL) AS issues
FROM quality_summary
WHERE overall_status = 'Needs review'
ORDER BY order_date DESC;
```

### 5.4 文本数据清洗

**场景**：清洗文本数据，去除噪声，标准化格式。

```sql
-- 文本数据清洗：去除噪声、标准化格式
WITH text_data AS (
    SELECT
        id,
        raw_text,
        description
    FROM product_descriptions
),
cleaned_text AS (
    SELECT
        id,
        raw_text,
        -- 去除多余空格
        REGEXP_REPLACE(raw_text, '\s+', ' ', 'g') AS cleaned_spaces,
        -- 去除特殊字符（保留字母、数字、基本标点）
        REGEXP_REPLACE(raw_text, '[^\w\s.,!?-]', '', 'g') AS cleaned_special,
        -- 去除HTML标签
        REGEXP_REPLACE(raw_text, '<[^>]+>', '', 'g') AS cleaned_html,
        -- 去除URL
        REGEXP_REPLACE(raw_text, 'https?://\S+', '', 'g') AS cleaned_urls,
        -- 去除邮箱地址
        REGEXP_REPLACE(raw_text, '\S+@\S+', '', 'g') AS cleaned_emails,
        -- 标准化大小写（首字母大写）
        INITCAP(raw_text) AS capitalized_text,
        -- 去除前后空格
        TRIM(raw_text) AS trimmed_text
    FROM text_data
),
-- 综合清洗
fully_cleaned AS (
    SELECT
        id,
        raw_text AS original_text,
        TRIM(REGEXP_REPLACE(
            REGEXP_REPLACE(
                REGEXP_REPLACE(
                    REGEXP_REPLACE(raw_text, '<[^>]+>', '', 'g'),
                    'https?://\S+', '', 'g'
                ),
                '\S+@\S+', '', 'g'
            ),
            '\s+', ' ', 'g'
        )) AS cleaned_text,
        LENGTH(raw_text) AS original_length,
        LENGTH(TRIM(REGEXP_REPLACE(
            REGEXP_REPLACE(
                REGEXP_REPLACE(
                    REGEXP_REPLACE(raw_text, '<[^>]+>', '', 'g'),
                    'https?://\S+', '', 'g'
                ),
                '\S+@\S+', '', 'g'
            ),
            '\s+', ' ', 'g'
        ))) AS cleaned_length
    FROM text_data
)
SELECT
    id,
    LEFT(original_text, 50) AS original_preview,
    LEFT(cleaned_text, 50) AS cleaned_preview,
    original_length,
    cleaned_length,
    original_length - cleaned_length AS removed_chars
FROM fully_cleaned
WHERE original_length != cleaned_length
ORDER BY removed_chars DESC;
```

### 5.5 数据质量监控

**场景**：建立数据质量监控系统，持续跟踪数据质量指标。

```sql
-- 数据质量监控：持续跟踪质量指标
WITH quality_metrics AS (
    SELECT
        DATE_TRUNC('day', created_at) AS check_date,
        -- 完整性指标
        COUNT(*) AS total_records,
        COUNT(name) AS name_filled,
        COUNT(email) AS email_filled,
        COUNT(phone) AS phone_filled,
        -- 准确性指标（示例：邮箱格式验证）
        COUNT(*) FILTER (WHERE email ~* '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}$') AS valid_emails,
        -- 唯一性指标
        COUNT(DISTINCT email) FILTER (WHERE email IS NOT NULL) AS unique_emails,
        COUNT(*) FILTER (WHERE email IS NOT NULL) AS total_emails,
        -- 时效性指标（最近7天更新的记录）
        COUNT(*) FILTER (WHERE updated_at >= CURRENT_DATE - INTERVAL '7 days') AS recent_updates
    FROM customer_data
    WHERE created_at >= CURRENT_DATE - INTERVAL '30 days'
    GROUP BY DATE_TRUNC('day', created_at)
),
quality_scores AS (
    SELECT
        check_date,
        total_records,
        -- 完整性得分
        ROUND((name_filled + email_filled + phone_filled)::numeric / (total_records * 3) * 100, 2) AS completeness_score,
        -- 准确性得分
        ROUND(valid_emails::numeric / NULLIF(total_emails, 0) * 100, 2) AS accuracy_score,
        -- 唯一性得分
        ROUND(unique_emails::numeric / NULLIF(total_emails, 0) * 100, 2) AS uniqueness_score,
        -- 时效性得分
        ROUND(recent_updates::numeric / NULLIF(total_records, 0) * 100, 2) AS timeliness_score,
        -- 综合得分
        ROUND((
            (name_filled + email_filled + phone_filled)::numeric / (total_records * 3) * 0.3 +
            valid_emails::numeric / NULLIF(total_emails, 0) * 0.3 +
            unique_emails::numeric / NULLIF(total_emails, 0) * 0.2 +
            recent_updates::numeric / NULLIF(total_records, 0) * 0.2
        ) * 100, 2) AS overall_quality_score
    FROM quality_metrics
)
SELECT
    check_date,
    total_records,
    ROUND(completeness_score::numeric, 2) AS completeness_pct,
    ROUND(accuracy_score::numeric, 2) AS accuracy_pct,
    ROUND(uniqueness_score::numeric, 2) AS uniqueness_pct,
    ROUND(timeliness_score::numeric, 2) AS timeliness_pct,
    ROUND(overall_quality_score::numeric, 2) AS overall_score,
    CASE
        WHEN overall_quality_score >= 90 THEN 'Excellent'
        WHEN overall_quality_score >= 80 THEN 'Good'
        WHEN overall_quality_score >= 70 THEN 'Fair'
        ELSE 'Poor'
    END AS quality_grade
FROM quality_scores
ORDER BY check_date DESC;
```

---

## 6. 算法性能对比与优化

### 6.1 清洗方法对比

| 方法 | 适用场景 | 优点 | 缺点 | 复杂度 |
|------|---------|------|------|--------|
| **删除缺失值** | MCAR缺失 | 简单、无偏差 | 丢失信息 | $O(n)$ |
| **均值填充** | 连续数值 | 快速、简单 | 低估方差 | $O(n)$ |
| **回归填充** | 有相关性 | 利用关系 | 可能过拟合 | $O(n \cdot p)$ |
| **IQR异常检测** | 单变量 | 稳健、无分布假设 | 只适用单变量 | $O(n \log n)$ |
| **Z-score检测** | 正态分布 | 快速、标准化 | 假设分布 | $O(n)$ |
| **哈希重复检测** | 完全重复 | 快速 | 可能冲突 | $O(n)$ |
| **编辑距离** | 近似重复 | 处理拼写错误 | 计算慢 | $O(n^2)$ |

### 6.2 性能优化建议

1. **索引优化**：在关键字段上创建索引加速重复检测
2. **批量处理**：批量处理大量数据，减少I/O
3. **并行处理**：利用PostgreSQL并行查询加速处理
4. **物化视图**：缓存清洗结果，避免重复计算
5. **增量处理**：只处理新增或变更的数据

### 6.3 常见问题与解决方案

**问题1**：缺失值填充后模型性能下降

- **解决方案**：检查缺失机制、尝试不同填充方法、使用缺失值指示变量

**问题2**：异常值检测误删正常数据

- **解决方案**：调整阈值、使用多种方法交叉验证、结合业务知识

**问题3**：重复检测计算时间过长

- **解决方案**：使用哈希方法、采样检测、并行处理

**问题4**：清洗后数据量大幅减少

- **解决方案**：检查清洗规则是否过严、考虑保留部分异常值、使用软删除

---

## 7. 最佳实践

### 7.1 数据清洗流程

1. **数据探索**：
   - 了解数据结构和分布
   - 识别数据质量问题
   - 确定清洗优先级

2. **清洗策略**：
   - 根据业务需求制定规则
   - 平衡数据质量和数据量
   - 记录清洗决策

3. **清洗执行**：
   - 分步骤执行清洗
   - 保留原始数据备份
   - 记录清洗日志

4. **质量验证**：
   - 验证清洗效果
   - 检查清洗后数据分布
   - 评估对后续分析的影响

### 7.2 缺失值处理策略

1. **分析缺失模式**：
   - 计算缺失比例
   - 分析缺失相关性
   - 判断缺失机制

2. **选择填充方法**：
   - MCAR：可以使用删除或简单填充
   - MAR：使用模型填充
   - MNAR：需要领域知识

3. **验证填充效果**：
   - 比较填充前后分布
   - 评估对模型的影响
   - 使用交叉验证

### 7.3 异常值处理策略

1. **异常值识别**：
   - 使用多种方法检测
   - 结合业务知识判断
   - 区分错误和真实异常

2. **异常值处理**：
   - **删除**：确认是错误数据
   - **修正**：可以修正的错误
   - **保留**：真实的异常值（可能包含重要信息）
   - **转换**：使用对数变换等

3. **异常值分析**：
   - 分析异常值产生原因
   - 改进数据收集过程
   - 建立异常值监控

### 7.4 重复数据处理策略

1. **重复检测**：
   - 确定关键字段
   - 选择检测方法
   - 设置相似度阈值

2. **重复处理**：
   - **删除**：完全重复的记录
   - **合并**：部分重复的记录
   - **标记**：保留但标记为重复

3. **预防措施**：
   - 数据入库前验证
   - 使用唯一约束
   - 建立数据标准

### 7.5 SQL实现注意事项

1. **性能考虑**：
   - 创建必要的索引
   - 使用批量操作
   - 避免全表扫描

2. **数据安全**：
   - 保留原始数据备份
   - 使用事务确保一致性
   - 记录清洗操作日志

3. **可追溯性**：
   - 记录清洗规则
   - 保存清洗历史
   - 建立审计跟踪

---

## 📚 参考资源

### 学术文献

1. **Little, R.J.A., Rubin, D.B. (2019)**: "Statistical Analysis with Missing Data", 3rd Edition, Wiley.

2. **Hawkins, D.M. (1980)**: "Identification of Outliers", Chapman and Hall.

3. **Elmagarmid, A.K., Ipeirotis, P.G., Verykios, V.S. (2007)**: "Duplicate Record Detection: A Survey", *IEEE Transactions on Knowledge and Data Engineering*.

4. **《数据挖掘：概念与技术》**（Han, Kamber, Pei, 2012）- Chapter 3: Data Preprocessing

5. **《数据科学手册》**（Jake VanderPlas, 2016）- Chapter 3: Data Manipulation with Pandas

### 在线资源

- **PostgreSQL字符串函数**: <https://www.postgresql.org/docs/current/functions-string.html>
- **PostgreSQL正则表达式**: <https://www.postgresql.org/docs/current/functions-matching.html>
- **数据清洗最佳实践**: <https://towardsdatascience.com/data-cleaning-in-python-the-ultimate-guide-2020-c63b88bf0a0>

### 相关算法

- **数据验证算法**：数据格式和规则验证
- **数据标准化算法**：数据格式统一
- **数据完整性检查**：约束和规则检查
- **异常检测算法**：高级异常检测方法

---

**最后更新**: 2025年1月
**文档状态**: ✅ 已完成
