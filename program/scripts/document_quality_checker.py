#!/usr/bin/env python3
"""
æ–‡æ¡£è´¨é‡æ£€æŸ¥å·¥å…·

åŠŸèƒ½:
1. æ‰«ææ‰€æœ‰Markdownæ–‡æ¡£
2. è¯„ä¼°æ–‡æ¡£è´¨é‡ï¼ˆA/B/Cçº§ï¼‰
3. ç”Ÿæˆè´¨é‡æŠ¥å‘Š
4. è¯†åˆ«éœ€è¦æ”¹è¿›çš„æ–‡æ¡£

ä½¿ç”¨æ–¹æ³•:
    python document_quality_checker.py [--output quality_report.md]
"""

import os
import re
from pathlib import Path
from typing import Dict, List, Tuple
from collections import defaultdict
import argparse


class DocumentQualityChecker:
    """æ–‡æ¡£è´¨é‡æ£€æŸ¥å™¨"""

    def __init__(self, root_dir: str):
        """åˆå§‹åŒ–æ£€æŸ¥å™¨"""
        self.root_dir = Path(root_dir)
        self.documents: Dict[str, Dict] = {}

    def scan_documents(self) -> None:
        """æ‰«ææ‰€æœ‰Markdownæ–‡æ¡£"""
        print(f"æ‰«æç›®å½•: {self.root_dir}")

        md_files = list(self.root_dir.rglob("*.md"))
        print(f"æ‰¾åˆ° {len(md_files)} ä¸ªMarkdownæ–‡ä»¶")

        for md_file in md_files:
            # è·³è¿‡æŸäº›ç›®å½•
            if any(skip in str(md_file) for skip in ['.git', 'node_modules', '__pycache__', '99-Archive']):
                continue

            try:
                content = md_file.read_text(encoding='utf-8')
                rel_path = str(md_file.relative_to(self.root_dir))

                quality = self.assess_quality(content, rel_path)
                self.documents[rel_path] = quality
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•è¯»å– {md_file}: {e}")

        print(f"æˆåŠŸè¯„ä¼° {len(self.documents)} ä¸ªæ–‡æ¡£")

    def assess_quality(self, content: str, filepath: str) -> Dict:
        """è¯„ä¼°æ–‡æ¡£è´¨é‡"""
        metrics = {
            'filepath': filepath,
            'total_lines': len(content.split('\n')),
            'code_blocks': len(re.findall(r'```', content)) // 2,
            'code_examples': len(re.findall(r'```(?:python|sql|bash|sh)', content, re.IGNORECASE)),
            'headings': len(re.findall(r'^#+\s+', content, re.MULTILINE)),
            'links': len(re.findall(r'\[([^\]]+)\]\([^\)]+\)', content)),
            'images': len(re.findall(r'!\[([^\]]*)\]\([^\)]+\)', content)),
            'tables': len(re.findall(r'\|.*\|', content)),
            'has_toc': bool(re.search(r'^##?\s+[ç›®å½•|Table of Contents]', content, re.MULTILINE | re.IGNORECASE)),
            'has_summary': bool(re.search(r'^##?\s+[æ‘˜è¦|Summary|æ¦‚è¿°]', content, re.MULTILINE | re.IGNORECASE)),
            'has_references': bool(re.search(r'^##?\s+[å‚è€ƒ|References|å‚è€ƒæ–‡çŒ®]', content, re.MULTILINE | re.IGNORECASE)),
            'placeholder_count': len(re.findall(r'(?:å¾…è¡¥å……|å¾…å®Œæˆ|TODO|FIXME|è¯¦ç»†å†…å®¹è§|è§æ–‡æ¡£)', content, re.IGNORECASE)),
            'substantive_content': self.calculate_substantive_content(content),
        }

        # è®¡ç®—è´¨é‡åˆ†æ•°
        score = self.calculate_score(metrics)
        metrics['score'] = score
        metrics['grade'] = self.assign_grade(score, metrics)

        return metrics

    def calculate_substantive_content(self, content: str) -> int:
        """è®¡ç®—å®è´¨æ€§å†…å®¹é•¿åº¦ï¼ˆå»é™¤ä»£ç å—ã€é“¾æ¥ç­‰ï¼‰"""
        # ç§»é™¤ä»£ç å—
        text = re.sub(r'```[\s\S]*?```', '', content)
        # ç§»é™¤è¡Œå†…ä»£ç 
        text = re.sub(r'`[^`]+`', '', text)
        # ç§»é™¤é“¾æ¥
        text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)
        # ç§»é™¤å›¾ç‰‡
        text = re.sub(r'!\[([^\]]*)\]\([^\)]+\)', '', text)
        # ç§»é™¤æ ‡é¢˜æ ‡è®°
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        return len(text.strip())

    def calculate_score(self, metrics: Dict) -> float:
        """è®¡ç®—è´¨é‡åˆ†æ•°ï¼ˆ0-100ï¼‰"""
        score = 0.0

        # åŸºç¡€åˆ†æ•°ï¼šå®è´¨æ€§å†…å®¹
        if metrics['substantive_content'] > 5000:
            score += 30
        elif metrics['substantive_content'] > 2000:
            score += 20
        elif metrics['substantive_content'] > 500:
            score += 10

        # ä»£ç ç¤ºä¾‹
        if metrics['code_examples'] >= 5:
            score += 20
        elif metrics['code_examples'] >= 3:
            score += 15
        elif metrics['code_examples'] >= 1:
            score += 10

        # ç»“æ„å®Œæ•´æ€§
        if metrics['has_toc']:
            score += 5
        if metrics['has_summary']:
            score += 5
        if metrics['has_references']:
            score += 5

        # å›¾è¡¨å’Œé“¾æ¥
        if metrics['tables'] >= 3:
            score += 10
        elif metrics['tables'] >= 1:
            score += 5

        if metrics['links'] >= 10:
            score += 10
        elif metrics['links'] >= 5:
            score += 5

        # æ‰£åˆ†é¡¹ï¼šå ä½ç¬¦
        if metrics['placeholder_count'] > 5:
            score -= 20
        elif metrics['placeholder_count'] > 2:
            score -= 10
        elif metrics['placeholder_count'] > 0:
            score -= 5

        # æ‰£åˆ†é¡¹ï¼šå†…å®¹è¿‡å°‘
        if metrics['total_lines'] < 50:
            score -= 15
        elif metrics['total_lines'] < 100:
            score -= 10

        return max(0, min(100, score))

    def assign_grade(self, score: float, metrics: Dict) -> str:
        """åˆ†é…è´¨é‡ç­‰çº§"""
        # Cçº§ï¼šåªæœ‰æ¡†æ¶æˆ–å ä½ç¬¦
        if (metrics['placeholder_count'] > 3 or
            metrics['substantive_content'] < 500 or
            score < 40):
            return 'C'

        # Açº§ï¼šé«˜è´¨é‡æ–‡æ¡£
        if (score >= 70 and
            metrics['code_examples'] >= 3 and
            metrics['substantive_content'] > 2000 and
            metrics['placeholder_count'] == 0):
            return 'A'

        # Bçº§ï¼šä¸­ç­‰è´¨é‡
        return 'B'

    def generate_report(self, output_file: str = "quality_report.md") -> None:
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        print(f"ç”ŸæˆæŠ¥å‘Š: {output_file}")

        # æŒ‰ç­‰çº§åˆ†ç»„
        by_grade = defaultdict(list)
        for filepath, metrics in self.documents.items():
            by_grade[metrics['grade']].append((filepath, metrics))

        # ç»Ÿè®¡
        total = len(self.documents)
        grade_counts = {grade: len(docs) for grade, docs in by_grade.items()}

        report_lines = [
            "# æ–‡æ¡£è´¨é‡æ£€æŸ¥æŠ¥å‘Š",
            "",
            f"**ç”Ÿæˆæ—¶é—´**: {Path(__file__).stat().st_mtime}",
            f"**æ£€æŸ¥æ–‡æ¡£æ•°**: {total}",
            "",
            "---",
            "",
            "## ğŸ“Š è´¨é‡ç»Ÿè®¡",
            "",
            f"- **Açº§æ–‡æ¡£ï¼ˆä¼˜ç§€ï¼‰**: {grade_counts.get('A', 0)} ({grade_counts.get('A', 0)*100//total if total > 0 else 0}%)",
            f"- **Bçº§æ–‡æ¡£ï¼ˆè‰¯å¥½ï¼‰**: {grade_counts.get('B', 0)} ({grade_counts.get('B', 0)*100//total if total > 0 else 0}%)",
            f"- **Cçº§æ–‡æ¡£ï¼ˆéœ€æ”¹è¿›ï¼‰**: {grade_counts.get('C', 0)} ({grade_counts.get('C', 0)*100//total if total > 0 else 0}%)",
            "",
            "---",
            "",
        ]

        # Açº§æ–‡æ¡£åˆ—è¡¨
        if 'A' in by_grade:
            report_lines.extend([
                "## âœ… Açº§æ–‡æ¡£ï¼ˆä¼˜ç§€ï¼‰",
                "",
                "è¿™äº›æ–‡æ¡£è´¨é‡ä¼˜ç§€ï¼ŒåŒ…å«æ·±å…¥çš„æŠ€æœ¯åŸç†ã€å®Œæ•´çš„ä»£ç ç¤ºä¾‹å’Œä¸°å¯Œçš„æ¡ˆä¾‹ã€‚",
                ""
            ])
            for filepath, metrics in sorted(by_grade['A'], key=lambda x: x[1]['score'], reverse=True):
                report_lines.extend([
                    f"### `{filepath}`",
                    f"- **è´¨é‡åˆ†æ•°**: {metrics['score']:.1f}/100",
                    f"- **å®è´¨æ€§å†…å®¹**: {metrics['substantive_content']} å­—ç¬¦",
                    f"- **ä»£ç ç¤ºä¾‹**: {metrics['code_examples']} ä¸ª",
                    f"- **è¡¨æ ¼**: {metrics['tables']} ä¸ª",
                    f"- **é“¾æ¥**: {metrics['links']} ä¸ª",
                    "",
                ])

        # Bçº§æ–‡æ¡£åˆ—è¡¨
        if 'B' in by_grade:
            report_lines.extend([
                "## âš ï¸ Bçº§æ–‡æ¡£ï¼ˆè‰¯å¥½ï¼‰",
                "",
                "è¿™äº›æ–‡æ¡£æœ‰åŸºç¡€å†…å®¹ï¼Œä½†éœ€è¦è¡¥å……è®ºè¯ã€å¼•ç”¨æˆ–æ€§èƒ½æ•°æ®ã€‚",
                ""
            ])
            for filepath, metrics in sorted(by_grade['B'], key=lambda x: x[1]['score'], reverse=True):
                report_lines.extend([
                    f"### `{filepath}`",
                    f"- **è´¨é‡åˆ†æ•°**: {metrics['score']:.1f}/100",
                    f"- **å®è´¨æ€§å†…å®¹**: {metrics['substantive_content']} å­—ç¬¦",
                    f"- **ä»£ç ç¤ºä¾‹**: {metrics['code_examples']} ä¸ª",
                    f"- **å ä½ç¬¦**: {metrics['placeholder_count']} ä¸ª",
                    "",
                    "**éœ€è¦è¡¥å……**:",
                    "- [ ] å­¦æœ¯è®ºæ–‡å¼•ç”¨",
                    "- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•æ•°æ®",
                    "- [ ] å®é™…æ¡ˆä¾‹è®ºè¯",
                    "- [ ] æ·±å…¥çš„æŠ€æœ¯åŸç†åˆ†æ",
                    "",
                ])

        # Cçº§æ–‡æ¡£åˆ—è¡¨ï¼ˆéœ€è¦ä¼˜å…ˆæ”¹è¿›ï¼‰
        if 'C' in by_grade:
            report_lines.extend([
                "## âŒ Cçº§æ–‡æ¡£ï¼ˆéœ€æ”¹è¿›ï¼‰",
                "",
                "è¿™äº›æ–‡æ¡£åªæœ‰æ¡†æ¶ï¼Œç¼ºä¹å®è´¨å†…å®¹ã€‚éœ€è¦ä¼˜å…ˆæ”¹è¿›ã€‚",
                "",
                f"**æ€»è®¡**: {len(by_grade['C'])} ä¸ªæ–‡æ¡£",
                ""
            ])

            # æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆå ä½ç¬¦å¤šã€å†…å®¹å°‘çš„ä¼˜å…ˆï¼‰
            priority_docs = sorted(
                by_grade['C'],
                key=lambda x: (x[1]['placeholder_count'], -x[1]['substantive_content']),
                reverse=True
            )

            report_lines.append("### é«˜ä¼˜å…ˆçº§æ”¹è¿›æ–‡æ¡£ï¼ˆå‰20ä¸ªï¼‰\n")
            for i, (filepath, metrics) in enumerate(priority_docs[:20], 1):
                report_lines.extend([
                    f"#### {i}. `{filepath}`",
                    f"- **è´¨é‡åˆ†æ•°**: {metrics['score']:.1f}/100",
                    f"- **å®è´¨æ€§å†…å®¹**: {metrics['substantive_content']} å­—ç¬¦",
                    f"- **å ä½ç¬¦**: {metrics['placeholder_count']} ä¸ª",
                    f"- **ä»£ç ç¤ºä¾‹**: {metrics['code_examples']} ä¸ª",
                    "",
                    "**éœ€è¦è¡¥å……**:",
                    "- [ ] å®Œæ•´çš„ä¸šåŠ¡åœºæ™¯æè¿°",
                    "- [ ] è¯¦ç»†çš„æ¶æ„è®¾è®¡å›¾",
                    "- [ ] ä»£ç ç¤ºä¾‹å’Œé…ç½®æ–‡ä»¶",
                    "- [ ] æ€§èƒ½æµ‹è¯•ç»“æœ",
                    "- [ ] æ•…éšœå¤„ç†ç»éªŒ",
                    "- [ ] FAQç« èŠ‚",
                    "",
                ])

        report_lines.extend([
            "",
            "---",
            "",
            "## ğŸ”§ ä½¿ç”¨è¯´æ˜",
            "",
            "1. ä¼˜å…ˆæ”¹è¿›Cçº§æ–‡æ¡£ï¼ˆç‰¹åˆ«æ˜¯é«˜ä¼˜å…ˆçº§çš„å‰20ä¸ªï¼‰",
            "2. ä¸ºBçº§æ–‡æ¡£è¡¥å……ç¼ºå¤±çš„å†…å®¹",
            "3. ä¿æŒAçº§æ–‡æ¡£çš„è´¨é‡",
            "",
            "---",
            "",
            "**æŠ¥å‘Šç”Ÿæˆå·¥å…·**: `scripts/document_quality_checker.py`",
            ""
        ])

        output_path = self.root_dir / output_file
        output_path.write_text('\n'.join(report_lines), encoding='utf-8')
        print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")


def main():
    parser = argparse.ArgumentParser(description='æ–‡æ¡£è´¨é‡æ£€æŸ¥å·¥å…·')
    parser.add_argument('--root', type=str, default='.', help='é¡¹ç›®æ ¹ç›®å½•')
    parser.add_argument('--output', type=str, default='quality_report.md', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶å')

    args = parser.parse_args()

    checker = DocumentQualityChecker(args.root)
    checker.scan_documents()
    checker.generate_report(args.output)

    print("\nâœ… æ£€æŸ¥å®Œæˆï¼")
    print(f"ğŸ“Š å·²è¯„ä¼° {len(checker.documents)} ä¸ªæ–‡æ¡£")
    print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")


if __name__ == '__main__':
    main()
