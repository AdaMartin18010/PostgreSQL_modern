#!/usr/bin/env python3
"""
å†…å®¹å»é‡æ£€æŸ¥å·¥å…·

åŠŸèƒ½:
1. æ‰«æé¡¹ç›®ä¸­çš„æ‰€æœ‰Markdownæ–‡æ¡£
2. è®¡ç®—æ–‡æ¡£é—´çš„ç›¸ä¼¼åº¦
3. ç”Ÿæˆé‡å¤å†…å®¹æŠ¥å‘Š
4. è¯†åˆ«éœ€è¦åˆå¹¶çš„æ–‡æ¡£

ä½¿ç”¨æ–¹æ³•:
    python content_deduplication_checker.py [--threshold 0.6] [--output report.md]
"""

import os
import re
import hashlib
from pathlib import Path
from typing import List, Dict, Tuple
from collections import defaultdict
import argparse
from difflib import SequenceMatcher


class ContentDeduplicationChecker:
    """å†…å®¹å»é‡æ£€æŸ¥å™¨"""

    def __init__(self, root_dir: str, threshold: float = 0.6):
        """
        åˆå§‹åŒ–æ£€æŸ¥å™¨

        Args:
            root_dir: é¡¹ç›®æ ¹ç›®å½•
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰ï¼Œè¶…è¿‡æ­¤å€¼è®¤ä¸ºé‡å¤
        """
        self.root_dir = Path(root_dir)
        self.threshold = threshold
        self.documents: Dict[str, str] = {}
        self.similarities: List[Tuple[str, str, float]] = []

    def scan_documents(self) -> None:
        """æ‰«ææ‰€æœ‰Markdownæ–‡æ¡£"""
        print(f"æ‰«æç›®å½•: {self.root_dir}")

        md_files = list(self.root_dir.rglob("*.md"))
        print(f"æ‰¾åˆ° {len(md_files)} ä¸ªMarkdownæ–‡ä»¶")

        for md_file in md_files:
            # è·³è¿‡æŸäº›ç›®å½•
            if any(skip in str(md_file) for skip in ['.git', 'node_modules', '__pycache__', '99-Archive']):
                continue

            try:
                content = md_file.read_text(encoding='utf-8')
                # æå–çº¯æ–‡æœ¬å†…å®¹ï¼ˆå»é™¤Markdownè¯­æ³•ï¼‰
                text_content = self.extract_text(content)

                if len(text_content) > 100:  # åªå¤„ç†æœ‰å®è´¨å†…å®¹çš„æ–‡æ¡£
                    rel_path = str(md_file.relative_to(self.root_dir))
                    self.documents[rel_path] = text_content
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•è¯»å– {md_file}: {e}")

        print(f"æˆåŠŸåŠ è½½ {len(self.documents)} ä¸ªæ–‡æ¡£")

    def extract_text(self, content: str) -> str:
        """ä»Markdownå†…å®¹ä¸­æå–çº¯æ–‡æœ¬"""
        # ç§»é™¤ä»£ç å—
        content = re.sub(r'```[\s\S]*?```', '', content)
        # ç§»é™¤è¡Œå†…ä»£ç 
        content = re.sub(r'`[^`]+`', '', content)
        # ç§»é™¤é“¾æ¥
        content = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', content)
        # ç§»é™¤å›¾ç‰‡
        content = re.sub(r'!\[([^\]]*)\]\([^\)]+\)', '', content)
        # ç§»é™¤æ ‡é¢˜æ ‡è®°
        content = re.sub(r'^#+\s+', '', content, flags=re.MULTILINE)
        # ç§»é™¤ç²—ä½“/æ–œä½“æ ‡è®°
        content = re.sub(r'\*\*([^\*]+)\*\*', r'\1', content)
        content = re.sub(r'\*([^\*]+)\*', r'\1', content)
        # ç§»é™¤å¤šä½™ç©ºç™½
        content = re.sub(r'\s+', ' ', content)
        return content.strip()

    def calculate_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
        return SequenceMatcher(None, text1, text2).ratio()

    def find_duplicates(self) -> None:
        """æŸ¥æ‰¾é‡å¤å†…å®¹"""
        print("è®¡ç®—æ–‡æ¡£ç›¸ä¼¼åº¦...")

        doc_list = list(self.documents.items())
        total = len(doc_list) * (len(doc_list) - 1) // 2
        processed = 0

        for i, (path1, content1) in enumerate(doc_list):
            for j, (path2, content2) in enumerate(doc_list[i+1:], start=i+1):
                similarity = self.calculate_similarity(content1, content2)

                if similarity >= self.threshold:
                    self.similarities.append((path1, path2, similarity))

                processed += 1
                if processed % 100 == 0:
                    print(f"è¿›åº¦: {processed}/{total} ({processed*100//total}%)")

        print(f"æ‰¾åˆ° {len(self.similarities)} å¯¹ç›¸ä¼¼æ–‡æ¡£ï¼ˆç›¸ä¼¼åº¦ >= {self.threshold})")

    def generate_report(self, output_file: str = "duplication_report.md") -> None:
        """ç”Ÿæˆé‡å¤å†…å®¹æŠ¥å‘Š"""
        print(f"ç”ŸæˆæŠ¥å‘Š: {output_file}")

        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        self.similarities.sort(key=lambda x: x[2], reverse=True)

        report_lines = [
            "# å†…å®¹å»é‡æ£€æŸ¥æŠ¥å‘Š",
            "",
            f"**ç”Ÿæˆæ—¶é—´**: {Path(__file__).stat().st_mtime}",
            f"**æ£€æŸ¥æ–‡æ¡£æ•°**: {len(self.documents)}",
            f"**ç›¸ä¼¼åº¦é˜ˆå€¼**: {self.threshold}",
            f"**å‘ç°é‡å¤æ–‡æ¡£å¯¹**: {len(self.similarities)}",
            "",
            "---",
            "",
            "## ğŸ“Š ç»Ÿè®¡æ‘˜è¦",
            "",
            f"- æ€»æ–‡æ¡£æ•°: {len(self.documents)}",
            f"- é‡å¤æ–‡æ¡£å¯¹: {len(self.similarities)}",
            f"- é«˜ç›¸ä¼¼åº¦æ–‡æ¡£å¯¹ (>=0.8): {len([s for s in self.similarities if s[2] >= 0.8])}",
            f"- ä¸­ç­‰ç›¸ä¼¼åº¦æ–‡æ¡£å¯¹ (0.6-0.8): {len([s for s in self.similarities if 0.6 <= s[2] < 0.8])}",
            "",
            "---",
            "",
            "## ğŸ” é‡å¤æ–‡æ¡£è¯¦æƒ…",
            "",
        ]

        # æŒ‰ç›¸ä¼¼åº¦åˆ†ç»„
        high_similarity = [(p1, p2, sim) for p1, p2, sim in self.similarities if sim >= 0.8]
        medium_similarity = [(p1, p2, sim) for p1, p2, sim in self.similarities if 0.6 <= sim < 0.8]

        if high_similarity:
            report_lines.extend([
                "### ğŸ”´ é«˜ç›¸ä¼¼åº¦æ–‡æ¡£ (ç›¸ä¼¼åº¦ >= 0.8) - å»ºè®®ç«‹å³åˆå¹¶",
                ""
            ])
            for path1, path2, similarity in high_similarity:
                report_lines.extend([
                    f"#### ç›¸ä¼¼åº¦: {similarity:.2%}",
                    "",
                    f"- **æ–‡æ¡£1**: `{path1}`",
                    f"- **æ–‡æ¡£2**: `{path2}`",
                    "",
                    "**å»ºè®®æ“ä½œ**:",
                    f"- [ ] å¯¹æ¯”ä¸¤ä¸ªæ–‡æ¡£å†…å®¹",
                    f"- [ ] åˆå¹¶é‡å¤å†…å®¹",
                    f"- [ ] ä¿ç•™æ›´å®Œæ•´çš„ç‰ˆæœ¬",
                    f"- [ ] æ›´æ–°æ‰€æœ‰å¼•ç”¨é“¾æ¥",
                    "",
                    "---",
                    ""
                ])

        if medium_similarity:
            report_lines.extend([
                "### ğŸŸ¡ ä¸­ç­‰ç›¸ä¼¼åº¦æ–‡æ¡£ (ç›¸ä¼¼åº¦ 0.6-0.8) - å»ºè®®å®¡æŸ¥",
                ""
            ])
            for path1, path2, similarity in medium_similarity:
                report_lines.extend([
                    f"#### ç›¸ä¼¼åº¦: {similarity:.2%}",
                    "",
                    f"- **æ–‡æ¡£1**: `{path1}`",
                    f"- **æ–‡æ¡£2**: `{path2}`",
                    "",
                    "**å»ºè®®æ“ä½œ**:",
                    f"- [ ] å®¡æŸ¥ä¸¤ä¸ªæ–‡æ¡£æ˜¯å¦æœ‰é‡å¤ç« èŠ‚",
                    f"- [ ] è€ƒè™‘åˆå¹¶æˆ–äº¤å‰å¼•ç”¨",
                    "",
                    "---",
                    ""
                ])

        # ç”Ÿæˆå»ºè®®çš„åˆå¹¶æ¸…å•
        report_lines.extend([
            "",
            "## ğŸ“‹ å»ºè®®çš„åˆå¹¶æ¸…å•",
            "",
            "### é«˜ä¼˜å…ˆçº§åˆå¹¶ï¼ˆç›¸ä¼¼åº¦ >= 0.8ï¼‰",
            ""
        ])

        merged_docs = set()
        merge_groups = []

        for path1, path2, similarity in high_similarity:
            if path1 not in merged_docs and path2 not in merged_docs:
                merge_groups.append([path1, path2])
                merged_docs.add(path1)
                merged_docs.add(path2)

        for i, group in enumerate(merge_groups, 1):
            report_lines.extend([
                f"#### åˆå¹¶ç»„ {i}",
                "",
                "**æ–‡æ¡£åˆ—è¡¨**:",
            ])
            for doc in group:
                report_lines.append(f"- `{doc}`")
            report_lines.extend([
                "",
                "**åˆå¹¶å»ºè®®**:",
                f"- ä¿ç•™: `{group[0]}` (å»ºè®®ä¿ç•™æ›´å®Œæ•´çš„ç‰ˆæœ¬)",
                f"- åˆå¹¶åˆ°: `{group[0]}`",
                f"- åˆ é™¤: `{group[1]}` (åˆå¹¶ååˆ é™¤)",
                "",
            ])

        report_lines.extend([
            "",
            "---",
            "",
            "## ğŸ”§ ä½¿ç”¨è¯´æ˜",
            "",
            "1. å®¡æŸ¥é«˜ç›¸ä¼¼åº¦æ–‡æ¡£å¯¹",
            "2. å†³å®šä¿ç•™å“ªä¸ªæ–‡æ¡£",
            "3. åˆå¹¶å†…å®¹åˆ°ä¿ç•™çš„æ–‡æ¡£",
            "4. æ›´æ–°æ‰€æœ‰å¼•ç”¨é“¾æ¥",
            "5. åˆ é™¤é‡å¤çš„æ–‡æ¡£",
            "",
            "---",
            "",
            "**æŠ¥å‘Šç”Ÿæˆå·¥å…·**: `scripts/content_deduplication_checker.py`",
            ""
        ])

        output_path = self.root_dir / output_file
        output_path.write_text('\n'.join(report_lines), encoding='utf-8')
        print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")


def main():
    parser = argparse.ArgumentParser(description='å†…å®¹å»é‡æ£€æŸ¥å·¥å…·')
    parser.add_argument('--root', type=str, default='.', help='é¡¹ç›®æ ¹ç›®å½•')
    parser.add_argument('--threshold', type=float, default=0.6, help='ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1)')
    parser.add_argument('--output', type=str, default='duplication_report.md', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶å')

    args = parser.parse_args()

    checker = ContentDeduplicationChecker(args.root, args.threshold)
    checker.scan_documents()
    checker.find_duplicates()
    checker.generate_report(args.output)

    print("\nâœ… æ£€æŸ¥å®Œæˆï¼")
    print(f"ğŸ“Š å‘ç° {len(checker.similarities)} å¯¹ç›¸ä¼¼æ–‡æ¡£")
    print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")


if __name__ == '__main__':
    main()
