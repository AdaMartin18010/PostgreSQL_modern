#!/usr/bin/env python3
"""
PostgreSQL æŸ¥è¯¢æ€§èƒ½è¿½è¸ªå™¨
æŒç»­ç›‘æ§æŸ¥è¯¢æ€§èƒ½ï¼Œè‡ªåŠ¨å‘ç°æ€§èƒ½é€€åŒ–
"""

import psycopg2
from psycopg2.extras import RealDictCursor
import argparse
import time
from datetime import datetime, timedelta
import json
import hashlib

class QueryPerformanceTracker:
    """æŸ¥è¯¢æ€§èƒ½è¿½è¸ªå™¨"""

    def __init__(self, conn_str: str):
        self.conn = psycopg2.connect(conn_str, cursor_factory=RealDictCursor)
        self.cursor = self.conn.cursor()
        self.baseline = {}
        self.alerts = []

    def ensure_pg_stat_statements(self):
        """ç¡®ä¿pg_stat_statementsæ‰©å±•å·²å®‰è£…"""
        self.cursor.execute("""
            SELECT COUNT(*) as count
            FROM pg_extension
            WHERE extname = 'pg_stat_statements';
        """)

        if self.cursor.fetchone()['count'] == 0:
            print("âš ï¸  pg_stat_statementsæœªå®‰è£…")
            print("   å®‰è£…æ–¹æ³•:")
            print("   CREATE EXTENSION pg_stat_statements;")
            return False

        return True

    def collect_query_stats(self):
        """æ”¶é›†æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯"""
        self.cursor.execute("""
            SELECT
                queryid,
                LEFT(query, 100) AS query_short,
                calls,
                total_exec_time,
                mean_exec_time,
                min_exec_time,
                max_exec_time,
                stddev_exec_time,
                rows,
                shared_blks_hit,
                shared_blks_read,
                CASE
                    WHEN (shared_blks_hit + shared_blks_read) > 0
                    THEN ROUND(100.0 * shared_blks_hit / (shared_blks_hit + shared_blks_read), 2)
                    ELSE 100
                END AS cache_hit_ratio
            FROM pg_stat_statements
            WHERE calls > 10  -- è¿‡æ»¤å¶å‘æŸ¥è¯¢
            ORDER BY mean_exec_time DESC
            LIMIT 100;
        """)

        return self.cursor.fetchall()

    def create_baseline(self):
        """åˆ›å»ºæ€§èƒ½åŸºçº¿"""
        print("åˆ›å»ºæ€§èƒ½åŸºçº¿...")

        stats = self.collect_query_stats()

        for stat in stats:
            query_id = str(stat['queryid'])
            self.baseline[query_id] = {
                'query': stat['query_short'],
                'mean_time': float(stat['mean_exec_time']),
                'max_time': float(stat['max_exec_time']),
                'cache_hit_ratio': float(stat['cache_hit_ratio']),
                'calls': stat['calls'],
                'timestamp': datetime.now().isoformat()
            }

        # ä¿å­˜åŸºçº¿
        filename = f"query_baseline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.baseline, f, indent=2, ensure_ascii=False)

        print(f"âœ“ åŸºçº¿å·²åˆ›å»º: {len(self.baseline)} ä¸ªæŸ¥è¯¢")
        print(f"  ä¿å­˜åˆ°: {filename}")

        return filename

    def load_baseline(self, filename: str):
        """åŠ è½½åŸºçº¿"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                self.baseline = json.load(f)
            print(f"âœ“ åŸºçº¿å·²åŠ è½½: {len(self.baseline)} ä¸ªæŸ¥è¯¢")
            return True
        except FileNotFoundError:
            print(f"âœ— åŸºçº¿æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
            return False

    def compare_performance(self, threshold: float = 1.5):
        """æ¯”è¾ƒæ€§èƒ½å˜åŒ–"""
        print(f"\nåˆ†ææ€§èƒ½å˜åŒ–ï¼ˆé˜ˆå€¼: {threshold}xï¼‰...")

        current_stats = self.collect_query_stats()
        regressions = []
        improvements = []

        for stat in current_stats:
            query_id = str(stat['queryid'])

            if query_id not in self.baseline:
                continue

            baseline = self.baseline[query_id]
            current_mean = float(stat['mean_exec_time'])
            baseline_mean = baseline['mean_time']

            # è®¡ç®—æ€§èƒ½å˜åŒ–
            if baseline_mean > 0:
                ratio = current_mean / baseline_mean

                # æ€§èƒ½é€€åŒ–
                if ratio > threshold:
                    regression = {
                        'query': stat['query_short'],
                        'baseline_mean': baseline_mean,
                        'current_mean': current_mean,
                        'ratio': ratio,
                        'degradation_pct': (ratio - 1) * 100,
                        'calls': stat['calls'],
                        'cache_hit_ratio': float(stat['cache_hit_ratio']),
                        'severity': 'critical' if ratio > 3 else 'warning'
                    }
                    regressions.append(regression)

                # æ€§èƒ½æ”¹å–„
                elif ratio < 0.7:
                    improvement = {
                        'query': stat['query_short'],
                        'baseline_mean': baseline_mean,
                        'current_mean': current_mean,
                        'ratio': ratio,
                        'improvement_pct': (1 - ratio) * 100,
                        'calls': stat['calls']
                    }
                    improvements.append(improvement)

        return regressions, improvements

    def analyze_slow_queries(self, threshold_ms: float = 1000):
        """åˆ†ææ…¢æŸ¥è¯¢"""
        print(f"\nåˆ†ææ…¢æŸ¥è¯¢ï¼ˆé˜ˆå€¼: {threshold_ms}msï¼‰...")

        self.cursor.execute("""
            SELECT
                queryid,
                LEFT(query, 150) AS query,
                calls,
                ROUND(mean_exec_time::numeric, 2) AS mean_ms,
                ROUND(max_exec_time::numeric, 2) AS max_ms,
                ROUND((total_exec_time / 1000)::numeric, 2) AS total_seconds,
                CASE
                    WHEN (shared_blks_hit + shared_blks_read) > 0
                    THEN ROUND(100.0 * shared_blks_hit / (shared_blks_hit + shared_blks_read), 2)
                    ELSE 100
                END AS cache_hit_ratio
            FROM pg_stat_statements
            WHERE mean_exec_time > %s
            ORDER BY mean_exec_time DESC
            LIMIT 20;
        """, (threshold_ms,))

        return self.cursor.fetchall()

    def analyze_frequent_queries(self, min_calls: int = 1000):
        """åˆ†æé«˜é¢‘æŸ¥è¯¢"""
        print(f"\nåˆ†æé«˜é¢‘æŸ¥è¯¢ï¼ˆé˜ˆå€¼: {min_calls}æ¬¡ï¼‰...")

        self.cursor.execute("""
            SELECT
                queryid,
                LEFT(query, 150) AS query,
                calls,
                ROUND(mean_exec_time::numeric, 2) AS mean_ms,
                ROUND((total_exec_time / 1000)::numeric, 2) AS total_seconds,
                ROUND((calls * mean_exec_time / 1000)::numeric, 2) AS impact_seconds
            FROM pg_stat_statements
            WHERE calls > %s
            ORDER BY calls DESC
            LIMIT 20;
        """, (min_calls,))

        return self.cursor.fetchall()

    def generate_report(self, regressions, improvements, slow_queries, frequent_queries):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("\n" + "="*80)
        print("æŸ¥è¯¢æ€§èƒ½åˆ†ææŠ¥å‘Š")
        print("="*80)

        # æ€§èƒ½é€€åŒ–
        if regressions:
            print(f"\nğŸ”´ æ€§èƒ½é€€åŒ– ({len(regressions)}ä¸ªæŸ¥è¯¢):")
            for r in regressions[:10]:
                severity = "ğŸ”´" if r['severity'] == 'critical' else "âš ï¸"
                print(f"\n{severity} æ€§èƒ½ä¸‹é™ {r['degradation_pct']:.1f}%")
                print(f"   æŸ¥è¯¢: {r['query']}...")
                print(f"   åŸºçº¿: {r['baseline_mean']:.2f}ms â†’ å½“å‰: {r['current_mean']:.2f}ms")
                print(f"   è°ƒç”¨æ¬¡æ•°: {r['calls']}, ç¼“å­˜å‘½ä¸­ç‡: {r['cache_hit_ratio']:.1f}%")
        else:
            print("\nâœ“ æ— æ€§èƒ½é€€åŒ–")

        # æ€§èƒ½æ”¹å–„
        if improvements:
            print(f"\nâœ“ æ€§èƒ½æ”¹å–„ ({len(improvements)}ä¸ªæŸ¥è¯¢):")
            for i in improvements[:5]:
                print(f"\n  æ€§èƒ½æå‡ {i['improvement_pct']:.1f}%")
                print(f"   æŸ¥è¯¢: {i['query']}...")
                print(f"   åŸºçº¿: {i['baseline_mean']:.2f}ms â†’ å½“å‰: {i['current_mean']:.2f}ms")

        # æ…¢æŸ¥è¯¢
        if slow_queries:
            print(f"\nâš ï¸  æ…¢æŸ¥è¯¢ ({len(slow_queries)}ä¸ª):")
            for q in slow_queries[:10]:
                print(f"\n  å¹³å‡: {q['mean_ms']}ms, æœ€å¤§: {q['max_ms']}ms")
                print(f"   æŸ¥è¯¢: {q['query']}...")
                print(f"   è°ƒç”¨: {q['calls']}æ¬¡, æ€»è€—æ—¶: {q['total_seconds']}ç§’")
                print(f"   ç¼“å­˜å‘½ä¸­ç‡: {q['cache_hit_ratio']}%")

        # é«˜é¢‘æŸ¥è¯¢
        if frequent_queries:
            print(f"\nğŸ“Š é«˜é¢‘æŸ¥è¯¢ ({len(frequent_queries)}ä¸ª):")
            for q in frequent_queries[:10]:
                print(f"\n  è°ƒç”¨: {q['calls']}æ¬¡, å¹³å‡: {q['mean_ms']}ms")
                print(f"   æŸ¥è¯¢: {q['query']}...")
                print(f"   æ€»è€—æ—¶: {q['total_seconds']}ç§’")

        print("\n" + "="*80)

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report = {
            'timestamp': datetime.now().isoformat(),
            'regressions': regressions,
            'improvements': improvements,
            'slow_queries': [dict(q) for q in slow_queries],
            'frequent_queries': [dict(q) for q in frequent_queries]
        }

        filename = f"query_performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)

        print(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {filename}")

    def monitor_realtime(self, interval: int = 60, duration: int = 3600):
        """å®æ—¶ç›‘æ§"""
        print(f"\nå®æ—¶ç›‘æ§æ¨¡å¼")
        print(f"é—´éš”: {interval}ç§’, æŒç»­: {duration}ç§’")
        print("æŒ‰Ctrl+Cåœæ­¢\n")

        start_time = datetime.now()
        iteration = 0

        try:
            while (datetime.now() - start_time).total_seconds() < duration:
                iteration += 1
                print(f"\n[{datetime.now().strftime('%H:%M:%S')}] è¿­ä»£ #{iteration}")

                # åˆ†ææ…¢æŸ¥è¯¢
                slow = self.analyze_slow_queries(threshold_ms=1000)
                if slow:
                    print(f"  æ…¢æŸ¥è¯¢: {len(slow)}ä¸ª")
                    for q in slow[:3]:
                        print(f"    - {q['mean_ms']}ms: {q['query'][:60]}...")

                time.sleep(interval)

        except KeyboardInterrupt:
            print("\n\nç›‘æ§å·²åœæ­¢")

    def close(self):
        self.cursor.close()
        self.conn.close()

def main():
    parser = argparse.ArgumentParser(
        description='PostgreSQL æŸ¥è¯¢æ€§èƒ½è¿½è¸ªå™¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åˆ›å»ºåŸºçº¿
  python3 query-performance-tracker.py --dbname mydb --create-baseline

  # æ¯”è¾ƒæ€§èƒ½
  python3 query-performance-tracker.py --dbname mydb --baseline baseline.json

  # åˆ†ææ…¢æŸ¥è¯¢
  python3 query-performance-tracker.py --dbname mydb --analyze-slow

  # å®æ—¶ç›‘æ§
  python3 query-performance-tracker.py --dbname mydb --monitor --interval 60
        """
    )

    parser.add_argument('--host', default='localhost')
    parser.add_argument('--port', type=int, default=5432)
    parser.add_argument('--dbname', required=True)
    parser.add_argument('--user', default='postgres')
    parser.add_argument('--password')

    parser.add_argument('--create-baseline', action='store_true',
                       help='åˆ›å»ºæ€§èƒ½åŸºçº¿')
    parser.add_argument('--baseline', type=str,
                       help='åŸºçº¿æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæ¯”è¾ƒï¼‰')
    parser.add_argument('--analyze-slow', action='store_true',
                       help='åˆ†ææ…¢æŸ¥è¯¢')
    parser.add_argument('--threshold', type=float, default=1.5,
                       help='æ€§èƒ½é€€åŒ–é˜ˆå€¼ï¼ˆé»˜è®¤1.5xï¼‰')
    parser.add_argument('--monitor', action='store_true',
                       help='å®æ—¶ç›‘æ§æ¨¡å¼')
    parser.add_argument('--interval', type=int, default=60,
                       help='ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰')
    parser.add_argument('--duration', type=int, default=3600,
                       help='ç›‘æ§æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰')

    args = parser.parse_args()

    conn_str = f"host={args.host} port={args.port} dbname={args.dbname} user={args.user}"
    if args.password:
        conn_str += f" password={args.password}"

    try:
        tracker = QueryPerformanceTracker(conn_str)

        # æ£€æŸ¥æ‰©å±•
        if not tracker.ensure_pg_stat_statements():
            exit(1)

        # åˆ›å»ºåŸºçº¿
        if args.create_baseline:
            tracker.create_baseline()

        # æ¯”è¾ƒæ€§èƒ½
        elif args.baseline:
            if tracker.load_baseline(args.baseline):
                regressions, improvements = tracker.compare_performance(args.threshold)
                slow = tracker.analyze_slow_queries()
                frequent = tracker.analyze_frequent_queries()
                tracker.generate_report(regressions, improvements, slow, frequent)

        # åˆ†ææ…¢æŸ¥è¯¢
        elif args.analyze_slow:
            slow = tracker.analyze_slow_queries()
            frequent = tracker.analyze_frequent_queries()
            tracker.generate_report([], [], slow, frequent)

        # å®æ—¶ç›‘æ§
        elif args.monitor:
            tracker.monitor_realtime(args.interval, args.duration)

        else:
            print("è¯·æŒ‡å®šæ“ä½œ: --create-baseline, --baseline, --analyze-slow, æˆ– --monitor")
            parser.print_help()

        tracker.close()

    except Exception as e:
        print(f"é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

if __name__ == '__main__':
    main()
