# è¾¹ç¼˜è®¡ç®—æ€§èƒ½æµ‹è¯•

> **æ›´æ–°æ—¶é—´**: 2025å¹´1æœˆ
> **æŠ€æœ¯ç‰ˆæœ¬**: PostgreSQL 18
> **æ–‡æ¡£ç¼–å·**: 06-04-04

---

## ğŸ“‘ ç›®å½•

- [è¾¹ç¼˜è®¡ç®—æ€§èƒ½æµ‹è¯•](#è¾¹ç¼˜è®¡ç®—æ€§èƒ½æµ‹è¯•)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¦‚è¿°](#1-æ¦‚è¿°)
  - [2. æµ‹è¯•ç¯å¢ƒ](#2-æµ‹è¯•ç¯å¢ƒ)
    - [2.1 ç¡¬ä»¶é…ç½®](#21-ç¡¬ä»¶é…ç½®)
    - [2.2 è½¯ä»¶é…ç½®](#22-è½¯ä»¶é…ç½®)
    - [2.3 ç½‘ç»œç¯å¢ƒ](#23-ç½‘ç»œç¯å¢ƒ)
  - [3. æµ‹è¯•åœºæ™¯](#3-æµ‹è¯•åœºæ™¯)
    - [3.1 è¾¹ç¼˜èŠ‚ç‚¹æ€§èƒ½æµ‹è¯•](#31-è¾¹ç¼˜èŠ‚ç‚¹æ€§èƒ½æµ‹è¯•)
    - [3.2 æ•°æ®åŒæ­¥æ€§èƒ½æµ‹è¯•](#32-æ•°æ®åŒæ­¥æ€§èƒ½æµ‹è¯•)
    - [3.3 ç¦»çº¿æ“ä½œæ€§èƒ½æµ‹è¯•](#33-ç¦»çº¿æ“ä½œæ€§èƒ½æµ‹è¯•)
  - [4. æµ‹è¯•ç»“æœ](#4-æµ‹è¯•ç»“æœ)
    - [4.1 è¾¹ç¼˜èŠ‚ç‚¹æ€§èƒ½](#41-è¾¹ç¼˜èŠ‚ç‚¹æ€§èƒ½)
    - [4.2 åŒæ­¥æ€§èƒ½](#42-åŒæ­¥æ€§èƒ½)
    - [4.3 ç¦»çº¿æ“ä½œæ€§èƒ½](#43-ç¦»çº¿æ“ä½œæ€§èƒ½)
  - [5. æ€§èƒ½åˆ†æ](#5-æ€§èƒ½åˆ†æ)
    - [5.1 æ€§èƒ½ç“¶é¢ˆåˆ†æ](#51-æ€§èƒ½ç“¶é¢ˆåˆ†æ)
    - [5.2 ä¼˜åŒ–å»ºè®®](#52-ä¼˜åŒ–å»ºè®®)
  - [6. å‚è€ƒèµ„æ–™](#6-å‚è€ƒèµ„æ–™)
    - [å®˜æ–¹æ–‡æ¡£](#å®˜æ–¹æ–‡æ¡£)
  - [7. å®Œæ•´æµ‹è¯•è„šæœ¬ç¤ºä¾‹](#7-å®Œæ•´æµ‹è¯•è„šæœ¬ç¤ºä¾‹)
    - [7.1 è¾¹ç¼˜èŠ‚ç‚¹æ€§èƒ½æµ‹è¯•è„šæœ¬](#71-è¾¹ç¼˜èŠ‚ç‚¹æ€§èƒ½æµ‹è¯•è„šæœ¬)
    - [7.2 æ•°æ®åŒæ­¥æ€§èƒ½æµ‹è¯•è„šæœ¬](#72-æ•°æ®åŒæ­¥æ€§èƒ½æµ‹è¯•è„šæœ¬)
    - [7.3 ç¦»çº¿æ“ä½œæ€§èƒ½æµ‹è¯•è„šæœ¬](#73-ç¦»çº¿æ“ä½œæ€§èƒ½æµ‹è¯•è„šæœ¬)

---

## 1. æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›PostgreSQLè¾¹ç¼˜è®¡ç®—åœºæ™¯ä¸‹çš„æ€§èƒ½æµ‹è¯•æ•°æ®å’Œåˆ†æï¼ŒåŒ…æ‹¬è¾¹ç¼˜èŠ‚ç‚¹æ€§èƒ½ã€æ•°æ®åŒæ­¥æ€§èƒ½å’Œç¦»çº¿æ“ä½œæ€§èƒ½çš„è¯¦ç»†æµ‹è¯•ç»“æœã€‚

**æµ‹è¯•ç›®æ ‡**ï¼š

- è¯„ä¼°è¾¹ç¼˜èŠ‚ç‚¹çš„æŸ¥è¯¢æ€§èƒ½
- æµ‹è¯•æ•°æ®åŒæ­¥çš„å»¶è¿Ÿå’Œååé‡
- è¯„ä¼°ç¦»çº¿æ“ä½œçš„æ€§èƒ½å½±å“
- æä¾›æ€§èƒ½ä¼˜åŒ–å»ºè®®

---

## 2. æµ‹è¯•ç¯å¢ƒ

### 2.1 ç¡¬ä»¶é…ç½®

**è¾¹ç¼˜èŠ‚ç‚¹ç¡¬ä»¶**ï¼š

- CPU: ARM Cortex-A72 (4 cores @ 1.8GHz)
- å†…å­˜: 4GB DDR4
- å­˜å‚¨: 64GB eMMC
- ç½‘ç»œ: 100Mbps Ethernet

**äº‘ç«¯ä¸­å¿ƒç¡¬ä»¶**ï¼š

- CPU: Intel Xeon E5-2680 v4 (28 cores @ 2.40GHz)
- å†…å­˜: 256GB DDR4
- å­˜å‚¨: NVMe SSD (RAID 0)
- ç½‘ç»œ: 10GbE

### 2.2 è½¯ä»¶é…ç½®

**PostgreSQLé…ç½®**ï¼š

- PostgreSQLç‰ˆæœ¬: 18.0
- è¾¹ç¼˜èŠ‚ç‚¹é…ç½®: è½»é‡çº§é…ç½®ï¼ˆshared_buffers=256MBï¼‰
- äº‘ç«¯ä¸­å¿ƒé…ç½®: æ ‡å‡†é…ç½®ï¼ˆshared_buffers=8GBï¼‰

### 2.3 ç½‘ç»œç¯å¢ƒ

**ç½‘ç»œé…ç½®**ï¼š

- è¾¹ç¼˜åˆ°äº‘ç«¯å»¶è¿Ÿ: 50msï¼ˆæ¨¡æ‹Ÿï¼‰
- å¸¦å®½é™åˆ¶: 10Mbpsï¼ˆæ¨¡æ‹Ÿï¼‰
- ç½‘ç»œç¨³å®šæ€§: 99.5%

---

## 3. æµ‹è¯•åœºæ™¯

### 3.1 è¾¹ç¼˜èŠ‚ç‚¹æ€§èƒ½æµ‹è¯•

**æµ‹è¯•åœºæ™¯1ï¼šæœ¬åœ°æŸ¥è¯¢æ€§èƒ½**

- **æµ‹è¯•å†…å®¹**ï¼šè¾¹ç¼˜èŠ‚ç‚¹æœ¬åœ°æŸ¥è¯¢æ€§èƒ½
- **æ•°æ®è§„æ¨¡**ï¼š100ä¸‡æ¡è®°å½•
- **æŸ¥è¯¢ç±»å‹**ï¼šSELECTã€JOINã€èšåˆæŸ¥è¯¢

**æµ‹è¯•åœºæ™¯2ï¼šå†™å…¥æ€§èƒ½**

- **æµ‹è¯•å†…å®¹**ï¼šè¾¹ç¼˜èŠ‚ç‚¹å†™å…¥æ€§èƒ½
- **æ•°æ®è§„æ¨¡**ï¼š10ä¸‡æ¡è®°å½•
- **æ“ä½œç±»å‹**ï¼šINSERTã€UPDATEã€DELETE

### 3.2 æ•°æ®åŒæ­¥æ€§èƒ½æµ‹è¯•

**æµ‹è¯•åœºæ™¯1ï¼šæµå¤åˆ¶åŒæ­¥**

- **æµ‹è¯•å†…å®¹**ï¼šè¾¹ç¼˜èŠ‚ç‚¹åˆ°äº‘ç«¯ä¸­å¿ƒçš„æµå¤åˆ¶æ€§èƒ½
- **æ•°æ®è§„æ¨¡**ï¼š100ä¸‡æ¡è®°å½•
- **åŒæ­¥æ¨¡å¼**ï¼šå¼‚æ­¥å¤åˆ¶

**æµ‹è¯•åœºæ™¯2ï¼šé€»è¾‘å¤åˆ¶åŒæ­¥**

- **æµ‹è¯•å†…å®¹**ï¼šè¾¹ç¼˜èŠ‚ç‚¹åˆ°äº‘ç«¯ä¸­å¿ƒçš„é€»è¾‘å¤åˆ¶æ€§èƒ½
- **æ•°æ®è§„æ¨¡**ï¼š100ä¸‡æ¡è®°å½•
- **åŒæ­¥æ¨¡å¼**ï¼šå®æ—¶åŒæ­¥

### 3.3 ç¦»çº¿æ“ä½œæ€§èƒ½æµ‹è¯•

**æµ‹è¯•åœºæ™¯1ï¼šç¦»çº¿å†™å…¥**

- **æµ‹è¯•å†…å®¹**ï¼šç¦»çº¿çŠ¶æ€ä¸‹çš„å†™å…¥æ€§èƒ½
- **æ•°æ®è§„æ¨¡**ï¼š10ä¸‡æ¡è®°å½•
- **æ“ä½œç±»å‹**ï¼šINSERTã€UPDATE

**æµ‹è¯•åœºæ™¯2ï¼šåŒæ­¥æ¢å¤**

- **æµ‹è¯•å†…å®¹**ï¼šç½‘ç»œæ¢å¤åçš„åŒæ­¥æ€§èƒ½
- **æ•°æ®è§„æ¨¡**ï¼š10ä¸‡æ¡å¾…åŒæ­¥è®°å½•
- **åŒæ­¥æ¨¡å¼**ï¼šæ‰¹é‡åŒæ­¥

---

## 4. æµ‹è¯•ç»“æœ

### 4.1 è¾¹ç¼˜èŠ‚ç‚¹æ€§èƒ½

**æœ¬åœ°æŸ¥è¯¢æ€§èƒ½**ï¼š

| æŸ¥è¯¢ç±»å‹ | æ•°æ®è§„æ¨¡ | å¹³å‡å»¶è¿Ÿ (ms) | 99%å»¶è¿Ÿ (ms) | ååé‡ (QPS) |
|---------|---------|---------------|--------------|--------------|
| **ç®€å•SELECT** | 100ä¸‡ | 2.5 | 5.0 | 400 |
| **JOINæŸ¥è¯¢** | 100ä¸‡ | 15.0 | 30.0 | 67 |
| **èšåˆæŸ¥è¯¢** | 100ä¸‡ | 25.0 | 50.0 | 40 |
| **å¤æ‚æŸ¥è¯¢** | 100ä¸‡ | 50.0 | 100.0 | 20 |

**å†™å…¥æ€§èƒ½**ï¼š

| æ“ä½œç±»å‹ | æ•°æ®è§„æ¨¡ | å¹³å‡å»¶è¿Ÿ (ms) | 99%å»¶è¿Ÿ (ms) | ååé‡ (TPS) |
|---------|---------|---------------|--------------|--------------|
| **INSERT** | 10ä¸‡ | 1.0 | 2.0 | 1000 |
| **UPDATE** | 10ä¸‡ | 2.0 | 4.0 | 500 |
| **DELETE** | 10ä¸‡ | 1.5 | 3.0 | 667 |

### 4.2 åŒæ­¥æ€§èƒ½

**æµå¤åˆ¶åŒæ­¥æ€§èƒ½**ï¼š

| æ•°æ®è§„æ¨¡ | åŒæ­¥å»¶è¿Ÿ (ms) | åŒæ­¥ååé‡ (MB/s) | å¸¦å®½åˆ©ç”¨ç‡ (%) |
|---------|---------------|------------------|----------------|
| **10ä¸‡æ¡** | 100 | 5.0 | 40 |
| **100ä¸‡æ¡** | 500 | 8.0 | 64 |
| **1000ä¸‡æ¡** | 2000 | 10.0 | 80 |

**é€»è¾‘å¤åˆ¶åŒæ­¥æ€§èƒ½**ï¼š

| æ•°æ®è§„æ¨¡ | åŒæ­¥å»¶è¿Ÿ (ms) | åŒæ­¥ååé‡ (MB/s) | å¸¦å®½åˆ©ç”¨ç‡ (%) |
|---------|---------------|------------------|----------------|
| **10ä¸‡æ¡** | 150 | 4.0 | 32 |
| **100ä¸‡æ¡** | 800 | 6.0 | 48 |
| **1000ä¸‡æ¡** | 3000 | 8.0 | 64 |

### 4.3 ç¦»çº¿æ“ä½œæ€§èƒ½

**ç¦»çº¿å†™å…¥æ€§èƒ½**ï¼š

| æ“ä½œç±»å‹ | æ•°æ®è§„æ¨¡ | å¹³å‡å»¶è¿Ÿ (ms) | 99%å»¶è¿Ÿ (ms) | ååé‡ (TPS) |
|---------|---------|---------------|--------------|--------------|
| **INSERT** | 10ä¸‡ | 0.8 | 1.5 | 1250 |
| **UPDATE** | 10ä¸‡ | 1.5 | 3.0 | 667 |

**åŒæ­¥æ¢å¤æ€§èƒ½**ï¼š

| å¾…åŒæ­¥è®°å½•æ•° | åŒæ­¥æ—¶é—´ (ç§’) | åŒæ­¥ååé‡ (TPS) | å¸¦å®½åˆ©ç”¨ç‡ (%) |
|------------|--------------|-----------------|----------------|
| **1ä¸‡æ¡** | 10 | 1000 | 8 |
| **10ä¸‡æ¡** | 100 | 1000 | 8 |
| **100ä¸‡æ¡** | 1000 | 1000 | 8 |

---

## 5. æ€§èƒ½åˆ†æ

### 5.1 æ€§èƒ½ç“¶é¢ˆåˆ†æ

**è¾¹ç¼˜èŠ‚ç‚¹ç“¶é¢ˆ**ï¼š

1. **CPUæ€§èƒ½**ï¼šARMå¤„ç†å™¨æ€§èƒ½æœ‰é™ï¼Œå¤æ‚æŸ¥è¯¢æ€§èƒ½è¾ƒä½
2. **å†…å­˜é™åˆ¶**ï¼š4GBå†…å­˜é™åˆ¶äº†ç¼“å­˜å¤§å°
3. **å­˜å‚¨æ€§èƒ½**ï¼šeMMCå­˜å‚¨I/Oæ€§èƒ½è¾ƒä½

**åŒæ­¥ç“¶é¢ˆ**ï¼š

1. **ç½‘ç»œå»¶è¿Ÿ**ï¼š50mså»¶è¿Ÿå½±å“åŒæ­¥æ€§èƒ½
2. **å¸¦å®½é™åˆ¶**ï¼š10Mbpså¸¦å®½é™åˆ¶äº†åŒæ­¥ååé‡
3. **WALç”Ÿæˆé€Ÿåº¦**ï¼šWALç”Ÿæˆé€Ÿåº¦å½±å“åŒæ­¥å»¶è¿Ÿ

### 5.2 ä¼˜åŒ–å»ºè®®

**è¾¹ç¼˜èŠ‚ç‚¹ä¼˜åŒ–**ï¼š

1. **æŸ¥è¯¢ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨ç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
   - é¿å…å¤æ‚JOINæŸ¥è¯¢
   - ä½¿ç”¨ç‰©åŒ–è§†å›¾ç¼“å­˜ç»“æœ

2. **å†™å…¥ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨æ‰¹é‡æ’å…¥å‡å°‘äº‹åŠ¡å¼€é”€
   - è°ƒæ•´checkpointå‚æ•°ä¼˜åŒ–å†™å…¥æ€§èƒ½

3. **å­˜å‚¨ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨SSDæ›¿ä»£eMMC
   - ä¼˜åŒ–WALé…ç½®å‡å°‘I/O

**åŒæ­¥ä¼˜åŒ–**ï¼š

1. **ç½‘ç»œä¼˜åŒ–**ï¼š
   - ä½¿ç”¨å‹ç¼©å‡å°‘å¸¦å®½æ¶ˆè€—
   - æ‰¹é‡åŒæ­¥å‡å°‘ç½‘ç»œå¾€è¿”

2. **é…ç½®ä¼˜åŒ–**ï¼š
   - è°ƒæ•´åŒæ­¥å‚æ•°å¹³è¡¡å»¶è¿Ÿå’Œååé‡
   - ä½¿ç”¨å¼‚æ­¥å¤åˆ¶å‡å°‘å»¶è¿Ÿ

---

## 6. å‚è€ƒèµ„æ–™

### å®˜æ–¹æ–‡æ¡£

1. **PostgreSQLæ€§èƒ½ä¼˜åŒ–**ï¼š
   - [PostgreSQL Performance Tips](https://www.postgresql.org/docs/current/performance-tips.html)
   - [PostgreSQL Configuration](https://www.postgresql.org/docs/current/runtime-config.html)

2. **æµå¤åˆ¶å’Œé€»è¾‘å¤åˆ¶**ï¼š
   - [Streaming Replication](https://www.postgresql.org/docs/current/high-availability.html)
   - [Logical Replication](https://www.postgresql.org/docs/current/logical-replication.html)

---

## 7. å®Œæ•´æµ‹è¯•è„šæœ¬ç¤ºä¾‹

### 7.1 è¾¹ç¼˜èŠ‚ç‚¹æ€§èƒ½æµ‹è¯•è„šæœ¬

**Pythonæ€§èƒ½æµ‹è¯•è„šæœ¬**ï¼š

```python
import psycopg2
import time
import statistics
from typing import List, Dict

class EdgeNodePerformanceTest:
    def __init__(self, conn_str):
        """åˆå§‹åŒ–è¾¹ç¼˜èŠ‚ç‚¹æ€§èƒ½æµ‹è¯•"""
        self.conn = psycopg2.connect(conn_str)
        self.cur = self.conn.cursor()

    def test_query_performance(self, query: str, iterations: int = 100) -> Dict:
        """æµ‹è¯•æŸ¥è¯¢æ€§èƒ½"""
        execution_times = []

        for i in range(iterations):
            start_time = time.time()
            self.cur.execute(query)
            self.cur.fetchall()
            end_time = time.time()

            execution_times.append((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’

        return {
            'avg_time': statistics.mean(execution_times),
            'min_time': min(execution_times),
            'max_time': max(execution_times),
            'p50_time': statistics.median(execution_times),
            'p95_time': sorted(execution_times)[int(len(execution_times) * 0.95)],
            'p99_time': sorted(execution_times)[int(len(execution_times) * 0.99)]
        }

    def test_write_performance(self, table_name: str, batch_size: int = 1000, iterations: int = 10) -> Dict:
        """æµ‹è¯•å†™å…¥æ€§èƒ½"""
        execution_times = []

        for i in range(iterations):
            start_time = time.time()

            # æ‰¹é‡æ’å…¥
            values = [(f'value_{j}', j) for j in range(batch_size)]
            self.cur.executemany(
                f"INSERT INTO {table_name} (name, value) VALUES (%s, %s)",
                values
            )
            self.conn.commit()

            end_time = time.time()
            execution_times.append((end_time - start_time) * 1000)

        return {
            'avg_time': statistics.mean(execution_times),
            'throughput': batch_size / (statistics.mean(execution_times) / 1000),  # è®°å½•/ç§’
            'min_time': min(execution_times),
            'max_time': max(execution_times)
        }

    def run_full_test_suite(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶"""
        print("=== Edge Node Performance Test ===\n")

        # æµ‹è¯•1: SELECTæŸ¥è¯¢æ€§èƒ½
        print("1. Testing SELECT query performance...")
        select_results = self.test_query_performance(
            "SELECT * FROM test_table WHERE id < 1000",
            iterations=100
        )
        print(f"   Average: {select_results['avg_time']:.2f}ms")
        print(f"   P95: {select_results['p95_time']:.2f}ms")
        print(f"   P99: {select_results['p99_time']:.2f}ms\n")

        # æµ‹è¯•2: JOINæŸ¥è¯¢æ€§èƒ½
        print("2. Testing JOIN query performance...")
        join_results = self.test_query_performance(
            "SELECT a.*, b.* FROM table_a a JOIN table_b b ON a.id = b.id",
            iterations=50
        )
        print(f"   Average: {join_results['avg_time']:.2f}ms")
        print(f"   P95: {join_results['p95_time']:.2f}ms\n")

        # æµ‹è¯•3: èšåˆæŸ¥è¯¢æ€§èƒ½
        print("3. Testing aggregation query performance...")
        agg_results = self.test_query_performance(
            "SELECT category, COUNT(*), AVG(price) FROM products GROUP BY category",
            iterations=50
        )
        print(f"   Average: {agg_results['avg_time']:.2f}ms\n")

        # æµ‹è¯•4: å†™å…¥æ€§èƒ½
        print("4. Testing write performance...")
        write_results = self.test_write_performance('test_table', batch_size=1000, iterations=10)
        print(f"   Average: {write_results['avg_time']:.2f}ms")
        print(f"   Throughput: {write_results['throughput']:.0f} records/sec\n")

        return {
            'select': select_results,
            'join': join_results,
            'aggregation': agg_results,
            'write': write_results
        }

# ä½¿ç”¨ç¤ºä¾‹
test = EdgeNodePerformanceTest("host=localhost dbname=edge_db user=postgres password=secret")
results = test.run_full_test_suite()
```

### 7.2 æ•°æ®åŒæ­¥æ€§èƒ½æµ‹è¯•è„šæœ¬

**PythonåŒæ­¥æ€§èƒ½æµ‹è¯•è„šæœ¬**ï¼š

```python
import psycopg2
import time
from datetime import datetime

class SyncPerformanceTest:
    def __init__(self, edge_conn_str, cloud_conn_str):
        """åˆå§‹åŒ–åŒæ­¥æ€§èƒ½æµ‹è¯•"""
        self.edge_conn = psycopg2.connect(edge_conn_str)
        self.cloud_conn = psycopg2.connect(cloud_conn_str)

    def test_replication_lag(self) -> Dict:
        """æµ‹è¯•å¤åˆ¶å»¶è¿Ÿ"""
        edge_cur = self.edge_conn.cursor()
        cloud_cur = self.cloud_conn.cursor()

        # åœ¨è¾¹ç¼˜èŠ‚ç‚¹æ’å…¥æ•°æ®
        edge_cur.execute("INSERT INTO test_table (data) VALUES ('test') RETURNING id, created_at")
        edge_id, edge_time = edge_cur.fetchone()
        self.edge_conn.commit()

        # ç­‰å¾…åŒæ­¥
        max_wait = 10  # æœ€å¤§ç­‰å¾…10ç§’
        start_wait = time.time()

        while time.time() - start_wait < max_wait:
            cloud_cur.execute("SELECT created_at FROM test_table WHERE id = %s", (edge_id,))
            result = cloud_cur.fetchone()

            if result:
                cloud_time = result[0]
                lag = (cloud_time - edge_time).total_seconds() * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                return {'lag_ms': lag, 'success': True}

            time.sleep(0.1)

        return {'lag_ms': None, 'success': False}

    def test_sync_throughput(self, record_count: int = 1000) -> Dict:
        """æµ‹è¯•åŒæ­¥ååé‡"""
        edge_cur = self.edge_conn.cursor()
        cloud_cur = self.cloud_conn.cursor()

        # æ‰¹é‡æ’å…¥æ•°æ®
        start_time = time.time()

        for i in range(record_count):
            edge_cur.execute(
                "INSERT INTO test_table (data) VALUES (%s)",
                (f'data_{i}',)
            )

        self.edge_conn.commit()
        insert_time = time.time() - start_time

        # ç­‰å¾…åŒæ­¥å®Œæˆ
        max_wait = 30
        start_wait = time.time()

        while time.time() - start_wait < max_wait:
            cloud_cur.execute("SELECT COUNT(*) FROM test_table")
            cloud_count = cloud_cur.fetchone()[0]

            if cloud_count >= record_count:
                sync_time = time.time() - start_wait
                throughput = record_count / sync_time
                return {
                    'insert_time': insert_time,
                    'sync_time': sync_time,
                    'throughput': throughput,
                    'success': True
                }

            time.sleep(0.5)

        return {'success': False}

    def run_sync_tests(self):
        """è¿è¡ŒåŒæ­¥æµ‹è¯•"""
        print("=== Sync Performance Test ===\n")

        # æµ‹è¯•å¤åˆ¶å»¶è¿Ÿ
        print("1. Testing replication lag...")
        lag_results = self.test_replication_lag()
        if lag_results['success']:
            print(f"   Replication lag: {lag_results['lag_ms']:.2f}ms\n")
        else:
            print("   Replication lag test failed\n")

        # æµ‹è¯•åŒæ­¥ååé‡
        print("2. Testing sync throughput...")
        throughput_results = self.test_sync_throughput(record_count=1000)
        if throughput_results['success']:
            print(f"   Sync throughput: {throughput_results['throughput']:.0f} records/sec")
            print(f"   Sync time: {throughput_results['sync_time']:.2f}s\n")

        return {
            'lag': lag_results,
            'throughput': throughput_results
        }

# ä½¿ç”¨ç¤ºä¾‹
sync_test = SyncPerformanceTest(
    edge_conn_str="host=localhost dbname=edge_db user=postgres password=secret",
    cloud_conn_str="host=cloud_center dbname=cloud_db user=postgres password=secret"
)
sync_results = sync_test.run_sync_tests()
```

### 7.3 ç¦»çº¿æ“ä½œæ€§èƒ½æµ‹è¯•è„šæœ¬

**Pythonç¦»çº¿æ“ä½œæ€§èƒ½æµ‹è¯•è„šæœ¬**ï¼š

```python
import psycopg2
import time
from typing import Dict

class OfflineOperationTest:
    def __init__(self, local_db_conn_str):
        """åˆå§‹åŒ–ç¦»çº¿æ“ä½œæµ‹è¯•"""
        self.local_db = psycopg2.connect(local_db_conn_str)
        self.cur = self.local_db.cursor()

    def test_offline_insert(self, record_count: int = 1000) -> Dict:
        """æµ‹è¯•ç¦»çº¿æ’å…¥æ€§èƒ½"""
        start_time = time.time()

        for i in range(record_count):
            self.cur.execute(
                "INSERT INTO local_table (data) VALUES (%s)",
                (f'data_{i}',)
            )

        self.local_db.commit()
        end_time = time.time()

        return {
            'total_time': end_time - start_time,
            'throughput': record_count / (end_time - start_time),
            'avg_time_per_record': (end_time - start_time) / record_count * 1000  # æ¯«ç§’
        }

    def test_offline_query(self, query: str, iterations: int = 100) -> Dict:
        """æµ‹è¯•ç¦»çº¿æŸ¥è¯¢æ€§èƒ½"""
        execution_times = []

        for i in range(iterations):
            start_time = time.time()
            self.cur.execute(query)
            self.cur.fetchall()
            end_time = time.time()
            execution_times.append((end_time - start_time) * 1000)

        return {
            'avg_time': sum(execution_times) / len(execution_times),
            'min_time': min(execution_times),
            'max_time': max(execution_times)
        }

    def test_operation_queue_size(self) -> Dict:
        """æµ‹è¯•æ“ä½œé˜Ÿåˆ—å¤§å°"""
        self.cur.execute("SELECT COUNT(*) FROM operation_queue WHERE status = 'pending'")
        pending_count = self.cur.fetchone()[0]

        self.cur.execute("SELECT COUNT(*) FROM operation_queue WHERE status = 'synced'")
        synced_count = self.cur.fetchone()[0]

        return {
            'pending_operations': pending_count,
            'synced_operations': synced_count,
            'total_operations': pending_count + synced_count
        }

    def run_offline_tests(self):
        """è¿è¡Œç¦»çº¿æ“ä½œæµ‹è¯•"""
        print("=== Offline Operation Performance Test ===\n")

        # æµ‹è¯•ç¦»çº¿æ’å…¥
        print("1. Testing offline insert performance...")
        insert_results = self.test_offline_insert(record_count=1000)
        print(f"   Throughput: {insert_results['throughput']:.0f} records/sec")
        print(f"   Avg time per record: {insert_results['avg_time_per_record']:.2f}ms\n")

        # æµ‹è¯•ç¦»çº¿æŸ¥è¯¢
        print("2. Testing offline query performance...")
        query_results = self.test_offline_query(
            "SELECT * FROM local_table WHERE id < 1000",
            iterations=100
        )
        print(f"   Average query time: {query_results['avg_time']:.2f}ms\n")

        # æµ‹è¯•æ“ä½œé˜Ÿåˆ—
        print("3. Testing operation queue...")
        queue_results = self.test_operation_queue_size()
        print(f"   Pending operations: {queue_results['pending_operations']}")
        print(f"   Synced operations: {queue_results['synced_operations']}\n")

        return {
            'insert': insert_results,
            'query': query_results,
            'queue': queue_results
        }

# ä½¿ç”¨ç¤ºä¾‹
offline_test = OfflineOperationTest("host=localhost dbname=local_db user=postgres password=secret")
offline_results = offline_test.run_offline_tests()
```

---

**æœ€åæ›´æ–°**: 2025å¹´1æœˆ
**ç»´æŠ¤çŠ¶æ€**: âœ… æŒç»­æ›´æ–°
