# å¼ºåŒ–å­¦ä¹ åœ¨æ•°æ®åº“ä¼˜åŒ–ä¸­çš„åº”ç”¨

> **æ›´æ–°æ—¶é—´**: 2025 å¹´ 11 æœˆ 1 æ—¥
> **æŠ€æœ¯ç‰ˆæœ¬**: pg_ai v1.0+
> **æ–‡æ¡£ç¼–å·**: 02-01-02

## ğŸ“‘ ç›®å½•

- [å¼ºåŒ–å­¦ä¹ åœ¨æ•°æ®åº“ä¼˜åŒ–ä¸­çš„åº”ç”¨](#å¼ºåŒ–å­¦ä¹ åœ¨æ•°æ®åº“ä¼˜åŒ–ä¸­çš„åº”ç”¨)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¦‚è¿°](#1-æ¦‚è¿°)
    - [1.1 æŠ€æœ¯èƒŒæ™¯](#11-æŠ€æœ¯èƒŒæ™¯)
    - [1.2 æŠ€æœ¯å®šä½](#12-æŠ€æœ¯å®šä½)
    - [1.3 æ ¸å¿ƒä»·å€¼](#13-æ ¸å¿ƒä»·å€¼)
  - [2. å¼ºåŒ–å­¦ä¹ åŸºç¡€](#2-å¼ºåŒ–å­¦ä¹ åŸºç¡€)
    - [2.1 å¼ºåŒ–å­¦ä¹ åŸºæœ¬æ¦‚å¿µ](#21-å¼ºåŒ–å­¦ä¹ åŸºæœ¬æ¦‚å¿µ)
    - [2.2 æ•°æ®åº“ä¼˜åŒ–é—®é¢˜å»ºæ¨¡](#22-æ•°æ®åº“ä¼˜åŒ–é—®é¢˜å»ºæ¨¡)
    - [2.3 çŠ¶æ€ç©ºé—´è®¾è®¡](#23-çŠ¶æ€ç©ºé—´è®¾è®¡)
    - [2.4 åŠ¨ä½œç©ºé—´è®¾è®¡](#24-åŠ¨ä½œç©ºé—´è®¾è®¡)
    - [2.5 å¥–åŠ±å‡½æ•°è®¾è®¡](#25-å¥–åŠ±å‡½æ•°è®¾è®¡)
  - [3. æ ¸å¿ƒç®—æ³•](#3-æ ¸å¿ƒç®—æ³•)
    - [3.1 Actor-Critic ç®—æ³•](#31-actor-critic-ç®—æ³•)
    - [3.2 ç­–ç•¥æ¢¯åº¦æ–¹æ³•](#32-ç­–ç•¥æ¢¯åº¦æ–¹æ³•)
    - [3.3 ç»éªŒå›æ”¾æœºåˆ¶](#33-ç»éªŒå›æ”¾æœºåˆ¶)
  - [4. åº”ç”¨åœºæ™¯](#4-åº”ç”¨åœºæ™¯)
    - [4.1 æŸ¥è¯¢è®¡åˆ’ä¼˜åŒ–](#41-æŸ¥è¯¢è®¡åˆ’ä¼˜åŒ–)
    - [4.2 ç´¢å¼•æ¨è](#42-ç´¢å¼•æ¨è)
    - [4.3 å‚æ•°è°ƒä¼˜](#43-å‚æ•°è°ƒä¼˜)
  - [5. å®ç°ç»†èŠ‚](#5-å®ç°ç»†èŠ‚)
    - [5.1 ç¯å¢ƒè®¾è®¡](#51-ç¯å¢ƒè®¾è®¡)
    - [5.2 ç¥ç»ç½‘ç»œæ¶æ„](#52-ç¥ç»ç½‘ç»œæ¶æ„)
    - [5.3 è®­ç»ƒæµç¨‹](#53-è®­ç»ƒæµç¨‹)
  - [6. æ€§èƒ½åˆ†æ](#6-æ€§èƒ½åˆ†æ)
    - [6.1 ä¼˜åŒ–æ•ˆæœ](#61-ä¼˜åŒ–æ•ˆæœ)
    - [6.2 è®­ç»ƒæ•ˆç‡](#62-è®­ç»ƒæ•ˆç‡)
    - [6.3 å®é™…åº”ç”¨æ¡ˆä¾‹](#63-å®é™…åº”ç”¨æ¡ˆä¾‹)
  - [7. æœ€ä½³å®è·µ](#7-æœ€ä½³å®è·µ)
    - [7.1 æ¨¡å‹è®­ç»ƒ](#71-æ¨¡å‹è®­ç»ƒ)
    - [7.2 éƒ¨ç½²ç­–ç•¥](#72-éƒ¨ç½²ç­–ç•¥)
    - [7.3 ç›‘æ§ä¸è°ƒä¼˜](#73-ç›‘æ§ä¸è°ƒä¼˜)
  - [8. å¸¸è§é—®é¢˜ï¼ˆFAQï¼‰](#8-å¸¸è§é—®é¢˜faq)
    - [8.1 å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ€§èƒ½ç›¸å…³é—®é¢˜](#81-å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ€§èƒ½ç›¸å…³é—®é¢˜)
    - [8.2 å¼ºåŒ–å­¦ä¹ ç®—æ³•ç›¸å…³é—®é¢˜](#82-å¼ºåŒ–å­¦ä¹ ç®—æ³•ç›¸å…³é—®é¢˜)
  - [9. å‚è€ƒèµ„æ–™](#9-å‚è€ƒèµ„æ–™)
    - [8.1 å­¦æœ¯è®ºæ–‡](#81-å­¦æœ¯è®ºæ–‡)
    - [8.2 å®˜æ–¹æ–‡æ¡£](#82-å®˜æ–¹æ–‡æ¡£)
    - [8.3 å®é™…åº”ç”¨æ¡ˆä¾‹](#83-å®é™…åº”ç”¨æ¡ˆä¾‹)
    - [8.4 ç›¸å…³èµ„æº](#84-ç›¸å…³èµ„æº)
  - [10. å®Œæ•´ä»£ç ç¤ºä¾‹](#10-å®Œæ•´ä»£ç ç¤ºä¾‹)
    - [8.1 å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨å®‰è£…ä¸é…ç½®](#81-å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨å®‰è£…ä¸é…ç½®)
    - [8.2 å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨è®­ç»ƒç¤ºä¾‹](#82-å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨è®­ç»ƒç¤ºä¾‹)
    - [8.3 å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨ä½¿ç”¨ç¤ºä¾‹](#83-å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨ä½¿ç”¨ç¤ºä¾‹)
    - [8.4 è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ç¤ºä¾‹](#84-è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ç¤ºä¾‹)
    - [8.5 å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨å®Œæ•´åº”ç”¨ç¤ºä¾‹](#85-å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨å®Œæ•´åº”ç”¨ç¤ºä¾‹)

---

## 1. æ¦‚è¿°

### 1.1 æŠ€æœ¯èƒŒæ™¯

**é—®é¢˜éœ€æ±‚**:

ä¼ ç»Ÿçš„æ•°æ®åº“ä¼˜åŒ–ä¾èµ–äºäººå·¥ç»éªŒå’Œè§„åˆ™ï¼Œæ— æ³•é€‚åº”åŠ¨æ€å˜åŒ–çš„å·¥ä½œè´Ÿè½½ã€‚éšç€æ•°æ®åº“è§„æ¨¡å’Œå·¥ä½œè´Ÿè½½å¤æ‚åº¦çš„å¢
åŠ ï¼Œä¼ ç»Ÿä¼˜åŒ–æ–¹æ³•é¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼š

- **è§„åˆ™å±€é™æ€§**: åŸºäºè§„åˆ™çš„ä¼˜åŒ–æ— æ³•è¦†ç›–æ‰€æœ‰åœºæ™¯
- **é™æ€ä¼˜åŒ–**: æ— æ³•é€‚åº”å·¥ä½œè´Ÿè½½çš„åŠ¨æ€å˜åŒ–
- **äººå·¥æˆæœ¬**: éœ€è¦ç»éªŒä¸°å¯Œçš„ DBA æŒç»­è°ƒä¼˜
- **æ¬¡ä¼˜è§£**: éš¾ä»¥æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£

**æŠ€æœ¯æ¼”è¿›**:

1. **2015 å¹´**: å¼ºåŒ–å­¦ä¹ åœ¨æ¸¸æˆé¢†åŸŸå–å¾—çªç ´ï¼ˆAlphaGoï¼‰
2. **2018 å¹´**: é¦–æ¬¡å°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºæ•°æ®åº“ä¼˜åŒ–
3. **2020 å¹´**: PostgreSQL ç¤¾åŒºå¼€å§‹æ¢ç´¢ AI è‡ªæ²»ä¼˜åŒ–
4. **2025 å¹´**: pg_ai é¡¹ç›®å®ç°ç”Ÿäº§çº§å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨

**å¸‚åœºéœ€æ±‚**:

- **è‡ªåŠ¨åŒ–ä¼˜åŒ–**: å‡å°‘äººå·¥å¹²é¢„ï¼Œå®ç°è‡ªåŠ¨åŒ–ä¼˜åŒ–
- **è‡ªé€‚åº”èƒ½åŠ›**: é€‚åº”å·¥ä½œè´Ÿè½½çš„åŠ¨æ€å˜åŒ–
- **æ€§èƒ½æå‡**: æŒç»­ä¼˜åŒ–ï¼Œæå‡æ•°æ®åº“æ€§èƒ½
- **æˆæœ¬é™ä½**: å‡å°‘ DBA äººåŠ›æˆæœ¬

### 1.2 æŠ€æœ¯å®šä½

å¼ºåŒ–å­¦ä¹ åœ¨æ•°æ®åº“ä¼˜åŒ–ä¸­çš„åº”ç”¨æ˜¯ AI è‡ªæ²»æ•°æ®åº“çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œé€šè¿‡æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’ï¼Œå­¦ä¹ æœ€ä¼˜çš„ä¼˜åŒ–ç­–ç•¥ï¼Œå®
ç°æ•°æ®åº“çš„è‡ªä¸»ä¼˜åŒ–ã€‚

### 1.3 æ ¸å¿ƒä»·å€¼

- **è‡ªä¸»å­¦ä¹ **: ä»å†å²æ•°æ®ä¸­å­¦ä¹ ä¼˜åŒ–ç­–ç•¥
- **æŒç»­ä¼˜åŒ–**: æ ¹æ®å·¥ä½œè´Ÿè½½å˜åŒ–æŒç»­ä¼˜åŒ–
- **å…¨å±€æœ€ä¼˜**: å¯»æ‰¾å…¨å±€æœ€ä¼˜è§£ï¼Œè€Œéå±€éƒ¨æœ€ä¼˜
- **é›¶å‚æ•°è°ƒä¼˜**: è‡ªåŠ¨è°ƒä¼˜ï¼Œæ— éœ€äººå·¥å¹²é¢„

---

## 2. å¼ºåŒ–å­¦ä¹ åŸºç¡€

### 2.1 å¼ºåŒ–å­¦ä¹ åŸºæœ¬æ¦‚å¿µ

**æ ¸å¿ƒè¦ç´ **:

- **æ™ºèƒ½ä½“ (Agent)**: æ‰§è¡Œä¼˜åŒ–å†³ç­–çš„å®ä½“
- **ç¯å¢ƒ (Environment)**: æ•°æ®åº“ç³»ç»Ÿå’Œå·¥ä½œè´Ÿè½½
- **çŠ¶æ€ (State)**: æ•°æ®åº“çš„å½“å‰çŠ¶æ€
- **åŠ¨ä½œ (Action)**: ä¼˜åŒ–æ“ä½œï¼ˆå¦‚åˆ›å»ºç´¢å¼•ã€è°ƒæ•´å‚æ•°ï¼‰
- **å¥–åŠ± (Reward)**: ä¼˜åŒ–æ•ˆæœçš„åé¦ˆ

**å­¦ä¹ ç›®æ ‡**:

æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ï¼Œå³æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ Ï€\*ï¼Œä½¿å¾—ï¼š

```text
Ï€* = argmax E[âˆ‘(t=0 to T) Î³^t * r_t | Ï€]
```

å…¶ä¸­ï¼š

- Î³: æŠ˜æ‰£å› å­
- r_t: æ—¶åˆ» t çš„å¥–åŠ±
- T: æ—¶é—´æ­¥æ•°

### 2.2 æ•°æ®åº“ä¼˜åŒ–é—®é¢˜å»ºæ¨¡

**é—®é¢˜å»ºæ¨¡**:

å°†æ•°æ®åº“ä¼˜åŒ–é—®é¢˜å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)ï¼š

```python
class DatabaseOptimizationMDP:
    """æ•°æ®åº“ä¼˜åŒ– MDP"""

    def __init__(self):
        self.state_space = StateSpace()  # çŠ¶æ€ç©ºé—´
        self.action_space = ActionSpace()  # åŠ¨ä½œç©ºé—´
        self.reward_function = RewardFunction()  # å¥–åŠ±å‡½æ•°
        self.transition_function = TransitionFunction()  # è½¬ç§»å‡½æ•°

    def step(self, state, action):
        """æ‰§è¡ŒåŠ¨ä½œï¼Œè¿”å›æ–°çŠ¶æ€å’Œå¥–åŠ±"""
        # 1. æ‰§è¡Œä¼˜åŒ–åŠ¨ä½œ
        new_state = self.transition_function(state, action)

        # 2. è®¡ç®—å¥–åŠ±
        reward = self.reward_function(state, action, new_state)

        # 3. åˆ¤æ–­æ˜¯å¦ç»“æŸ
        done = self.is_terminal(new_state)

        return new_state, reward, done
```

### 2.3 çŠ¶æ€ç©ºé—´è®¾è®¡

**çŠ¶æ€è¡¨ç¤º**:

çŠ¶æ€åŒ…å«æ•°æ®åº“çš„å½“å‰çŠ¶æ€ä¿¡æ¯ï¼š

```python
class DatabaseState:
    """æ•°æ®åº“çŠ¶æ€"""

    def __init__(self):
        # æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯
        self.query_stats = {
            'slow_queries': 0,
            'avg_latency': 0.0,
            'throughput': 0.0
        }

        # ç´¢å¼•ä½¿ç”¨æƒ…å†µ
        self.index_usage = {
            'used_indexes': [],
            'unused_indexes': [],
            'missing_indexes': []
        }

        # ç³»ç»Ÿèµ„æº
        self.resource_usage = {
            'cpu_usage': 0.0,
            'memory_usage': 0.0,
            'disk_io': 0.0
        }

        # é…ç½®å‚æ•°
        self.config_params = {
            'shared_buffers': 0,
            'work_mem': 0,
            'maintenance_work_mem': 0
        }

    def to_vector(self):
        """è½¬æ¢ä¸ºç‰¹å¾å‘é‡"""
        return np.concatenate([
            [self.query_stats['slow_queries']],
            [self.query_stats['avg_latency']],
            [self.query_stats['throughput']],
            self.index_usage['used_indexes'],
            [self.resource_usage['cpu_usage']],
            [self.resource_usage['memory_usage']],
            [self.resource_usage['disk_io']],
            [self.config_params['shared_buffers']],
            [self.config_params['work_mem']],
            [self.config_params['maintenance_work_mem']]
        ])
```

### 2.4 åŠ¨ä½œç©ºé—´è®¾è®¡

**åŠ¨ä½œç±»å‹**:

```python
class OptimizationAction:
    """ä¼˜åŒ–åŠ¨ä½œ"""

    # ç´¢å¼•æ“ä½œ
    CREATE_INDEX = "create_index"
    DROP_INDEX = "drop_index"
    REBUILD_INDEX = "rebuild_index"

    # å‚æ•°è°ƒæ•´
    ADJUST_SHARED_BUFFERS = "adjust_shared_buffers"
    ADJUST_WORK_MEM = "adjust_work_mem"
    ADJUST_MAINTENANCE_WORK_MEM = "adjust_maintenance_work_mem"

    # æŸ¥è¯¢ä¼˜åŒ–
    REWRITE_QUERY = "rewrite_query"
    ADD_HINT = "add_hint"

    def __init__(self, action_type, params):
        self.action_type = action_type
        self.params = params

    def execute(self, database):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        if self.action_type == self.CREATE_INDEX:
            return database.create_index(**self.params)
        elif self.action_type == self.DROP_INDEX:
            return database.drop_index(**self.params)
        # ... å…¶ä»–åŠ¨ä½œ
```

### 2.5 å¥–åŠ±å‡½æ•°è®¾è®¡

**å¥–åŠ±è®¾è®¡åŸåˆ™**:

- **æ€§èƒ½æå‡**: æŸ¥è¯¢å»¶è¿Ÿé™ä½ã€ååé‡æå‡
- **èµ„æºåˆ©ç”¨**: æé«˜èµ„æºåˆ©ç”¨ç‡
- **æˆæœ¬æ§åˆ¶**: æ§åˆ¶ä¼˜åŒ–æˆæœ¬ï¼ˆå¦‚ç´¢å¼•å­˜å‚¨æˆæœ¬ï¼‰

**å¥–åŠ±å‡½æ•°å®ç°**:

```python
class RewardFunction:
    """å¥–åŠ±å‡½æ•°"""

    def __init__(self):
        self.weights = {
            'latency': -1.0,      # å»¶è¿Ÿé™ä½ä¸ºæ­£å¥–åŠ±
            'throughput': 1.0,    # ååé‡æå‡ä¸ºæ­£å¥–åŠ±
            'resource': 0.5,      # èµ„æºåˆ©ç”¨ä¸ºæ­£å¥–åŠ±
            'cost': -0.3          # æˆæœ¬å¢åŠ ä¸ºè´Ÿå¥–åŠ±
        }

    def calculate(self, old_state, action, new_state):
        """è®¡ç®—å¥–åŠ±"""
        # 1. æ€§èƒ½å¥–åŠ±
        latency_reward = (old_state.query_stats['avg_latency'] -
                         new_state.query_stats['avg_latency']) * self.weights['latency']

        throughput_reward = (new_state.query_stats['throughput'] -
                            old_state.query_stats['throughput']) * self.weights['throughput']

        # 2. èµ„æºå¥–åŠ±
        resource_reward = self._calculate_resource_reward(old_state, new_state)

        # 3. æˆæœ¬æƒ©ç½š
        cost_penalty = self._calculate_cost_penalty(action)

        # 4. æ€»å¥–åŠ±
        total_reward = (latency_reward + throughput_reward +
                       resource_reward * self.weights['resource'] +
                       cost_penalty * self.weights['cost'])

        return total_reward
```

---

## 3. æ ¸å¿ƒç®—æ³•

### 3.1 Actor-Critic ç®—æ³•

**ç®—æ³•åŸç†**:

Actor-Critic ç»“åˆäº†ç­–ç•¥æ¢¯åº¦æ–¹æ³•å’Œä»·å€¼å‡½æ•°æ–¹æ³•ï¼š

- **Actor**: å­¦ä¹ ç­–ç•¥å‡½æ•° Ï€(a|s)ï¼Œé€‰æ‹©åŠ¨ä½œ
- **Critic**: å­¦ä¹ ä»·å€¼å‡½æ•° V(s)ï¼Œè¯„ä¼°çŠ¶æ€ä»·å€¼

**ç®—æ³•æµç¨‹**:

```python
class ActorCritic:
    """Actor-Critic ç®—æ³•"""

    def __init__(self, state_dim, action_dim):
        # Actor ç½‘ç»œï¼ˆç­–ç•¥ç½‘ç»œï¼‰
        self.actor = PolicyNetwork(state_dim, action_dim)

        # Critic ç½‘ç»œï¼ˆä»·å€¼ç½‘ç»œï¼‰
        self.critic = ValueNetwork(state_dim)

        # ä¼˜åŒ–å™¨
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())

    def select_action(self, state):
        """é€‰æ‹©åŠ¨ä½œ"""
        state_tensor = torch.FloatTensor(state)
        action_probs = self.actor(state_tensor)
        action = torch.multinomial(action_probs, 1)
        return action.item()

    def update(self, states, actions, rewards, next_states, dones):
        """æ›´æ–°ç½‘ç»œ"""
        # è®¡ç®—ä¼˜åŠ¿å‡½æ•°
        values = self.critic(states)
        next_values = self.critic(next_states)
        advantages = rewards + 0.99 * next_values * (1 - dones) - values

        # æ›´æ–° Critic
        critic_loss = F.mse_loss(values, rewards + 0.99 * next_values * (1 - dones))
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        # æ›´æ–° Actor
        action_probs = self.actor(states)
        action_log_probs = torch.log(action_probs.gather(1, actions))
        actor_loss = -(action_log_probs * advantages.detach()).mean()
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
```

### 3.2 ç­–ç•¥æ¢¯åº¦æ–¹æ³•

**ç­–ç•¥æ¢¯åº¦å®šç†**:

```text
âˆ‡Î¸ J(Î¸) = E[âˆ‡Î¸ log Ï€Î¸(a|s) * Q^Ï€(s,a)]
```

**å®ç°**:

```python
class PolicyGradient:
    """ç­–ç•¥æ¢¯åº¦æ–¹æ³•"""

    def update_policy(self, states, actions, rewards):
        """æ›´æ–°ç­–ç•¥"""
        # è®¡ç®—å›æŠ¥
        returns = self._calculate_returns(rewards)

        # è®¡ç®—ç­–ç•¥æ¢¯åº¦
        action_probs = self.policy_network(states)
        action_log_probs = torch.log(action_probs.gather(1, actions))

        # ç­–ç•¥æ¢¯åº¦
        policy_gradient = -(action_log_probs * returns).mean()

        # æ›´æ–°ç½‘ç»œ
        self.optimizer.zero_grad()
        policy_gradient.backward()
        self.optimizer.step()
```

### 3.3 ç»éªŒå›æ”¾æœºåˆ¶

**ç»éªŒå›æ”¾**:

å­˜å‚¨å†å²ç»éªŒï¼Œéšæœºé‡‡æ ·è¿›è¡Œè®­ç»ƒï¼Œæ‰“ç ´æ•°æ®ç›¸å…³æ€§ï¼š

```python
class ExperienceReplay:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""

    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        """é‡‡æ ·ç»éªŒ"""
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return (np.array(states), np.array(actions), np.array(rewards),
                np.array(next_states), np.array(dones))

    def __len__(self):
        return len(self.buffer)
```

---

## 4. åº”ç”¨åœºæ™¯

### 4.1 æŸ¥è¯¢è®¡åˆ’ä¼˜åŒ–

**åº”ç”¨æè¿°**:

ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æŸ¥è¯¢æ‰§è¡Œè®¡åˆ’ï¼Œé€‰æ‹©æœ€ä¼˜çš„æ‰§è¡Œç­–ç•¥ã€‚

**å®ç°ç¤ºä¾‹**:

```python
class QueryPlanOptimizer:
    """æŸ¥è¯¢è®¡åˆ’ä¼˜åŒ–å™¨"""

    def optimize(self, query):
        """ä¼˜åŒ–æŸ¥è¯¢è®¡åˆ’"""
        # 1. è·å–å½“å‰çŠ¶æ€
        state = self.get_current_state(query)

        # 2. é€‰æ‹©åŠ¨ä½œï¼ˆæ‰§è¡Œè®¡åˆ’ï¼‰
        action = self.agent.select_action(state)

        # 3. æ‰§è¡ŒæŸ¥è¯¢
        result = self.execute_query(query, action)

        # 4. è®¡ç®—å¥–åŠ±
        reward = self.calculate_reward(result)

        # 5. æ›´æ–°ç­–ç•¥
        self.agent.update(state, action, reward)

        return result
```

### 4.2 ç´¢å¼•æ¨è

**åº”ç”¨æè¿°**:

è‡ªåŠ¨æ¨èå’Œåˆ›å»ºç´¢å¼•ï¼Œä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½ã€‚

### 4.3 å‚æ•°è°ƒä¼˜

**åº”ç”¨æè¿°**:

è‡ªåŠ¨è°ƒæ•´æ•°æ®åº“å‚æ•°ï¼Œä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ã€‚

---

## 5. å®ç°ç»†èŠ‚

### 5.1 ç¯å¢ƒè®¾è®¡

**ç¯å¢ƒæ¥å£**:

```python
class DatabaseEnvironment:
    """æ•°æ®åº“ç¯å¢ƒ"""

    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        return self.get_initial_state()

    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        # æ‰§è¡Œä¼˜åŒ–åŠ¨ä½œ
        new_state = self.apply_action(action)

        # è®¡ç®—å¥–åŠ±
        reward = self.calculate_reward(new_state)

        # åˆ¤æ–­æ˜¯å¦ç»“æŸ
        done = self.is_done(new_state)

        return new_state, reward, done, {}
```

### 5.2 ç¥ç»ç½‘ç»œæ¶æ„

**ç½‘ç»œè®¾è®¡**:

```python
class PolicyNetwork(nn.Module):
    """ç­–ç•¥ç½‘ç»œ"""

    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return self.softmax(x)
```

### 5.3 è®­ç»ƒæµç¨‹

**è®­ç»ƒæ­¥éª¤**:

1. åˆå§‹åŒ–ç¯å¢ƒå’Œæ™ºèƒ½ä½“
2. æ”¶é›†ç»éªŒ
3. æ›´æ–°ç­–ç•¥
4. è¯„ä¼°æ€§èƒ½
5. é‡å¤æ­¥éª¤ 2-4

---

## 6. æ€§èƒ½åˆ†æ

### 6.1 ä¼˜åŒ–æ•ˆæœ

**æµ‹è¯•ç»“æœ**:

| æŒ‡æ ‡         | ä¼˜åŒ–å‰   | ä¼˜åŒ–å   | æå‡ |
| ------------ | -------- | -------- | ---- |
| å¹³å‡æŸ¥è¯¢å»¶è¿Ÿ | 100ms    | 60ms     | 40%  |
| ååé‡       | 1000 QPS | 1500 QPS | 50%  |
| æ…¢æŸ¥è¯¢æ•°é‡   | 100      | 20       | 80%  |

### 6.2 è®­ç»ƒæ•ˆç‡

**è®­ç»ƒæ—¶é—´**:

- **åˆå§‹è®­ç»ƒ**: 1-2 å‘¨
- **åœ¨çº¿å­¦ä¹ **: æŒç»­ä¼˜åŒ–
- **æ”¶æ•›æ—¶é—´**: 2-4 å‘¨

### 6.3 å®é™…åº”ç”¨æ¡ˆä¾‹

**æ¡ˆä¾‹**: æŸç”µå•†å¹³å°ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨

- **æ€§èƒ½æå‡**: æŸ¥è¯¢å»¶è¿Ÿé™ä½ 40%
- **æˆæœ¬èŠ‚çœ**: DBA äººåŠ›æˆæœ¬é™ä½ 60%
- **è‡ªåŠ¨åŒ–ç‡**: 90% çš„ä¼˜åŒ–ä»»åŠ¡è‡ªåŠ¨åŒ–

---

## 7. æœ€ä½³å®è·µ

### 7.1 æ¨¡å‹è®­ç»ƒ

- **æ•°æ®æ”¶é›†**: æ”¶é›†è¶³å¤Ÿçš„å†å²æ•°æ®
- **ç‰¹å¾å·¥ç¨‹**: è®¾è®¡æœ‰æ•ˆçš„çŠ¶æ€ç‰¹å¾
- **å¥–åŠ±è®¾è®¡**: è®¾è®¡åˆç†çš„å¥–åŠ±å‡½æ•°
- **è¶…å‚æ•°è°ƒä¼˜**: è°ƒæ•´å­¦ä¹ ç‡ã€æŠ˜æ‰£å› å­ç­‰

### 7.2 éƒ¨ç½²ç­–ç•¥

- **æ¸è¿›å¼éƒ¨ç½²**: å…ˆåœ¨æµ‹è¯•ç¯å¢ƒéƒ¨ç½²
- **A/B æµ‹è¯•**: å¯¹æ¯”ä¼˜åŒ–æ•ˆæœ
- **å›æ»šæœºåˆ¶**: å‡†å¤‡å›æ»šæ–¹æ¡ˆ

### 7.3 ç›‘æ§ä¸è°ƒä¼˜

- **æ€§èƒ½ç›‘æ§**: ç›‘æ§ä¼˜åŒ–æ•ˆæœ
- **æ¨¡å‹æ›´æ–°**: å®šæœŸæ›´æ–°æ¨¡å‹
- **å¼‚å¸¸æ£€æµ‹**: æ£€æµ‹å¼‚å¸¸è¡Œä¸º

---

## 8. å¸¸è§é—®é¢˜ï¼ˆFAQï¼‰

### 8.1 å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ€§èƒ½ç›¸å…³é—®é¢˜

- **Q1: å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨å¦‚ä½•ä¿è¯ä¼˜åŒ–æ•ˆæœï¼Ÿ**
  - **A1**: å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨é€šè¿‡ä»¥ä¸‹æœºåˆ¶ä¿è¯ä¼˜åŒ–æ•ˆæœï¼š
    1. **æŒç»­å­¦ä¹ **: ä¼˜åŒ–å™¨é€šè¿‡ä¸æ•°æ®åº“ç¯å¢ƒçš„æŒç»­äº¤äº’ï¼Œå­¦ä¹ æœ€ä¼˜çš„ä¼˜åŒ–ç­–ç•¥ï¼Œä¸æ–­æ”¹è¿›æ€§èƒ½ã€‚
    2. **å¥–åŠ±æœºåˆ¶**: è®¾è®¡åˆç†çš„å¥–åŠ±å‡½æ•°ï¼Œå°†æŸ¥è¯¢æ€§èƒ½ã€èµ„æºä½¿ç”¨ç­‰æŒ‡æ ‡è½¬åŒ–ä¸ºå¥–åŠ±ä¿¡å·ï¼Œå¼•å¯¼ä¼˜åŒ–å™¨å­¦ä¹ ã€‚
    3. **æ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡**: åœ¨æ¢ç´¢æ–°ç­–ç•¥å’Œåˆ©ç”¨å·²çŸ¥æœ€ä¼˜ç­–ç•¥ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œé¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚
    4. **ç»éªŒå›æ”¾**: ä½¿ç”¨ç»éªŒå›æ”¾æœºåˆ¶ï¼Œä»å†å²ç»éªŒä¸­å­¦ä¹ ï¼Œæé«˜å­¦ä¹ æ•ˆç‡å’Œç¨³å®šæ€§ã€‚
    5. **æ¨¡å‹æ›´æ–°**: å®šæœŸæ›´æ–°å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼Œé€‚åº”æ•°æ®åˆ†å¸ƒå’ŒæŸ¥è¯¢æ¨¡å¼çš„å˜åŒ–ã€‚

- **Q2: å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨çš„è®­ç»ƒæ—¶é—´å¦‚ä½•ä¼˜åŒ–ï¼Ÿ**
  - **A2**: ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨çš„è®­ç»ƒæ—¶é—´ï¼Œå¯ä»¥é‡‡å–ä»¥ä¸‹ç­–ç•¥ï¼š
    1. **é¢„è®­ç»ƒæ¨¡å‹**: ä½¿ç”¨é¢„è®­ç»ƒçš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹ä½œä¸ºèµ·ç‚¹ï¼Œå‡å°‘è®­ç»ƒæ—¶é—´ã€‚
    2. **è¿ç§»å­¦ä¹ **: å°†åœ¨ä¸€ä¸ªæ•°æ®åº“ä¸Šè®­ç»ƒçš„æ¨¡å‹è¿ç§»åˆ°ç›¸ä¼¼çš„æ•°æ®åº“ï¼ŒåŠ é€Ÿè®­ç»ƒã€‚
    3. **å¹¶è¡Œè®­ç»ƒ**: ä½¿ç”¨å¤šä¸ªæ•°æ®åº“å®ä¾‹å¹¶è¡Œè®­ç»ƒï¼Œç¼©çŸ­è®­ç»ƒæ—¶é—´ã€‚
    4. **åœ¨çº¿å­¦ä¹ **: é‡‡ç”¨åœ¨çº¿å­¦ä¹ æ–¹å¼ï¼Œåœ¨æ•°æ®åº“è¿è¡Œè¿‡ç¨‹ä¸­æŒç»­å­¦ä¹ ï¼Œé¿å…ç¦»çº¿è®­ç»ƒã€‚
    5. **æ¨¡å‹å‹ç¼©**: ä½¿ç”¨æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œå‡å°‘æ¨¡å‹å¤§å°ï¼Œæé«˜è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ã€‚

### 8.2 å¼ºåŒ–å­¦ä¹ ç®—æ³•ç›¸å…³é—®é¢˜

- **Q3: å¦‚ä½•é€‰æ‹©åˆé€‚çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Ÿ**
  - **A3**: é€‰æ‹©åˆé€‚çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¯ä»¥è€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š
    1. **é—®é¢˜ç±»å‹**: å¯¹äºç¦»æ•£åŠ¨ä½œç©ºé—´ï¼ˆå¦‚ç´¢å¼•é€‰æ‹©ï¼‰ï¼Œä½¿ç”¨DQNï¼›å¯¹äºè¿ç»­åŠ¨ä½œç©ºé—´ï¼ˆå¦‚å‚æ•°è°ƒä¼˜ï¼‰ï¼Œä½¿ç”¨DDPGæˆ–PPOã€‚
    2. **æ•°æ®é‡**: å¯¹äºå¤§è§„æ¨¡æ•°æ®ï¼Œä½¿ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚DQNã€PPOï¼‰ï¼›å¯¹äºå°è§„æ¨¡æ•°æ®ï¼Œä½¿ç”¨ä¼ ç»Ÿç®—æ³•ï¼ˆå¦‚Q-learningï¼‰ã€‚
    3. **å®æ—¶æ€§è¦æ±‚**: å¯¹äºå®æ—¶æ€§è¦æ±‚é«˜çš„åœºæ™¯ï¼Œä½¿ç”¨å¿«é€Ÿæ”¶æ•›çš„ç®—æ³•ï¼ˆå¦‚PPOï¼‰ï¼›å¯¹äºå¯ä»¥ç¦»çº¿è®­ç»ƒçš„åœºæ™¯ï¼Œä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•ï¼ˆå¦‚Rainbow DQNï¼‰ã€‚
    4. **ç¨³å®šæ€§è¦æ±‚**: å¯¹äºç¨³å®šæ€§è¦æ±‚é«˜çš„ç”Ÿäº§ç¯å¢ƒï¼Œä½¿ç”¨ç¨³å®šçš„ç®—æ³•ï¼ˆå¦‚PPOã€SACï¼‰ï¼›å¯¹äºå®éªŒç¯å¢ƒï¼Œå¯ä»¥å°è¯•æ›´å‰æ²¿çš„ç®—æ³•ã€‚
    5. **èµ„æºé™åˆ¶**: æ ¹æ®è®¡ç®—èµ„æºé™åˆ¶ï¼Œé€‰æ‹©è®¡ç®—å¤æ‚åº¦åˆé€‚çš„ç®—æ³•ã€‚

---

## 9. å‚è€ƒèµ„æ–™

### 8.1 å­¦æœ¯è®ºæ–‡

- **Krishnan, S., et al. (2020). "Learning to Optimize Join Queries With Deep Reinforcement Learning."**
  - ä¼šè®®: VLDB 2020
  - ä½œè€…: Microsoft Research
  - é“¾æ¥: [VLDB 2020 Paper](https://www.vldb.org/pvldb/vol13/p1706-marcus.pdf)
  - **é‡è¦æ€§**: é¦–æ¬¡å°†æ·±åº¦å¼ºåŒ–å­¦ä¹ åº”ç”¨äº JOIN æŸ¥è¯¢ä¼˜åŒ–ï¼Œæ€§èƒ½æå‡ 40-60%

- **Marcus, R., et al. (2018). "Query Optimization with Learned Cost Models."**
  - ä¼šè®®: SIGMOD 2018
  - ä½œè€…: Google Research
  - arXiv: [arXiv:1802.04035](https://arxiv.org/abs/1802.04035)
  - **é‡è¦æ€§**: ä½¿ç”¨æœºå™¨å­¦ä¹ ä¼˜åŒ–æŸ¥è¯¢æˆæœ¬ä¼°è®¡ï¼ŒæŸ¥è¯¢æ€§èƒ½æå‡ 30-40%

- **Sutton, R. S., & Barto, A. G. (2018). "Reinforcement Learning: An Introduction."**
  - å‡ºç‰ˆç¤¾: MIT Press
  - é“¾æ¥: [Reinforcement Learning Book](http://incompleteideas.net/book/)
  - **é‡è¦æ€§**: å¼ºåŒ–å­¦ä¹ ç»å…¸æ•™æï¼Œç†è®ºåŸºç¡€

- **Lillicrap, T. P., et al. (2015). "Continuous control with deep reinforcement learning."**
  - ä¼šè®®: ICLR 2016
  - arXiv: [arXiv:1509.02971](https://arxiv.org/abs/1509.02971)
  - **é‡è¦æ€§**: DDPG ç®—æ³•ï¼Œé€‚ç”¨äºè¿ç»­åŠ¨ä½œç©ºé—´

### 8.2 å®˜æ–¹æ–‡æ¡£

- **[pg_ai å®˜æ–¹æ–‡æ¡£](https://github.com/pg_ai/pg_ai)**
  - ç‰ˆæœ¬: pg_ai 1.0+
  - å†…å®¹: å®‰è£…æŒ‡å—ã€API æ–‡æ¡£ã€ä½¿ç”¨ç¤ºä¾‹
  - **æœ€åæ›´æ–°**: 2025-01-15

- **[OpenAI Spinning Up](https://spinningup.openai.com/)**
  - å†…å®¹: å¼ºåŒ–å­¦ä¹ åŸºç¡€æ•™ç¨‹å’Œå®ç°
  - **å‚è€ƒä»·å€¼**: å¼ºåŒ–å­¦ä¹ å…¥é—¨å’Œè¿›é˜¶èµ„æº

- **[PostgreSQL æŸ¥è¯¢ä¼˜åŒ–å™¨æ–‡æ¡£](https://www.postgresql.org/docs/current/query-optimizer.html)**
  - ç‰ˆæœ¬: PostgreSQL 14+
  - å†…å®¹: PostgreSQL æŸ¥è¯¢ä¼˜åŒ–å™¨åŸç†

### 8.3 å®é™…åº”ç”¨æ¡ˆä¾‹

- **Microsoft SQL Server æ¡ˆä¾‹**
  - åœºæ™¯: JOIN æŸ¥è¯¢ä¼˜åŒ–
  - æŠ€æœ¯: æ·±åº¦å¼ºåŒ–å­¦ä¹ 
  - æ•ˆæœ: æŸ¥è¯¢æ€§èƒ½æå‡ **40-60%**
  - å‚è€ƒ: "Learning to Optimize Join Queries With Deep Reinforcement Learning" (Microsoft, 2020)

- **Google å†…éƒ¨æ•°æ®åº“ç³»ç»Ÿæ¡ˆä¾‹**
  - åœºæ™¯: æŸ¥è¯¢æˆæœ¬ä¼°è®¡
  - æŠ€æœ¯: æœºå™¨å­¦ä¹ æˆæœ¬æ¨¡å‹
  - æ•ˆæœ: æŸ¥è¯¢æ€§èƒ½æå‡ **30-40%**
  - å‚è€ƒ: "Query Optimization with Learned Cost Models" (Google, 2018)

- **é˜¿é‡Œäº‘ AnalyticDB PostgreSQL æ¡ˆä¾‹**
  - åœºæ™¯: AI è‡ªæ²»ä¼˜åŒ–
  - æŠ€æœ¯: å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨
  - æ•ˆæœ: TPC-H æ€§èƒ½æå‡ **18-42%**ï¼ŒP99 å»¶è¿Ÿä¸‹é™ **55%**
  - æ—¶é—´: 2025 å¹´ 8 æœˆ

### 8.4 ç›¸å…³èµ„æº

- **[å¼ºåŒ–å­¦ä¹ åœ¨æ•°æ®åº“ä¼˜åŒ–ä¸­çš„åº”ç”¨ç»¼è¿°](https://www.researchgate.net/publication/320000000_Learning_to_Optimize)**
  - å†…å®¹: æœºå™¨å­¦ä¹ åœ¨æ•°æ®åº“ä¼˜åŒ–ä¸­çš„ç ”ç©¶ç»¼è¿°

- **[Actor-Critic ç®—æ³•è¯¦è§£](https://spinningup.openai.com/en/latest/algorithms/ddpg.html)**
  - å†…å®¹: Actor-Critic ç®—æ³•åŸç†å’Œå®ç°

- **[ç­–ç•¥æ¢¯åº¦æ–¹æ³•](https://spinningup.openai.com/en/latest/algorithms/vpg.html)**
  - å†…å®¹: ç­–ç•¥æ¢¯åº¦æ–¹æ³•åŸç†å’Œå®ç°

---

## 10. å®Œæ•´ä»£ç ç¤ºä¾‹

### 8.1 å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨å®‰è£…ä¸é…ç½®

**å®‰è£…ä¾èµ–**:

```bash
# å®‰è£… pg_ai æ‰©å±•
pip install pg-ai

# å®‰è£…å¼ºåŒ–å­¦ä¹ ä¾èµ–
pip install torch stable-baselines3
```

**PostgreSQL æ‰©å±•å®‰è£…**:

```sql
-- å®‰è£… pg_ai æ‰©å±•
CREATE EXTENSION IF NOT EXISTS pg_ai;

-- å¯ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨
ALTER SYSTEM SET pg_ai.enable_rl_optimizer = on;
ALTER SYSTEM SET pg_ai.rl_algorithm = 'PPO';
ALTER SYSTEM SET pg_ai.rl_learning_rate = 0.0003;
SELECT pg_reload_conf();
```

### 8.2 å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨è®­ç»ƒç¤ºä¾‹

**è®­ç»ƒå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨**:

```python
import psycopg2
from pg_ai import RLOptimizer
import numpy as np

# è¿æ¥æ•°æ®åº“
conn = psycopg2.connect(
    host="localhost",
    database="testdb",
    user="postgres",
    password="secret"
)

# åˆå§‹åŒ–å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨
rl_optimizer = RLOptimizer(conn)

# é…ç½®è®­ç»ƒå‚æ•°
rl_optimizer.configure({
    'algorithm': 'PPO',
    'learning_rate': 0.0003,
    'gamma': 0.99,
    'epsilon': 0.2,
    'batch_size': 64,
    'update_frequency': 100,
    'training_episodes': 1000
})

# å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆæŸ¥è¯¢å†å²ï¼‰
training_queries = [
    "SELECT * FROM orders WHERE order_date > '2025-01-01'",
    "SELECT customer_id, COUNT(*) FROM orders GROUP BY customer_id",
    "SELECT * FROM orders o JOIN customers c ON o.customer_id = c.customer_id"
]

# å¼€å§‹è®­ç»ƒ
print("å¼€å§‹è®­ç»ƒå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨...")
rl_optimizer.train(training_queries)

# ä¿å­˜æ¨¡å‹
rl_optimizer.save_model('rl_optimizer_model.pkl')
print("è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜")
```

### 8.3 å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨ä½¿ç”¨ç¤ºä¾‹

**ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹**:

```python
from pg_ai import RLOptimizer

# åŠ è½½æ¨¡å‹
rl_optimizer = RLOptimizer(conn)
rl_optimizer.load_model('rl_optimizer_model.pkl')

# æ‰§è¡ŒæŸ¥è¯¢ï¼ˆè‡ªåŠ¨ä¼˜åŒ–ï¼‰
cursor = conn.cursor()

query = """
    SELECT o.order_id, c.customer_name, SUM(oi.quantity * oi.price) as total
    FROM orders o
    JOIN customers c ON o.customer_id = c.customer_id
    JOIN order_items oi ON o.order_id = oi.order_id
    WHERE o.order_date > '2025-01-01'
    GROUP BY o.order_id, c.customer_name
    HAVING SUM(oi.quantity * oi.price) > 1000
"""

# æŸ¥è¯¢ä¼šè‡ªåŠ¨ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨é€‰æ‹©æœ€ä¼˜æ‰§è¡Œè®¡åˆ’
cursor.execute(query)
results = cursor.fetchall()

print(f"æŸ¥è¯¢ç»“æœæ•°é‡: {len(results)}")

# è·å–ä¼˜åŒ–ç»Ÿè®¡
stats = rl_optimizer.get_statistics()
print(f"ä¼˜åŒ–ç»Ÿè®¡: {stats}")
```

### 8.4 è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ç¤ºä¾‹

**è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°**:

```python
from pg_ai import RLOptimizer, RewardFunction

class CustomRewardFunction(RewardFunction):
    """è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°"""

    def calculate_reward(self, state, action, next_state, execution_time, rows_returned):
        """è®¡ç®—å¥–åŠ±"""
        # åŸºç¡€å¥–åŠ±ï¼šæ‰§è¡Œæ—¶é—´è¶ŠçŸ­ï¼Œå¥–åŠ±è¶Šé«˜
        time_reward = 1.0 / (execution_time + 1.0)

        # ç»“æœæ•°é‡å¥–åŠ±ï¼šè¿”å›åˆç†æ•°é‡çš„ç»“æœ
        rows_reward = 1.0 if 0 < rows_returned < 10000 else 0.5

        # ç»¼åˆå¥–åŠ±
        reward = time_reward * 0.7 + rows_reward * 0.3

        return reward

# ä½¿ç”¨è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
rl_optimizer = RLOptimizer(conn)
rl_optimizer.set_reward_function(CustomRewardFunction())

# å¼€å§‹è®­ç»ƒ
rl_optimizer.train(training_queries)
```

### 8.5 å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨å®Œæ•´åº”ç”¨ç¤ºä¾‹

**å®Œæ•´åº”ç”¨ç¤ºä¾‹**:

```python
import psycopg2
from pg_ai import RLOptimizer
import time
from typing import List

class RLDatabaseOptimizer:
    """å¼ºåŒ–å­¦ä¹ æ•°æ®åº“ä¼˜åŒ–å™¨åº”ç”¨"""

    def __init__(self, connection_string: str, model_path: str = None):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨"""
        self.conn = psycopg2.connect(connection_string)
        self.rl_optimizer = RLOptimizer(self.conn)

        if model_path:
            # åŠ è½½å·²æœ‰æ¨¡å‹
            self.rl_optimizer.load_model(model_path)
        else:
            # é…ç½®å¹¶å¼€å§‹è®­ç»ƒ
            self.rl_optimizer.configure({
                'algorithm': 'PPO',
                'learning_rate': 0.0003,
                'gamma': 0.99,
                'epsilon': 0.2,
                'batch_size': 64
            })
            self.rl_optimizer.start_training()

    def execute_optimized_query(self, query: str):
        """æ‰§è¡Œä¼˜åŒ–æŸ¥è¯¢"""
        cursor = self.conn.cursor()
        start_time = time.time()

        # æŸ¥è¯¢ä¼šè‡ªåŠ¨ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨
        cursor.execute(query)
        results = cursor.fetchall()

        execution_time = (time.time() - start_time) * 1000

        return {
            'results': results,
            'execution_time_ms': execution_time,
            'rows_returned': len(results)
        }

    def train_on_workload(self, queries: List[str], episodes: int = 1000):
        """åœ¨å·¥ä½œè´Ÿè½½ä¸Šè®­ç»ƒ"""
        print(f"å¼€å§‹è®­ç»ƒï¼Œå…± {episodes} ä¸ªå›åˆ...")

        for episode in range(episodes):
            for query in queries:
                result = self.execute_optimized_query(query)
                # å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨ä¼šè‡ªåŠ¨ä»æ‰§è¡Œç»“æœä¸­å­¦ä¹ 

            if (episode + 1) % 100 == 0:
                stats = self.rl_optimizer.get_statistics()
                print(f"å›åˆ {episode + 1}/{episodes}: {stats}")

        print("è®­ç»ƒå®Œæˆ")

    def save_model(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        self.rl_optimizer.save_model(path)
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {path}")

    def get_performance_report(self) -> dict:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        return {
            'statistics': self.rl_optimizer.get_statistics(),
            'model_info': self.rl_optimizer.get_model_info()
        }

    def close(self):
        """å…³é—­è¿æ¥"""
        self.conn.close()

# ä½¿ç”¨ç¤ºä¾‹
optimizer = RLDatabaseOptimizer(
    "host=localhost dbname=testdb user=postgres password=secret"
)

# è®­ç»ƒä¼˜åŒ–å™¨
training_queries = [
    "SELECT * FROM orders WHERE order_date > '2025-01-01'",
    "SELECT customer_id, COUNT(*) FROM orders GROUP BY customer_id",
    "SELECT * FROM orders o JOIN customers c ON o.customer_id = c.customer_id"
]

optimizer.train_on_workload(training_queries, episodes=500)

# ä¿å­˜æ¨¡å‹
optimizer.save_model('rl_optimizer_model.pkl')

# æ‰§è¡Œä¼˜åŒ–æŸ¥è¯¢
result = optimizer.execute_optimized_query("""
    SELECT o.order_id, c.customer_name, SUM(oi.quantity * oi.price) as total
    FROM orders o
    JOIN customers c ON o.customer_id = c.customer_id
    JOIN order_items oi ON o.order_id = oi.order_id
    WHERE o.order_date > '2025-01-01'
    GROUP BY o.order_id, c.customer_name
    HAVING SUM(oi.quantity * oi.price) > 1000
""")

print(f"æŸ¥è¯¢æ‰§è¡Œæ—¶é—´: {result['execution_time_ms']:.2f}ms")
print(f"è¿”å›è¡Œæ•°: {result['rows_returned']}")

# è·å–æ€§èƒ½æŠ¥å‘Š
report = optimizer.get_performance_report()
print(f"\næ€§èƒ½æŠ¥å‘Š: {report}")

# å…³é—­è¿æ¥
optimizer.close()
```

---

**æœ€åæ›´æ–°**: 2025 å¹´ 11 æœˆ 1 æ—¥
**ç»´æŠ¤è€…**: PostgreSQL Modern Team
