# 2.1.2 å¼ºåŒ–å­¦ä¹ åœ¨æ•°æ®åº“ä¼˜åŒ–ä¸­çš„åº”ç”¨

> **æ›´æ–°æ—¶é—´**: 2025 å¹´ 11 æœˆ 1 æ—¥  
> **æŠ€æœ¯ç‰ˆæœ¬**: pg_ai v1.0+  
> **æ–‡æ¡£ç¼–å·**: 02-01-02

## ğŸ“‘ ç›®å½•

- [2.1.2 å¼ºåŒ–å­¦ä¹ åœ¨æ•°æ®åº“ä¼˜åŒ–ä¸­çš„åº”ç”¨](#212-å¼ºåŒ–å­¦ä¹ åœ¨æ•°æ®åº“ä¼˜åŒ–ä¸­çš„åº”ç”¨)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¦‚è¿°](#1-æ¦‚è¿°)
    - [1.1 æŠ€æœ¯èƒŒæ™¯](#11-æŠ€æœ¯èƒŒæ™¯)
    - [1.2 æŠ€æœ¯å®šä½](#12-æŠ€æœ¯å®šä½)
    - [1.3 æ ¸å¿ƒä»·å€¼](#13-æ ¸å¿ƒä»·å€¼)
  - [2. å¼ºåŒ–å­¦ä¹ åŸºç¡€](#2-å¼ºåŒ–å­¦ä¹ åŸºç¡€)
    - [2.1 å¼ºåŒ–å­¦ä¹ åŸºæœ¬æ¦‚å¿µ](#21-å¼ºåŒ–å­¦ä¹ åŸºæœ¬æ¦‚å¿µ)
    - [2.2 æ•°æ®åº“ä¼˜åŒ–é—®é¢˜å»ºæ¨¡](#22-æ•°æ®åº“ä¼˜åŒ–é—®é¢˜å»ºæ¨¡)
    - [2.3 çŠ¶æ€ç©ºé—´è®¾è®¡](#23-çŠ¶æ€ç©ºé—´è®¾è®¡)
    - [2.4 åŠ¨ä½œç©ºé—´è®¾è®¡](#24-åŠ¨ä½œç©ºé—´è®¾è®¡)
    - [2.5 å¥–åŠ±å‡½æ•°è®¾è®¡](#25-å¥–åŠ±å‡½æ•°è®¾è®¡)
  - [3. æ ¸å¿ƒç®—æ³•](#3-æ ¸å¿ƒç®—æ³•)
    - [3.1 Actor-Critic ç®—æ³•](#31-actor-critic-ç®—æ³•)
    - [3.2 ç­–ç•¥æ¢¯åº¦æ–¹æ³•](#32-ç­–ç•¥æ¢¯åº¦æ–¹æ³•)
    - [3.3 ç»éªŒå›æ”¾æœºåˆ¶](#33-ç»éªŒå›æ”¾æœºåˆ¶)
  - [4. åº”ç”¨åœºæ™¯](#4-åº”ç”¨åœºæ™¯)
    - [4.1 æŸ¥è¯¢è®¡åˆ’ä¼˜åŒ–](#41-æŸ¥è¯¢è®¡åˆ’ä¼˜åŒ–)
    - [4.2 ç´¢å¼•æ¨è](#42-ç´¢å¼•æ¨è)
    - [4.3 å‚æ•°è°ƒä¼˜](#43-å‚æ•°è°ƒä¼˜)
  - [5. å®ç°ç»†èŠ‚](#5-å®ç°ç»†èŠ‚)
    - [5.1 ç¯å¢ƒè®¾è®¡](#51-ç¯å¢ƒè®¾è®¡)
    - [5.2 ç¥ç»ç½‘ç»œæ¶æ„](#52-ç¥ç»ç½‘ç»œæ¶æ„)
    - [5.3 è®­ç»ƒæµç¨‹](#53-è®­ç»ƒæµç¨‹)
  - [6. æ€§èƒ½åˆ†æ](#6-æ€§èƒ½åˆ†æ)
    - [6.1 ä¼˜åŒ–æ•ˆæœ](#61-ä¼˜åŒ–æ•ˆæœ)
    - [6.2 è®­ç»ƒæ•ˆç‡](#62-è®­ç»ƒæ•ˆç‡)
    - [6.3 å®é™…åº”ç”¨æ¡ˆä¾‹](#63-å®é™…åº”ç”¨æ¡ˆä¾‹)
  - [7. æœ€ä½³å®è·µ](#7-æœ€ä½³å®è·µ)
    - [7.1 æ¨¡å‹è®­ç»ƒ](#71-æ¨¡å‹è®­ç»ƒ)
    - [7.2 éƒ¨ç½²ç­–ç•¥](#72-éƒ¨ç½²ç­–ç•¥)
    - [7.3 ç›‘æ§ä¸è°ƒä¼˜](#73-ç›‘æ§ä¸è°ƒä¼˜)
  - [8. å‚è€ƒèµ„æ–™](#8-å‚è€ƒèµ„æ–™)

---

## 1. æ¦‚è¿°

### 1.1 æŠ€æœ¯èƒŒæ™¯

**é—®é¢˜éœ€æ±‚**:

ä¼ ç»Ÿçš„æ•°æ®åº“ä¼˜åŒ–ä¾èµ–äºäººå·¥ç»éªŒå’Œè§„åˆ™ï¼Œæ— æ³•é€‚åº”åŠ¨æ€å˜åŒ–çš„å·¥ä½œè´Ÿè½½ã€‚éšç€æ•°æ®åº“è§„æ¨¡å’Œå·¥ä½œè´Ÿè½½å¤æ‚åº¦çš„å¢
åŠ ï¼Œä¼ ç»Ÿä¼˜åŒ–æ–¹æ³•é¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼š

- **è§„åˆ™å±€é™æ€§**: åŸºäºè§„åˆ™çš„ä¼˜åŒ–æ— æ³•è¦†ç›–æ‰€æœ‰åœºæ™¯
- **é™æ€ä¼˜åŒ–**: æ— æ³•é€‚åº”å·¥ä½œè´Ÿè½½çš„åŠ¨æ€å˜åŒ–
- **äººå·¥æˆæœ¬**: éœ€è¦ç»éªŒä¸°å¯Œçš„ DBA æŒç»­è°ƒä¼˜
- **æ¬¡ä¼˜è§£**: éš¾ä»¥æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£

**æŠ€æœ¯æ¼”è¿›**:

1. **2015 å¹´**: å¼ºåŒ–å­¦ä¹ åœ¨æ¸¸æˆé¢†åŸŸå–å¾—çªç ´ï¼ˆAlphaGoï¼‰
2. **2018 å¹´**: é¦–æ¬¡å°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºæ•°æ®åº“ä¼˜åŒ–
3. **2020 å¹´**: PostgreSQL ç¤¾åŒºå¼€å§‹æ¢ç´¢ AI è‡ªæ²»ä¼˜åŒ–
4. **2025 å¹´**: pg_ai é¡¹ç›®å®ç°ç”Ÿäº§çº§å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨

**å¸‚åœºéœ€æ±‚**:

- **è‡ªåŠ¨åŒ–ä¼˜åŒ–**: å‡å°‘äººå·¥å¹²é¢„ï¼Œå®ç°è‡ªåŠ¨åŒ–ä¼˜åŒ–
- **è‡ªé€‚åº”èƒ½åŠ›**: é€‚åº”å·¥ä½œè´Ÿè½½çš„åŠ¨æ€å˜åŒ–
- **æ€§èƒ½æå‡**: æŒç»­ä¼˜åŒ–ï¼Œæå‡æ•°æ®åº“æ€§èƒ½
- **æˆæœ¬é™ä½**: å‡å°‘ DBA äººåŠ›æˆæœ¬

### 1.2 æŠ€æœ¯å®šä½

å¼ºåŒ–å­¦ä¹ åœ¨æ•°æ®åº“ä¼˜åŒ–ä¸­çš„åº”ç”¨æ˜¯ AI è‡ªæ²»æ•°æ®åº“çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œé€šè¿‡æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’ï¼Œå­¦ä¹ æœ€ä¼˜çš„ä¼˜åŒ–ç­–ç•¥ï¼Œå®
ç°æ•°æ®åº“çš„è‡ªä¸»ä¼˜åŒ–ã€‚

### 1.3 æ ¸å¿ƒä»·å€¼

- **è‡ªä¸»å­¦ä¹ **: ä»å†å²æ•°æ®ä¸­å­¦ä¹ ä¼˜åŒ–ç­–ç•¥
- **æŒç»­ä¼˜åŒ–**: æ ¹æ®å·¥ä½œè´Ÿè½½å˜åŒ–æŒç»­ä¼˜åŒ–
- **å…¨å±€æœ€ä¼˜**: å¯»æ‰¾å…¨å±€æœ€ä¼˜è§£ï¼Œè€Œéå±€éƒ¨æœ€ä¼˜
- **é›¶å‚æ•°è°ƒä¼˜**: è‡ªåŠ¨è°ƒä¼˜ï¼Œæ— éœ€äººå·¥å¹²é¢„

---

## 2. å¼ºåŒ–å­¦ä¹ åŸºç¡€

### 2.1 å¼ºåŒ–å­¦ä¹ åŸºæœ¬æ¦‚å¿µ

**æ ¸å¿ƒè¦ç´ **:

- **æ™ºèƒ½ä½“ (Agent)**: æ‰§è¡Œä¼˜åŒ–å†³ç­–çš„å®ä½“
- **ç¯å¢ƒ (Environment)**: æ•°æ®åº“ç³»ç»Ÿå’Œå·¥ä½œè´Ÿè½½
- **çŠ¶æ€ (State)**: æ•°æ®åº“çš„å½“å‰çŠ¶æ€
- **åŠ¨ä½œ (Action)**: ä¼˜åŒ–æ“ä½œï¼ˆå¦‚åˆ›å»ºç´¢å¼•ã€è°ƒæ•´å‚æ•°ï¼‰
- **å¥–åŠ± (Reward)**: ä¼˜åŒ–æ•ˆæœçš„åé¦ˆ

**å­¦ä¹ ç›®æ ‡**:

æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ï¼Œå³æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ Ï€\*ï¼Œä½¿å¾—ï¼š

```
Ï€* = argmax E[âˆ‘(t=0 to T) Î³^t * r_t | Ï€]
```

å…¶ä¸­ï¼š

- Î³: æŠ˜æ‰£å› å­
- r_t: æ—¶åˆ» t çš„å¥–åŠ±
- T: æ—¶é—´æ­¥æ•°

### 2.2 æ•°æ®åº“ä¼˜åŒ–é—®é¢˜å»ºæ¨¡

**é—®é¢˜å»ºæ¨¡**:

å°†æ•°æ®åº“ä¼˜åŒ–é—®é¢˜å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)ï¼š

```python
class DatabaseOptimizationMDP:
    """æ•°æ®åº“ä¼˜åŒ– MDP"""

    def __init__(self):
        self.state_space = StateSpace()  # çŠ¶æ€ç©ºé—´
        self.action_space = ActionSpace()  # åŠ¨ä½œç©ºé—´
        self.reward_function = RewardFunction()  # å¥–åŠ±å‡½æ•°
        self.transition_function = TransitionFunction()  # è½¬ç§»å‡½æ•°

    def step(self, state, action):
        """æ‰§è¡ŒåŠ¨ä½œï¼Œè¿”å›æ–°çŠ¶æ€å’Œå¥–åŠ±"""
        # 1. æ‰§è¡Œä¼˜åŒ–åŠ¨ä½œ
        new_state = self.transition_function(state, action)

        # 2. è®¡ç®—å¥–åŠ±
        reward = self.reward_function(state, action, new_state)

        # 3. åˆ¤æ–­æ˜¯å¦ç»“æŸ
        done = self.is_terminal(new_state)

        return new_state, reward, done
```

### 2.3 çŠ¶æ€ç©ºé—´è®¾è®¡

**çŠ¶æ€è¡¨ç¤º**:

çŠ¶æ€åŒ…å«æ•°æ®åº“çš„å½“å‰çŠ¶æ€ä¿¡æ¯ï¼š

```python
class DatabaseState:
    """æ•°æ®åº“çŠ¶æ€"""

    def __init__(self):
        # æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯
        self.query_stats = {
            'slow_queries': 0,
            'avg_latency': 0.0,
            'throughput': 0.0
        }

        # ç´¢å¼•ä½¿ç”¨æƒ…å†µ
        self.index_usage = {
            'used_indexes': [],
            'unused_indexes': [],
            'missing_indexes': []
        }

        # ç³»ç»Ÿèµ„æº
        self.resource_usage = {
            'cpu_usage': 0.0,
            'memory_usage': 0.0,
            'disk_io': 0.0
        }

        # é…ç½®å‚æ•°
        self.config_params = {
            'shared_buffers': 0,
            'work_mem': 0,
            'maintenance_work_mem': 0
        }

    def to_vector(self):
        """è½¬æ¢ä¸ºç‰¹å¾å‘é‡"""
        return np.concatenate([
            [self.query_stats['slow_queries']],
            [self.query_stats['avg_latency']],
            [self.query_stats['throughput']],
            self.index_usage['used_indexes'],
            [self.resource_usage['cpu_usage']],
            [self.resource_usage['memory_usage']],
            [self.resource_usage['disk_io']],
            [self.config_params['shared_buffers']],
            [self.config_params['work_mem']],
            [self.config_params['maintenance_work_mem']]
        ])
```

### 2.4 åŠ¨ä½œç©ºé—´è®¾è®¡

**åŠ¨ä½œç±»å‹**:

```python
class OptimizationAction:
    """ä¼˜åŒ–åŠ¨ä½œ"""

    # ç´¢å¼•æ“ä½œ
    CREATE_INDEX = "create_index"
    DROP_INDEX = "drop_index"
    REBUILD_INDEX = "rebuild_index"

    # å‚æ•°è°ƒæ•´
    ADJUST_SHARED_BUFFERS = "adjust_shared_buffers"
    ADJUST_WORK_MEM = "adjust_work_mem"
    ADJUST_MAINTENANCE_WORK_MEM = "adjust_maintenance_work_mem"

    # æŸ¥è¯¢ä¼˜åŒ–
    REWRITE_QUERY = "rewrite_query"
    ADD_HINT = "add_hint"

    def __init__(self, action_type, params):
        self.action_type = action_type
        self.params = params

    def execute(self, database):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        if self.action_type == self.CREATE_INDEX:
            return database.create_index(**self.params)
        elif self.action_type == self.DROP_INDEX:
            return database.drop_index(**self.params)
        # ... å…¶ä»–åŠ¨ä½œ
```

### 2.5 å¥–åŠ±å‡½æ•°è®¾è®¡

**å¥–åŠ±è®¾è®¡åŸåˆ™**:

- **æ€§èƒ½æå‡**: æŸ¥è¯¢å»¶è¿Ÿé™ä½ã€ååé‡æå‡
- **èµ„æºåˆ©ç”¨**: æé«˜èµ„æºåˆ©ç”¨ç‡
- **æˆæœ¬æ§åˆ¶**: æ§åˆ¶ä¼˜åŒ–æˆæœ¬ï¼ˆå¦‚ç´¢å¼•å­˜å‚¨æˆæœ¬ï¼‰

**å¥–åŠ±å‡½æ•°å®ç°**:

```python
class RewardFunction:
    """å¥–åŠ±å‡½æ•°"""

    def __init__(self):
        self.weights = {
            'latency': -1.0,      # å»¶è¿Ÿé™ä½ä¸ºæ­£å¥–åŠ±
            'throughput': 1.0,    # ååé‡æå‡ä¸ºæ­£å¥–åŠ±
            'resource': 0.5,      # èµ„æºåˆ©ç”¨ä¸ºæ­£å¥–åŠ±
            'cost': -0.3          # æˆæœ¬å¢åŠ ä¸ºè´Ÿå¥–åŠ±
        }

    def calculate(self, old_state, action, new_state):
        """è®¡ç®—å¥–åŠ±"""
        # 1. æ€§èƒ½å¥–åŠ±
        latency_reward = (old_state.query_stats['avg_latency'] -
                         new_state.query_stats['avg_latency']) * self.weights['latency']

        throughput_reward = (new_state.query_stats['throughput'] -
                            old_state.query_stats['throughput']) * self.weights['throughput']

        # 2. èµ„æºå¥–åŠ±
        resource_reward = self._calculate_resource_reward(old_state, new_state)

        # 3. æˆæœ¬æƒ©ç½š
        cost_penalty = self._calculate_cost_penalty(action)

        # 4. æ€»å¥–åŠ±
        total_reward = (latency_reward + throughput_reward +
                       resource_reward * self.weights['resource'] +
                       cost_penalty * self.weights['cost'])

        return total_reward
```

---

## 3. æ ¸å¿ƒç®—æ³•

### 3.1 Actor-Critic ç®—æ³•

**ç®—æ³•åŸç†**:

Actor-Critic ç»“åˆäº†ç­–ç•¥æ¢¯åº¦æ–¹æ³•å’Œä»·å€¼å‡½æ•°æ–¹æ³•ï¼š

- **Actor**: å­¦ä¹ ç­–ç•¥å‡½æ•° Ï€(a|s)ï¼Œé€‰æ‹©åŠ¨ä½œ
- **Critic**: å­¦ä¹ ä»·å€¼å‡½æ•° V(s)ï¼Œè¯„ä¼°çŠ¶æ€ä»·å€¼

**ç®—æ³•æµç¨‹**:

```python
class ActorCritic:
    """Actor-Critic ç®—æ³•"""

    def __init__(self, state_dim, action_dim):
        # Actor ç½‘ç»œï¼ˆç­–ç•¥ç½‘ç»œï¼‰
        self.actor = PolicyNetwork(state_dim, action_dim)

        # Critic ç½‘ç»œï¼ˆä»·å€¼ç½‘ç»œï¼‰
        self.critic = ValueNetwork(state_dim)

        # ä¼˜åŒ–å™¨
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())

    def select_action(self, state):
        """é€‰æ‹©åŠ¨ä½œ"""
        state_tensor = torch.FloatTensor(state)
        action_probs = self.actor(state_tensor)
        action = torch.multinomial(action_probs, 1)
        return action.item()

    def update(self, states, actions, rewards, next_states, dones):
        """æ›´æ–°ç½‘ç»œ"""
        # è®¡ç®—ä¼˜åŠ¿å‡½æ•°
        values = self.critic(states)
        next_values = self.critic(next_states)
        advantages = rewards + 0.99 * next_values * (1 - dones) - values

        # æ›´æ–° Critic
        critic_loss = F.mse_loss(values, rewards + 0.99 * next_values * (1 - dones))
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        # æ›´æ–° Actor
        action_probs = self.actor(states)
        action_log_probs = torch.log(action_probs.gather(1, actions))
        actor_loss = -(action_log_probs * advantages.detach()).mean()
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
```

### 3.2 ç­–ç•¥æ¢¯åº¦æ–¹æ³•

**ç­–ç•¥æ¢¯åº¦å®šç†**:

```
âˆ‡Î¸ J(Î¸) = E[âˆ‡Î¸ log Ï€Î¸(a|s) * Q^Ï€(s,a)]
```

**å®ç°**:

```python
class PolicyGradient:
    """ç­–ç•¥æ¢¯åº¦æ–¹æ³•"""

    def update_policy(self, states, actions, rewards):
        """æ›´æ–°ç­–ç•¥"""
        # è®¡ç®—å›æŠ¥
        returns = self._calculate_returns(rewards)

        # è®¡ç®—ç­–ç•¥æ¢¯åº¦
        action_probs = self.policy_network(states)
        action_log_probs = torch.log(action_probs.gather(1, actions))

        # ç­–ç•¥æ¢¯åº¦
        policy_gradient = -(action_log_probs * returns).mean()

        # æ›´æ–°ç½‘ç»œ
        self.optimizer.zero_grad()
        policy_gradient.backward()
        self.optimizer.step()
```

### 3.3 ç»éªŒå›æ”¾æœºåˆ¶

**ç»éªŒå›æ”¾**:

å­˜å‚¨å†å²ç»éªŒï¼Œéšæœºé‡‡æ ·è¿›è¡Œè®­ç»ƒï¼Œæ‰“ç ´æ•°æ®ç›¸å…³æ€§ï¼š

```python
class ExperienceReplay:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""

    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        """é‡‡æ ·ç»éªŒ"""
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return (np.array(states), np.array(actions), np.array(rewards),
                np.array(next_states), np.array(dones))

    def __len__(self):
        return len(self.buffer)
```

---

## 4. åº”ç”¨åœºæ™¯

### 4.1 æŸ¥è¯¢è®¡åˆ’ä¼˜åŒ–

**åº”ç”¨æè¿°**:

ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æŸ¥è¯¢æ‰§è¡Œè®¡åˆ’ï¼Œé€‰æ‹©æœ€ä¼˜çš„æ‰§è¡Œç­–ç•¥ã€‚

**å®ç°ç¤ºä¾‹**:

```python
class QueryPlanOptimizer:
    """æŸ¥è¯¢è®¡åˆ’ä¼˜åŒ–å™¨"""

    def optimize(self, query):
        """ä¼˜åŒ–æŸ¥è¯¢è®¡åˆ’"""
        # 1. è·å–å½“å‰çŠ¶æ€
        state = self.get_current_state(query)

        # 2. é€‰æ‹©åŠ¨ä½œï¼ˆæ‰§è¡Œè®¡åˆ’ï¼‰
        action = self.agent.select_action(state)

        # 3. æ‰§è¡ŒæŸ¥è¯¢
        result = self.execute_query(query, action)

        # 4. è®¡ç®—å¥–åŠ±
        reward = self.calculate_reward(result)

        # 5. æ›´æ–°ç­–ç•¥
        self.agent.update(state, action, reward)

        return result
```

### 4.2 ç´¢å¼•æ¨è

**åº”ç”¨æè¿°**:

è‡ªåŠ¨æ¨èå’Œåˆ›å»ºç´¢å¼•ï¼Œä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½ã€‚

### 4.3 å‚æ•°è°ƒä¼˜

**åº”ç”¨æè¿°**:

è‡ªåŠ¨è°ƒæ•´æ•°æ®åº“å‚æ•°ï¼Œä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ã€‚

---

## 5. å®ç°ç»†èŠ‚

### 5.1 ç¯å¢ƒè®¾è®¡

**ç¯å¢ƒæ¥å£**:

```python
class DatabaseEnvironment:
    """æ•°æ®åº“ç¯å¢ƒ"""

    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        return self.get_initial_state()

    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        # æ‰§è¡Œä¼˜åŒ–åŠ¨ä½œ
        new_state = self.apply_action(action)

        # è®¡ç®—å¥–åŠ±
        reward = self.calculate_reward(new_state)

        # åˆ¤æ–­æ˜¯å¦ç»“æŸ
        done = self.is_done(new_state)

        return new_state, reward, done, {}
```

### 5.2 ç¥ç»ç½‘ç»œæ¶æ„

**ç½‘ç»œè®¾è®¡**:

```python
class PolicyNetwork(nn.Module):
    """ç­–ç•¥ç½‘ç»œ"""

    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return self.softmax(x)
```

### 5.3 è®­ç»ƒæµç¨‹

**è®­ç»ƒæ­¥éª¤**:

1. åˆå§‹åŒ–ç¯å¢ƒå’Œæ™ºèƒ½ä½“
2. æ”¶é›†ç»éªŒ
3. æ›´æ–°ç­–ç•¥
4. è¯„ä¼°æ€§èƒ½
5. é‡å¤æ­¥éª¤ 2-4

---

## 6. æ€§èƒ½åˆ†æ

### 6.1 ä¼˜åŒ–æ•ˆæœ

**æµ‹è¯•ç»“æœ**:

| æŒ‡æ ‡         | ä¼˜åŒ–å‰   | ä¼˜åŒ–å   | æå‡ |
| ------------ | -------- | -------- | ---- |
| å¹³å‡æŸ¥è¯¢å»¶è¿Ÿ | 100ms    | 60ms     | 40%  |
| ååé‡       | 1000 QPS | 1500 QPS | 50%  |
| æ…¢æŸ¥è¯¢æ•°é‡   | 100      | 20       | 80%  |

### 6.2 è®­ç»ƒæ•ˆç‡

**è®­ç»ƒæ—¶é—´**:

- **åˆå§‹è®­ç»ƒ**: 1-2 å‘¨
- **åœ¨çº¿å­¦ä¹ **: æŒç»­ä¼˜åŒ–
- **æ”¶æ•›æ—¶é—´**: 2-4 å‘¨

### 6.3 å®é™…åº”ç”¨æ¡ˆä¾‹

**æ¡ˆä¾‹**: æŸç”µå•†å¹³å°ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨

- **æ€§èƒ½æå‡**: æŸ¥è¯¢å»¶è¿Ÿé™ä½ 40%
- **æˆæœ¬èŠ‚çœ**: DBA äººåŠ›æˆæœ¬é™ä½ 60%
- **è‡ªåŠ¨åŒ–ç‡**: 90% çš„ä¼˜åŒ–ä»»åŠ¡è‡ªåŠ¨åŒ–

---

## 7. æœ€ä½³å®è·µ

### 7.1 æ¨¡å‹è®­ç»ƒ

- **æ•°æ®æ”¶é›†**: æ”¶é›†è¶³å¤Ÿçš„å†å²æ•°æ®
- **ç‰¹å¾å·¥ç¨‹**: è®¾è®¡æœ‰æ•ˆçš„çŠ¶æ€ç‰¹å¾
- **å¥–åŠ±è®¾è®¡**: è®¾è®¡åˆç†çš„å¥–åŠ±å‡½æ•°
- **è¶…å‚æ•°è°ƒä¼˜**: è°ƒæ•´å­¦ä¹ ç‡ã€æŠ˜æ‰£å› å­ç­‰

### 7.2 éƒ¨ç½²ç­–ç•¥

- **æ¸è¿›å¼éƒ¨ç½²**: å…ˆåœ¨æµ‹è¯•ç¯å¢ƒéƒ¨ç½²
- **A/B æµ‹è¯•**: å¯¹æ¯”ä¼˜åŒ–æ•ˆæœ
- **å›æ»šæœºåˆ¶**: å‡†å¤‡å›æ»šæ–¹æ¡ˆ

### 7.3 ç›‘æ§ä¸è°ƒä¼˜

- **æ€§èƒ½ç›‘æ§**: ç›‘æ§ä¼˜åŒ–æ•ˆæœ
- **æ¨¡å‹æ›´æ–°**: å®šæœŸæ›´æ–°æ¨¡å‹
- **å¼‚å¸¸æ£€æµ‹**: æ£€æµ‹å¼‚å¸¸è¡Œä¸º

---

## 8. å‚è€ƒèµ„æ–™

- [å¼ºåŒ–å­¦ä¹ åŸºç¡€](https://spinningup.openai.com/)
- [pg_ai é¡¹ç›®](https://github.com/pg_ai/pg_ai)
- [æ•°æ®åº“ä¼˜åŒ–ä¸­çš„æœºå™¨å­¦ä¹ ](https://arxiv.org/abs/1802.04035)

---

**æœ€åæ›´æ–°**: 2025 å¹´ 11 æœˆ 1 æ—¥  
**ç»´æŠ¤è€…**: PostgreSQL Modern Team
