# 奖励函数设计

> **更新时间**: 2025 年 11 月 1 日  
> **技术版本**: pg_ai v1.0+  
> **文档编号**: 02-02-02

## 📑 目录

- [奖励函数设计](#奖励函数设计)
  - [📑 目录](#-目录)
  - [1. 概述](#1-概述)
    - [1.1 设计目标](#11-设计目标)
    - [1.2 设计原则](#12-设计原则)
  - [2. 奖励函数组成](#2-奖励函数组成)
    - [2.1 性能奖励](#21-性能奖励)
    - [2.2 资源奖励](#22-资源奖励)
    - [2.3 成本惩罚](#23-成本惩罚)
    - [2.4 稳定性奖励](#24-稳定性奖励)
  - [3. 奖励函数实现](#3-奖励函数实现)
    - [3.1 基础奖励函数](#31-基础奖励函数)
    - [3.2 复合奖励函数](#32-复合奖励函数)
    - [3.3 自适应奖励函数](#33-自适应奖励函数)
  - [4. 权重调优](#4-权重调优)
    - [4.1 权重设计原则](#41-权重设计原则)
    - [4.2 权重调优方法](#42-权重调优方法)
  - [5. 最佳实践](#5-最佳实践)
    - [5.1 奖励函数设计建议](#51-奖励函数设计建议)
    - [5.2 权重调优建议](#52-权重调优建议)
  - [6. 参考资料](#6-参考资料)

---

## 1. 概述

### 1.1 设计目标

奖励函数是强化学习优化器的核心组件，用于评估优化动作的效果。设计良好的奖励函数能够：

- **引导学习方向**: 引导智能体学习最优策略
- **平衡多目标**: 平衡性能、资源、成本等多个目标
- **快速收敛**: 帮助模型快速收敛到最优解
- **稳定训练**: 保证训练过程的稳定性

### 1.2 设计原则

**设计原则**:

1. **目标对齐**: 奖励函数与优化目标一致
2. **尺度合理**: 奖励值在合理范围内
3. **平滑连续**: 奖励函数平滑，避免突变
4. **可解释性**: 奖励函数易于理解和调试

---

## 2. 奖励函数组成

### 2.1 性能奖励

**性能指标**:

- **查询延迟**: 查询执行时间
- **吞吐量**: 每秒查询数
- **慢查询率**: 慢查询占比

**性能奖励计算**:

```python
class PerformanceReward:
    """性能奖励"""

    def calculate(self, old_metrics, new_metrics):
        """计算性能奖励"""
        # 1. 延迟奖励（延迟降低为正奖励）
        latency_improvement = (old_metrics['avg_latency'] -
                              new_metrics['avg_latency']) / old_metrics['avg_latency']
        latency_reward = latency_improvement * 100

        # 2. 吞吐量奖励（吞吐量提升为正奖励）
        throughput_improvement = (new_metrics['throughput'] -
                                 old_metrics['throughput']) / old_metrics['throughput']
        throughput_reward = throughput_improvement * 50

        # 3. 慢查询奖励（慢查询减少为正奖励）
        slow_query_reduction = (old_metrics['slow_query_rate'] -
                               new_metrics['slow_query_rate'])
        slow_query_reward = slow_query_reduction * 200

        return latency_reward + throughput_reward + slow_query_reward
```

### 2.2 资源奖励

**资源指标**:

- **CPU 使用率**: CPU 利用率
- **内存使用率**: 内存利用率
- **I/O 使用率**: 磁盘 I/O 使用率

**资源奖励计算**:

```python
class ResourceReward:
    """资源奖励"""

    def calculate(self, old_metrics, new_metrics):
        """计算资源奖励"""
        # 资源利用率提升为正奖励
        cpu_improvement = new_metrics['cpu_utilization'] - old_metrics['cpu_utilization']
        memory_improvement = new_metrics['memory_utilization'] - old_metrics['memory_utilization']

        # 资源利用率在合理范围内为正奖励
        resource_reward = 0
        if 0.5 <= new_metrics['cpu_utilization'] <= 0.8:
            resource_reward += 10
        if 0.5 <= new_metrics['memory_utilization'] <= 0.8:
            resource_reward += 10

        return resource_reward
```

### 2.3 成本惩罚

**成本指标**:

- **索引存储成本**: 索引占用的存储空间
- **计算成本**: 额外的计算开销
- **维护成本**: 索引维护成本

**成本惩罚计算**:

```python
class CostPenalty:
    """成本惩罚"""

    def calculate(self, action, old_state, new_state):
        """计算成本惩罚"""
        cost = 0

        # 索引存储成本
        if action.type == 'create_index':
            index_size = self.estimate_index_size(action)
            cost += index_size * 0.1  # 每 GB 0.1 单位成本

        # 索引维护成本
        if action.type in ['create_index', 'rebuild_index']:
            cost += 5  # 维护成本

        return -cost  # 返回负值作为惩罚
```

### 2.4 稳定性奖励

**稳定性指标**:

- **性能波动**: 性能指标的方差
- **错误率**: 查询错误率
- **可用性**: 系统可用性

**稳定性奖励计算**:

```python
class StabilityReward:
    """稳定性奖励"""

    def calculate(self, old_metrics, new_metrics):
        """计算稳定性奖励"""
        # 性能波动降低为正奖励
        old_variance = np.var(old_metrics['latency_history'])
        new_variance = np.var(new_metrics['latency_history'])
        variance_reduction = (old_variance - new_variance) / old_variance
        stability_reward = variance_reduction * 20

        # 错误率降低为正奖励
        error_reduction = old_metrics['error_rate'] - new_metrics['error_rate']
        error_reward = error_reduction * 100

        return stability_reward + error_reward
```

---

## 3. 奖励函数实现

### 3.1 基础奖励函数

**简单奖励函数**:

```python
class SimpleRewardFunction:
    """简单奖励函数"""

    def __init__(self):
        self.weights = {
            'latency': -1.0,
            'throughput': 1.0,
            'cost': -0.3
        }

    def calculate(self, old_state, action, new_state):
        """计算奖励"""
        # 性能奖励
        latency_reward = (old_state.avg_latency - new_state.avg_latency) * self.weights['latency']
        throughput_reward = (new_state.throughput - old_state.throughput) * self.weights['throughput']

        # 成本惩罚
        cost_penalty = self._calculate_cost(action) * self.weights['cost']

        return latency_reward + throughput_reward + cost_penalty
```

### 3.2 复合奖励函数

**复合奖励函数**:

```python
class CompositeRewardFunction:
    """复合奖励函数"""

    def __init__(self):
        self.performance_reward = PerformanceReward()
        self.resource_reward = ResourceReward()
        self.cost_penalty = CostPenalty()
        self.stability_reward = StabilityReward()

        self.weights = {
            'performance': 1.0,
            'resource': 0.5,
            'cost': -0.3,
            'stability': 0.3
        }

    def calculate(self, old_state, action, new_state):
        """计算复合奖励"""
        # 各组件奖励
        perf_reward = self.performance_reward.calculate(old_state.metrics, new_state.metrics)
        resource_reward = self.resource_reward.calculate(old_state.metrics, new_state.metrics)
        cost_penalty = self.cost_penalty.calculate(action, old_state, new_state)
        stability_reward = self.stability_reward.calculate(old_state.metrics, new_state.metrics)

        # 加权求和
        total_reward = (
            perf_reward * self.weights['performance'] +
            resource_reward * self.weights['resource'] +
            cost_penalty * self.weights['cost'] +
            stability_reward * self.weights['stability']
        )

        return total_reward
```

### 3.3 自适应奖励函数

**自适应奖励函数**:

```python
class AdaptiveRewardFunction:
    """自适应奖励函数"""

    def __init__(self):
        self.base_weights = {
            'performance': 1.0,
            'resource': 0.5,
            'cost': -0.3
        }
        self.adaptation_rate = 0.1

    def calculate(self, old_state, action, new_state):
        """计算自适应奖励"""
        # 根据当前状态调整权重
        weights = self._adapt_weights(old_state, new_state)

        # 计算奖励
        reward = self._calculate_with_weights(old_state, action, new_state, weights)

        return reward

    def _adapt_weights(self, old_state, new_state):
        """自适应调整权重"""
        weights = self.base_weights.copy()

        # 如果性能已经很好，增加成本和稳定性权重
        if new_state.avg_latency < 50:  # 延迟已经很低
            weights['cost'] *= 1.5
            weights['stability'] *= 1.2

        # 如果资源使用率很高，增加资源权重
        if new_state.cpu_utilization > 0.9:
            weights['resource'] *= 1.5

        return weights
```

---

## 4. 权重调优

### 4.1 权重设计原则

**设计原则**:

1. **目标优先级**: 根据业务目标确定权重优先级
2. **尺度平衡**: 确保各组件奖励在相似尺度
3. **动态调整**: 根据训练进度动态调整权重

### 4.2 权重调优方法

**调优方法**:

```python
class WeightTuner:
    """权重调优器"""

    def __init__(self):
        self.weight_ranges = {
            'performance': (0.5, 2.0),
            'resource': (0.1, 1.0),
            'cost': (-1.0, -0.1),
            'stability': (0.1, 0.5)
        }

    def tune_weights(self, training_history):
        """调优权重"""
        # 使用网格搜索或贝叶斯优化
        best_weights = None
        best_performance = -float('inf')

        for weights in self._generate_weight_combinations():
            performance = self._evaluate_weights(weights, training_history)
            if performance > best_performance:
                best_performance = performance
                best_weights = weights

        return best_weights
```

---

## 5. 最佳实践

### 5.1 奖励函数设计建议

1. **明确目标**: 明确优化目标，设计对应的奖励组件
2. **合理尺度**: 确保奖励值在合理范围内（如 -100 到 100）
3. **平滑函数**: 使用平滑的奖励函数，避免突变
4. **可解释性**: 奖励函数易于理解和调试

### 5.2 权重调优建议

1. **初始权重**: 从经验值开始
2. **逐步调优**: 逐步调整权重，观察效果
3. **A/B 测试**: 对比不同权重配置的效果
4. **定期评估**: 定期评估和调整权重

---

## 6. 参考资料

- [强化学习奖励函数设计](https://spinningup.openai.com/)
- [pg_ai 奖励函数文档](https://github.com/pg_ai/pg_ai)

---

**最后更新**: 2025 年 11 月 1 日  
**维护者**: PostgreSQL Modern Team
