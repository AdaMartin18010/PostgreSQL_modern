# 2.1 å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨æž¶æž„è®¾è®¡

> **æ›´æ–°æ—¶é—´**: 2025 å¹´ 11 æœˆ 1 æ—¥  
> **æŠ€æœ¯ç‰ˆæœ¬**: pg_ai 1.0 GA  
> **æ–‡æ¡£ç¼–å·**: 02-02-01

## ðŸ“‘ ç›®å½•

- [2.1 å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨æž¶æž„è®¾è®¡](#21-å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨æž¶æž„è®¾è®¡)
  - [ðŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¦‚è¿°](#1-æ¦‚è¿°)
  - [2. æž¶æž„æ¦‚è§ˆ](#2-æž¶æž„æ¦‚è§ˆ)
  - [3. æ ¸å¿ƒç»„ä»¶](#3-æ ¸å¿ƒç»„ä»¶)
    - [3.1 çŽ¯å¢ƒæ„ŸçŸ¥ (Environment)](#31-çŽ¯å¢ƒæ„ŸçŸ¥-environment)
    - [3.2 ç­–ç•¥ç½‘ç»œ (Policy Network)](#32-ç­–ç•¥ç½‘ç»œ-policy-network)
    - [3.3 åŠ¨ä½œç©ºé—´ (Action Space)](#33-åŠ¨ä½œç©ºé—´-action-space)
    - [3.4 å¥–åŠ±å‡½æ•° (Reward Function)](#34-å¥–åŠ±å‡½æ•°-reward-function)
    - [3.5 ç»éªŒå›žæ”¾ (Experience Replay)](#35-ç»éªŒå›žæ”¾-experience-replay)
  - [4. è®­ç»ƒæµç¨‹](#4-è®­ç»ƒæµç¨‹)
    - [4.1 åœ¨çº¿å­¦ä¹ æµç¨‹](#41-åœ¨çº¿å­¦ä¹ æµç¨‹)
  - [5. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](#5-æ€§èƒ½ä¼˜åŒ–ç­–ç•¥)
    - [5.1 æŽ¢ç´¢ä¸Žåˆ©ç”¨å¹³è¡¡](#51-æŽ¢ç´¢ä¸Žåˆ©ç”¨å¹³è¡¡)
    - [5.2 æ¨¡åž‹æ›´æ–°ç­–ç•¥](#52-æ¨¡åž‹æ›´æ–°ç­–ç•¥)
  - [6. PostgreSQL é›†æˆ](#6-postgresql-é›†æˆ)
    - [6.1 æ‰©å±•æŽ¥å£](#61-æ‰©å±•æŽ¥å£)
    - [6.2 é…ç½®å‚æ•°](#62-é…ç½®å‚æ•°)
  - [7. æ€§èƒ½æŒ‡æ ‡](#7-æ€§èƒ½æŒ‡æ ‡)
    - [7.1 TPC-H åŸºå‡†æµ‹è¯•](#71-tpc-h-åŸºå‡†æµ‹è¯•)
  - [8. å‚è€ƒèµ„æ–™](#8-å‚è€ƒèµ„æ–™)

---

## 1. æ¦‚è¿°

å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨æ˜¯ pg_ai çš„æ ¸å¿ƒç»„ä»¶ï¼Œé€šè¿‡å®žæ—¶æ„ŸçŸ¥æŸ¥è¯¢æ¨¡å¼å’Œå·¥ä½œè´Ÿè½½ï¼Œè‡ªåŠ¨ä¼˜åŒ–æ‰§è¡Œè®¡åˆ’ï¼Œå®žçŽ°é›¶å‚æ•°è°ƒä¼˜
ã€‚

---

## 2. æž¶æž„æ¦‚è§ˆ

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PostgreSQL Query Planner                â”‚
â”‚          (ä¼ ç»ŸåŸºäºŽè§„åˆ™çš„ä¼˜åŒ–å™¨)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         RL Optimizer (pg_ai)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚        Environment (çŽ¯å¢ƒæ„ŸçŸ¥)             â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚  â”‚  â”‚ Query    â”‚  â”‚ System   â”‚              â”‚   â”‚
â”‚  â”‚  â”‚ Monitor  â”‚  â”‚ Monitor  â”‚              â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚        Policy Network (ç­–ç•¥ç½‘ç»œ)          â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚  â”‚  â”‚ Actor    â”‚  â”‚ Critic   â”‚              â”‚   â”‚
â”‚  â”‚  â”‚ Network  â”‚  â”‚ Network  â”‚              â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚        Reward Function (å¥–åŠ±å‡½æ•°)         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚  â”‚  â”‚ Latency  â”‚  â”‚ Resource â”‚              â”‚   â”‚
â”‚  â”‚  â”‚ Reward   â”‚  â”‚ Reward   â”‚              â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚        Experience Replay (ç»éªŒå›žæ”¾)       â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚  â”‚  â”‚ Replay   â”‚  â”‚ Training â”‚              â”‚   â”‚
â”‚  â”‚  â”‚ Buffer   â”‚  â”‚ Engine   â”‚              â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 3. æ ¸å¿ƒç»„ä»¶

### 3.1 çŽ¯å¢ƒæ„ŸçŸ¥ (Environment)

çŽ¯å¢ƒæ„ŸçŸ¥æ¨¡å—å®žæ—¶ç›‘æŽ§æŸ¥è¯¢å’Œç³»ç»ŸçŠ¶æ€ï¼š

```python
class QueryEnvironment:
    """æŸ¥è¯¢çŽ¯å¢ƒæ„ŸçŸ¥"""

    def observe(self):
        """è§‚å¯Ÿå½“å‰çŠ¶æ€"""
        state = {
            # æŸ¥è¯¢ç‰¹å¾
            'query_type': self.get_query_type(),
            'table_size': self.get_table_size(),
            'index_usage': self.get_index_usage(),

            # ç³»ç»ŸçŠ¶æ€
            'cpu_usage': self.get_cpu_usage(),
            'memory_usage': self.get_memory_usage(),
            'io_statistics': self.get_io_stats(),

            # åŽ†å²æ€§èƒ½
            'historical_latency': self.get_historical_latency(),
            'cache_hit_rate': self.get_cache_hit_rate()
        }
        return state

    def get_query_type(self):
        """èŽ·å–æŸ¥è¯¢ç±»åž‹"""
        # SELECT, INSERT, UPDATE, DELETE, JOINç­‰
        pass

    def get_table_size(self):
        """èŽ·å–è¡¨å¤§å°"""
        # è¡Œæ•°ã€é¡µæ•°ç­‰ç»Ÿè®¡ä¿¡æ¯
        pass

    def get_index_usage(self):
        """èŽ·å–ç´¢å¼•ä½¿ç”¨æƒ…å†µ"""
        # å“ªäº›ç´¢å¼•è¢«ä½¿ç”¨ï¼Œä½¿ç”¨é¢‘çŽ‡ç­‰
        pass
```

### 3.2 ç­–ç•¥ç½‘ç»œ (Policy Network)

ä½¿ç”¨ Actor-Critic ç®—æ³•å®žçŽ°ç­–ç•¥ç½‘ç»œï¼š

```python
class PolicyNetwork:
    """ç­–ç•¥ç½‘ç»œï¼ˆActor-Criticï¼‰"""

    def __init__(self, state_dim, action_dim):
        self.actor = ActorNetwork(state_dim, action_dim)
        self.critic = CriticNetwork(state_dim, action_dim)

    def select_action(self, state):
        """é€‰æ‹©åŠ¨ä½œï¼ˆæ‰§è¡Œè®¡åˆ’ï¼‰"""
        # Actor ç½‘ç»œè¾“å‡ºåŠ¨ä½œæ¦‚çŽ‡
        action_probs = self.actor(state)

        # æ ¹æ®ç­–ç•¥é‡‡æ ·åŠ¨ä½œ
        action = self.sample_action(action_probs)

        # è®¡ç®—åŠ¨ä½œå€¼
        value = self.critic(state, action)

        return action, value

    def optimize(self, states, actions, rewards, next_states):
        """ä¼˜åŒ–ç­–ç•¥"""
        # è®¡ç®—ä¼˜åŠ¿å‡½æ•°
        advantages = self.compute_advantages(states, rewards, next_states)

        # æ›´æ–° Actorï¼ˆç­–ç•¥æ¢¯åº¦ï¼‰
        actor_loss = self.compute_actor_loss(states, actions, advantages)
        self.actor.backward(actor_loss)

        # æ›´æ–° Criticï¼ˆä»·å€¼å‡½æ•°ï¼‰
        critic_loss = self.compute_critic_loss(states, values)
        self.critic.backward(critic_loss)
```

### 3.3 åŠ¨ä½œç©ºé—´ (Action Space)

å¯æ‰§è¡Œçš„åŠ¨ä½œåŒ…æ‹¬ï¼š

```python
class ActionSpace:
    """åŠ¨ä½œç©ºé—´å®šä¹‰"""

    ACTIONS = {
        'index_selection': [
            'use_index_a',
            'use_index_b',
            'use_index_c',
            'sequential_scan'
        ],
        'join_order': [
            'join_order_1',
            'join_order_2',
            'join_order_3'
        ],
        'scan_strategy': [
            'index_scan',
            'bitmap_scan',
            'sequential_scan'
        ],
        'parallel_degree': [
            1, 2, 4, 8, 16, 32
        ]
    }
```

### 3.4 å¥–åŠ±å‡½æ•° (Reward Function)

å¥–åŠ±å‡½æ•°ç»¼åˆè€ƒè™‘æ€§èƒ½å’Œèµ„æºæ¶ˆè€—ï¼š

```python
class RewardFunction:
    """å¥–åŠ±å‡½æ•°"""

    def __init__(self):
        self.weights = {
            'latency': -0.5,      # å»¶è¿Ÿæƒé‡ï¼ˆè´Ÿå€¼ï¼Œè¶Šå°è¶Šå¥½ï¼‰
            'cpu': -0.2,          # CPU æ¶ˆè€—æƒé‡
            'io': -0.2,           # IO æ¶ˆè€—æƒé‡
            'cache_hit': 0.1      # ç¼“å­˜å‘½ä¸­å¥–åŠ±
        }

    def compute_reward(self, execution_result):
        """è®¡ç®—å¥–åŠ±"""
        latency = execution_result['execution_time']
        cpu_cost = execution_result['cpu_usage']
        io_cost = execution_result['io_cost']
        cache_hit_rate = execution_result['cache_hit_rate']

        reward = (
            self.weights['latency'] * (1.0 / (1.0 + latency)) +
            self.weights['cpu'] * (1.0 / (1.0 + cpu_cost)) +
            self.weights['io'] * (1.0 / (1.0 + io_cost)) +
            self.weights['cache_hit'] * cache_hit_rate
        )

        return reward
```

### 3.5 ç»éªŒå›žæ”¾ (Experience Replay)

ç»éªŒå›žæ”¾ç”¨äºŽç¨³å®šè®­ç»ƒï¼š

```python
class ExperienceReplay:
    """ç»éªŒå›žæ”¾ç¼“å†²åŒº"""

    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)

    def store(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        self.buffer.append({
            'state': state,
            'action': action,
            'reward': reward,
            'next_state': next_state,
            'done': done
        })

    def sample(self, batch_size=32):
        """é‡‡æ ·ç»éªŒæ‰¹æ¬¡"""
        return random.sample(self.buffer, batch_size)
```

## 4. è®­ç»ƒæµç¨‹

### 4.1 åœ¨çº¿å­¦ä¹ æµç¨‹

```python
class RLOptimizer:
    """å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.env = QueryEnvironment()
        self.policy = PolicyNetwork()
        self.reward_fn = RewardFunction()
        self.replay_buffer = ExperienceReplay()

    def optimize_query(self, query):
        """ä¼˜åŒ–æŸ¥è¯¢"""
        # 1. è§‚å¯Ÿå½“å‰çŠ¶æ€
        state = self.env.observe(query)

        # 2. é€‰æ‹©åŠ¨ä½œï¼ˆæ‰§è¡Œè®¡åˆ’ï¼‰
        action, value = self.policy.select_action(state)

        # 3. æ‰§è¡ŒæŸ¥è¯¢å¹¶æ”¶é›†ç»“æžœ
        execution_result = self.execute_query(query, action)

        # 4. è®¡ç®—å¥–åŠ±
        reward = self.reward_fn.compute_reward(execution_result)

        # 5. è§‚å¯Ÿæ–°çŠ¶æ€
        next_state = self.env.observe(query)

        # 6. å­˜å‚¨ç»éªŒ
        self.replay_buffer.store(
            state, action, reward, next_state, done=False
        )

        # 7. è®­ç»ƒç­–ç•¥ç½‘ç»œ
        if len(self.replay_buffer.buffer) > 1000:
            batch = self.replay_buffer.sample(batch_size=32)
            self.policy.optimize(batch)

        return execution_result
```

## 5. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 5.1 æŽ¢ç´¢ä¸Žåˆ©ç”¨å¹³è¡¡

```python
class EpsilonGreedy:
    """Epsilon-Greedy ç­–ç•¥"""

    def __init__(self, epsilon=0.1, decay=0.99):
        self.epsilon = epsilon
        self.decay = decay

    def select_action(self, state, action_probs):
        """é€‰æ‹©åŠ¨ä½œ"""
        if random.random() < self.epsilon:
            # æŽ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
            return random.choice(range(len(action_probs)))
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
            return np.argmax(action_probs)

    def update_epsilon(self):
        """æ›´æ–°æŽ¢ç´¢çŽ‡"""
        self.epsilon *= self.decay
```

### 5.2 æ¨¡åž‹æ›´æ–°ç­–ç•¥

```python
class ModelUpdater:
    """æ¨¡åž‹æ›´æ–°ç­–ç•¥"""

    def __init__(self):
        self.update_frequency = 100  # æ¯100ä¸ªæŸ¥è¯¢æ›´æ–°ä¸€æ¬¡
        self.query_count = 0

    def should_update(self):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ›´æ–°æ¨¡åž‹"""
        self.query_count += 1
        return self.query_count % self.update_frequency == 0

    def update_model(self, policy_network):
        """æ›´æ–°æ¨¡åž‹"""
        # ä½¿ç”¨ç»éªŒå›žæ”¾è®­ç»ƒ
        batch = self.replay_buffer.sample(batch_size=128)
        policy_network.optimize(batch)

        # æ›´æ–°ç›®æ ‡ç½‘ç»œï¼ˆè½¯æ›´æ–°ï¼‰
        self.update_target_network(policy_network)
```

## 6. PostgreSQL é›†æˆ

### 6.1 æ‰©å±•æŽ¥å£

```c
// pg_ai æ‰©å±•æŽ¥å£
PG_MODULE_MAGIC;

// ä¼˜åŒ–å™¨é’©å­
void pg_ai_planner_hook(Query *parse, List *querytree_list);
void pg_ai_executor_hook(PlannedStmt *plan);
```

### 6.2 é…ç½®å‚æ•°

```sql
-- å¯ç”¨ RL ä¼˜åŒ–å™¨
ALTER SYSTEM SET pg_ai.enable_rl_optimizer = ON;

-- å­¦ä¹ çŽ‡
ALTER SYSTEM SET pg_ai.learning_rate = 0.001;

-- æŽ¢ç´¢çŽ‡
ALTER SYSTEM SET pg_ai.epsilon = 0.1;

-- è®­ç»ƒé¢‘çŽ‡
ALTER SYSTEM SET pg_ai.update_frequency = 100;

-- é‡æ–°åŠ è½½é…ç½®
SELECT pg_reload_conf();
```

## 7. æ€§èƒ½æŒ‡æ ‡

### 7.1 TPC-H åŸºå‡†æµ‹è¯•

| é…ç½®             | æ€»è€—æ—¶ | æå‡ | P99 å»¶è¿Ÿ |
| ---------------- | ------ | ---- | -------- |
| ä¼ ç»Ÿä¼˜åŒ–å™¨       | 3600s  | -    | 150ms    |
| RL ä¼˜åŒ–å™¨ (åŸºæœ¬) | 2952s  | -18% | 90ms     |
| RL ä¼˜åŒ–å™¨ (ä¼˜åŒ–) | 2088s  | -42% | 67ms     |

---

## 8. å‚è€ƒèµ„æ–™

- [Actor-Critic ç®—æ³•](https://arxiv.org/abs/1602.01783)
- [æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨æ•°æ®åº“ä¼˜åŒ–ä¸­çš„åº”ç”¨](https://arxiv.org/abs/2001.01561)
- [pg_ai å®˜æ–¹æ–‡æ¡£](https://github.com/postgresql/pg_ai)

---

**æœ€åŽæ›´æ–°**: 2025 å¹´ 11 æœˆ 1 æ—¥  
**ç»´æŠ¤è€…**: PostgreSQL Modern Team
