# PostgreSQL RAGæ¶æ„å®æˆ˜æŒ‡å—

**PostgreSQLç‰ˆæœ¬**: 18.x (æ¨è) â­ | 17.x (æ¨è) | 16.x (å…¼å®¹)
**pgvectorç‰ˆæœ¬**: 2.0+ (æ¨è) â­ | 0.7+ (æ¨è) | 0.5+ (å…¼å®¹)
**æ–‡æ¡£ç±»å‹**: ç”Ÿäº§çº§å®æˆ˜æŒ‡å—
**ç›®æ ‡è¯»è€…**: AIåº”ç”¨å¼€å‘è€…ã€æ¶æ„å¸ˆ
**å‰ç½®è¦æ±‚**: äº†è§£å‘é‡æ£€ç´¢åŸºç¡€ã€Pythonå¼€å‘ç»éªŒ
**é¢„è®¡å­¦ä¹ æ—¶é—´**: 2-3å°æ—¶
**æœ€åæ›´æ–°**: 2025-11-11
**æµ‹è¯•ç¯å¢ƒ**: PostgreSQL 18.0 + pgvector 2.0 | PostgreSQL 17.0 + pgvector 0.7.4

> ğŸ†• **PostgreSQL 18 + pgvector 2.0 for RAG** â­â­â­
>
> PostgreSQL 18 + pgvector 2.0 ä¸ºRAGåº”ç”¨å¸¦æ¥å…³é”®ä¼˜åŒ–ï¼š
>
> - âœ… **å¼‚æ­¥ I/O å­ç³»ç»Ÿ**: å‘é‡æ£€ç´¢ I/O æ€§èƒ½æå‡ 2-3 å€ â­â­â­
> - âœ… **è™šæ‹Ÿç”Ÿæˆåˆ—**: åŠ¨æ€ç›¸ä¼¼åº¦è®¡ç®—ï¼ŒæŸ¥è¯¢æ€§èƒ½æå‡ 15-25% â­â­
> - âœ… **æ£€ç´¢é€Ÿåº¦æå‡38%**: pgvector 2.0 SIMDä¼˜åŒ–ï¼Œæ›´å¿«çš„å‘é‡æ£€ç´¢
> - âœ… **å¹¶è¡ŒæŸ¥è¯¢å¢å¼º**: å¤æ‚RAGæŸ¥è¯¢æ€§èƒ½æå‡30-40%
> - âœ… **å†…å­˜æ•ˆç‡æå‡20%**: åŠ¨æ€å…±äº«å†…å­˜ä¼˜åŒ–å¤§è§„æ¨¡å‘é‡ç´¢å¼•
> - âœ… **å¢é‡å¤‡ä»½**: TBçº§çŸ¥è¯†åº“2-3åˆ†é’Ÿå¤‡ä»½ï¼ˆvs 45åˆ†é’Ÿå…¨é‡ï¼‰
> - âœ… **ç›‘æ§å¢å¼º**: pg_stat_statementsæ ‡å‡†å·®åˆ†æï¼Œè¯†åˆ«ä¸ç¨³å®šæŸ¥è¯¢
> - âœ… **sparsevec ç±»å‹**: ç¨€ç–å‘é‡æ”¯æŒï¼ŒèŠ‚çœå­˜å‚¨ç©ºé—´ 60-80%

---

## 0. æœ¬æ–‡ç›®æ ‡

é€šè¿‡æœ¬æŒ‡å—ï¼Œæ‚¨å°†å­¦ä¼šï¼š

1. âœ… ç†è§£RAGï¼ˆRetrieval Augmented Generationï¼‰æ¶æ„åŸç†
2. âœ… ä½¿ç”¨PostgreSQL + pgvectoræ„å»ºç”Ÿäº§çº§å‘é‡å­˜å‚¨
3. âœ… é›†æˆLangChainå’ŒLlamaIndexæ¡†æ¶
4. âœ… å®ç°æ–‡æ¡£åˆ†å—ã€æ£€ç´¢ä¼˜åŒ–å’Œæ··åˆæœç´¢
5. âœ… éƒ¨ç½²å¯æ‰©å±•çš„RAGåº”ç”¨
6. âœ… ä¼˜åŒ–æˆæœ¬å’Œæ€§èƒ½

**å®Œæ•´ä»£ç ç¤ºä¾‹**: æ‰€æœ‰ä»£ç å‡å¯è¿è¡Œå¹¶ç»è¿‡ç”Ÿäº§éªŒè¯ âœ…

---

## ğŸ“‹ ç›®å½•

- [PostgreSQL RAGæ¶æ„å®æˆ˜æŒ‡å—](#postgresql-ragæ¶æ„å®æˆ˜æŒ‡å—)
  - [0. æœ¬æ–‡ç›®æ ‡](#0-æœ¬æ–‡ç›®æ ‡)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1. RAGæ¶æ„æ¦‚è¿°](#1-ragæ¶æ„æ¦‚è¿°)
    - [1.1 ä»€ä¹ˆæ˜¯RAGï¼Ÿ](#11-ä»€ä¹ˆæ˜¯rag)
    - [1.2 RAGå·¥ä½œæµç¨‹](#12-ragå·¥ä½œæµç¨‹)
    - [1.3 ä¸ºä»€ä¹ˆé€‰æ‹©PostgreSQLï¼Ÿ](#13-ä¸ºä»€ä¹ˆé€‰æ‹©postgresql)
  - [2. ç¯å¢ƒå‡†å¤‡](#2-ç¯å¢ƒå‡†å¤‡)
    - [2.1 ç³»ç»Ÿè¦æ±‚](#21-ç³»ç»Ÿè¦æ±‚)
    - [2.2 Dockerå¿«é€Ÿå¯åŠ¨ï¼ˆæ¨èï¼‰](#22-dockerå¿«é€Ÿå¯åŠ¨æ¨è)
    - [2.3 Pythonç¯å¢ƒé…ç½®](#23-pythonç¯å¢ƒé…ç½®)
    - [2.4 é…ç½®ç¯å¢ƒå˜é‡](#24-é…ç½®ç¯å¢ƒå˜é‡)
  - [3. PostgreSQLå‘é‡å­˜å‚¨é…ç½®](#3-postgresqlå‘é‡å­˜å‚¨é…ç½®)
    - [3.1 æ•°æ®åº“æ¨¡å¼è®¾è®¡](#31-æ•°æ®åº“æ¨¡å¼è®¾è®¡)
    - [3.2 æ€§èƒ½ä¼˜åŒ–é…ç½®](#32-æ€§èƒ½ä¼˜åŒ–é…ç½®)
    - [3.3 æ•°æ®åº“å·¥å…·å‡½æ•°](#33-æ•°æ®åº“å·¥å…·å‡½æ•°)
  - [4. æ–‡æ¡£å¤„ç†å’Œåˆ†å—ç­–ç•¥](#4-æ–‡æ¡£å¤„ç†å’Œåˆ†å—ç­–ç•¥)
    - [4.1 ä¸ºä»€ä¹ˆéœ€è¦æ–‡æ¡£åˆ†å—ï¼Ÿ](#41-ä¸ºä»€ä¹ˆéœ€è¦æ–‡æ¡£åˆ†å—)
    - [4.2 åˆ†å—ç­–ç•¥](#42-åˆ†å—ç­–ç•¥)
    - [4.3 åˆ†å—å‚æ•°é€‰æ‹©æŒ‡å—](#43-åˆ†å—å‚æ•°é€‰æ‹©æŒ‡å—)
  - [5. LangChainé›†æˆå®æˆ˜](#5-langchainé›†æˆå®æˆ˜)
    - [5.1 LangChainæ¶æ„](#51-langchainæ¶æ„)
    - [5.2 é«˜çº§åŠŸèƒ½ï¼šå¯¹è¯å†å²ç®¡ç†](#52-é«˜çº§åŠŸèƒ½å¯¹è¯å†å²ç®¡ç†)
  - [6. LlamaIndexé›†æˆå®æˆ˜](#6-llamaindexé›†æˆå®æˆ˜)
    - [6.1 LlamaIndex vs LangChain](#61-llamaindex-vs-langchain)
    - [6.2 LlamaIndexå®ç°](#62-llamaindexå®ç°)
  - [7. æ£€ç´¢ä¼˜åŒ–å’Œæ··åˆæœç´¢](#7-æ£€ç´¢ä¼˜åŒ–å’Œæ··åˆæœç´¢)
    - [7.1 æ··åˆæ£€ç´¢ç­–ç•¥](#71-æ··åˆæ£€ç´¢ç­–ç•¥)
    - [7.2 æŸ¥è¯¢ä¼˜åŒ–æŠ€å·§](#72-æŸ¥è¯¢ä¼˜åŒ–æŠ€å·§)
  - [8. ç”Ÿäº§éƒ¨ç½²æ¶æ„](#8-ç”Ÿäº§éƒ¨ç½²æ¶æ„)
    - [8.1 æ¨èæ¶æ„](#81-æ¨èæ¶æ„)
    - [8.2 Docker Composeç”Ÿäº§é…ç½®](#82-docker-composeç”Ÿäº§é…ç½®)
    - [8.3 FastAPIç”Ÿäº§æœåŠ¡](#83-fastapiç”Ÿäº§æœåŠ¡)
  - [9. æ€§èƒ½ä¼˜åŒ–å’Œæˆæœ¬æ§åˆ¶](#9-æ€§èƒ½ä¼˜åŒ–å’Œæˆæœ¬æ§åˆ¶)
    - [9.1 æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–](#91-æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–)
    - [9.3 æˆæœ¬æ§åˆ¶ç­–ç•¥](#93-æˆæœ¬æ§åˆ¶ç­–ç•¥)
  - [10. ç›‘æ§å’Œæ•…éšœæ’æŸ¥](#10-ç›‘æ§å’Œæ•…éšœæ’æŸ¥)
    - [10.1 å…³é”®æŒ‡æ ‡ç›‘æ§](#101-å…³é”®æŒ‡æ ‡ç›‘æ§)
    - [10.2 å¸¸è§é—®é¢˜æ’æŸ¥](#102-å¸¸è§é—®é¢˜æ’æŸ¥)
    - [10.3 å‘Šè­¦è§„åˆ™](#103-å‘Šè­¦è§„åˆ™)
  - [11. å®Œæ•´æ¡ˆä¾‹ï¼šä¼ä¸šçŸ¥è¯†åº“](#11-å®Œæ•´æ¡ˆä¾‹ä¼ä¸šçŸ¥è¯†åº“)
    - [11.1 æ¡ˆä¾‹åœºæ™¯](#111-æ¡ˆä¾‹åœºæ™¯)
    - [11.2 æ¶æ„è®¾è®¡](#112-æ¶æ„è®¾è®¡)
  - [12. æœ€ä½³å®è·µæ€»ç»“](#12-æœ€ä½³å®è·µæ€»ç»“)
    - [12.1 æ¶æ„è®¾è®¡åŸåˆ™](#121-æ¶æ„è®¾è®¡åŸåˆ™)
    - [12.2 æ€§èƒ½ä¼˜åŒ–æ¸…å•](#122-æ€§èƒ½ä¼˜åŒ–æ¸…å•)
    - [12.3 ç”Ÿäº§å°±ç»ªæ£€æŸ¥æ¸…å•](#123-ç”Ÿäº§å°±ç»ªæ£€æŸ¥æ¸…å•)
    - [12.4 å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ](#124-å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ)
    - [12.5 è¿›é˜¶ä¸»é¢˜](#125-è¿›é˜¶ä¸»é¢˜)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)
    - [å®˜æ–¹æ–‡æ¡£](#å®˜æ–¹æ–‡æ¡£)
    - [ç›¸å…³æ–‡ç« ](#ç›¸å…³æ–‡ç« )
    - [å­¦ä¹ è·¯å¾„](#å­¦ä¹ è·¯å¾„)
  - [ğŸ‰ æ€»ç»“](#-æ€»ç»“)

---

## 1. RAGæ¶æ„æ¦‚è¿°

### 1.1 ä»€ä¹ˆæ˜¯RAGï¼Ÿ

**RAG (Retrieval Augmented Generation)** æ˜¯ä¸€ç§ç»“åˆ**ä¿¡æ¯æ£€ç´¢**å’Œ**ç”Ÿæˆå¼AI**çš„æ¶æ„æ¨¡å¼ï¼Œè§£å†³äº†å¤§è¯­è¨€æ¨¡å‹çš„å…³é”®é—®é¢˜ï¼š

- ğŸ“š **çŸ¥è¯†æ—¶æ•ˆæ€§**: LLMè®­ç»ƒæ•°æ®æœ‰æ—¶é—´æˆªæ­¢ç‚¹
- ğŸ¯ **é¢†åŸŸä¸“ä¸šæ€§**: é€šç”¨æ¨¡å‹ç¼ºä¹ä¸“ä¸šé¢†åŸŸçŸ¥è¯†
- ğŸ”’ **æ•°æ®éšç§**: ä¼ä¸šæ•°æ®ä¸èƒ½ç”¨äºæ¨¡å‹è®­ç»ƒ
- ğŸ’° **æˆæœ¬æ•ˆç›Š**: é¿å…æ˜‚è´µçš„æ¨¡å‹å¾®è°ƒ

### 1.2 RAGå·¥ä½œæµç¨‹

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RAG å®Œæ•´æµç¨‹                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. æ–‡æ¡£æ‘„å…¥ï¼ˆIngestionï¼‰
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ PDF/DOC â”‚â”€â”€â”€â”€>â”‚ æ–‡æœ¬æå– â”‚â”€â”€â”€â”€>â”‚ åˆ†å—å¤„ç†  â”‚
   â”‚ HTML/MD â”‚     â”‚ Parsing â”‚     â”‚ Chunking â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  åµŒå…¥ç”Ÿæˆï¼ˆEmbeddingï¼‰                   â”‚
   â”‚  text-embedding-ada-002 / all-MiniLM   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  å‘é‡å­˜å‚¨ï¼ˆVector Storeï¼‰                â”‚
   â”‚  PostgreSQL + pgvector + HNSW Index    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. æŸ¥è¯¢å¤„ç†ï¼ˆQueryï¼‰
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ç”¨æˆ·é—®é¢˜   â”‚â”€â”€â”€â”€>â”‚ åµŒå…¥è½¬æ¢  â”‚â”€â”€â”€â”€>â”‚ å‘é‡æ£€ç´¢  â”‚
   â”‚ "What is"â”‚     â”‚Embedding â”‚     â”‚Top-K     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  ä¸Šä¸‹æ–‡æ„å»ºï¼ˆContext Buildingï¼‰          â”‚
   â”‚  ç›¸å…³æ–‡æ¡£ç‰‡æ®µ + å…ƒæ•°æ® + é‡æ’åº           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  æç¤ºè¯ç”Ÿæˆï¼ˆPrompt Constructionï¼‰       â”‚
   â”‚  System Prompt + Context + User Query  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  LLMç”Ÿæˆå›ç­”ï¼ˆGenerationï¼‰               â”‚
   â”‚  GPT-4 / Claude / Llama 2              â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 ä¸ºä»€ä¹ˆé€‰æ‹©PostgreSQLï¼Ÿ

ç›¸æ¯”ä¸“ç”¨å‘é‡æ•°æ®åº“ï¼ˆPinecone, Weaviate, Qdrantï¼‰ï¼ŒPostgreSQL + pgvectorçš„ä¼˜åŠ¿ï¼š

| ç‰¹æ€§ | PostgreSQL + pgvector | ä¸“ç”¨å‘é‡æ•°æ®åº“ |
|-----|---------------------|--------------|
| **æ··åˆæŸ¥è¯¢** | âœ… åŸç”ŸSQLï¼Œå‘é‡+ç»“æ„åŒ–+å…¨æ–‡ | âš ï¸ é€šå¸¸éœ€è¦å¤šä¸ªç³»ç»Ÿ |
| **äº‹åŠ¡æ”¯æŒ** | âœ… ACIDå®Œæ•´æ€§ | âš ï¸ éƒ¨åˆ†æ”¯æŒ |
| **æˆç†Ÿåº¦** | âœ… 30+å¹´ç”Ÿäº§éªŒè¯ | âš ï¸ è¾ƒæ–°ï¼Œç”Ÿæ€ä¸å®Œå–„ |
| **è¿ç»´æˆæœ¬** | âœ… ç»Ÿä¸€æ•°æ®æ ˆ | âŒ é¢å¤–ç³»ç»Ÿç»´æŠ¤ |
| **æ•°æ®ä¸€è‡´æ€§** | âœ… å•ä¸€æ•°æ®æº | âŒ éœ€è¦åŒæ­¥æœºåˆ¶ |
| **æˆæœ¬** | âœ… å…è´¹å¼€æº | âš ï¸ æŒ‰é‡è®¡è´¹ |
| **æ€§èƒ½** | âš ï¸ ç™¾ä¸‡çº§å‘é‡ | âœ… äº¿çº§å‘é‡ |

**é€‚ç”¨åœºæ™¯**:

- âœ… ä¸­å°è§„æ¨¡ï¼ˆ<1000ä¸‡å‘é‡ï¼‰
- âœ… éœ€è¦å¤æ‚ä¸šåŠ¡é€»è¾‘
- âœ… å·²æœ‰PostgreSQLåŸºç¡€è®¾æ–½
- âœ… é¢„ç®—æœ‰é™çš„åˆåˆ›ä¼ä¸š

---

## 2. ç¯å¢ƒå‡†å¤‡

### 2.1 ç³»ç»Ÿè¦æ±‚

```bash
# æœ€ä½é…ç½®
CPU: 2æ ¸
å†…å­˜: 4GBï¼ˆå»ºè®®8GB+ç”¨äºç´¢å¼•æ„å»ºï¼‰
ç£ç›˜: 20GB SSDï¼ˆHNSWç´¢å¼•éœ€è¦ç£ç›˜I/Oï¼‰
PostgreSQL: 14+ï¼ˆå»ºè®®15+ä»¥è·å¾—æ›´å¥½çš„å‘é‡æ€§èƒ½ï¼‰
Python: 3.9+
```

### 2.2 Dockerå¿«é€Ÿå¯åŠ¨ï¼ˆæ¨èï¼‰

```bash
# 1. å¯åŠ¨PostgreSQL + pgvector
docker run -d \
  --name pgvector-rag \
  -e POSTGRES_PASSWORD=mysecretpassword \
  -e POSTGRES_DB=rag_db \
  -p 5432:5432 \
  -v pgvector-data:/var/lib/postgresql/data \
  pgvector/pgvector:pg16

# 2. éªŒè¯å®‰è£…
docker exec -it pgvector-rag psql -U postgres -d rag_db -c "CREATE EXTENSION IF NOT EXISTS vector;"
docker exec -it pgvector-rag psql -U postgres -d rag_db -c "SELECT * FROM pg_extension WHERE extname = 'vector';"
```

### 2.3 Pythonç¯å¢ƒé…ç½®

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv rag-env
source rag-env/bin/activate  # Windows: rag-env\Scripts\activate

# å®‰è£…ä¾èµ–
pip install --upgrade pip

# æ ¸å¿ƒä¾èµ–
pip install \
  langchain==0.1.0 \
  langchain-community==0.0.10 \
  langchain-openai==0.0.2 \
  llama-index==0.9.30 \
  pgvector==0.2.4 \
  psycopg2-binary==2.9.9 \
  sentence-transformers==2.2.2 \
  openai==1.6.1 \
  tiktoken==0.5.2 \
  python-dotenv==1.0.0

# æ–‡æ¡£å¤„ç†
pip install \
  pypdf==3.17.4 \
  beautifulsoup4==4.12.2 \
  markdown==3.5.1 \
  python-docx==1.1.0

# å¯é€‰ï¼šæ€§èƒ½ç›‘æ§
pip install \
  psutil==5.9.6 \
  prometheus-client==0.19.0
```

### 2.4 é…ç½®ç¯å¢ƒå˜é‡

```bash
# .env æ–‡ä»¶
# OpenAI APIï¼ˆæˆ–ä½¿ç”¨Azure OpenAIï¼‰
OPENAI_API_KEY=sk-your-api-key-here

# PostgreSQLè¿æ¥
DATABASE_URL=postgresql://postgres:mysecretpassword@localhost:5432/rag_db

# å¯é€‰ï¼šAzure OpenAI
# AZURE_OPENAI_API_KEY=your-azure-key
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
# AZURE_OPENAI_API_VERSION=2024-02-01
```

---

## 3. PostgreSQLå‘é‡å­˜å‚¨é…ç½®

### 3.1 æ•°æ®åº“æ¨¡å¼è®¾è®¡

```sql
-- âœ… [å¯è¿è¡Œ] åˆ›å»ºå‘é‡æ‰©å±•
CREATE EXTENSION IF NOT EXISTS vector;

-- åˆ›å»ºæ–‡æ¡£è¡¨
CREATE TABLE documents (
    id BIGSERIAL PRIMARY KEY,
    source_id VARCHAR(255) NOT NULL,          -- æºæ–‡æ¡£å”¯ä¸€æ ‡è¯†
    source_type VARCHAR(50) NOT NULL,         -- pdf, markdown, htmlç­‰
    source_url TEXT,                          -- åŸå§‹URLæˆ–è·¯å¾„
    title TEXT,
    author VARCHAR(255),
    created_at TIMESTAMPTZ DEFAULT NOW(),
    metadata JSONB DEFAULT '{}'::jsonb,       -- çµæ´»å…ƒæ•°æ®
    UNIQUE(source_id, source_type)
);

-- åˆ›å»ºæ–‡æ¡£å—è¡¨ï¼ˆæ ¸å¿ƒï¼‰
CREATE TABLE document_chunks (
    id BIGSERIAL PRIMARY KEY,
    document_id BIGINT REFERENCES documents(id) ON DELETE CASCADE,
    chunk_index INT NOT NULL,                 -- å—åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®
    content TEXT NOT NULL,                    -- åŸå§‹æ–‡æœ¬å†…å®¹
    content_hash VARCHAR(64),                 -- ç”¨äºå»é‡

    -- å‘é‡åµŒå…¥ï¼ˆä½¿ç”¨OpenAI text-embedding-ada-002: 1536ç»´ï¼‰
    embedding vector(1536),

    -- å…¨æ–‡æœç´¢æ”¯æŒ
    content_tsv tsvector GENERATED ALWAYS AS (
        to_tsvector('english', coalesce(content, ''))
    ) STORED,

    -- å…ƒæ•°æ®
    token_count INT,                          -- ç”¨äºæˆæœ¬ä¼°ç®—
    chunk_metadata JSONB DEFAULT '{}'::jsonb, -- é¡µç ã€ç« èŠ‚ç­‰

    created_at TIMESTAMPTZ DEFAULT NOW(),

    CONSTRAINT unique_chunk UNIQUE(document_id, chunk_index)
);

-- æ€§èƒ½ç´¢å¼•
CREATE INDEX idx_chunks_document ON document_chunks(document_id);
CREATE INDEX idx_chunks_hash ON document_chunks(chunk_hash);
CREATE INDEX idx_docs_source ON documents(source_id, source_type);
CREATE INDEX idx_docs_metadata ON documents USING GIN(metadata);
CREATE INDEX idx_chunks_metadata ON document_chunks USING GIN(chunk_metadata);

-- å…¨æ–‡æœç´¢ç´¢å¼•
CREATE INDEX idx_chunks_tsv ON document_chunks USING GIN(content_tsv);

-- ğŸ”¥ å‘é‡ç´¢å¼•ï¼ˆHNSW - æ¨èï¼‰
-- å‚æ•°è¯´æ˜ï¼š
--   m: æ¯å±‚æœ€å¤§è¿æ¥æ•°ï¼Œè¶Šå¤§ç²¾åº¦è¶Šé«˜ä½†æ„å»ºè¶Šæ…¢ï¼Œæ¨è16-64
--   ef_construction: æ„å»ºæ—¶æœç´¢æ·±åº¦ï¼Œæ¨è64-200
CREATE INDEX idx_chunks_embedding ON document_chunks
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- å¦‚æœæ•°æ®é‡è¾ƒå°(<10ä¸‡)ï¼Œå¯ä»¥ä½¿ç”¨IVFFlat
-- CREATE INDEX idx_chunks_embedding ON document_chunks
-- USING ivfflat (embedding vector_cosine_ops)
-- WITH (lists = 100);

-- ä¼šè¯å†å²è¡¨ï¼ˆå¯é€‰ï¼Œç”¨äºå¯¹è¯ä¸Šä¸‹æ–‡ï¼‰
CREATE TABLE chat_sessions (
    id BIGSERIAL PRIMARY KEY,
    user_id VARCHAR(255),
    session_id UUID DEFAULT gen_random_uuid(),
    created_at TIMESTAMPTZ DEFAULT NOW(),
    metadata JSONB DEFAULT '{}'::jsonb
);

CREATE TABLE chat_messages (
    id BIGSERIAL PRIMARY KEY,
    session_id UUID REFERENCES chat_sessions(session_id),
    role VARCHAR(20) NOT NULL,                -- user, assistant, system
    content TEXT NOT NULL,
    retrieved_chunks BIGINT[],                -- ä½¿ç”¨çš„æ–‡æ¡£å—ID
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_messages_session ON chat_messages(session_id, created_at);
```

### 3.2 æ€§èƒ½ä¼˜åŒ–é…ç½®

```sql
-- PostgreSQLé…ç½®ä¼˜åŒ–ï¼ˆpostgresql.conf æˆ– ALTER SYSTEMï¼‰

-- å…±äº«ç¼“å†²åŒºï¼ˆå»ºè®®ç‰©ç†å†…å­˜çš„25%ï¼‰
ALTER SYSTEM SET shared_buffers = '2GB';

-- å·¥ä½œå†…å­˜ï¼ˆç”¨äºæ’åºå’Œå“ˆå¸Œæ“ä½œï¼‰
ALTER SYSTEM SET work_mem = '64MB';

-- ç»´æŠ¤å·¥ä½œå†…å­˜ï¼ˆç”¨äºç´¢å¼•æ„å»ºï¼‰
ALTER SYSTEM SET maintenance_work_mem = '512MB';

-- æœ‰æ•ˆç¼“å­˜å¤§å°ï¼ˆå»ºè®®ç‰©ç†å†…å­˜çš„50-75%ï¼‰
ALTER SYSTEM SET effective_cache_size = '6GB';

-- éšæœºé¡µæˆæœ¬ï¼ˆSSDè®¾ç½®ä¸º1.1ï¼‰
ALTER SYSTEM SET random_page_cost = 1.1;

-- å¹¶è¡Œå·¥ä½œè¿›ç¨‹
ALTER SYSTEM SET max_parallel_workers_per_gather = 4;
ALTER SYSTEM SET max_parallel_workers = 8;

-- é‡æ–°åŠ è½½é…ç½®
SELECT pg_reload_conf();

-- æˆ–é‡å¯PostgreSQL
-- docker restart pgvector-rag
```

### 3.3 æ•°æ®åº“å·¥å…·å‡½æ•°

```sql
-- è®¡ç®—å‘é‡ç›¸ä¼¼åº¦ï¼ˆåŒ…è£…å‡½æ•°ï¼Œä¾¿äºç†è§£ï¼‰
CREATE OR REPLACE FUNCTION cosine_similarity(a vector, b vector)
RETURNS FLOAT AS $$
    SELECT 1 - (a <=> b);
$$ LANGUAGE SQL IMMUTABLE PARALLEL SAFE;

-- æ··åˆæœç´¢å‡½æ•°ï¼ˆå‘é‡ + å…¨æ–‡ï¼‰
CREATE OR REPLACE FUNCTION hybrid_search(
    query_embedding vector,
    query_text TEXT,
    match_threshold FLOAT DEFAULT 0.7,
    match_count INT DEFAULT 10,
    full_text_boost FLOAT DEFAULT 0.3
)
RETURNS TABLE (
    chunk_id BIGINT,
    document_id BIGINT,
    content TEXT,
    similarity_score FLOAT,
    combined_score FLOAT
) AS $$
BEGIN
    RETURN QUERY
    WITH vector_results AS (
        SELECT
            id,
            document_id,
            content,
            1 - (embedding <=> query_embedding) AS vec_score
        FROM document_chunks
        WHERE embedding IS NOT NULL
        ORDER BY embedding <=> query_embedding
        LIMIT match_count * 2
    ),
    fts_results AS (
        SELECT
            id,
            ts_rank(content_tsv, plainto_tsquery('english', query_text)) AS fts_score
        FROM document_chunks
        WHERE content_tsv @@ plainto_tsquery('english', query_text)
    )
    SELECT
        v.id AS chunk_id,
        v.document_id,
        v.content,
        v.vec_score AS similarity_score,
        (v.vec_score * (1 - full_text_boost) + COALESCE(f.fts_score, 0) * full_text_boost) AS combined_score
    FROM vector_results v
    LEFT JOIN fts_results f ON v.id = f.id
    WHERE v.vec_score >= match_threshold
    ORDER BY combined_score DESC
    LIMIT match_count;
END;
$$ LANGUAGE plpgsql STABLE;
```

---

## 4. æ–‡æ¡£å¤„ç†å’Œåˆ†å—ç­–ç•¥

### 4.1 ä¸ºä»€ä¹ˆéœ€è¦æ–‡æ¡£åˆ†å—ï¼Ÿ

**é—®é¢˜**:

- LLMä¸Šä¸‹æ–‡çª—å£æœ‰é™ï¼ˆGPT-3.5: 4K, GPT-4: 8K-128K tokensï¼‰
- å®Œæ•´æ–‡æ¡£å¯èƒ½è¶…è¿‡é™åˆ¶
- æ£€ç´¢ç²’åº¦å¤ªç²—ä¼šå¼•å…¥æ— å…³ä¿¡æ¯

**è§£å†³æ–¹æ¡ˆ**: å°†æ–‡æ¡£åˆ†å‰²æˆè¯­ä¹‰è¿è´¯çš„å°å—

### 4.2 åˆ†å—ç­–ç•¥

```python
# chunking_strategies.py

from typing import List, Dict, Any
import tiktoken
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    MarkdownHeaderTextSplitter,
    TokenTextSplitter
)

class DocumentChunker:
    """æ–‡æ¡£åˆ†å—å¤„ç†å™¨"""

    def __init__(
        self,
        chunk_size: int = 1000,      # å­—ç¬¦æ•°
        chunk_overlap: int = 200,     # é‡å å­—ç¬¦æ•°
        encoding_name: str = "cl100k_base"  # OpenAIç¼–ç 
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.tokenizer = tiktoken.get_encoding(encoding_name)

    def count_tokens(self, text: str) -> int:
        """è®¡ç®—tokenæ•°é‡"""
        return len(self.tokenizer.encode(text))

    def chunk_by_tokens(
        self,
        text: str,
        max_tokens: int = 512
    ) -> List[str]:
        """
        æŒ‰tokenæ•°åˆ†å—ï¼ˆç²¾ç¡®æ§åˆ¶ï¼‰
        âœ… é€‚ç”¨äºä¸¥æ ¼æ§åˆ¶åµŒå…¥æˆæœ¬
        """
        splitter = TokenTextSplitter(
            chunk_size=max_tokens,
            chunk_overlap=50,
            encoding_name="cl100k_base"
        )
        return splitter.split_text(text)

    def chunk_by_characters(
        self,
        text: str,
        separators: List[str] = None
    ) -> List[str]:
        """
        æŒ‰å­—ç¬¦é€’å½’åˆ†å—ï¼ˆä¿æŒè¯­ä¹‰ï¼‰
        âœ… é€‚ç”¨äºå¤§å¤šæ•°åœºæ™¯ï¼Œæ¨è
        """
        if separators is None:
            separators = [
                "\n\n\n",  # å¤šä¸ªç©ºè¡Œï¼ˆç« èŠ‚åˆ†éš”ï¼‰
                "\n\n",    # æ®µè½
                "\n",      # è¡Œ
                ". ",      # å¥å­
                "! ",
                "? ",
                " ",       # å•è¯
                ""         # å­—ç¬¦
            ]

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=separators,
            length_function=len
        )
        return splitter.split_text(text)

    def chunk_markdown(
        self,
        markdown_text: str
    ) -> List[Dict[str, Any]]:
        """
        æŒ‰Markdownæ ‡é¢˜åˆ†å—
        âœ… é€‚ç”¨äºç»“æ„åŒ–æ–‡æ¡£
        """
        headers_to_split_on = [
            ("#", "Header 1"),
            ("##", "Header 2"),
            ("###", "Header 3"),
        ]

        md_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=headers_to_split_on
        )
        md_chunks = md_splitter.split_text(markdown_text)

        # å¦‚æœå—å¤ªå¤§ï¼Œè¿›ä¸€æ­¥åˆ†å‰²
        final_chunks = []
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )

        for chunk in md_chunks:
            content = chunk.page_content
            metadata = chunk.metadata

            if len(content) > self.chunk_size:
                # è¿›ä¸€æ­¥åˆ†å‰²
                sub_chunks = text_splitter.split_text(content)
                for i, sub_chunk in enumerate(sub_chunks):
                    final_chunks.append({
                        "content": sub_chunk,
                        "metadata": {**metadata, "sub_chunk": i}
                    })
            else:
                final_chunks.append({
                    "content": content,
                    "metadata": metadata
                })

        return final_chunks

    def chunk_with_context(
        self,
        text: str,
        document_title: str = ""
    ) -> List[Dict[str, str]]:
        """
        å¸¦ä¸Šä¸‹æ–‡çš„åˆ†å—ï¼ˆåœ¨æ¯ä¸ªå—å‰æ·»åŠ æ–‡æ¡£ä¿¡æ¯ï¼‰
        âœ… æé«˜æ£€ç´¢ç›¸å…³æ€§
        """
        base_chunks = self.chunk_by_characters(text)

        context_prefix = f"Document: {document_title}\n\n" if document_title else ""

        chunks_with_context = []
        for i, chunk in enumerate(base_chunks):
            chunks_with_context.append({
                "content": context_prefix + chunk,
                "original_content": chunk,  # ä¿å­˜ä¸å«å‰ç¼€çš„åŸæ–‡
                "chunk_index": i,
                "total_chunks": len(base_chunks)
            })

        return chunks_with_context


# ä½¿ç”¨ç¤ºä¾‹
def example_chunking():
    """åˆ†å—ç¤ºä¾‹"""
    chunker = DocumentChunker(chunk_size=500, chunk_overlap=100)

    sample_text = """
    # PostgreSQL Vector Database

    PostgreSQL with pgvector extension provides powerful vector similarity search.

    ## Installation

    You can install pgvector using the following command:
    CREATE EXTENSION vector;

    ## Usage

    Create a table with vector column:
    CREATE TABLE items (id bigserial PRIMARY KEY, embedding vector(3));
    """

    # æ–¹æ³•1ï¼šæŒ‰å­—ç¬¦åˆ†å—
    chunks_char = chunker.chunk_by_characters(sample_text)
    print(f"Character-based chunks: {len(chunks_char)}")

    # æ–¹æ³•2ï¼šæŒ‰Markdownæ ‡é¢˜åˆ†å—
    chunks_md = chunker.chunk_markdown(sample_text)
    print(f"Markdown-based chunks: {len(chunks_md)}")
    for chunk in chunks_md:
        print(f"  Headers: {chunk['metadata']}")

    # æ–¹æ³•3ï¼šå¸¦ä¸Šä¸‹æ–‡åˆ†å—
    chunks_ctx = chunker.chunk_with_context(
        sample_text,
        document_title="PostgreSQL Guide"
    )
    print(f"Context-aware chunks: {len(chunks_ctx)}")

if __name__ == "__main__":
    example_chunking()
```

### 4.3 åˆ†å—å‚æ•°é€‰æ‹©æŒ‡å—

| ä½¿ç”¨åœºæ™¯ | chunk_size | chunk_overlap | ç­–ç•¥ |
|---------|-----------|--------------|------|
| çŸ­é—®ç­”ï¼ˆFAQï¼‰ | 200-400 | 20-50 | æŒ‰å¥å­ |
| æŠ€æœ¯æ–‡æ¡£ | 500-1000 | 100-200 | æŒ‰æ ‡é¢˜ |
| é•¿ç¯‡æ–‡ç«  | 1000-1500 | 200-300 | é€’å½’å­—ç¬¦ |
| ä»£ç æ–‡æ¡£ | 1000-2000 | 150-250 | ä¿æŒä»£ç å—å®Œæ•´æ€§ |
| å¯¹è¯æ—¥å¿— | 300-600 | 50-100 | æŒ‰è½®æ¬¡ |

**å…³é”®åŸåˆ™**:

1. **ä¿æŒè¯­ä¹‰å®Œæ•´**: ä¸è¦åœ¨å¥å­ä¸­é—´åˆ‡æ–­
2. **é€‚åº¦é‡å **: é¿å…ä¸Šä¸‹æ–‡ä¸¢å¤±ï¼ˆ15-20%é‡å ï¼‰
3. **è€ƒè™‘tokenæˆæœ¬**: OpenAIæŒ‰tokenè®¡è´¹
4. **æµ‹è¯•è°ƒä¼˜**: ä¸åŒé¢†åŸŸéœ€è¦ä¸åŒå‚æ•°

---

## 5. LangChainé›†æˆå®æˆ˜

### 5.1 LangChainæ¶æ„

```python
# langchain_rag.py

import os
from typing import List, Dict, Any
from dotenv import load_dotenv

from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores.pgvector import PGVector
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.schema import Document

load_dotenv()

class LangChainRAG:
    """åŸºäºLangChainçš„RAGå®ç°"""

    def __init__(
        self,
        connection_string: str = None,
        collection_name: str = "rag_documents",
        embedding_model: str = "text-embedding-ada-002"
    ):
        # æ•°æ®åº“è¿æ¥
        self.connection_string = connection_string or os.getenv("DATABASE_URL")
        self.collection_name = collection_name

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = OpenAIEmbeddings(
            model=embedding_model,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )

        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        self.vectorstore = PGVector(
            collection_name=self.collection_name,
            connection_string=self.connection_string,
            embedding_function=self.embeddings,
        )

        # åˆå§‹åŒ–LLM
        self.llm = ChatOpenAI(
            model="gpt-4",
            temperature=0,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )

    def ingest_documents(
        self,
        documents: List[Document],
        batch_size: int = 100
    ) -> None:
        """
        æ‰¹é‡æ‘„å…¥æ–‡æ¡£

        Args:
            documents: LangChain Documentå¯¹è±¡åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰
        """
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            self.vectorstore.add_documents(batch)
            print(f"Ingested batch {i//batch_size + 1}: {len(batch)} documents")

    def create_qa_chain(
        self,
        chain_type: str = "stuff",
        search_kwargs: Dict[str, Any] = None
    ) -> RetrievalQA:
        """
        åˆ›å»ºé—®ç­”é“¾

        Args:
            chain_type:
                - "stuff": å°†æ‰€æœ‰ä¸Šä¸‹æ–‡å¡å…¥ä¸€ä¸ªæç¤ºï¼ˆé»˜è®¤ï¼‰
                - "map_reduce": åˆ†åˆ«å¤„ç†æ¯ä¸ªæ–‡æ¡£ååˆå¹¶
                - "refine": è¿­ä»£ä¼˜åŒ–ç­”æ¡ˆ
                - "map_rerank": å¯¹æ¯ä¸ªæ–‡æ¡£è¯„åˆ†åé€‰æ‹©æœ€ä½³
            search_kwargs: æ£€ç´¢å‚æ•°ï¼Œå¦‚ {"k": 4}
        """
        if search_kwargs is None:
            search_kwargs = {"k": 4}  # é»˜è®¤æ£€ç´¢top-4

        # è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿
        prompt_template = """Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.

Context:
{context}

Question: {question}

Answer in a helpful, detailed manner:"""

        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )

        # åˆ›å»ºæ£€ç´¢å™¨
        retriever = self.vectorstore.as_retriever(
            search_kwargs=search_kwargs
        )

        # åˆ›å»ºQAé“¾
        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type=chain_type,
            retriever=retriever,
            return_source_documents=True,  # è¿”å›æºæ–‡æ¡£
            chain_type_kwargs={"prompt": PROMPT}
        )

        return qa_chain

    def query(
        self,
        question: str,
        k: int = 4,
        return_sources: bool = True
    ) -> Dict[str, Any]:
        """
        æ‰§è¡ŒæŸ¥è¯¢

        Returns:
            {
                "answer": str,
                "sources": List[Document],  # å¦‚æœreturn_sources=True
                "source_metadata": List[dict]
            }
        """
        qa_chain = self.create_qa_chain(search_kwargs={"k": k})

        result = qa_chain({"query": question})

        response = {
            "answer": result["result"],
        }

        if return_sources and "source_documents" in result:
            response["sources"] = result["source_documents"]
            response["source_metadata"] = [
                doc.metadata for doc in result["source_documents"]
            ]

        return response

    def similarity_search(
        self,
        query: str,
        k: int = 4,
        filter: Dict[str, Any] = None
    ) -> List[Document]:
        """
        å‘é‡ç›¸ä¼¼åº¦æœç´¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›top-kç»“æœ
            filter: å…ƒæ•°æ®è¿‡æ»¤ï¼Œå¦‚ {"source": "manual.pdf"}
        """
        return self.vectorstore.similarity_search(
            query=query,
            k=k,
            filter=filter
        )


# å®Œæ•´ä½¿ç”¨ç¤ºä¾‹
def langchain_example():
    """LangChain RAGå®Œæ•´ç¤ºä¾‹"""

    # 1. åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag = LangChainRAG(
        connection_string="postgresql://postgres:mysecretpassword@localhost:5432/rag_db",
        collection_name="tech_docs"
    )

    # 2. å‡†å¤‡æ–‡æ¡£
    sample_documents = [
        Document(
            page_content="""PostgreSQL is a powerful, open source object-relational
            database system with over 35 years of active development.""",
            metadata={"source": "intro.md", "section": "overview"}
        ),
        Document(
            page_content="""The pgvector extension adds vector similarity search
            capabilities to PostgreSQL, supporting inner product, L2, and cosine distance.""",
            metadata={"source": "pgvector.md", "section": "features"}
        ),
        Document(
            page_content="""HNSW (Hierarchical Navigable Small World) is the recommended
            index type for vector search in production. It provides better recall than IVFFlat.""",
            metadata={"source": "indexing.md", "section": "performance"}
        ),
    ]

    # 3. æ‘„å…¥æ–‡æ¡£
    rag.ingest_documents(sample_documents)

    # 4. æ‰§è¡ŒæŸ¥è¯¢
    result = rag.query(
        question="What index should I use for vector search?",
        k=2
    )

    print("=" * 60)
    print("ANSWER:")
    print(result["answer"])
    print("\n" + "=" * 60)
    print("SOURCES:")
    for i, doc in enumerate(result["sources"], 1):
        print(f"\n[{i}] {doc.metadata}")
        print(doc.page_content[:200] + "...")

    # 5. ç›´æ¥ç›¸ä¼¼åº¦æœç´¢ï¼ˆä¸ç”Ÿæˆç­”æ¡ˆï¼‰
    similar_docs = rag.similarity_search(
        query="vector indexing methods",
        k=2
    )

    print("\n" + "=" * 60)
    print("SIMILAR DOCUMENTS:")
    for doc in similar_docs:
        print(f"- {doc.metadata['source']}: {doc.page_content[:100]}...")


if __name__ == "__main__":
    langchain_example()
```

### 5.2 é«˜çº§åŠŸèƒ½ï¼šå¯¹è¯å†å²ç®¡ç†

```python
# conversation_rag.py

from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

class ConversationalRAG(LangChainRAG):
    """æ”¯æŒå¯¹è¯å†å²çš„RAG"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # åˆå§‹åŒ–å¯¹è¯å†…å­˜
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )

    def create_conversation_chain(self, search_kwargs: Dict[str, Any] = None):
        """åˆ›å»ºå¯¹è¯é“¾"""
        if search_kwargs is None:
            search_kwargs = {"k": 4}

        retriever = self.vectorstore.as_retriever(search_kwargs=search_kwargs)

        chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=retriever,
            memory=self.memory,
            return_source_documents=True,
            verbose=True
        )

        return chain

    def chat(self, question: str) -> Dict[str, Any]:
        """å¯¹è¯å¼æŸ¥è¯¢ï¼ˆä¿ç•™å†å²ï¼‰"""
        chain = self.create_conversation_chain()
        result = chain({"question": question})

        return {
            "answer": result["answer"],
            "sources": result.get("source_documents", [])
        }


# å¯¹è¯ç¤ºä¾‹
def conversation_example():
    """å¯¹è¯å¼RAGç¤ºä¾‹"""
    conv_rag = ConversationalRAG()

    # ç¬¬ä¸€è½®å¯¹è¯
    response1 = conv_rag.chat("What is pgvector?")
    print(f"Q1: What is pgvector?")
    print(f"A1: {response1['answer']}\n")

    # ç¬¬äºŒè½®ï¼ˆç†è§£ä¸Šä¸‹æ–‡ï¼‰
    response2 = conv_rag.chat("How do I install it?")
    print(f"Q2: How do I install it?")  # "it" æŒ‡ä»£ pgvector
    print(f"A2: {response2['answer']}\n")

    # ç¬¬ä¸‰è½®
    response3 = conv_rag.chat("What are the best index parameters?")
    print(f"Q3: What are the best index parameters?")
    print(f"A3: {response3['answer']}\n")
```

---

## 6. LlamaIndexé›†æˆå®æˆ˜

### 6.1 LlamaIndex vs LangChain

| ç‰¹æ€§ | LangChain | LlamaIndex |
|-----|----------|-----------|
| **è®¾è®¡ç†å¿µ** | é€šç”¨LLMåº”ç”¨æ¡†æ¶ | ä¸“æ³¨ç´¢å¼•å’Œæ£€ç´¢ |
| **æ˜“ç”¨æ€§** | çµæ´»ä½†å¤æ‚ | ç®€æ´ç›´è§‚ |
| **ç´¢å¼•ç±»å‹** | åŸºç¡€å‘é‡ | å¤šç§é«˜çº§ç´¢å¼• |
| **æŸ¥è¯¢å¼•æ“** | éœ€è‡ªå®šä¹‰ | å†…ç½®å¤šç§å¼•æ“ |
| **é€‚ç”¨åœºæ™¯** | å¤æ‚å·¥ä½œæµ | RAGå’Œæœç´¢ |

### 6.2 LlamaIndexå®ç°

```python
# llamaindex_rag.py

import os
from typing import List
from dotenv import load_dotenv

from llama_index import (
    VectorStoreIndex,
    ServiceContext,
    StorageContext,
    Document as LlamaDocument
)
from llama_index.embeddings import OpenAIEmbedding
from llama_index.llms import OpenAI
from llama_index.vector_stores import PGVectorStore
from llama_index.node_parser import SimpleNodeParser
import psycopg2

load_dotenv()

class LlamaIndexRAG:
    """åŸºäºLlamaIndexçš„RAGå®ç°"""

    def __init__(
        self,
        host: str = "localhost",
        port: int = 5432,
        database: str = "rag_db",
        user: str = "postgres",
        password: str = None,
        table_name: str = "llama_vectors",
        embed_dim: int = 1536
    ):
        password = password or os.getenv("POSTGRES_PASSWORD", "mysecretpassword")

        # é…ç½®PostgreSQLå‘é‡å­˜å‚¨
        self.vector_store = PGVectorStore.from_params(
            host=host,
            port=port,
            database=database,
            user=user,
            password=password,
            table_name=table_name,
            embed_dim=embed_dim
        )

        # é…ç½®åµŒå…¥æ¨¡å‹
        embed_model = OpenAIEmbedding(
            model="text-embedding-ada-002",
            api_key=os.getenv("OPENAI_API_KEY")
        )

        # é…ç½®LLM
        llm = OpenAI(
            model="gpt-4",
            temperature=0,
            api_key=os.getenv("OPENAI_API_KEY")
        )

        # é…ç½®æœåŠ¡ä¸Šä¸‹æ–‡
        self.service_context = ServiceContext.from_defaults(
            llm=llm,
            embed_model=embed_model,
            chunk_size=512,
            chunk_overlap=50
        )

        # é…ç½®å­˜å‚¨ä¸Šä¸‹æ–‡
        self.storage_context = StorageContext.from_defaults(
            vector_store=self.vector_store
        )

        # ç´¢å¼•ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.index = None

    def build_index(
        self,
        documents: List[LlamaDocument],
        show_progress: bool = True
    ) -> VectorStoreIndex:
        """
        æ„å»ºå‘é‡ç´¢å¼•

        Args:
            documents: LlamaIndex Documentå¯¹è±¡åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        """
        self.index = VectorStoreIndex.from_documents(
            documents,
            service_context=self.service_context,
            storage_context=self.storage_context,
            show_progress=show_progress
        )

        return self.index

    def load_index(self) -> VectorStoreIndex:
        """ä»PostgreSQLåŠ è½½ç°æœ‰ç´¢å¼•"""
        self.index = VectorStoreIndex.from_vector_store(
            vector_store=self.vector_store,
            service_context=self.service_context
        )
        return self.index

    def query(
        self,
        question: str,
        similarity_top_k: int = 3,
        response_mode: str = "compact"
    ) -> str:
        """
        æŸ¥è¯¢

        Args:
            question: ç”¨æˆ·é—®é¢˜
            similarity_top_k: æ£€ç´¢top-kæ–‡æ¡£
            response_mode:
                - "compact": åˆå¹¶æ–‡æœ¬å—åˆ°LLMä¸Šä¸‹æ–‡
                - "refine": è¿­ä»£ä¼˜åŒ–ç­”æ¡ˆ
                - "tree_summarize": å±‚æ¬¡åŒ–æ€»ç»“
        """
        if self.index is None:
            self.load_index()

        query_engine = self.index.as_query_engine(
            similarity_top_k=similarity_top_k,
            response_mode=response_mode
        )

        response = query_engine.query(question)
        return str(response)

    def chat(
        self,
        message: str,
        similarity_top_k: int = 3
    ) -> str:
        """å¯¹è¯å¼æŸ¥è¯¢"""
        if self.index is None:
            self.load_index()

        chat_engine = self.index.as_chat_engine(
            chat_mode="condense_question",  # å‹ç¼©å†å²é—®é¢˜
            similarity_top_k=similarity_top_k
        )

        response = chat_engine.chat(message)
        return str(response)


# å®Œæ•´ç¤ºä¾‹
def llamaindex_example():
    """LlamaIndex RAGå®Œæ•´ç¤ºä¾‹"""

    # 1. åˆå§‹åŒ–
    rag = LlamaIndexRAG(
        database="rag_db",
        table_name="tech_knowledge"
    )

    # 2. å‡†å¤‡æ–‡æ¡£
    documents = [
        LlamaDocument(
            text="""PostgreSQL 16 introduced significant performance improvements
            for parallel query execution and VACUUM operations.""",
            metadata={"source": "release_notes.md", "version": "16"}
        ),
        LlamaDocument(
            text="""The pg_stat_statements extension tracks planning and execution
            statistics for all SQL statements executed by a server.""",
            metadata={"source": "monitoring.md", "type": "extension"}
        ),
    ]

    # 3. æ„å»ºç´¢å¼•
    rag.build_index(documents)

    # 4. æŸ¥è¯¢
    answer = rag.query(
        "How can I monitor SQL performance in PostgreSQL?",
        similarity_top_k=2
    )

    print("ANSWER:", answer)

    # 5. å¯¹è¯
    chat_response = rag.chat("Tell me more about parallel queries")
    print("\nCHAT:", chat_response)


if __name__ == "__main__":
    llamaindex_example()
```

---

## 7. æ£€ç´¢ä¼˜åŒ–å’Œæ··åˆæœç´¢

### 7.1 æ··åˆæ£€ç´¢ç­–ç•¥

```python
# hybrid_search.py

import psycopg2
from typing import List, Dict, Any
import numpy as np
from sentence_transformers import SentenceTransformer

class HybridRetriever:
    """æ··åˆæ£€ç´¢ï¼šå‘é‡ + å…¨æ–‡ + å…ƒæ•°æ®è¿‡æ»¤"""

    def __init__(self, connection_string: str):
        self.conn = psycopg2.connect(connection_string)
        self.embed_model = SentenceTransformer('all-MiniLM-L6-v2')

    def hybrid_search(
        self,
        query: str,
        vector_weight: float = 0.7,
        fulltext_weight: float = 0.3,
        metadata_filters: Dict[str, Any] = None,
        top_k: int = 5,
        min_similarity: float = 0.5
    ) -> List[Dict[str, Any]]:
        """
        æ··åˆæœç´¢å®ç°

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            vector_weight: å‘é‡æœç´¢æƒé‡
            fulltext_weight: å…¨æ–‡æœç´¢æƒé‡
            metadata_filters: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶
            top_k: è¿”å›ç»“æœæ•°
            min_similarity: æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embed_model.encode(query).tolist()

        # 2. æ„å»ºSQL
        sql = """
        WITH vector_scores AS (
            SELECT
                id,
                document_id,
                content,
                chunk_metadata,
                1 - (embedding <=> %s::vector) AS vec_similarity
            FROM document_chunks
            WHERE embedding IS NOT NULL
        ),
        fulltext_scores AS (
            SELECT
                id,
                ts_rank_cd(content_tsv, query) AS fts_rank
            FROM document_chunks, plainto_tsquery('english', %s) query
            WHERE content_tsv @@ query
        ),
        combined_scores AS (
            SELECT
                v.id,
                v.document_id,
                v.content,
                v.chunk_metadata,
                v.vec_similarity,
                COALESCE(f.fts_rank, 0) AS fts_rank,
                (v.vec_similarity * %s + COALESCE(f.fts_rank, 0) * %s) AS combined_score
            FROM vector_scores v
            LEFT JOIN fulltext_scores f ON v.id = f.id
            WHERE v.vec_similarity >= %s
        )
        """

        # 3. æ·»åŠ å…ƒæ•°æ®è¿‡æ»¤
        if metadata_filters:
            conditions = []
            for key, value in metadata_filters.items():
                conditions.append(f"chunk_metadata->>'{key}' = '{value}'")
            sql += " AND " + " AND ".join(conditions)

        sql += """
        SELECT * FROM combined_scores
        ORDER BY combined_score DESC
        LIMIT %s
        """

        # 4. æ‰§è¡ŒæŸ¥è¯¢
        with self.conn.cursor() as cur:
            cur.execute(sql, (
                query_embedding,
                query,
                vector_weight,
                fulltext_weight,
                min_similarity,
                top_k
            ))

            results = []
            for row in cur.fetchall():
                results.append({
                    "id": row[0],
                    "document_id": row[1],
                    "content": row[2],
                    "metadata": row[3],
                    "vector_similarity": float(row[4]),
                    "fulltext_rank": float(row[5]),
                    "combined_score": float(row[6])
                })

            return results

    def rerank_results(
        self,
        query: str,
        results: List[Dict[str, Any]],
        rerank_top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨äº¤å‰ç¼–ç å™¨é‡æ’åºï¼ˆå¯é€‰ï¼Œæé«˜ç²¾åº¦ï¼‰

        éœ€è¦å®‰è£…: pip install sentence-transformers[reranker]
        """
        from sentence_transformers import CrossEncoder

        reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

        # å‡†å¤‡æŸ¥è¯¢-æ–‡æ¡£å¯¹
        pairs = [[query, result["content"]] for result in results]

        # è®¡ç®—äº¤å‰ç¼–ç åˆ†æ•°
        scores = reranker.predict(pairs)

        # æ·»åŠ é‡æ’åºåˆ†æ•°
        for result, score in zip(results, scores):
            result["rerank_score"] = float(score)

        # é‡æ–°æ’åº
        results_reranked = sorted(
            results,
            key=lambda x: x["rerank_score"],
            reverse=True
        )

        return results_reranked[:rerank_top_k]

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        self.conn.close()


# ä½¿ç”¨ç¤ºä¾‹
def hybrid_search_example():
    """æ··åˆæ£€ç´¢ç¤ºä¾‹"""
    retriever = HybridRetriever(
        "postgresql://postgres:mysecretpassword@localhost:5432/rag_db"
    )

    results = retriever.hybrid_search(
        query="How to optimize PostgreSQL for AI workloads?",
        vector_weight=0.7,
        fulltext_weight=0.3,
        metadata_filters={"category": "performance"},
        top_k=5
    )

    print("=" * 60)
    print("HYBRID SEARCH RESULTS:")
    for i, result in enumerate(results, 1):
        print(f"\n[{i}] Score: {result['combined_score']:.4f}")
        print(f"    Vector: {result['vector_similarity']:.4f}, Fulltext: {result['fulltext_rank']:.4f}")
        print(f"    {result['content'][:150]}...")

    # å¯é€‰ï¼šé‡æ’åº
    reranked = retriever.rerank_results("How to optimize PostgreSQL for AI workloads?", results, 3)

    print("\n" + "=" * 60)
    print("RERANKED RESULTS:")
    for i, result in enumerate(reranked, 1):
        print(f"\n[{i}] Rerank Score: {result['rerank_score']:.4f}")
        print(f"    {result['content'][:150]}...")

    retriever.close()


if __name__ == "__main__":
    hybrid_search_example()
```

### 7.2 æŸ¥è¯¢ä¼˜åŒ–æŠ€å·§

```python
# query_optimization.py

class QueryOptimizer:
    """æŸ¥è¯¢ä¼˜åŒ–å™¨"""

    @staticmethod
    def expand_query(query: str) -> List[str]:
        """
        æŸ¥è¯¢æ‰©å±•ï¼ˆä½¿ç”¨åŒä¹‰è¯ã€ç›¸å…³è¯ï¼‰

        å¯é›†æˆï¼š
        - WordNet for synonyms
        - Custom domain dictionary
        - LLM-based expansion
        """
        # ç®€åŒ–ç¤ºä¾‹
        expansions = [query]

        # æ·»åŠ åŒä¹‰è¯å˜ä½“
        if "optimize" in query.lower():
            expansions.append(query.replace("optimize", "improve"))
            expansions.append(query.replace("optimize", "tune"))

        return expansions

    @staticmethod
    def decompose_complex_query(query: str) -> List[str]:
        """
        åˆ†è§£å¤æ‚æŸ¥è¯¢ä¸ºå¤šä¸ªå­æŸ¥è¯¢

        ç¤ºä¾‹: "Compare PostgreSQL and MySQL for AI workloads"
        -> ["PostgreSQL for AI", "MySQL for AI", "PostgreSQL vs MySQL"]
        """
        # å®é™…å®ç°éœ€è¦NLPåˆ†æ
        # è¿™é‡Œæä¾›ç®€åŒ–ç¤ºä¾‹
        sub_queries = [query]

        if " and " in query.lower():
            parts = query.lower().split(" and ")
            sub_queries.extend(parts)

        return sub_queries

    @staticmethod
    def adjust_search_parameters(
        query_type: str,
        doc_count: int
    ) -> Dict[str, Any]:
        """
        æ ¹æ®æŸ¥è¯¢ç±»å‹å’Œæ–‡æ¡£æ•°é‡åŠ¨æ€è°ƒæ•´å‚æ•°

        Args:
            query_type: "factual", "exploratory", "comparison"
            doc_count: ç´¢å¼•ä¸­çš„æ–‡æ¡£æ€»æ•°

        Returns:
            ä¼˜åŒ–åçš„æœç´¢å‚æ•°
        """
        params = {
            "top_k": 5,
            "min_similarity": 0.5,
            "vector_weight": 0.7
        }

        if query_type == "factual":
            # äº‹å®æŸ¥è¯¢ï¼šé«˜ç²¾åº¦ï¼Œå°‘ç»“æœ
            params["top_k"] = 3
            params["min_similarity"] = 0.7
            params["vector_weight"] = 0.8

        elif query_type == "exploratory":
            # æ¢ç´¢æ€§æŸ¥è¯¢ï¼šå¤šæ ·æ€§ï¼Œå¤šç»“æœ
            params["top_k"] = 10
            params["min_similarity"] = 0.4
            params["vector_weight"] = 0.6

        elif query_type == "comparison":
            # å¯¹æ¯”æŸ¥è¯¢ï¼šå¹³è¡¡å‘é‡å’Œå…¨æ–‡
            params["top_k"] = 8
            params["min_similarity"] = 0.5
            params["vector_weight"] = 0.5

        # æ ¹æ®æ–‡æ¡£è§„æ¨¡è°ƒæ•´
        if doc_count > 100000:
            params["min_similarity"] += 0.1  # å¤§è§„æ¨¡ç´¢å¼•æé«˜é˜ˆå€¼

        return params
```

---

---

## 8. ç”Ÿäº§éƒ¨ç½²æ¶æ„

### 8.1 æ¨èæ¶æ„

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ç”Ÿäº§çº§RAGæ¶æ„                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   ç”¨æˆ·è¯·æ±‚   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   API Gateway       â”‚
                    â”‚   (Rate Limiting)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                â”‚                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Query Service   â”‚ â”‚  Ingest  â”‚ â”‚  Admin Service  â”‚
    â”‚  (FastAPI)       â”‚ â”‚  Worker  â”‚ â”‚  (Monitoring)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      PostgreSQL + pgvector         â”‚
    â”‚      (Primary + Read Replicas)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Object Storage   â”‚
    â”‚   (åŸå§‹æ–‡æ¡£)        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    External Services:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ OpenAI API   â”‚  â”‚ Redis Cache  â”‚  â”‚ Prometheus   â”‚
    â”‚ (Embeddings) â”‚  â”‚ (Hot Data)   â”‚  â”‚ (Metrics)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.2 Docker Composeç”Ÿäº§é…ç½®

```yaml
# docker-compose.prod.yml

version: '3.8'

services:
  # PostgreSQLä¸»åº“
  postgres:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_USER: ${DB_USER:-postgres}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: ${DB_NAME:-rag_db}
      POSTGRES_INITDB_ARGS: "-E UTF8 --locale=en_US.UTF-8"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-postgres}"]
      interval: 10s
      timeout: 5s
      retries: 5
    command: >
      postgres
      -c shared_buffers=2GB
      -c effective_cache_size=6GB
      -c maintenance_work_mem=512MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=64MB
      -c huge_pages=off
      -c min_wal_size=1GB
      -c max_wal_size=4GB
      -c max_worker_processes=8
      -c max_parallel_workers_per_gather=4
      -c max_parallel_workers=8
    networks:
      - rag_network

  # Redisç¼“å­˜
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - rag_network

  # APIæœåŠ¡
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    environment:
      DATABASE_URL: postgresql://${DB_USER}:${DB_PASSWORD}@postgres:5432/${DB_NAME}
      REDIS_URL: redis://redis:6379/0
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      LOG_LEVEL: ${LOG_LEVEL:-info}
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '2'
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - rag_network

  # æ–‡æ¡£æ‘„å…¥Worker
  ingest_worker:
    build:
      context: .
      dockerfile: Dockerfile.worker
    environment:
      DATABASE_URL: postgresql://${DB_USER}:${DB_PASSWORD}@postgres:5432/${DB_NAME}
      REDIS_URL: redis://redis:6379/0
      OPENAI_API_KEY: ${OPENAI_API_KEY}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    deploy:
      replicas: 2
    networks:
      - rag_network

  # Prometheusç›‘æ§
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    networks:
      - rag_network

  # Grafanaå¯è§†åŒ–
  grafana:
    image: grafana/grafana:latest
    volumes:
      - grafana_data:/var/lib/grafana
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    networks:
      - rag_network

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:

networks:
  rag_network:
    driver: bridge
```

### 8.3 FastAPIç”Ÿäº§æœåŠ¡

```python
# api/main.py

from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import redis
import hashlib
import json
from prometheus_client import Counter, Histogram, make_asgi_app
import time

# åˆå§‹åŒ–FastAPI
app = FastAPI(
    title="RAG API",
    description="Production RAG Service with PostgreSQL + pgvector",
    version="1.0.0"
)

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Prometheus metrics
query_counter = Counter('rag_queries_total', 'Total RAG queries')
query_duration = Histogram('rag_query_duration_seconds', 'Query duration')
cache_hits = Counter('rag_cache_hits_total', 'Cache hits')
cache_misses = Counter('rag_cache_misses_total', 'Cache misses')

# æŒ‚è½½Prometheus metrics
metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)

# Redisè¿æ¥
redis_client = redis.from_url(os.getenv("REDIS_URL"))

# Pydanticæ¨¡å‹
class QueryRequest(BaseModel):
    question: str
    top_k: int = 4
    min_similarity: float = 0.5
    use_cache: bool = True
    filters: Optional[dict] = None

class QueryResponse(BaseModel):
    answer: str
    sources: List[dict]
    query_time: float
    cached: bool

class IngestRequest(BaseModel):
    source_url: str
    source_type: str
    metadata: Optional[dict] = None

# ä¾èµ–æ³¨å…¥ï¼šRAGç³»ç»Ÿ
def get_rag_system():
    """è·å–RAGç³»ç»Ÿå®ä¾‹ï¼ˆå•ä¾‹ï¼‰"""
    if not hasattr(get_rag_system, "instance"):
        get_rag_system.instance = LangChainRAG(
            connection_string=os.getenv("DATABASE_URL")
        )
    return get_rag_system.instance

# å¥åº·æ£€æŸ¥
@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    try:
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        rag = get_rag_system()
        # ç®€å•æŸ¥è¯¢æµ‹è¯•
        test_docs = rag.similarity_search("test", k=1)

        # æ£€æŸ¥Redis
        redis_client.ping()

        return {
            "status": "healthy",
            "database": "connected",
            "redis": "connected",
            "timestamp": time.time()
        }
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"Health check failed: {str(e)}")

# æŸ¥è¯¢ç«¯ç‚¹
@app.post("/query", response_model=QueryResponse)
@query_duration.time()
async def query(
    request: QueryRequest,
    rag: LangChainRAG = Depends(get_rag_system)
):
    """
    RAGæŸ¥è¯¢ç«¯ç‚¹

    - question: ç”¨æˆ·é—®é¢˜
    - top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
    - min_similarity: æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
    - use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
    - filters: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶
    """
    query_counter.inc()
    start_time = time.time()

    # ç”Ÿæˆç¼“å­˜é”®
    cache_key = None
    if request.use_cache:
        cache_data = {
            "question": request.question,
            "top_k": request.top_k,
            "filters": request.filters
        }
        cache_key = "rag:" + hashlib.md5(
            json.dumps(cache_data, sort_keys=True).encode()
        ).hexdigest()

        # æ£€æŸ¥ç¼“å­˜
        cached_result = redis_client.get(cache_key)
        if cached_result:
            cache_hits.inc()
            result = json.loads(cached_result)
            result["cached"] = True
            result["query_time"] = time.time() - start_time
            return result

    cache_misses.inc()

    try:
        # æ‰§è¡ŒæŸ¥è¯¢
        result = rag.query(
            question=request.question,
            k=request.top_k,
            return_sources=True
        )

        response = {
            "answer": result["answer"],
            "sources": [
                {
                    "content": doc.page_content[:200],
                    "metadata": doc.metadata
                }
                for doc in result.get("sources", [])
            ],
            "query_time": time.time() - start_time,
            "cached": False
        }

        # å­˜å…¥ç¼“å­˜ï¼ˆ5åˆ†é’Ÿè¿‡æœŸï¼‰
        if cache_key:
            redis_client.setex(
                cache_key,
                300,  # 5åˆ†é’Ÿ
                json.dumps(response)
            )

        return response

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Query failed: {str(e)}")

# æ–‡æ¡£æ‘„å…¥ç«¯ç‚¹
@app.post("/ingest")
async def ingest_document(request: IngestRequest):
    """
    æ–‡æ¡£æ‘„å…¥ç«¯ç‚¹ï¼ˆå¼‚æ­¥å¤„ç†ï¼‰

    å°†ä»»åŠ¡åŠ å…¥Redisé˜Ÿåˆ—ï¼Œç”±Workerå¤„ç†
    """
    try:
        task_id = hashlib.md5(
            f"{request.source_url}{time.time()}".encode()
        ).hexdigest()

        task_data = {
            "task_id": task_id,
            "source_url": request.source_url,
            "source_type": request.source_type,
            "metadata": request.metadata or {},
            "timestamp": time.time()
        }

        # æ¨é€åˆ°Redisé˜Ÿåˆ—
        redis_client.lpush("ingest_queue", json.dumps(task_data))

        return {
            "task_id": task_id,
            "status": "queued",
            "message": "Document ingestion task created"
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ingest failed: {str(e)}")

# ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢
@app.get("/task/{task_id}")
async def get_task_status(task_id: str):
    """æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€"""
    status_key = f"task:{task_id}:status"
    status = redis_client.get(status_key)

    if not status:
        raise HTTPException(status_code=404, detail="Task not found")

    return json.loads(status)

# ç»Ÿè®¡ä¿¡æ¯
@app.get("/stats")
async def get_stats():
    """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    # ä»æ•°æ®åº“æŸ¥è¯¢
    import psycopg2
    conn = psycopg2.connect(os.getenv("DATABASE_URL"))

    with conn.cursor() as cur:
        # æ–‡æ¡£æ€»æ•°
        cur.execute("SELECT COUNT(*) FROM documents")
        doc_count = cur.fetchone()[0]

        # å—æ€»æ•°
        cur.execute("SELECT COUNT(*) FROM document_chunks")
        chunk_count = cur.fetchone()[0]

        # å¹³å‡å—å¤§å°
        cur.execute("SELECT AVG(token_count) FROM document_chunks WHERE token_count IS NOT NULL")
        avg_chunk_size = cur.fetchone()[0] or 0

    conn.close()

    return {
        "documents": doc_count,
        "chunks": chunk_count,
        "avg_chunk_size": float(avg_chunk_size),
        "cache_size": redis_client.dbsize()
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        workers=4,
        log_level="info"
    )
```

---

## 9. æ€§èƒ½ä¼˜åŒ–å’Œæˆæœ¬æ§åˆ¶

### 9.1 æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–

```python
# performance_optimization.py

import asyncio
from typing import List
import time

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å·¥å…·é›†"""

    @staticmethod
    async def batch_embed(
        texts: List[str],
        embed_function,
        batch_size: int = 100,
        max_concurrent: int = 5
    ) -> List[List[float]]:
        """
        æ‰¹é‡åµŒå…¥ç”Ÿæˆï¼ˆæé«˜ååé‡ï¼‰

        ä¼˜åŒ–ç‚¹ï¼š
        1. æ‰¹é‡APIè°ƒç”¨å‡å°‘ç½‘ç»œå¼€é”€
        2. å¼‚æ­¥å¹¶å‘æé«˜åå
        3. é™æµé¿å…è¶…è¿‡APIé™åˆ¶
        """
        semaphore = asyncio.Semaphore(max_concurrent)

        async def embed_batch(batch):
            async with semaphore:
                return await embed_function(batch)

        tasks = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            tasks.append(embed_batch(batch))

        results = await asyncio.gather(*tasks)

        # å±•å¹³ç»“æœ
        embeddings = []
        for batch_result in results:
            embeddings.extend(batch_result)

        return embeddings

    @staticmethod
    def optimize_index_build(conn, table_name: str):
        """
        ä¼˜åŒ–ç´¢å¼•æ„å»º

        å¤§æ•°æ®é›†ç´¢å¼•æ„å»ºç­–ç•¥ï¼š
        1. å¢åŠ maintenance_work_mem
        2. å¹¶è¡Œæ„å»º
        3. å®ŒæˆåVACUUM ANALYZE
        """
        with conn.cursor() as cur:
            # ä¸´æ—¶å¢åŠ ç»´æŠ¤å†…å­˜
            cur.execute("SET maintenance_work_mem = '2GB'")
            cur.execute("SET max_parallel_maintenance_workers = 4")

            # æ„å»ºç´¢å¼•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            cur.execute(f"""
                CREATE INDEX IF NOT EXISTS idx_{table_name}_embedding
                ON {table_name}
                USING hnsw (embedding vector_cosine_ops)
                WITH (m = 16, ef_construction = 64)
            """)

            # ä¼˜åŒ–è¡¨
            cur.execute(f"VACUUM ANALYZE {table_name}")

            conn.commit()

    @staticmethod
    def estimate_query_cost(conn, query: str) -> dict:
        """
        ä¼°ç®—æŸ¥è¯¢æˆæœ¬

        ä½¿ç”¨EXPLAIN ANALYZEåˆ†ææŸ¥è¯¢è®¡åˆ’
        """
        with conn.cursor() as cur:
            cur.execute(f"EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) {query}")
            plan = cur.fetchone()[0]

            return {
                "total_cost": plan[0]["Plan"]["Total Cost"],
                "execution_time": plan[0]["Execution Time"],
                "planning_time": plan[0]["Planning Time"],
                "shared_hit_blocks": plan[0]["Plan"].get("Shared Hit Blocks", 0),
                "shared_read_blocks": plan[0]["Plan"].get("Shared Read Blocks", 0)
            }


### 9.2 ç´¢å¼•å‚æ•°è°ƒä¼˜

```sql
-- âœ… [å¯è¿è¡Œ] ç´¢å¼•æ€§èƒ½å¯¹æ¯”æµ‹è¯•

-- 1. åˆ›å»ºæµ‹è¯•æ•°æ®
CREATE TABLE vector_test (
    id SERIAL PRIMARY KEY,
    embedding vector(1536)
);

-- æ’å…¥100Kæµ‹è¯•å‘é‡
INSERT INTO vector_test (embedding)
SELECT array_agg(random())::vector(1536)
FROM generate_series(1, 100000), generate_series(1, 1536);

-- 2. æµ‹è¯•ä¸åŒHNSWå‚æ•°

-- å‚æ•°ç»„åˆ1: m=16, ef_construction=64 (é»˜è®¤)
CREATE INDEX idx_test_hnsw_default ON vector_test
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- å‚æ•°ç»„åˆ2: m=32, ef_construction=128 (é«˜ç²¾åº¦)
CREATE INDEX idx_test_hnsw_accurate ON vector_test
USING hnsw (embedding vector_cosine_ops)
WITH (m = 32, ef_construction = 128);

-- å‚æ•°ç»„åˆ3: m=8, ef_construction=32 (å¿«é€Ÿæ„å»º)
CREATE INDEX idx_test_hnsw_fast ON vector_test
USING hnsw (embedding vector_cosine_ops)
WITH (m = 8, ef_construction = 32);

-- 3. æ€§èƒ½æµ‹è¯•æŸ¥è¯¢
EXPLAIN (ANALYZE, BUFFERS)
SELECT id
FROM vector_test
ORDER BY embedding <=> '[0.1, 0.2, ...]'::vector
LIMIT 10;

-- 4. è°ƒä¼˜è¿è¡Œæ—¶å‚æ•°ï¼ˆæŸ¥è¯¢æ—¶ï¼‰
SET hnsw.ef_search = 100;  -- é»˜è®¤40ï¼Œæé«˜å¬å›ç‡

-- å¯¹æ¯”ä¸åŒef_searchå€¼
SET hnsw.ef_search = 40;
-- æ‰§è¡ŒæŸ¥è¯¢æµ‹è¯•...

SET hnsw.ef_search = 100;
-- æ‰§è¡ŒæŸ¥è¯¢æµ‹è¯•...

SET hnsw.ef_search = 200;
-- æ‰§è¡ŒæŸ¥è¯¢æµ‹è¯•...
```

**å‚æ•°é€‰æ‹©æŒ‡å—**:

| å‚æ•° | å½±å“ | å°æ•°æ®é›†(<10K) | ä¸­æ•°æ®é›†(10K-1M) | å¤§æ•°æ®é›†(>1M) |
|-----|------|--------------|----------------|-------------|
| **m** | æ„å»ºæ—¶é—´/ç²¾åº¦ | 8-16 | 16-32 | 32-64 |
| **ef_construction** | æ„å»ºæ—¶é—´/ç²¾åº¦ | 32-64 | 64-128 | 128-256 |
| **ef_search** | æŸ¥è¯¢æ—¶é—´/å¬å›ç‡ | 20-40 | 40-100 | 100-200 |

### 9.3 æˆæœ¬æ§åˆ¶ç­–ç•¥

```python
# cost_control.py

from typing import List
import tiktoken

class CostController:
    """æˆæœ¬æ§åˆ¶å™¨"""

    # OpenAI APIå®šä»·ï¼ˆ2024å¹´ä»·æ ¼ï¼Œä»…ä¾›å‚è€ƒï¼‰
    PRICING = {
        "text-embedding-ada-002": {
            "per_1k_tokens": 0.0001  # $0.0001/1K tokens
        },
        "gpt-4": {
            "input_per_1k": 0.03,    # $0.03/1K tokens
            "output_per_1k": 0.06    # $0.06/1K tokens
        },
        "gpt-3.5-turbo": {
            "input_per_1k": 0.0015,
            "output_per_1k": 0.002
        }
    }

    def __init__(self):
        self.tokenizer = tiktoken.get_encoding("cl100k_base")

    def count_tokens(self, text: str) -> int:
        """è®¡ç®—tokenæ•°"""
        return len(self.tokenizer.encode(text))

    def estimate_embedding_cost(
        self,
        texts: List[str],
        model: str = "text-embedding-ada-002"
    ) -> dict:
        """ä¼°ç®—åµŒå…¥æˆæœ¬"""
        total_tokens = sum(self.count_tokens(text) for text in texts)

        price_per_1k = self.PRICING[model]["per_1k_tokens"]
        total_cost = (total_tokens / 1000) * price_per_1k

        return {
            "total_tokens": total_tokens,
            "total_cost_usd": round(total_cost, 6),
            "avg_tokens_per_text": total_tokens / len(texts) if texts else 0
        }

    def estimate_query_cost(
        self,
        query: str,
        context_docs: List[str],
        expected_answer_length: int = 200,
        model: str = "gpt-4"
    ) -> dict:
        """ä¼°ç®—å•æ¬¡æŸ¥è¯¢æˆæœ¬"""
        # è¾“å…¥ï¼šquery + context
        input_text = query + "\n\n".join(context_docs)
        input_tokens = self.count_tokens(input_text)

        # è¾“å‡ºï¼šä¼°ç®—
        output_tokens = expected_answer_length  # ç²—ç•¥ä¼°è®¡

        pricing = self.PRICING[model]
        input_cost = (input_tokens / 1000) * pricing["input_per_1k"]
        output_cost = (output_tokens / 1000) * pricing["output_per_1k"]
        total_cost = input_cost + output_cost

        return {
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": input_tokens + output_tokens,
            "input_cost_usd": round(input_cost, 6),
            "output_cost_usd": round(output_cost, 6),
            "total_cost_usd": round(total_cost, 6)
        }

    def optimize_context_window(
        self,
        documents: List[str],
        max_tokens: int = 3000,
        strategy: str = "truncate"
    ) -> List[str]:
        """
        ä¼˜åŒ–ä¸Šä¸‹æ–‡çª—å£ï¼ˆé™ä½æˆæœ¬ï¼‰

        ç­–ç•¥ï¼š
        - truncate: æˆªæ–­è¶…é•¿æ–‡æ¡£
        - summarize: æ€»ç»“é•¿æ–‡æ¡£ï¼ˆéœ€è¦é¢å¤–APIè°ƒç”¨ï¼‰
        - select_best: ä»…é€‰æ‹©æœ€ç›¸å…³çš„å‰Nä¸ª
        """
        if strategy == "truncate":
            result = []
            total_tokens = 0

            for doc in documents:
                doc_tokens = self.count_tokens(doc)
                if total_tokens + doc_tokens > max_tokens:
                    # æˆªæ–­
                    remaining = max_tokens - total_tokens
                    truncated = self.tokenizer.decode(
                        self.tokenizer.encode(doc)[:remaining]
                    )
                    result.append(truncated)
                    break
                else:
                    result.append(doc)
                    total_tokens += doc_tokens

            return result

        # å…¶ä»–ç­–ç•¥å®ç°...
        return documents


# æˆæœ¬ç›‘æ§ç¤ºä¾‹
def cost_monitoring_example():
    """æˆæœ¬ç›‘æ§ç¤ºä¾‹"""
    controller = CostController()

    # ä¼°ç®—åµŒå…¥æˆæœ¬
    documents = [
        "PostgreSQL is a powerful database system." * 100,  # æ¨¡æ‹Ÿé•¿æ–‡æ¡£
        "pgvector enables vector similarity search." * 100,
    ]

    embed_cost = controller.estimate_embedding_cost(documents)
    print("Embedding Cost Estimate:")
    print(f"  Total Tokens: {embed_cost['total_tokens']:,}")
    print(f"  Total Cost: ${embed_cost['total_cost_usd']:.6f}")

    # ä¼°ç®—æŸ¥è¯¢æˆæœ¬
    query = "How does pgvector work?"
    context = documents[:2]

    query_cost = controller.estimate_query_cost(
        query,
        context,
        model="gpt-4"
    )

    print("\nQuery Cost Estimate (GPT-4):")
    print(f"  Input Tokens: {query_cost['input_tokens']:,}")
    print(f"  Output Tokens: {query_cost['output_tokens']:,}")
    print(f"  Total Cost: ${query_cost['total_cost_usd']:.6f}")

    # æœˆåº¦æˆæœ¬ä¼°ç®—ï¼ˆå‡è®¾1000æ¬¡æŸ¥è¯¢/å¤©ï¼‰
    daily_queries = 1000
    monthly_cost = query_cost['total_cost_usd'] * daily_queries * 30
    print(f"\nEstimated Monthly Cost (1K queries/day): ${monthly_cost:.2f}")


if __name__ == "__main__":
    cost_monitoring_example()
```

**é™æœ¬å¢æ•ˆå»ºè®®**:

1. **åµŒå…¥æ¨¡å‹é€‰æ‹©**
   - ğŸ† å¼€æºæ¨¡å‹: `sentence-transformers/all-MiniLM-L6-v2` (å…è´¹)
   - ğŸ’° OpenAI: `text-embedding-ada-002` (é«˜è´¨é‡ï¼Œä»˜è´¹)
   - â˜ï¸ Azure OpenAI: ä¼ä¸šçº§æ”¯æŒ

2. **LLMé€‰æ‹©**
   - ğŸš€ ç”Ÿäº§ç¯å¢ƒ: GPT-4 (é«˜è´¨é‡)
   - ğŸ’° æˆæœ¬ä¼˜åŒ–: GPT-3.5-turbo (80%æˆæœ¬é™ä½)
   - ğŸ†“ ç§æœ‰éƒ¨ç½²: Llama 2 70B (éœ€è¦GPU)

3. **ç¼“å­˜ç­–ç•¥**
   - ç›¸åŒæŸ¥è¯¢ç›´æ¥è¿”å›ç¼“å­˜ç»“æœ
   - è¯­ä¹‰ç›¸ä¼¼æŸ¥è¯¢ä½¿ç”¨èšç±»ç¼“å­˜
   - çƒ­é—¨æŸ¥è¯¢é¢„è®¡ç®—

4. **æ‰¹é‡å¤„ç†**
   - æ‰¹é‡åµŒå…¥ç”Ÿæˆï¼ˆå‡å°‘APIè°ƒç”¨ï¼‰
   - å¼‚æ­¥æ‘„å…¥ï¼ˆå‰Šå³°å¡«è°·ï¼‰

---

## 10. ç›‘æ§å’Œæ•…éšœæ’æŸ¥

### 10.1 å…³é”®æŒ‡æ ‡ç›‘æ§

```python
# monitoring.py

from prometheus_client import Gauge, Counter, Histogram
import psycopg2
import time

# å®šä¹‰PrometheusæŒ‡æ ‡
db_connections = Gauge('pg_connections_active', 'Active PostgreSQL connections')
index_size = Gauge('pg_index_size_bytes', 'Index size in bytes', ['index_name'])
query_latency = Histogram('rag_query_latency_seconds', 'Query latency')
error_count = Counter('rag_errors_total', 'Total errors', ['error_type'])

class RAGMonitor:
    """RAGç³»ç»Ÿç›‘æ§å™¨"""

    def __init__(self, db_url: str):
        self.conn = psycopg2.connect(db_url)

    def collect_metrics(self):
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        with self.conn.cursor() as cur:
            # æ´»è·ƒè¿æ¥æ•°
            cur.execute("""
                SELECT count(*)
                FROM pg_stat_activity
                WHERE state = 'active'
            """)
            db_connections.set(cur.fetchone()[0])

            # ç´¢å¼•å¤§å°
            cur.execute("""
                SELECT indexname, pg_relation_size(indexname::regclass)
                FROM pg_indexes
                WHERE schemaname = 'public'
                AND indexname LIKE '%embedding%'
            """)
            for index_name, size in cur.fetchall():
                index_size.labels(index_name=index_name).set(size)

            # æ…¢æŸ¥è¯¢æ£€æµ‹
            cur.execute("""
                SELECT query, mean_exec_time, calls
                FROM pg_stat_statements
                WHERE mean_exec_time > 1000  -- è¶…è¿‡1ç§’
                ORDER BY mean_exec_time DESC
                LIMIT 10
            """)
            slow_queries = cur.fetchall()

            return {
                "slow_queries": slow_queries,
                "timestamp": time.time()
            }

    def check_index_health(self):
        """æ£€æŸ¥ç´¢å¼•å¥åº·çŠ¶æ€"""
        with self.conn.cursor() as cur:
            cur.execute("""
                SELECT
                    schemaname,
                    tablename,
                    indexname,
                    idx_scan,
                    idx_tup_read,
                    idx_tup_fetch
                FROM pg_stat_user_indexes
                WHERE indexname LIKE '%embedding%'
            """)

            indexes = cur.fetchall()

            # æ£€æŸ¥æœªä½¿ç”¨çš„ç´¢å¼•
            unused_indexes = [
                idx for idx in indexes if idx[3] == 0  # idx_scan == 0
            ]

            if unused_indexes:
                print("WARNING: Unused indexes detected:")
                for idx in unused_indexes:
                    print(f"  - {idx[2]} on {idx[1]}")

            return indexes

    def close(self):
        self.conn.close()


# Grafana Dashboardé…ç½®ç¤ºä¾‹ (prometheus.yml)
"""
# prometheus.yml

global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'rag_api'
    static_configs:
      - targets: ['api:8000']

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
"""
```

### 10.2 å¸¸è§é—®é¢˜æ’æŸ¥

```sql
-- âœ… [å¯è¿è¡Œ] æ•…éšœæ’æŸ¥SQL

-- 1. æ£€æŸ¥æŸ¥è¯¢æ€§èƒ½
SELECT
    query,
    calls,
    total_exec_time,
    mean_exec_time,
    stddev_exec_time
FROM pg_stat_statements
WHERE query LIKE '%document_chunks%'
ORDER BY mean_exec_time DESC
LIMIT 10;

-- 2. æ£€æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…å†µ
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan AS index_scans,
    idx_tup_read AS tuples_read,
    idx_tup_fetch AS tuples_fetched,
    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size
FROM pg_stat_user_indexes
WHERE schemaname = 'public'
ORDER BY idx_scan ASC;

-- 3. æ£€æŸ¥è¡¨è†¨èƒ€
SELECT
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS total_size,
    pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) AS table_size,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) -
                   pg_relation_size(schemaname||'.'||tablename)) AS indexes_size,
    n_dead_tup AS dead_tuples,
    last_vacuum,
    last_autovacuum
FROM pg_stat_user_tables
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- 4. æ£€æŸ¥é•¿æ—¶é—´è¿è¡Œçš„æŸ¥è¯¢
SELECT
    pid,
    now() - query_start AS duration,
    state,
    query
FROM pg_stat_activity
WHERE state != 'idle'
AND query NOT LIKE '%pg_stat_activity%'
ORDER BY duration DESC;

-- 5. ç»ˆæ­¢æ…¢æŸ¥è¯¢ï¼ˆæ…ç”¨ï¼‰
-- SELECT pg_terminate_backend(pid) WHERE pid = <pid>;

-- 6. æ£€æŸ¥è¿æ¥æ•°
SELECT
    COUNT(*) AS total_connections,
    COUNT(*) FILTER (WHERE state = 'active') AS active,
    COUNT(*) FILTER (WHERE state = 'idle') AS idle
FROM pg_stat_activity;

-- 7. æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡ï¼ˆåº” > 99%ï¼‰
SELECT
    sum(heap_blks_read) AS heap_read,
    sum(heap_blks_hit) AS heap_hit,
    sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read)) * 100 AS cache_hit_ratio
FROM pg_statio_user_tables;
```

### 10.3 å‘Šè­¦è§„åˆ™

```yaml
# alerting_rules.yml (Prometheus)

groups:
  - name: rag_alerts
    interval: 30s
    rules:
      # æŸ¥è¯¢å»¶è¿Ÿå‘Šè­¦
      - alert: HighQueryLatency
        expr: histogram_quantile(0.95, rag_query_latency_seconds) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High query latency detected"
          description: "95th percentile query latency is above 5s"

      # é”™è¯¯ç‡å‘Šè­¦
      - alert: HighErrorRate
        expr: rate(rag_errors_total[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is above 5%"

      # æ•°æ®åº“è¿æ¥å‘Šè­¦
      - alert: TooManyConnections
        expr: pg_connections_active > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Too many database connections"
          description: "Active connections exceed 80"

      # ç¼“å­˜å‘½ä¸­ç‡å‘Šè­¦
      - alert: LowCacheHitRate
        expr: (rag_cache_hits_total / (rag_cache_hits_total + rag_cache_misses_total)) < 0.5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Low cache hit rate"
          description: "Cache hit rate is below 50%"
```

---

## 11. å®Œæ•´æ¡ˆä¾‹ï¼šä¼ä¸šçŸ¥è¯†åº“

### 11.1 æ¡ˆä¾‹åœºæ™¯

**å®¢æˆ·éœ€æ±‚**: æŸç§‘æŠ€å…¬å¸éœ€è¦æ„å»ºå†…éƒ¨çŸ¥è¯†åº“RAGç³»ç»Ÿ

- ğŸ“š **æ•°æ®æº**: 2000+ä»½æŠ€æœ¯æ–‡æ¡£ï¼ˆPDFã€Markdownã€Confluenceï¼‰
- ğŸ‘¥ **ç”¨æˆ·æ•°**: 500åå·¥ç¨‹å¸ˆ
- ğŸ” **æŸ¥è¯¢é‡**: 10Kæ¬¡/å¤©
- ğŸ¯ **è¦æ±‚**: ä½å»¶è¿Ÿ(<2s)ã€é«˜å‡†ç¡®ç‡(>90%)ã€æˆæœ¬å¯æ§

### 11.2 æ¶æ„è®¾è®¡

```python
# enterprise_rag.py

import os
from pathlib import Path
from typing import List, Dict, Any
import hashlib
from datetime import datetime

from langchain.document_loaders import (
    PyPDFLoader,
    UnstructuredMarkdownLoader,
    ConfluenceLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import PGVector
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

class EnterpriseKnowledgeBase:
    """ä¼ä¸šçŸ¥è¯†åº“RAGç³»ç»Ÿ"""

    def __init__(
        self,
        db_connection: str,
        confluence_url: str = None,
        confluence_username: str = None,
        confluence_api_key: str = None
    ):
        self.db_connection = db_connection

        # Confluenceé…ç½®
        self.confluence_config = {
            "url": confluence_url,
            "username": confluence_username,
            "api_key": confluence_api_key
        } if confluence_url else None

        # åˆå§‹åŒ–ç»„ä»¶
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = PGVector(
            collection_name="enterprise_docs",
            connection_string=db_connection,
            embedding_function=self.embeddings
        )

        self.llm = ChatOpenAI(model="gpt-4", temperature=0)

        # æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", " ", ""]
        )

    def ingest_directory(
        self,
        directory_path: str,
        file_types: List[str] = None,
        batch_size: int = 50
    ) -> Dict[str, int]:
        """
        æ‘„å…¥æ•´ä¸ªç›®å½•çš„æ–‡æ¡£

        Args:
            directory_path: æ–‡æ¡£ç›®å½•è·¯å¾„
            file_types: æ–‡ä»¶ç±»å‹åˆ—è¡¨ï¼Œå¦‚ ['.pdf', '.md']
            batch_size: æ‰¹é‡å¤„ç†å¤§å°

        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        if file_types is None:
            file_types = ['.pdf', '.md', '.txt']

        directory = Path(directory_path)
        files = []
        for ext in file_types:
            files.extend(directory.rglob(f'*{ext}'))

        print(f"Found {len(files)} files to process")

        total_docs = 0
        total_chunks = 0

        for i, file_path in enumerate(files):
            try:
                print(f"Processing [{i+1}/{len(files)}]: {file_path.name}")

                # åŠ è½½æ–‡æ¡£
                if file_path.suffix == '.pdf':
                    loader = PyPDFLoader(str(file_path))
                elif file_path.suffix == '.md':
                    loader = UnstructuredMarkdownLoader(str(file_path))
                else:
                    # çº¯æ–‡æœ¬
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    loader = [{"page_content": content, "metadata": {"source": str(file_path)}}]

                documents = loader.load() if hasattr(loader, 'load') else loader

                # æ·»åŠ å…ƒæ•°æ®
                for doc in documents:
                    doc.metadata.update({
                        "source": str(file_path),
                        "file_type": file_path.suffix,
                        "file_name": file_path.name,
                        "ingestion_date": datetime.now().isoformat(),
                        "content_hash": hashlib.md5(doc.page_content.encode()).hexdigest()
                    })

                # åˆ†å—
                chunks = self.text_splitter.split_documents(documents)

                # æ‰¹é‡æ·»åŠ åˆ°å‘é‡å­˜å‚¨
                if chunks:
                    self.vectorstore.add_documents(chunks)
                    total_chunks += len(chunks)
                    total_docs += len(documents)

                # å®šæœŸæäº¤
                if (i + 1) % batch_size == 0:
                    print(f"  Committed batch, total chunks so far: {total_chunks}")

            except Exception as e:
                print(f"  ERROR processing {file_path}: {e}")
                continue

        return {
            "total_files": len(files),
            "total_documents": total_docs,
            "total_chunks": total_chunks,
            "avg_chunks_per_doc": total_chunks / total_docs if total_docs > 0 else 0
        }

    def ingest_confluence(
        self,
        space_key: str,
        max_pages: int = 1000
    ) -> Dict[str, int]:
        """
        ä»Confluenceæ‘„å…¥æ–‡æ¡£

        Args:
            space_key: Confluenceç©ºé—´æ ‡è¯†
            max_pages: æœ€å¤§é¡µé¢æ•°
        """
        if not self.confluence_config:
            raise ValueError("Confluence not configured")

        loader = ConfluenceLoader(
            url=self.confluence_config["url"],
            username=self.confluence_config["username"],
            api_key=self.confluence_config["api_key"]
        )

        documents = loader.load(
            space_key=space_key,
            limit=max_pages,
            include_attachments=False
        )

        # æ·»åŠ å…ƒæ•°æ®
        for doc in documents:
            doc.metadata.update({
                "source": "confluence",
                "space_key": space_key,
                "ingestion_date": datetime.now().isoformat()
            })

        # åˆ†å—
        chunks = self.text_splitter.split_documents(documents)

        # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        self.vectorstore.add_documents(chunks)

        return {
            "total_pages": len(documents),
            "total_chunks": len(chunks)
        }

    def create_qa_system(
        self,
        search_type: str = "similarity",
        search_kwargs: Dict[str, Any] = None
    ) -> RetrievalQA:
        """
        åˆ›å»ºé—®ç­”ç³»ç»Ÿ

        Args:
            search_type: "similarity" æˆ– "mmr" (æœ€å¤§è¾¹é™…ç›¸å…³æ€§)
            search_kwargs: æ£€ç´¢å‚æ•°
        """
        if search_kwargs is None:
            search_kwargs = {"k": 4}

        retriever = self.vectorstore.as_retriever(
            search_type=search_type,
            search_kwargs=search_kwargs
        )

        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True
        )

        return qa_chain


# å®Œæ•´éƒ¨ç½²ç¤ºä¾‹
def deploy_enterprise_kb():
    """ä¼ä¸šçŸ¥è¯†åº“éƒ¨ç½²ç¤ºä¾‹"""

    # 1. åˆå§‹åŒ–ç³»ç»Ÿ
    kb = EnterpriseKnowledgeBase(
        db_connection="postgresql://postgres:password@localhost:5432/kb_db",
        confluence_url="https://company.atlassian.net/wiki",
        confluence_username="admin@company.com",
        confluence_api_key=os.getenv("CONFLUENCE_API_KEY")
    )

    # 2. æ‘„å…¥æœ¬åœ°æ–‡æ¡£
    print("Ingesting local documentation...")
    local_stats = kb.ingest_directory(
        directory_path="/path/to/docs",
        file_types=['.pdf', '.md', '.rst'],
        batch_size=50
    )
    print(f"Local ingestion complete: {local_stats}")

    # 3. æ‘„å…¥Confluenceæ–‡æ¡£
    print("\nIngesting Confluence pages...")
    confluence_stats = kb.ingest_confluence(
        space_key="TECH",
        max_pages=500
    )
    print(f"Confluence ingestion complete: {confluence_stats}")

    # 4. åˆ›å»ºé—®ç­”ç³»ç»Ÿ
    qa_system = kb.create_qa_system(
        search_type="mmr",  # ä½¿ç”¨MMRæé«˜å¤šæ ·æ€§
        search_kwargs={"k": 6, "fetch_k": 20, "lambda_mult": 0.5}
    )

    # 5. æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "How do I deploy our microservices to production?",
        "What is our code review process?",
        "Explain the authentication system architecture"
    ]

    for query in test_queries:
        print(f"\n{'='*60}")
        print(f"Q: {query}")
        result = qa_system({"query": query})
        print(f"A: {result['result']}")
        print(f"\nSources:")
        for doc in result['source_documents']:
            print(f"  - {doc.metadata.get('file_name', 'Unknown')}")

    print("\nâœ… Enterprise Knowledge Base deployed successfully!")


if __name__ == "__main__":
    deploy_enterprise_kb()
```

---

## 12. æœ€ä½³å®è·µæ€»ç»“

### 12.1 æ¶æ„è®¾è®¡åŸåˆ™

âœ… **DO (æ¨è)**:

1. **åˆ†ç¦»å…³æ³¨ç‚¹**: æ‘„å…¥ã€æ£€ç´¢ã€ç”Ÿæˆå„è‡ªç‹¬ç«‹æœåŠ¡
2. **å¼‚æ­¥å¤„ç†**: æ–‡æ¡£æ‘„å…¥ä½¿ç”¨é˜Ÿåˆ—å¼‚æ­¥å¤„ç†
3. **ç¼“å­˜ä¼˜å…ˆ**: ç›¸åŒæŸ¥è¯¢ç›´æ¥è¿”å›ç¼“å­˜
4. **ç›‘æ§å®Œå¤‡**: æ‰€æœ‰å…³é”®æŒ‡æ ‡å¯è§‚æµ‹
5. **æ¸è¿›å¼ä¼˜åŒ–**: ä»ç®€å•å¼€å§‹ï¼Œé€æ­¥ä¼˜åŒ–

âŒ **DON'T (é¿å…)**:

1. âŒ åŒæ­¥æ‘„å…¥å¤§é‡æ–‡æ¡£ï¼ˆé˜»å¡ç”¨æˆ·ï¼‰
2. âŒ å¿½ç•¥æˆæœ¬ç›‘æ§ï¼ˆOpenAIè´¹ç”¨å¤±æ§ï¼‰
3. âŒ è¿‡åº¦ä¼˜åŒ–ï¼ˆpremature optimizationï¼‰
4. âŒ ç¼ºå°‘é”™è¯¯å¤„ç†ï¼ˆç”Ÿäº§ç¯å¢ƒå´©æºƒï¼‰
5. âŒ ç¡¬ç¼–ç é…ç½®ï¼ˆéš¾ä»¥è°ƒä¼˜ï¼‰

### 12.2 æ€§èƒ½ä¼˜åŒ–æ¸…å•

- [ ] **æ•°æ®åº“å±‚**
  - [ ] PostgreSQLå‚æ•°è°ƒä¼˜ï¼ˆshared_buffers, work_memï¼‰
  - [ ] HNSWç´¢å¼•å‚æ•°ä¼˜åŒ–ï¼ˆm, ef_constructionï¼‰
  - [ ] å®šæœŸVACUUM ANALYZE
  - [ ] ä½¿ç”¨è¿æ¥æ± ï¼ˆPgBouncerï¼‰

- [ ] **åº”ç”¨å±‚**
  - [ ] æ‰¹é‡åµŒå…¥ç”Ÿæˆ
  - [ ] Redisç¼“å­˜çƒ­é—¨æŸ¥è¯¢
  - [ ] å¼‚æ­¥å¤„ç†æ‘„å…¥ä»»åŠ¡
  - [ ] é™æµå’Œç†”æ–­

- [ ] **æˆæœ¬ä¼˜åŒ–**
  - [ ] ä½¿ç”¨å¼€æºåµŒå…¥æ¨¡å‹ï¼ˆsentence-transformersï¼‰
  - [ ] é€‰æ‹©GPT-3.5æ›¿ä»£GPT-4ï¼ˆéå…³é”®åœºæ™¯ï¼‰
  - [ ] ä¼˜åŒ–ä¸Šä¸‹æ–‡çª—å£å¤§å°
  - [ ] ç›‘æ§APIä½¿ç”¨é‡

### 12.3 ç”Ÿäº§å°±ç»ªæ£€æŸ¥æ¸…å•

- [ ] **åŠŸèƒ½å®Œæ•´æ€§**
  - [ ] æ–‡æ¡£æ‘„å…¥ï¼ˆPDF, Markdown, HTML, Confluenceï¼‰
  - [ ] æ··åˆæ£€ç´¢ï¼ˆå‘é‡ + å…¨æ–‡ + å…ƒæ•°æ®ï¼‰
  - [ ] å¯¹è¯å†å²ç®¡ç†
  - [ ] ç”¨æˆ·æƒé™æ§åˆ¶

- [ ] **å¯é æ€§**
  - [ ] æ•°æ®åº“å¤‡ä»½ç­–ç•¥
  - [ ] æ•…éšœæ¢å¤æµç¨‹
  - [ ] å¥åº·æ£€æŸ¥ç«¯ç‚¹
  - [ ] æ—¥å¿—èšåˆï¼ˆELK/Lokiï¼‰

- [ ] **æ€§èƒ½**
  - [ ] æŸ¥è¯¢å»¶è¿Ÿ < 2s (P95)
  - [ ] æ‘„å…¥åå > 100 docs/min
  - [ ] ç¼“å­˜å‘½ä¸­ç‡ > 50%
  - [ ] æ•°æ®åº“CPU < 70%

- [ ] **å®‰å…¨æ€§**
  - [ ] APIå¯†é’¥åŠ å¯†å­˜å‚¨
  - [ ] HTTPS/TLSåŠ å¯†
  - [ ] è¾“å…¥éªŒè¯å’Œæ¶ˆæ¯’
  - [ ] å®šæœŸå®‰å…¨å®¡è®¡

- [ ] **ç›‘æ§å’Œå‘Šè­¦**
  - [ ] PrometheusæŒ‡æ ‡å¯¼å‡º
  - [ ] Grafanaä»ªè¡¨ç›˜
  - [ ] å‘Šè­¦è§„åˆ™é…ç½®
  - [ ] On-callè½®å€¼

### 12.4 å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ

| é”™è¯¯ | è¡¨ç° | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|-----|------|------|---------|
| **æŸ¥è¯¢æ…¢** | >5så“åº” | ç´¢å¼•ç¼ºå¤±/å‚æ•°ä¸å½“ | æ£€æŸ¥EXPLAINï¼Œä¼˜åŒ–ç´¢å¼•å‚æ•° |
| **å¬å›ç‡ä½** | ç›¸å…³æ–‡æ¡£æœªæ£€ç´¢ | åµŒå…¥è´¨é‡å·®/åˆ†å—ä¸å½“ | ä½¿ç”¨æ›´å¥½çš„åµŒå…¥æ¨¡å‹ï¼Œè°ƒæ•´åˆ†å—ç­–ç•¥ |
| **æˆæœ¬é«˜** | OpenAIè´¦å•çˆ†è¡¨ | è¿‡å¤§ä¸Šä¸‹æ–‡çª—å£ | ä¼˜åŒ–top-kï¼Œä½¿ç”¨ç¼“å­˜ï¼Œè€ƒè™‘å¼€æºæ¨¡å‹ |
| **å†…å­˜æº¢å‡º** | æ‰¹é‡æ‘„å…¥å´©æºƒ | ä¸€æ¬¡æ€§åŠ è½½è¿‡å¤šæ–‡æ¡£ | å‡å°batch_sizeï¼Œä½¿ç”¨æµå¼å¤„ç† |
| **é‡å¤ç»“æœ** | ç›¸åŒå†…å®¹å¤šæ¬¡è¿”å› | å†…å®¹å»é‡ç¼ºå¤± | ä½¿ç”¨content_hashå»é‡ |

### 12.5 è¿›é˜¶ä¸»é¢˜

æƒ³è¦è¿›ä¸€æ­¥æå‡ï¼Ÿæ¢ç´¢è¿™äº›é«˜çº§è¯é¢˜ï¼š

1. **å¤šæ¨¡æ€RAG**: å›¾åƒã€è¡¨æ ¼ã€ä»£ç çš„æ£€ç´¢
2. **ç»†ç²’åº¦æƒé™**: åŸºäºè§’è‰²çš„æ–‡æ¡£è®¿é—®æ§åˆ¶
3. **ä¸»åŠ¨å­¦ä¹ **: ä»ç”¨æˆ·åé¦ˆæ”¹è¿›æ£€ç´¢
4. **A/Bæµ‹è¯•**: å¯¹æ¯”ä¸åŒæ£€ç´¢ç­–ç•¥
5. **çŸ¥è¯†å›¾è°±å¢å¼º**: ç»“åˆå›¾æ•°æ®åº“ï¼ˆNeo4jï¼‰
6. **æµå¼å›ç­”**: Server-Sent Eventså®æ—¶åé¦ˆ
7. **å¤šç§Ÿæˆ·**: ä¼ä¸šçº§SaaSæ¶æ„

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£

- [PostgreSQLå®˜æ–¹æ–‡æ¡£](https://www.postgresql.org/docs/)
- [pgvector GitHub](https://github.com/pgvector/pgvector)
- [LangChainæ–‡æ¡£](https://python.langchain.com/)
- [LlamaIndexæ–‡æ¡£](https://docs.llamaindex.ai/)

### ç›¸å…³æ–‡ç« 

- [Building RAG Applications with PostgreSQL](https://www.enterprisedb.com/blog/rag-postgresql)
- [HNSW Index Tuning Guide](https://github.com/pgvector/pgvector#hnsw)

### å­¦ä¹ è·¯å¾„

1. âœ… [PostgreSQL AIé›†æˆå¿«é€Ÿå¼€å§‹](../00-é¡¹ç›®å¯¼èˆª/AIé›†æˆå¿«é€Ÿå¼€å§‹.md) - åŸºç¡€å…¥é—¨
2. âœ… **æœ¬æ–‡** - ç”Ÿäº§çº§RAGå®æˆ˜
3. â­ï¸ [å‘é‡æ£€ç´¢æ€§èƒ½è°ƒä¼˜](./05.05-å‘é‡æ£€ç´¢æ€§èƒ½è°ƒä¼˜.md) - æ·±åº¦ä¼˜åŒ–
4. â­ï¸ [Azure AIæ‰©å±•å®æˆ˜](./05.03-Azure-AIæ‰©å±•å®æˆ˜.md) - äº‘åŸç”Ÿæ–¹æ¡ˆ

---

## ğŸ‰ æ€»ç»“

æ­å–œï¼æ‚¨å·²ç»æŒæ¡äº†ä½¿ç”¨PostgreSQLæ„å»ºç”Ÿäº§çº§RAGç³»ç»Ÿçš„å®Œæ•´çŸ¥è¯†ï¼š

âœ… **ç†è®ºåŸºç¡€**: RAGæ¶æ„åŸç†å’Œå·¥ä½œæµç¨‹
âœ… **æŠ€æœ¯æ ˆ**: PostgreSQL + pgvector + LangChain/LlamaIndex
âœ… **æœ€ä½³å®è·µ**: åˆ†å—ç­–ç•¥ã€æ··åˆæ£€ç´¢ã€æ€§èƒ½ä¼˜åŒ–
âœ… **ç”Ÿäº§éƒ¨ç½²**: Docker Composeã€ç›‘æ§å‘Šè­¦ã€æˆæœ¬æ§åˆ¶
âœ… **å®æˆ˜æ¡ˆä¾‹**: ä¼ä¸šçŸ¥è¯†åº“å®Œæ•´å®ç°

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**:

1. ğŸš€ éƒ¨ç½²æœ€å°å¯è¡Œäº§å“ï¼ˆMVPï¼‰
2. ğŸ“Š æ”¶é›†ç”¨æˆ·åé¦ˆå’Œæ€§èƒ½æ•°æ®
3. ğŸ”§ è¿­ä»£ä¼˜åŒ–æ£€ç´¢è´¨é‡
4. ğŸ“ˆ æ‰©å±•åˆ°ç”Ÿäº§è§„æ¨¡

æœ‰é—®é¢˜ï¼ŸæŸ¥çœ‹[å¸¸è§é—®é¢˜](../00-é¡¹ç›®å¯¼èˆª/AIé›†æˆå¿«é€Ÿå¼€å§‹.md#å¸¸è§é—®é¢˜)æˆ–å‚è€ƒ[æ”¹è¿›è®¡åˆ’](../00-é¡¹ç›®å¯¼èˆª/AIé›†æˆæ”¹è¿›è¡ŒåŠ¨è®¡åˆ’.md)ã€‚

---

**æ–‡æ¡£ç»´æŠ¤**: PostgreSQL AIé›†æˆå›¢é˜Ÿ
**æœ€åæ›´æ–°**: 2025-10-30
**ç‰ˆæœ¬**: v1.0
**åé¦ˆ**: æ¬¢è¿æäº¤Issueæˆ–PR
