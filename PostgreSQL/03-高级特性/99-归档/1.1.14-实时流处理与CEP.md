# 1.1.14 å®æ—¶æµå¤„ç†ä¸CEP

## ğŸ“‹ æ¦‚è¿°

å®æ—¶æµå¤„ç†ä¸å¤æ‚äº‹ä»¶å¤„ç†(CEP)æ˜¯ç°ä»£æ•°æ®æ¶æ„çš„æ ¸å¿ƒç»„ä»¶ï¼ŒPostgreSQLé€šè¿‡å…¶å¼ºå¤§çš„æµå¤„ç†èƒ½åŠ›ä¸ºå®æ—¶æ•°æ®åˆ†ææä¾›äº†å®Œæ•´çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡æ¡£ç³»ç»Ÿæ€§åœ°é˜è¿°PostgreSQLåœ¨å®æ—¶æµå¤„ç†ã€å¤æ‚äº‹ä»¶å¤„ç†ã€æµè®¡ç®—æ¶æ„ç­‰æ–¹é¢çš„ç†è®ºåŸºç¡€ã€å®ç°æ–¹æ³•å’Œåº”ç”¨å®è·µã€‚

## ğŸ—ï¸ ç†è®ºåŸºç¡€

### 1. æµå¤„ç†åŸºç¡€æ¦‚å¿µ

#### 1.1 æµæ•°æ®æ¨¡å‹

æµæ•°æ®æ˜¯ä¸€ç§è¿ç»­ã€æœ‰åºã€å¿«é€Ÿã€å¤§é‡ã€æ½œåœ¨æ— é™çš„æ•°æ®åºåˆ—ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹å¾ï¼š

```mermaid
graph LR
    A[æµæ•°æ®ç‰¹å¾] --> B[è¿ç»­æ€§]
    A --> C[æœ‰åºæ€§]
    A --> D[å¿«é€Ÿæ€§]
    A --> E[å¤§é‡æ€§]
    A --> F[æ— é™æ€§]

    B --> G[æ—¶é—´åºåˆ—]
    B --> H[äº‹ä»¶åºåˆ—]

    C --> I[æ—¶é—´æˆ³]
    C --> J[åºåˆ—å·]

    D --> K[é«˜åå]
    D --> L[ä½å»¶è¿Ÿ]

    E --> M[å¤§æ•°æ®é‡]
    E --> N[é«˜å¹¶å‘]

    F --> O[æ— ç•Œæ•°æ®]
    F --> P[æŒç»­äº§ç”Ÿ]
```

#### 1.2 æµå¤„ç†æ¨¡å‹

**å®šä¹‰ 1.1 (æµå¤„ç†æ¨¡å‹)**
æµå¤„ç†æ¨¡å‹æ˜¯ä¸€ä¸ªäº”å…ƒç»„ $\mathcal{M} = (S, E, P, O, T)$ï¼Œå…¶ä¸­ï¼š

- $S$ æ˜¯æµæ•°æ®æºé›†åˆ
- $E$ æ˜¯äº‹ä»¶é›†åˆ
- $P$ æ˜¯å¤„ç†å‡½æ•°é›†åˆ
- $O$ æ˜¯è¾“å‡ºé›†åˆ
- $T$ æ˜¯æ—¶é—´åŸŸ

**å®šç† 1.1 (æµå¤„ç†ä¸€è‡´æ€§)**
å¯¹äºæµå¤„ç†æ¨¡å‹ $\mathcal{M}$ï¼Œå¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š

1. äº‹ä»¶é¡ºåºæ€§ï¼š$\forall e_1, e_2 \in E, t(e_1) < t(e_2) \Rightarrow e_1 \prec e_2$
2. å¤„ç†åŸå­æ€§ï¼š$\forall p \in P, p$ æ˜¯åŸå­çš„
3. è¾“å‡ºç¡®å®šæ€§ï¼š$\forall o \in O, o$ æ˜¯ç¡®å®šçš„

åˆ™ $\mathcal{M}$ å…·æœ‰ä¸€è‡´æ€§ä¿è¯ã€‚

#### 1.3 æ—¶é—´çª—å£ç†è®º

**å®šä¹‰ 1.2 (æ—¶é—´çª—å£)**
æ—¶é—´çª—å£æ˜¯ä¸€ä¸ªæ—¶é—´åŒºé—´ $W = [t_{start}, t_{end}]$ï¼Œå…¶ä¸­ï¼š

- $t_{start}$ æ˜¯çª—å£å¼€å§‹æ—¶é—´
- $t_{end}$ æ˜¯çª—å£ç»“æŸæ—¶é—´
- $|W| = t_{end} - t_{start}$ æ˜¯çª—å£å¤§å°

**æ»‘åŠ¨çª—å£ç®—æ³•**:

```rust
// Rustå®ç°çš„æ»‘åŠ¨çª—å£
use std::collections::VecDeque;
use std::time::{Duration, Instant};

#[derive(Debug, Clone)]
pub struct SlidingWindow<T> {
    window_size: Duration,
    events: VecDeque<(T, Instant)>,
}

impl<T> SlidingWindow<T> {
    pub fn new(window_size: Duration) -> Self {
        Self {
            window_size,
            events: VecDeque::new(),
        }
    }

    pub fn add_event(&mut self, event: T) {
        let now = Instant::now();
        self.events.push_back((event, now));
        self.cleanup_old_events(now);
    }

    pub fn get_events(&self) -> Vec<&T> {
        self.events.iter().map(|(event, _)| event).collect()
    }

    fn cleanup_old_events(&mut self, now: Instant) {
        while let Some((_, timestamp)) = self.events.front() {
            if now.duration_since(*timestamp) > self.window_size {
                self.events.pop_front();
            } else {
                break;
            }
        }
    }
}

// ä½¿ç”¨ç¤ºä¾‹
fn main() {
    let mut window = SlidingWindow::new(Duration::from_secs(60));

    // æ·»åŠ äº‹ä»¶
    window.add_event("transaction_1");
    window.add_event("transaction_2");

    // è·å–çª—å£å†…äº‹ä»¶
    let events = window.get_events();
    println!("Events in window: {:?}", events);
}
```

### 2. å¤æ‚äº‹ä»¶å¤„ç†(CEP)ç†è®º

#### 2.1 CEPåŸºç¡€æ¦‚å¿µ

**å®šä¹‰ 2.1 (å¤æ‚äº‹ä»¶)**
å¤æ‚äº‹ä»¶æ˜¯ç”±å¤šä¸ªç®€å•äº‹ä»¶é€šè¿‡æ—¶é—´å…³ç³»å’Œé€»è¾‘å…³ç³»ç»„åˆè€Œæˆçš„äº‹ä»¶æ¨¡å¼ã€‚

**å®šä¹‰ 2.2 (äº‹ä»¶æ¨¡å¼)**
äº‹ä»¶æ¨¡å¼æ˜¯ä¸€ä¸ªä¸‰å…ƒç»„ $P = (E, R, C)$ï¼Œå…¶ä¸­ï¼š

- $E$ æ˜¯äº‹ä»¶é›†åˆ
- $R$ æ˜¯äº‹ä»¶é—´å…³ç³»é›†åˆ
- $C$ æ˜¯çº¦æŸæ¡ä»¶é›†åˆ

#### 2.2 äº‹ä»¶å…³ç³»ç±»å‹

| å…³ç³»ç±»å‹ | æ•°å­¦è¡¨ç¤º | æè¿° | ç¤ºä¾‹ |
|---------|---------|------|------|
| åºåˆ—å…³ç³» | $e_1 \rightarrow e_2$ | äº‹ä»¶æŒ‰é¡ºåºå‘ç”Ÿ | ç™»å½• â†’ æµè§ˆ â†’ è´­ä¹° |
| å¹¶å‘å…³ç³» | $e_1 \parallel e_2$ | äº‹ä»¶åŒæ—¶å‘ç”Ÿ | å¤šè®¾å¤‡åŒæ—¶ç™»å½• |
| é€‰æ‹©å…³ç³» | $e_1 \vee e_2$ | äº‹ä»¶é€‰æ‹©å‘ç”Ÿ | æ”¯ä»˜æ–¹å¼é€‰æ‹© |
| é‡å¤å…³ç³» | $e_1^*$ | äº‹ä»¶é‡å¤å‘ç”Ÿ | å¤šæ¬¡å°è¯•ç™»å½• |

#### 2.3 CEPæ¨¡å¼åŒ¹é…ç®—æ³•

**ç®—æ³• 2.1 (NFAæ¨¡å¼åŒ¹é…)**:

```python
# Pythonå®ç°çš„NFAæ¨¡å¼åŒ¹é…
from typing import Dict, List, Set, Tuple
from dataclasses import dataclass
from enum import Enum

class EventType(Enum):
    LOGIN = "login"
    BROWSE = "browse"
    PURCHASE = "purchase"
    LOGOUT = "logout"

@dataclass
class Event:
    event_type: EventType
    timestamp: float
    user_id: str
    data: Dict

class NFAState:
    def __init__(self, state_id: int):
        self.state_id = state_id
        self.transitions: Dict[EventType, Set[int]] = {}
        self.is_accepting = False

    def add_transition(self, event_type: EventType, next_state: int):
        if event_type not in self.transitions:
            self.transitions[event_type] = set()
        self.transitions[event_type].add(next_state)

class CEPEngine:
    def __init__(self):
        self.states: Dict[int, NFAState] = {}
        self.initial_states: Set[int] = set()
        self.current_states: Set[int] = set()

    def add_state(self, state_id: int, is_accepting: bool = False):
        self.states[state_id] = NFAState(state_id)
        self.states[state_id].is_accepting = is_accepting

    def add_transition(self, from_state: int, event_type: EventType, to_state: int):
        self.states[from_state].add_transition(event_type, to_state)

    def set_initial_state(self, state_id: int):
        self.initial_states.add(state_id)

    def process_event(self, event: Event) -> bool:
        new_states = set()

        # å¤„ç†å½“å‰çŠ¶æ€çš„æ‰€æœ‰è½¬æ¢
        for current_state in self.current_states:
            if event.event_type in self.states[current_state].transitions:
                for next_state in self.states[current_state].transitions[event.event_type]:
                    new_states.add(next_state)

        # å¤„ç†åˆå§‹çŠ¶æ€çš„è½¬æ¢
        for initial_state in self.initial_states:
            if event.event_type in self.states[initial_state].transitions:
                for next_state in self.states[initial_state].transitions[event.event_type]:
                    new_states.add(next_state)

        self.current_states = new_states

        # æ£€æŸ¥æ˜¯å¦æœ‰æ¥å—çŠ¶æ€
        return any(self.states[state].is_accepting for state in self.current_states)

# ä½¿ç”¨ç¤ºä¾‹ï¼šæ£€æµ‹ç™»å½•-æµè§ˆ-è´­ä¹°æ¨¡å¼
def create_login_browse_purchase_pattern():
    engine = CEPEngine()

    # åˆ›å»ºçŠ¶æ€
    engine.add_state(0)  # åˆå§‹çŠ¶æ€
    engine.add_state(1)  # ç™»å½•å
    engine.add_state(2)  # æµè§ˆå
    engine.add_state(3, is_accepting=True)  # è´­ä¹°åï¼ˆæ¥å—çŠ¶æ€ï¼‰

    # æ·»åŠ è½¬æ¢
    engine.add_transition(0, EventType.LOGIN, 1)
    engine.add_transition(1, EventType.BROWSE, 2)
    engine.add_transition(2, EventType.PURCHASE, 3)

    engine.set_initial_state(0)
    return engine

# æµ‹è¯•CEPå¼•æ“
def test_cep_engine():
    engine = create_login_browse_purchase_pattern()

    events = [
        Event(EventType.LOGIN, 1.0, "user1", {}),
        Event(EventType.BROWSE, 2.0, "user1", {}),
        Event(EventType.PURCHASE, 3.0, "user1", {}),
    ]

    for event in events:
        matched = engine.process_event(event)
        print(f"Event: {event.event_type.value}, Pattern matched: {matched}")
```

## ğŸ”§ PostgreSQLæµå¤„ç†å®ç°

### 1. è§¦å‘å™¨åŸºç¡€æ¶æ„

#### 1.1 è§¦å‘å™¨ç±»å‹

PostgreSQLæ”¯æŒå¤šç§è§¦å‘å™¨ç±»å‹ï¼Œç”¨äºå®ç°æµå¤„ç†ï¼š

```sql
-- 1. è¡Œçº§è§¦å‘å™¨
CREATE TRIGGER row_trigger
    AFTER INSERT OR UPDATE OR DELETE ON transactions
    FOR EACH ROW
    EXECUTE FUNCTION process_transaction_event();

-- 2. è¯­å¥çº§è§¦å‘å™¨
CREATE TRIGGER statement_trigger
    AFTER INSERT ON transactions
    FOR EACH STATEMENT
    EXECUTE FUNCTION process_batch_events();

-- 3. æ¡ä»¶è§¦å‘å™¨
CREATE TRIGGER conditional_trigger
    AFTER INSERT ON transactions
    FOR EACH ROW
    WHEN (NEW.amount > 10000)
    EXECUTE FUNCTION process_high_value_transaction();
```

#### 1.2 é«˜çº§è§¦å‘å™¨å®ç°

```sql
-- å¤æ‚äº‹ä»¶å¤„ç†è§¦å‘å™¨
CREATE OR REPLACE FUNCTION cep_transaction_monitor()
RETURNS TRIGGER AS $$
DECLARE
    user_risk_score FLOAT;
    recent_transactions INTEGER;
    total_amount DECIMAL;
    alert_message TEXT;
BEGIN
    -- è®¡ç®—ç”¨æˆ·é£é™©è¯„åˆ†
    SELECT
        COUNT(*) as transaction_count,
        SUM(amount) as total_amount
    INTO recent_transactions, total_amount
    FROM transactions
    WHERE user_id = NEW.user_id
    AND created_at >= NOW() - INTERVAL '1 hour';

    -- é£é™©è¯„åˆ†ç®—æ³•
    user_risk_score :=
        CASE
            WHEN recent_transactions > 10 THEN 0.8
            WHEN total_amount > 50000 THEN 0.7
            WHEN NEW.amount > 10000 THEN 0.6
            ELSE 0.2
        END;

    -- ç”Ÿæˆå‘Šè­¦
    IF user_risk_score > 0.5 THEN
        alert_message := format(
            'High risk transaction detected: User %s, Amount %s, Risk Score %.2f',
            NEW.user_id, NEW.amount, user_risk_score
        );

        INSERT INTO alerts (user_id, transaction_id, risk_score, message, created_at)
        VALUES (NEW.user_id, NEW.id, user_risk_score, alert_message, NOW());

        -- å‘é€é€šçŸ¥
        PERFORM pg_notify('transaction_alerts', alert_message);
    END IF;

    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- åˆ›å»ºè§¦å‘å™¨
CREATE TRIGGER transaction_cep_trigger
    AFTER INSERT ON transactions
    FOR EACH ROW
    EXECUTE FUNCTION cep_transaction_monitor();
```

### 2. é€šçŸ¥ç³»ç»Ÿ

#### 2.1 å¼‚æ­¥é€šçŸ¥æ¶æ„

```sql
-- é€šçŸ¥é€šé“å®šä¹‰
CREATE OR REPLACE FUNCTION setup_notification_channels()
RETURNS VOID AS $$
BEGIN
    -- åˆ›å»ºé€šçŸ¥é€šé“
    PERFORM pg_notify('transaction_events', 'channel_ready');
    PERFORM pg_notify('user_events', 'channel_ready');
    PERFORM pg_notify('system_events', 'channel_ready');
END;
$$ LANGUAGE plpgsql;

-- äº‹ä»¶å‘å¸ƒå‡½æ•°
CREATE OR REPLACE FUNCTION publish_transaction_event(
    event_type TEXT,
    event_data JSONB
)
RETURNS VOID AS $$
BEGIN
    PERFORM pg_notify(
        'transaction_events',
        json_build_object(
            'type', event_type,
            'data', event_data,
            'timestamp', NOW()
        )::TEXT
    );
END;
$$ LANGUAGE plpgsql;
```

#### 2.2 é€šçŸ¥ç›‘å¬å™¨

```python
# Pythoné€šçŸ¥ç›‘å¬å™¨
import psycopg2
import json
import threading
from typing import Callable, Dict, Any

class PostgreSQLNotificationListener:
    def __init__(self, connection_params: Dict[str, Any]):
        self.connection_params = connection_params
        self.connection = None
        self.running = False
        self.handlers: Dict[str, Callable] = {}

    def connect(self):
        self.connection = psycopg2.connect(**self.connection_params)
        self.connection.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)

    def add_handler(self, channel: str, handler: Callable):
        self.handlers[channel] = handler

    def listen(self, channel: str):
        cursor = self.connection.cursor()
        cursor.execute(f"LISTEN {channel}")

    def start_listening(self):
        self.running = True
        self.connect()

        # ç›‘å¬æ‰€æœ‰æ³¨å†Œçš„é€šé“
        for channel in self.handlers.keys():
            self.listen(channel)

        while self.running:
            if self.connection.poll():
                notify = self.connection.notifies.pop()
                channel = notify.channel
                payload = notify.payload

                if channel in self.handlers:
                    try:
                        event_data = json.loads(payload)
                        self.handlers[channel](event_data)
                    except json.JSONDecodeError:
                        print(f"Invalid JSON payload: {payload}")

    def stop(self):
        self.running = False
        if self.connection:
            self.connection.close()

# ä½¿ç”¨ç¤ºä¾‹
def transaction_handler(event_data):
    print(f"Transaction event: {event_data}")

def user_handler(event_data):
    print(f"User event: {event_data}")

# åˆ›å»ºç›‘å¬å™¨
listener = PostgreSQLNotificationListener({
    'host': 'localhost',
    'database': 'streaming_db',
    'user': 'postgres',
    'password': 'password'
})

# æ³¨å†Œå¤„ç†å™¨
listener.add_handler('transaction_events', transaction_handler)
listener.add_handler('user_events', user_handler)

# å¯åŠ¨ç›‘å¬
thread = threading.Thread(target=listener.start_listening)
thread.start()
```

### 3. çª—å£å‡½æ•°ä¸æµå¼SQL

#### 3.1 æ—¶é—´çª—å£èšåˆ

```sql
-- æ»‘åŠ¨çª—å£èšåˆæŸ¥è¯¢
WITH time_windows AS (
    SELECT
        transaction_id,
        amount,
        user_id,
        created_at,
        -- 1åˆ†é’Ÿæ»‘åŠ¨çª—å£
        date_trunc('minute', created_at) as window_start,
        date_trunc('minute', created_at) + INTERVAL '1 minute' as window_end
    FROM transactions
    WHERE created_at >= NOW() - INTERVAL '1 hour'
),
window_aggregates AS (
    SELECT
        window_start,
        window_end,
        COUNT(*) as transaction_count,
        SUM(amount) as total_amount,
        AVG(amount) as avg_amount,
        COUNT(DISTINCT user_id) as unique_users
    FROM time_windows
    GROUP BY window_start, window_end
    ORDER BY window_start
)
SELECT
    window_start,
    window_end,
    transaction_count,
    total_amount,
    avg_amount,
    unique_users,
    -- è®¡ç®—å˜åŒ–ç‡
    LAG(total_amount) OVER (ORDER BY window_start) as prev_total,
    (total_amount - LAG(total_amount) OVER (ORDER BY window_start)) /
    NULLIF(LAG(total_amount) OVER (ORDER BY window_start), 0) * 100 as growth_rate
FROM window_aggregates;
```

#### 3.2 å®æ—¶ç»Ÿè®¡è§†å›¾

```sql
-- åˆ›å»ºå®æ—¶ç»Ÿè®¡è§†å›¾
CREATE OR REPLACE VIEW real_time_stats AS
WITH recent_transactions AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY created_at DESC) as rn
    FROM transactions
    WHERE created_at >= NOW() - INTERVAL '5 minutes'
),
user_stats AS (
    SELECT
        user_id,
        COUNT(*) as transaction_count,
        SUM(amount) as total_amount,
        AVG(amount) as avg_amount,
        MAX(amount) as max_amount,
        MIN(created_at) as first_transaction,
        MAX(created_at) as last_transaction
    FROM recent_transactions
    GROUP BY user_id
),
global_stats AS (
    SELECT
        COUNT(*) as total_transactions,
        SUM(amount) as total_volume,
        AVG(amount) as avg_transaction,
        COUNT(DISTINCT user_id) as active_users
    FROM recent_transactions
)
SELECT
    gs.*,
    us.user_id,
    us.transaction_count as user_transactions,
    us.total_amount as user_total,
    us.avg_amount as user_avg
FROM global_stats gs
CROSS JOIN user_stats us
ORDER BY us.total_amount DESC;
```

## ğŸ­ è¡Œä¸šåº”ç”¨æ¡ˆä¾‹

### 1. é‡‘èè¡Œä¸šï¼šå®æ—¶é£æ§ç³»ç»Ÿ

#### 1.1 ç³»ç»Ÿæ¶æ„

```mermaid
graph TD
    A[äº¤æ˜“æ•°æ®æº] --> B[PostgreSQL]
    B --> C[CEPå¼•æ“]
    C --> D[é£æ§è§„åˆ™å¼•æ“]
    D --> E[é£é™©è¯„åˆ†]
    E --> F[å‘Šè­¦ç³»ç»Ÿ]
    E --> G[å†³ç­–å¼•æ“]

    B --> H[å®æ—¶ç»Ÿè®¡]
    H --> I[ä»ªè¡¨æ¿]

    C --> J[äº‹ä»¶å­˜å‚¨]
    J --> K[å†å²åˆ†æ]
```

#### 1.2 é£æ§è§„åˆ™å®ç°

```sql
-- é£æ§è§„åˆ™è¡¨
CREATE TABLE risk_rules (
    rule_id SERIAL PRIMARY KEY,
    rule_name VARCHAR(100) NOT NULL,
    rule_type VARCHAR(50) NOT NULL, -- 'threshold', 'pattern', 'anomaly'
    rule_condition JSONB NOT NULL,
    risk_score FLOAT NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT NOW()
);

-- æ’å…¥é£æ§è§„åˆ™
INSERT INTO risk_rules (rule_name, rule_type, rule_condition, risk_score) VALUES
('é«˜é¢‘äº¤æ˜“æ£€æµ‹', 'threshold',
 '{"metric": "transaction_count", "operator": ">", "value": 10, "window": "1_hour"}', 0.8),
('å¤§é¢äº¤æ˜“æ£€æµ‹', 'threshold',
 '{"metric": "amount", "operator": ">", "value": 10000}', 0.6),
('å¼‚å¸¸æ—¶é—´äº¤æ˜“', 'pattern',
 '{"pattern": "night_transaction", "time_range": ["22:00", "06:00"]}', 0.4),
('å¼‚åœ°ç™»å½•æ£€æµ‹', 'pattern',
 '{"pattern": "location_change", "time_window": "5_minutes"}', 0.7);

-- é£æ§è¯„ä¼°å‡½æ•°
CREATE OR REPLACE FUNCTION evaluate_risk_rules(
    p_user_id INTEGER,
    p_amount DECIMAL,
    p_location TEXT,
    p_timestamp TIMESTAMP
)
RETURNS TABLE(rule_id INTEGER, risk_score FLOAT, triggered BOOLEAN) AS $$
DECLARE
    rule_record RECORD;
    condition_met BOOLEAN;
    recent_count INTEGER;
    recent_amount DECIMAL;
BEGIN
    FOR rule_record IN
        SELECT * FROM risk_rules WHERE is_active = TRUE
    LOOP
        condition_met := FALSE;

        CASE rule_record.rule_type
            WHEN 'threshold' THEN
                -- é˜ˆå€¼è§„åˆ™è¯„ä¼°
                IF rule_record.rule_condition->>'metric' = 'transaction_count' THEN
                    SELECT COUNT(*) INTO recent_count
                    FROM transactions
                    WHERE user_id = p_user_id
                    AND created_at >= NOW() - INTERVAL '1 hour';

                    condition_met := recent_count > (rule_record.rule_condition->>'value')::INTEGER;
                ELSIF rule_record.rule_condition->>'metric' = 'amount' THEN
                    condition_met := p_amount > (rule_record.rule_condition->>'value')::DECIMAL;
                END IF;

            WHEN 'pattern' THEN
                -- æ¨¡å¼è§„åˆ™è¯„ä¼°
                IF rule_record.rule_condition->>'pattern' = 'night_transaction' THEN
                    condition_met := EXTRACT(HOUR FROM p_timestamp) >= 22 OR EXTRACT(HOUR FROM p_timestamp) <= 6;
                ELSIF rule_record.rule_condition->>'pattern' = 'location_change' THEN
                    -- æ£€æŸ¥5åˆ†é’Ÿå†…æ˜¯å¦æœ‰ä½ç½®å˜åŒ–
                    SELECT COUNT(*) INTO recent_count
                    FROM transactions
                    WHERE user_id = p_user_id
                    AND location != p_location
                    AND created_at >= p_timestamp - INTERVAL '5 minutes';

                    condition_met := recent_count > 0;
                END IF;
        END CASE;

        rule_id := rule_record.rule_id;
        risk_score := rule_record.risk_score;
        triggered := condition_met;

        RETURN NEXT;
    END LOOP;
END;
$$ LANGUAGE plpgsql;
```

#### 1.3 å®æ—¶é£æ§ç›‘æ§

```python
# Pythonå®æ—¶é£æ§ç›‘æ§ç³»ç»Ÿ
import psycopg2
import json
import time
from datetime import datetime
from typing import Dict, List, Any

class RealTimeRiskMonitor:
    def __init__(self, db_config: Dict[str, Any]):
        self.db_config = db_config
        self.connection = psycopg2.connect(**db_config)
        self.risk_thresholds = {
            'high_risk': 0.7,
            'medium_risk': 0.4,
            'low_risk': 0.2
        }

    def evaluate_transaction(self, transaction_data: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°å•ç¬”äº¤æ˜“é£é™©"""
        cursor = self.connection.cursor()

        # è°ƒç”¨PostgreSQLé£æ§å‡½æ•°
        cursor.execute("""
            SELECT rule_id, risk_score, triggered
            FROM evaluate_risk_rules(%s, %s, %s, %s)
        """, (
            transaction_data['user_id'],
            transaction_data['amount'],
            transaction_data['location'],
            transaction_data['timestamp']
        ))

        results = cursor.fetchall()

        # è®¡ç®—ç»¼åˆé£é™©è¯„åˆ†
        total_risk_score = 0.0
        triggered_rules = []

        for rule_id, risk_score, triggered in results:
            if triggered:
                total_risk_score += risk_score
                triggered_rules.append(rule_id)

        # ç¡®å®šé£é™©ç­‰çº§
        risk_level = 'low'
        if total_risk_score >= self.risk_thresholds['high_risk']:
            risk_level = 'high'
        elif total_risk_score >= self.risk_thresholds['medium_risk']:
            risk_level = 'medium'

        return {
            'transaction_id': transaction_data['id'],
            'user_id': transaction_data['user_id'],
            'total_risk_score': total_risk_score,
            'risk_level': risk_level,
            'triggered_rules': triggered_rules,
            'evaluation_time': datetime.now().isoformat()
        }

    def monitor_transactions(self):
        """æŒç»­ç›‘æ§äº¤æ˜“"""
        cursor = self.connection.cursor()

        # ç›‘å¬æ–°äº¤æ˜“
        cursor.execute("LISTEN transaction_events")

        while True:
            if self.connection.poll():
                notify = self.connection.notifies.pop()
                event_data = json.loads(notify.payload)

                if event_data['type'] == 'transaction_created':
                    risk_assessment = self.evaluate_transaction(event_data['data'])

                    # å¤„ç†é«˜é£é™©äº¤æ˜“
                    if risk_assessment['risk_level'] == 'high':
                        self.handle_high_risk_transaction(risk_assessment)

                    # è®°å½•é£é™©è¯„ä¼°ç»“æœ
                    self.log_risk_assessment(risk_assessment)

    def handle_high_risk_transaction(self, risk_assessment: Dict[str, Any]):
        """å¤„ç†é«˜é£é™©äº¤æ˜“"""
        print(f"ğŸš¨ High risk transaction detected: {risk_assessment}")

        # å‘é€å‘Šè­¦
        self.send_alert(risk_assessment)

        # å¯èƒ½çš„è‡ªåŠ¨å“åº”
        if risk_assessment['total_risk_score'] > 0.9:
            self.block_transaction(risk_assessment['transaction_id'])

    def send_alert(self, risk_assessment: Dict[str, Any]):
        """å‘é€å‘Šè­¦"""
        alert_message = {
            'type': 'risk_alert',
            'level': risk_assessment['risk_level'],
            'transaction_id': risk_assessment['transaction_id'],
            'user_id': risk_assessment['user_id'],
            'risk_score': risk_assessment['total_risk_score'],
            'timestamp': datetime.now().isoformat()
        }

        # è¿™é‡Œå¯ä»¥é›†æˆå„ç§å‘Šè­¦ç³»ç»Ÿ
        print(f"Alert: {alert_message}")

    def log_risk_assessment(self, risk_assessment: Dict[str, Any]):
        """è®°å½•é£é™©è¯„ä¼°ç»“æœ"""
        cursor = self.connection.cursor()
        cursor.execute("""
            INSERT INTO risk_assessments
            (transaction_id, user_id, risk_score, risk_level, triggered_rules, created_at)
            VALUES (%s, %s, %s, %s, %s, %s)
        """, (
            risk_assessment['transaction_id'],
            risk_assessment['user_id'],
            risk_assessment['total_risk_score'],
            risk_assessment['risk_level'],
            json.dumps(risk_assessment['triggered_rules']),
            datetime.now()
        ))
        self.connection.commit()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    db_config = {
        'host': 'localhost',
        'database': 'financial_db',
        'user': 'postgres',
        'password': 'password'
    }

    monitor = RealTimeRiskMonitor(db_config)

    # å¯åŠ¨ç›‘æ§
    try:
        monitor.monitor_transactions()
    except KeyboardInterrupt:
        print("Monitoring stopped")
```

### 2. äº’è”ç½‘è¡Œä¸šï¼šå®æ—¶æ¨èç³»ç»Ÿ

#### 2.1 æ¨èç³»ç»Ÿæ¶æ„

```mermaid
graph TD
    A[ç”¨æˆ·è¡Œä¸ºæ•°æ®] --> B[PostgreSQLæµå¤„ç†]
    B --> C[å®æ—¶ç‰¹å¾æå–]
    C --> D[æ¨èç®—æ³•å¼•æ“]
    D --> E[æ¨èç»“æœ]
    E --> F[ç”¨æˆ·ç•Œé¢]

    B --> G[ç”¨æˆ·ç”»åƒæ›´æ–°]
    G --> H[ä¸ªæ€§åŒ–æ¨è]

    C --> I[ç‰©å“ç‰¹å¾æ›´æ–°]
    I --> J[ååŒè¿‡æ»¤]
```

#### 2.2 å®æ—¶æ¨èç®—æ³•

```python
# Pythonå®æ—¶æ¨èç³»ç»Ÿ
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Dict, Any
import psycopg2
import json

class RealTimeRecommendationSystem:
    def __init__(self, db_config: Dict[str, Any]):
        self.db_config = db_config
        self.connection = psycopg2.connect(**db_config)
        self.user_profiles = {}
        self.item_profiles = {}
        self.similarity_matrix = None

    def update_user_profile(self, user_id: int, event_data: Dict[str, Any]):
        """æ›´æ–°ç”¨æˆ·ç”»åƒ"""
        event_type = event_data['type']
        item_id = event_data['item_id']
        timestamp = event_data['timestamp']

        if user_id not in self.user_profiles:
            self.user_profiles[user_id] = {
                'interactions': {},
                'preferences': {},
                'last_activity': timestamp
            }

        # æ›´æ–°äº¤äº’å†å²
        if item_id not in self.user_profiles[user_id]['interactions']:
            self.user_profiles[user_id]['interactions'][item_id] = []

        self.user_profiles[user_id]['interactions'][item_id].append({
            'type': event_type,
            'timestamp': timestamp
        })

        # æ›´æ–°åå¥½æƒé‡
        weight_map = {
            'view': 1,
            'like': 3,
            'share': 5,
            'purchase': 10
        }

        weight = weight_map.get(event_type, 1)
        if item_id not in self.user_profiles[user_id]['preferences']:
            self.user_profiles[user_id]['preferences'][item_id] = 0

        self.user_profiles[user_id]['preferences'][item_id] += weight
        self.user_profiles[user_id]['last_activity'] = timestamp

    def calculate_item_similarity(self):
        """è®¡ç®—ç‰©å“ç›¸ä¼¼åº¦çŸ©é˜µ"""
        # è·å–æ‰€æœ‰ç”¨æˆ·-ç‰©å“äº¤äº’æ•°æ®
        cursor = self.connection.cursor()
        cursor.execute("""
            SELECT user_id, item_id,
                   SUM(CASE WHEN event_type = 'purchase' THEN 10
                           WHEN event_type = 'like' THEN 3
                           WHEN event_type = 'view' THEN 1
                           ELSE 0 END) as weight
            FROM user_events
            WHERE created_at >= NOW() - INTERVAL '7 days'
            GROUP BY user_id, item_id
        """)

        interactions = cursor.fetchall()

        # æ„å»ºç”¨æˆ·-ç‰©å“çŸ©é˜µ
        df = pd.DataFrame(interactions, columns=['user_id', 'item_id', 'weight'])
        user_item_matrix = df.pivot_table(
            index='user_id',
            columns='item_id',
            values='weight',
            fill_value=0
        )

        # è®¡ç®—ç‰©å“ç›¸ä¼¼åº¦
        self.similarity_matrix = cosine_similarity(user_item_matrix.T)
        self.item_ids = user_item_matrix.columns.tolist()

    def get_recommendations(self, user_id: int, n_recommendations: int = 10) -> List[Dict[str, Any]]:
        """è·å–æ¨èç»“æœ"""
        if user_id not in self.user_profiles:
            return []

        user_preferences = self.user_profiles[user_id]['preferences']

        if not user_preferences:
            return []

        # è®¡ç®—æ¨èåˆ†æ•°
        recommendation_scores = {}

        for item_id in self.item_ids:
            if item_id in user_preferences:
                continue  # è·³è¿‡ç”¨æˆ·å·²ç»äº¤äº’è¿‡çš„ç‰©å“

            score = 0
            for interacted_item, weight in user_preferences.items():
                if interacted_item in self.item_ids:
                    item_idx = self.item_ids.index(interacted_item)
                    current_idx = self.item_ids.index(item_id)
                    similarity = self.similarity_matrix[item_idx][current_idx]
                    score += weight * similarity

            recommendation_scores[item_id] = score

        # æ’åºå¹¶è¿”å›æ¨èç»“æœ
        sorted_recommendations = sorted(
            recommendation_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )

        return [
            {
                'item_id': item_id,
                'score': score,
                'reason': 'Based on your preferences'
            }
            for item_id, score in sorted_recommendations[:n_recommendations]
        ]

    def process_user_event(self, event_data: Dict[str, Any]):
        """å¤„ç†ç”¨æˆ·äº‹ä»¶"""
        user_id = event_data['user_id']

        # æ›´æ–°ç”¨æˆ·ç”»åƒ
        self.update_user_profile(user_id, event_data)

        # å®šæœŸé‡æ–°è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        if len(self.user_profiles) % 100 == 0:  # æ¯100ä¸ªäº‹ä»¶é‡æ–°è®¡ç®—
            self.calculate_item_similarity()

        # ç”Ÿæˆå®æ—¶æ¨è
        recommendations = self.get_recommendations(user_id)

        # å­˜å‚¨æ¨èç»“æœ
        self.store_recommendations(user_id, recommendations)

        return recommendations

    def store_recommendations(self, user_id: int, recommendations: List[Dict[str, Any]]):
        """å­˜å‚¨æ¨èç»“æœ"""
        cursor = self.connection.cursor()

        # æ¸…é™¤æ—§æ¨è
        cursor.execute("DELETE FROM user_recommendations WHERE user_id = %s", (user_id,))

        # æ’å…¥æ–°æ¨è
        for i, rec in enumerate(recommendations):
            cursor.execute("""
                INSERT INTO user_recommendations
                (user_id, item_id, score, rank, created_at)
                VALUES (%s, %s, %s, %s, %s)
            """, (user_id, rec['item_id'], rec['score'], i+1, datetime.now()))

        self.connection.commit()

# ä½¿ç”¨ç¤ºä¾‹
def test_recommendation_system():
    db_config = {
        'host': 'localhost',
        'database': 'recommendation_db',
        'user': 'postgres',
        'password': 'password'
    }

    rec_system = RealTimeRecommendationSystem(db_config)

    # æ¨¡æ‹Ÿç”¨æˆ·äº‹ä»¶
    events = [
        {'user_id': 1, 'item_id': 101, 'type': 'view', 'timestamp': datetime.now()},
        {'user_id': 1, 'item_id': 102, 'type': 'like', 'timestamp': datetime.now()},
        {'user_id': 1, 'item_id': 103, 'type': 'purchase', 'timestamp': datetime.now()},
    ]

    for event in events:
        recommendations = rec_system.process_user_event(event)
        print(f"User {event['user_id']} recommendations: {recommendations}")
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–ä¸ç›‘æ§

### 1. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

#### 1.1 ç´¢å¼•ä¼˜åŒ–

```sql
-- æµå¤„ç†ç›¸å…³ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_transactions_user_time
ON transactions(user_id, created_at);

CREATE INDEX CONCURRENTLY idx_transactions_amount_time
ON transactions(amount, created_at)
WHERE amount > 1000;

CREATE INDEX CONCURRENTLY idx_user_events_user_type_time
ON user_events(user_id, event_type, created_at);

-- éƒ¨åˆ†ç´¢å¼•ä¼˜åŒ–
CREATE INDEX CONCURRENTLY idx_recent_transactions
ON transactions(created_at)
WHERE created_at >= NOW() - INTERVAL '24 hours';

-- å¤åˆç´¢å¼•ä¼˜åŒ–
CREATE INDEX CONCURRENTLY idx_transactions_composite
ON transactions(user_id, amount, created_at, status);
```

#### 1.2 åˆ†åŒºç­–ç•¥

```sql
-- æŒ‰æ—¶é—´åˆ†åŒº
CREATE TABLE transactions_partitioned (
    id SERIAL,
    user_id INTEGER,
    amount DECIMAL,
    created_at TIMESTAMP,
    status VARCHAR(20)
) PARTITION BY RANGE (created_at);

-- åˆ›å»ºåˆ†åŒº
CREATE TABLE transactions_2024_01 PARTITION OF transactions_partitioned
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE transactions_2024_02 PARTITION OF transactions_partitioned
FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');

-- è‡ªåŠ¨åˆ†åŒºç®¡ç†
CREATE OR REPLACE FUNCTION create_monthly_partition(table_name TEXT, month_date DATE)
RETURNS VOID AS $$
DECLARE
    partition_name TEXT;
    start_date DATE;
    end_date DATE;
BEGIN
    partition_name := table_name || '_' || to_char(month_date, 'YYYY_MM');
    start_date := date_trunc('month', month_date);
    end_date := start_date + INTERVAL '1 month';

    EXECUTE format(
        'CREATE TABLE IF NOT EXISTS %I PARTITION OF %I FOR VALUES FROM (%L) TO (%L)',
        partition_name, table_name, start_date, end_date
    );
END;
$$ LANGUAGE plpgsql;
```

### 2. ç›‘æ§ä¸è¯Šæ–­

#### 2.1 æ€§èƒ½ç›‘æ§è§†å›¾

```sql
-- æµå¤„ç†æ€§èƒ½ç›‘æ§è§†å›¾
CREATE OR REPLACE VIEW streaming_performance AS
WITH current_stats AS (
    SELECT
        COUNT(*) as total_transactions,
        COUNT(*) FILTER (WHERE created_at >= NOW() - INTERVAL '1 minute') as transactions_last_minute,
        AVG(EXTRACT(EPOCH FROM (NOW() - created_at))) as avg_processing_delay,
        COUNT(DISTINCT user_id) as active_users
    FROM transactions
    WHERE created_at >= NOW() - INTERVAL '1 hour'
),
trigger_stats AS (
    SELECT
        trigger_name,
        COUNT(*) as execution_count,
        AVG(execution_time_ms) as avg_execution_time,
        MAX(execution_time_ms) as max_execution_time
    FROM trigger_execution_log
    WHERE executed_at >= NOW() - INTERVAL '1 hour'
    GROUP BY trigger_name
),
notification_stats AS (
    SELECT
        channel,
        COUNT(*) as notification_count,
        AVG(EXTRACT(EPOCH FROM (processed_at - sent_at))) as avg_processing_time
    FROM notification_log
    WHERE sent_at >= NOW() - INTERVAL '1 hour'
    GROUP BY channel
)
SELECT
    cs.*,
    ts.trigger_name,
    ts.execution_count,
    ts.avg_execution_time,
    ns.channel,
    ns.notification_count,
    ns.avg_processing_time
FROM current_stats cs
CROSS JOIN trigger_stats ts
CROSS JOIN notification_stats ns;
```

#### 2.2 å®æ—¶ç›‘æ§ä»ªè¡¨æ¿

```python
# Pythonå®æ—¶ç›‘æ§ä»ªè¡¨æ¿
import dash
from dash import dcc, html
from dash.dependencies import Input, Output
import plotly.graph_objs as go
import pandas as pd
import psycopg2
from datetime import datetime, timedelta

class StreamingDashboard:
    def __init__(self, db_config):
        self.db_config = db_config
        self.app = dash.Dash(__name__)
        self.setup_layout()
        self.setup_callbacks()

    def setup_layout(self):
        self.app.layout = html.Div([
            html.H1("å®æ—¶æµå¤„ç†ç›‘æ§ä»ªè¡¨æ¿"),

            # å®æ—¶æŒ‡æ ‡å¡ç‰‡
            html.Div([
                html.Div([
                    html.H3(id='total-transactions'),
                    html.P("æ€»äº¤æ˜“æ•°")
                ], className='metric-card'),
                html.Div([
                    html.H3(id='tps'),
                    html.P("TPS")
                ], className='metric-card'),
                html.Div([
                    html.H3(id='avg-latency'),
                    html.P("å¹³å‡å»¶è¿Ÿ(ms)")
                ], className='metric-card'),
                html.Div([
                    html.H3(id='active-users'),
                    html.P("æ´»è·ƒç”¨æˆ·")
                ], className='metric-card'),
            ], className='metrics-row'),

            # å›¾è¡¨åŒºåŸŸ
            html.Div([
                dcc.Graph(id='transaction-timeline'),
                dcc.Graph(id='latency-distribution'),
                dcc.Graph(id='user-activity'),
            ], className='charts-container'),

            # è‡ªåŠ¨åˆ·æ–°
            dcc.Interval(
                id='interval-component',
                interval=5*1000,  # 5ç§’åˆ·æ–°
                n_intervals=0
            )
        ])

    def setup_callbacks(self):
        @self.app.callback(
            [Output('total-transactions', 'children'),
             Output('tps', 'children'),
             Output('avg-latency', 'children'),
             Output('active-users', 'children')],
            [Input('interval-component', 'n_intervals')]
        )
        def update_metrics(n):
            return self.get_current_metrics()

        @self.app.callback(
            Output('transaction-timeline', 'figure'),
            [Input('interval-component', 'n_intervals')]
        )
        def update_timeline(n):
            return self.get_transaction_timeline()

    def get_current_metrics(self):
        """è·å–å½“å‰æŒ‡æ ‡"""
        conn = psycopg2.connect(**self.db_config)
        cursor = conn.cursor()

        # è·å–å®æ—¶æŒ‡æ ‡
        cursor.execute("""
            SELECT
                COUNT(*) as total_transactions,
                COUNT(*) FILTER (WHERE created_at >= NOW() - INTERVAL '1 minute') as tps,
                AVG(EXTRACT(EPOCH FROM (NOW() - created_at)) * 1000) as avg_latency,
                COUNT(DISTINCT user_id) FILTER (WHERE created_at >= NOW() - INTERVAL '5 minutes') as active_users
            FROM transactions
            WHERE created_at >= NOW() - INTERVAL '1 hour'
        """)

        result = cursor.fetchone()
        conn.close()

        return [
            f"{result[0]:,}",
            f"{result[1]}",
            f"{result[2]:.2f}",
            f"{result[3]}"
        ]

    def get_transaction_timeline(self):
        """è·å–äº¤æ˜“æ—¶é—´çº¿"""
        conn = psycopg2.connect(**self.db_config)

        # è·å–æœ€è¿‘1å°æ—¶çš„äº¤æ˜“æ•°æ®
        df = pd.read_sql("""
            SELECT
                date_trunc('minute', created_at) as minute,
                COUNT(*) as transaction_count,
                SUM(amount) as total_amount
            FROM transactions
            WHERE created_at >= NOW() - INTERVAL '1 hour'
            GROUP BY minute
            ORDER BY minute
        """, conn)

        conn.close()

        return {
            'data': [
                go.Scatter(
                    x=df['minute'],
                    y=df['transaction_count'],
                    mode='lines+markers',
                    name='äº¤æ˜“æ•°é‡'
                ),
                go.Scatter(
                    x=df['minute'],
                    y=df['total_amount'],
                    mode='lines+markers',
                    name='äº¤æ˜“é‡‘é¢',
                    yaxis='y2'
                )
            ],
            'layout': go.Layout(
                title='äº¤æ˜“æ—¶é—´çº¿',
                xaxis={'title': 'æ—¶é—´'},
                yaxis={'title': 'äº¤æ˜“æ•°é‡'},
                yaxis2={'title': 'äº¤æ˜“é‡‘é¢', 'overlaying': 'y', 'side': 'right'}
            )
        }

    def run(self, debug=True, port=8050):
        self.app.run_server(debug=debug, port=port)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    db_config = {
        'host': 'localhost',
        'database': 'streaming_db',
        'user': 'postgres',
        'password': 'password'
    }

    dashboard = StreamingDashboard(db_config)
    dashboard.run()
```

## ğŸ”— ç›¸å…³é“¾æ¥

- [1.1.15-äº‘åŸç”Ÿä¸å®¹å™¨åŒ–éƒ¨ç½²](1.1.15-äº‘åŸç”Ÿä¸å®¹å™¨åŒ–éƒ¨ç½².md) - PostgreSQLäº‘åŸç”Ÿéƒ¨ç½²
- [1.1.16-æ€§èƒ½è°ƒä¼˜ä¸ç›‘æ§](1.1.16-æ€§èƒ½è°ƒä¼˜ä¸ç›‘æ§.md) - æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
- [1.1.17-å®‰å…¨ä¸åˆè§„](1.1.17-å®‰å…¨ä¸åˆè§„.md) - å®‰å…¨åˆè§„å®ç°
- [3.5.5-æ•°æ®æµå¤„ç†ä¸æ¶æ„](../../../3-æ•°æ®æ¨¡å‹ä¸ç®—æ³•/3.5-æ•°æ®åˆ†æä¸ETL/3.5.5-æ•°æ®æµå¤„ç†ä¸æ¶æ„.md) - æ•°æ®æµå¤„ç†æ¶æ„
- [4.3.1-å¾®æœåŠ¡æ¶æ„åŸºç¡€ç†è®º](../../../4-è½¯ä»¶æ¶æ„ä¸å·¥ç¨‹/4.3-å¾®æœåŠ¡æ¶æ„/4.3.1-å¾®æœåŠ¡æ¶æ„åŸºç¡€ç†è®º.md) - å¾®æœåŠ¡æ¶æ„ç†è®º

[è¿”å›PostgreSQLå¯¼èˆª](README.md)
