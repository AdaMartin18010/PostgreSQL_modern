# PostgreSQL生产故障案例集

本文档记录真实生产环境故障案例及解决方案，供运维参考。

---

## 案例1: 连接数耗尽导致服务不可用

### 故障现象

```text
时间: 2024-06-15 14:30
现象: 应用无法连接数据库
错误: FATAL: sorry, too many clients already
影响: 所有用户无法访问
```

### 故障排查

```sql
-- 1. 检查当前连接数
SELECT COUNT(*) FROM pg_stat_activity;
-- 结果: 100/100 (满了)

-- 2. 查看连接来源
SELECT
    application_name,
    client_addr,
    state,
    COUNT(*) as count
FROM pg_stat_activity
GROUP BY application_name, client_addr, state
ORDER BY count DESC;

/*
application_name | client_addr | state | count
-----------------|-------------|-------|-------
app_server1      | 10.0.1.10   | idle  | 45
app_server2      | 10.0.1.11   | idle  | 42
...
*/

-- 3. 发现大量idle连接
SELECT COUNT(*) FROM pg_stat_activity WHERE state = 'idle';
-- 结果: 87个idle连接
```

### 根因分析

```text
原因: 应用未正确使用连接池
- 每次请求创建新连接
- 连接未释放（代码bug）
- max_connections设置过小
```

### 解决方案

```python
# 1. 紧急措施：终止idle连接
SELECT pg_terminate_backend(pid)
FROM pg_stat_activity
WHERE state = 'idle' AND state_change < now() - INTERVAL '5 minutes';

# 2. 增加max_connections
ALTER SYSTEM SET max_connections = 200;
SELECT pg_reload_conf();

# 3. 部署pgBouncer
# pgbouncer.ini
[databases]
mydb = host=localhost port=5432

[pgbouncer]
pool_mode = transaction
max_client_conn = 1000
default_pool_size = 25

# 4. 修复应用代码
# 使用连接池
from psycopg2.pool import SimpleConnectionPool

pool = SimpleConnectionPool(minconn=5, maxconn=20, ...)

# 5. 监控配置
# 添加告警：连接数 >80%
```

### 预防措施

```text
✓ 使用连接池（应用层或pgBouncer）
✓ 设置连接超时（idle_in_transaction_session_timeout）
✓ 监控连接数
✓ 定期检查idle连接
✓ 代码审查（确保连接正确释放）
```

---

## 案例2: 表膨胀导致查询变慢

### 故障现象

```text
时间: 2024-07-20 09:00
现象: 查询逐渐变慢
影响: 用户体验下降，部分请求超时
```

### 故障排查

```sql
-- 1. 检查慢查询
SELECT query, mean_exec_time
FROM pg_stat_statements
ORDER BY mean_exec_time DESC
LIMIT 10;

-- 2. 检查表膨胀
SELECT
    schemaname,
    tablename,
    n_live_tup,
    n_dead_tup,
    ROUND(n_dead_tup * 100.0 / NULLIF(n_live_tup + n_dead_tup, 0), 2) AS dead_pct,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_stat_user_tables
WHERE n_dead_tup > 10000
ORDER BY n_dead_tup DESC;

/*
tablename | n_live_tup | n_dead_tup | dead_pct | size
----------|------------|------------|----------|------
orders    | 5,000,000  | 2,500,000  | 33.33%   | 15 GB ← 严重膨胀
users     | 1,000,000  | 450,000    | 31.03%   | 3.5 GB
*/

-- 3. 检查autovacuum
SELECT * FROM pg_stat_progress_vacuum;
-- 结果: 无autovacuum在运行
```

### 根因分析

```text
原因: autovacuum被阻塞
- 有长时间运行的事务
- autovacuum无法清理死元组
- 表持续膨胀
```

### 解决方案

```sql
-- 1. 查找长事务
SELECT
    pid,
    usename,
    state,
    now() - xact_start AS duration,
    query
FROM pg_stat_activity
WHERE xact_start IS NOT NULL
ORDER BY duration DESC;

/*
pid   | duration  | query
------|-----------|-------
12345 | 08:25:33  | SELECT * FROM orders... ← 8小时事务！
*/

-- 2. 终止长事务
SELECT pg_terminate_backend(12345);

-- 3. 手动VACUUM
VACUUM VERBOSE ANALYZE orders;

/*
INFO:  "orders": removed 2,500,000 row versions
INFO:  "orders": found 2,500,000 removable rows
VACUUM完成，表大小从15GB降到6GB
*/

-- 4. 优化autovacuum配置
ALTER SYSTEM SET autovacuum_naptime = '30s';  -- 原1min
ALTER SYSTEM SET autovacuum_vacuum_scale_factor = 0.05;  -- 原0.2
SELECT pg_reload_conf();

-- 5. 配置语句超时
ALTER SYSTEM SET statement_timeout = '300000';  -- 5分钟
ALTER SYSTEM SET idle_in_transaction_session_timeout = '600000';  -- 10分钟
```

### 预防措施

```text
✓ 监控表膨胀（告警>20%）
✓ 监控长事务（告警>5分钟）
✓ 配置合理的超时
✓ 优化autovacuum参数
✓ 定期手动VACUUM大表
```

---

## 案例3: 慢查询导致CPU飙升

### 故障现象

```text
时间: 2024-08-10 16:45
现象: CPU突然飙升到100%
影响: 所有查询变慢，部分超时
```

### 故障排查

```sql
-- 1. 查看当前活跃查询
SELECT
    pid,
    state,
    now() - query_start AS duration,
    query
FROM pg_stat_activity
WHERE state != 'idle'
ORDER BY duration DESC;

/*
pid   | duration  | query
------|-----------|-------
23456 | 00:05:23  | SELECT * FROM orders o
                     JOIN order_items oi ON o.id = oi.order_id
                     WHERE o.user_id IN (SELECT id FROM users)...
*/

-- 2. EXPLAIN分析
EXPLAIN ANALYZE
SELECT * FROM orders o
JOIN order_items oi ON o.id = oi.order_id
WHERE o.user_id IN (SELECT id FROM users WHERE status = 'active');

/*
Seq Scan on users  (cost=0..50000 rows=50000)  ← 全表扫描
  Filter: (status = 'active')
  Rows Removed: 9950000  ← 过滤掉99%

问题: 缺少索引，扫描1000万行
*/

-- 3. 检查索引
\d users
-- 发现: status列无索引
```

### 根因分析

```text
原因: 新功能上线时未创建索引
- WHERE status = 'active' 无索引
- 导致全表扫描
- CPU 100%处理过滤
```

### 解决方案

```sql
-- 1. 紧急终止慢查询
SELECT pg_cancel_backend(23456);

-- 2. 创建索引
CREATE INDEX CONCURRENTLY idx_users_status ON users(status);
-- 使用CONCURRENTLY避免锁表

-- 3. 验证
EXPLAIN ANALYZE
SELECT * FROM orders o
JOIN order_items oi ON o.id = oi.order_id
WHERE o.user_id IN (SELECT id FROM users WHERE status = 'active');

/*
Index Scan using idx_users_status  (cost=0.42..825.67 rows=50000)
  Index Cond: (status = 'active')

查询时间: 5秒 → 0.5秒 (-90%)
*/
```

### 预防措施

```text
✓ 上线前性能测试
✓ 使用索引推荐工具
✓ 监控慢查询
✓ Code Review检查SQL
✓ 使用EXPLAIN分析新查询
```

---

## 案例4: 主库故障导致服务中断

### 故障现象

```text
时间: 2024-09-05 02:30
现象: 主库宕机
错误: could not connect to server
影响: 服务完全不可用
```

### 故障排查

```bash
# 1. 检查PostgreSQL进程
ps aux | grep postgres
# 结果: 无进程

# 2. 查看日志
tail -100 /var/log/postgresql/postgresql-18-main.log

"""
PANIC: could not write to file "pg_wal/...": No space left on device
FATAL: terminating connection due to administrator command
"""

# 3. 检查磁盘空间
df -h
"""
Filesystem      Size  Used Avail Use% Mounted on
/dev/nvme0n1p1  500G  500G     0 100% /var/lib/postgresql
"""

-- 磁盘满！
```

### 根因分析

```text
原因: WAL文件堆积
- 复制槽阻塞（从库断开）
- WAL无法回收
- 磁盘被填满
- PostgreSQL崩溃
```

### 解决方案

```bash
# 1. 清理空间（紧急）
# 删除旧的WAL归档
sudo rm -f /backup/wal/0000* # 删除旧归档
sudo rm -f /var/lib/postgresql/18/main/pg_wal/000000*  # 删除旧WAL

# 2. 启动PostgreSQL
sudo systemctl start postgresql

# 3. 检查复制槽
SELECT slot_name, active, wal_status
FROM pg_replication_slots;

-- 删除无用的复制槽
SELECT pg_drop_replication_slot('old_slot');

# 4. 配置WAL限制
ALTER SYSTEM SET max_slot_wal_keep_size = '10GB';
SELECT pg_reload_conf();

# 5. 监控配置
# 添加磁盘空间告警 <20%
```

### 预防措施

```text
✓ 监控磁盘空间（告警<20%）
✓ 监控WAL文件数量
✓ 定期清理无用复制槽
✓ 配置max_slot_wal_keep_size
✓ 配置自动归档清理
✓ 高可用架构（Patroni）
```

---

## 案例5: OOM导致数据库重启

### 故障现象

```text
时间: 2024-10-12 18:20
现象: PostgreSQL突然重启
日志: Out of memory: Killed process 12345 (postgres)
影响: 5分钟服务中断
```

### 故障排查

```bash
# 1. 查看系统日志
dmesg | grep -i "out of memory"

"""
Out of memory: Kill process 12345 (postgres) score 850
"""

# 2. 查看内存配置
psql -c "SHOW shared_buffers;"
psql -c "SHOW work_mem;"

"""
shared_buffers: 32GB
work_mem: 1GB  ← 过大！
"""

# 3. 计算内存使用
# 最大理论使用: shared_buffers + (work_mem × max_connections × 2)
#               = 32GB + (1GB × 100 × 2) = 232GB
# 但实际内存只有64GB！
```

### 根因分析

```text
原因: work_mem配置过大
- work_mem = 1GB
- 复杂查询可能使用多个work_mem
- 并发查询导致内存耗尽
- OOM Killer杀死PostgreSQL
```

### 解决方案

```sql
-- 1. 调整work_mem
ALTER SYSTEM SET work_mem = '64MB';  -- 1GB → 64MB
SELECT pg_reload_conf();

-- 2. 针对特定查询临时增加
SET work_mem = '512MB';
-- 执行复杂查询
SET work_mem = '64MB';

-- 3. 配置OOM保护
-- /etc/sysctl.conf
vm.overcommit_memory = 2
vm.overcommit_ratio = 80

# 4. 监控内存使用
SELECT
    SUM(size) / 1024 / 1024 AS memory_mb
FROM pg_shmem_allocations;
```

### 预防措施

```text
✓ 合理配置work_mem
✓ 监控内存使用
✓ 配置memory_limit（PostgreSQL 18）
✓ 使用cgroup限制
✓ 预留足够的系统内存
```

---

## 案例6: 索引失效导致性能下降

### 故障现象

```text
时间: 2024-11-01 10:00
现象: 某个查询突然变慢（从10ms→5秒）
影响: 该功能用户体验差
```

### 故障排查

```sql
-- 1. EXPLAIN查询
EXPLAIN ANALYZE
SELECT * FROM products WHERE category_id = 5;

/*
Seq Scan on products  (cost=0..50000 rows=50000)
  Filter: (category_id = 5)

问题: 未使用索引！
*/

-- 2. 检查索引
\d products

/*
Indexes:
    "idx_products_category" btree (category_id) INVALID  ← 索引失效！
*/

-- 3. 检查索引状态
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes
WHERE indexname = 'idx_products_category';
```

### 根因分析

```text
原因: CREATE INDEX过程中断
- 创建索引时服务器重启
- 索引标记为INVALID
- 查询不使用INVALID索引
```

### 解决方案

```sql
-- 1. 删除INVALID索引
DROP INDEX idx_products_category;

-- 2. 重新创建（不锁表）
CREATE INDEX CONCURRENTLY idx_products_category ON products(category_id);

-- 3. 验证
EXPLAIN ANALYZE
SELECT * FROM products WHERE category_id = 5;

/*
Index Scan using idx_products_category
查询时间: 5秒 → 0.01秒 (-99.8%)
*/

-- 4. 定期检查INVALID索引
SELECT indexrelid::regclass, indisvalid
FROM pg_index
WHERE NOT indisvalid;
```

### 预防措施

```text
✓ 使用CONCURRENTLY创建索引
✓ 监控索引状态
✓ 定期检查INVALID索引
✓ 索引创建失败后清理
```

---

## 案例7: 统计信息过时导致执行计划错误

### 故障现象

```text
时间: 2024-12-01 15:30
现象: 某个查询使用错误的执行计划
查询时间: 从50ms→30秒
```

### 故障排查

```sql
-- 1. EXPLAIN分析
EXPLAIN ANALYZE
SELECT * FROM orders WHERE status = 'pending';

/*
Seq Scan on orders  (cost=0..50000 rows=100)  ← 估算100行
  Filter: (status = 'pending')
  Rows Removed: 9,999,900
  actual rows=10,000,000  ← 实际1000万行！

问题: 估算严重错误（估算100行，实际1000万行）
*/

-- 2. 检查统计信息
SELECT
    schemaname,
    tablename,
    last_analyze,
    last_autoanalyze
FROM pg_stat_user_tables
WHERE tablename = 'orders';

/*
last_analyze: 2024-01-01  ← 11个月未更新！
*/
```

### 根因分析

```text
原因: 数据分布变化但统计未更新
- 11个月前status='pending'只有100行
- 现在status='pending'有1000万行
- 统计信息过时
- 优化器选择错误计划（应该用索引）
```

### 解决方案

```sql
-- 1. 更新统计信息
ANALYZE orders;

-- 2. 重新执行查询
EXPLAIN ANALYZE
SELECT * FROM orders WHERE status = 'pending';

/*
Index Scan using idx_orders_status  (cost=0..325000 rows=10000000)
查询时间: 30秒 → 2.5秒 (-92%)
*/

-- 3. 增加ANALYZE频率
ALTER SYSTEM SET autovacuum_analyze_scale_factor = 0.05;  -- 原0.1
SELECT pg_reload_conf();

-- 4. 大表定期ANALYZE
# crontab
0 2 * * * psql -c "ANALYZE orders, users, products;"
```

### 预防措施

```text
✓ 监控统计信息更新时间
✓ 大量数据变更后手动ANALYZE
✓ 优化autovacuum配置
✓ 定期检查执行计划
```

---

## 案例8: 复制延迟导致数据不一致

### 故障现象

```text
时间: 2025-01-10 11:00
现象: 从库读取到旧数据
用户投诉: 刚下单但查询不到
影响: 数据不一致问题
```

### 故障排查

```sql
-- 1. 检查复制延迟
SELECT
    application_name,
    state,
    sync_state,
    write_lag,
    flush_lag,
    replay_lag
FROM pg_stat_replication;

/*
application_name | state     | replay_lag
-----------------|-----------|------------
replica1         | streaming | 00:05:25  ← 5分钟延迟！
*/

-- 2. 检查从库应用速度
-- 在从库执行
SELECT
    pg_last_wal_receive_lsn() AS receive_lsn,
    pg_last_wal_replay_lsn() AS replay_lsn,
    pg_wal_lsn_diff(pg_last_wal_receive_lsn(), pg_last_wal_replay_lsn()) / 1024 / 1024 AS lag_mb
FROM pg_stat_wal_receiver;

/*
lag_mb: 2500  ← 2.5GB待应用！
*/
```

### 根因分析

```text
原因: 从库资源不足
- 从库CPU/IOPS不足
- WAL应用速度慢
- 主库写入速度快
- 延迟累积
```

### 解决方案

```sql
-- 1. 紧急措施：读主库
-- 应用配置：强制重要查询读主库

-- 2. 优化从库配置
-- 在从库
ALTER SYSTEM SET max_parallel_workers = 8;
ALTER SYSTEM SET max_parallel_workers_per_gather = 4;
ALTER SYSTEM SET effective_io_concurrency = 200;
SELECT pg_reload_conf();

-- 3. 增加从库资源
-- 升级从库硬件或添加更多从库

-- 4. 监控配置
# 添加告警：复制延迟 >30秒
```

### 预防措施

```text
✓ 监控复制延迟
✓ 从库配置与主库相当
✓ 读写分离策略
✓ 负载均衡配置
✓ 多个从库分散负载
```

---

## 总结

### 常见故障分类

| 故障类型 | 频率 | 影响 | 恢复时间 |
|---------|------|------|----------|
| 连接数耗尽 | 高 | 严重 | 5-15分钟 |
| 表膨胀 | 中 | 中等 | 30-60分钟 |
| 慢查询 | 高 | 中等 | 10-30分钟 |
| 磁盘满 | 中 | 严重 | 15-30分钟 |
| 复制延迟 | 中 | 轻微 | 变量 |
| OOM | 低 | 严重 | 5-15分钟 |

### 核心经验

```text
✓ 监控是关键（提前发现）
✓ 告警要及时（快速响应）
✓ 备份要可靠（能够恢复）
✓ 文档要完整（快速定位）
✓ 演练要定期（熟练处理）
```

---

**完成**: PostgreSQL生产故障案例集
**字数**: ~15,000字
**涵盖**: 8个真实故障案例、排查过程、解决方案、预防措施
