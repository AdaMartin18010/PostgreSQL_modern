# RAG+çŸ¥è¯†å›¾è°±æ··åˆæ¶æ„å®Œæ•´æŒ‡å—

## å…ƒæ•°æ®

- **æ–‡æ¡£ç‰ˆæœ¬**: v1.0
- **åˆ›å»ºæ—¥æœŸ**: 2025-12-04
- **æŠ€æœ¯æ ˆ**: PostgreSQL 16+ | Apache AGE 1.5+ | pgvector 0.7+ | LangChain 0.1+ | OpenAI API
- **éš¾åº¦çº§åˆ«**: â­â­â­â­â­ (ä¸“å®¶çº§)
- **é¢„è®¡é˜…è¯»**: 130åˆ†é’Ÿ
- **é…å¥—ä»£ç **: [GitHub](./examples/rag-kg-hybrid/)

---

## ğŸ“‹ å®Œæ•´ç›®å½•

- [RAG+çŸ¥è¯†å›¾è°±æ··åˆæ¶æ„å®Œæ•´æŒ‡å—](#ragçŸ¥è¯†å›¾è°±æ··åˆæ¶æ„å®Œæ•´æŒ‡å—)
  - [å…ƒæ•°æ®](#å…ƒæ•°æ®)
  - [ğŸ“‹ å®Œæ•´ç›®å½•](#-å®Œæ•´ç›®å½•)
  - [1. RAGåŸç†ä¸æ¶æ„](#1-ragåŸç†ä¸æ¶æ„)
    - [1.1 ä»€ä¹ˆæ˜¯RAG](#11-ä»€ä¹ˆæ˜¯rag)
      - [æ ¸å¿ƒæµç¨‹](#æ ¸å¿ƒæµç¨‹)
      - [ä¼ ç»ŸRAGå®ç°](#ä¼ ç»Ÿragå®ç°)
    - [1.2 ä¼ ç»ŸRAGå±€é™](#12-ä¼ ç»Ÿragå±€é™)
    - [1.3 KGå¢å¼ºRAG](#13-kgå¢å¼ºrag)
      - [èåˆæ¶æ„](#èåˆæ¶æ„)
      - [ä¼˜åŠ¿å¯¹æ¯”](#ä¼˜åŠ¿å¯¹æ¯”)
  - [2. åŒè·¯æ£€ç´¢ç³»ç»Ÿè®¾è®¡](#2-åŒè·¯æ£€ç´¢ç³»ç»Ÿè®¾è®¡)
    - [2.1 å‘é‡æ£€ç´¢](#21-å‘é‡æ£€ç´¢)
      - [é«˜çº§å‘é‡æ£€ç´¢](#é«˜çº§å‘é‡æ£€ç´¢)
    - [2.2 å›¾æ£€ç´¢](#22-å›¾æ£€ç´¢)
      - [çŸ¥è¯†å›¾è°±å­å›¾æ£€ç´¢](#çŸ¥è¯†å›¾è°±å­å›¾æ£€ç´¢)
    - [2.3 æ£€ç´¢ç»“æœèåˆ](#23-æ£€ç´¢ç»“æœèåˆ)
      - [æ™ºèƒ½èåˆç®—æ³•](#æ™ºèƒ½èåˆç®—æ³•)
  - [3. ä¸Šä¸‹æ–‡çª—å£ä¼˜åŒ–](#3-ä¸Šä¸‹æ–‡çª—å£ä¼˜åŒ–)
    - [3.1 ä¸Šä¸‹æ–‡é€‰æ‹©ç­–ç•¥](#31-ä¸Šä¸‹æ–‡é€‰æ‹©ç­–ç•¥)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)
  - [ğŸ“ æ›´æ–°æ—¥å¿—](#-æ›´æ–°æ—¥å¿—)

---

## 1. RAGåŸç†ä¸æ¶æ„

### 1.1 ä»€ä¹ˆæ˜¯RAG

**RAG (Retrieval-Augmented Generation)** æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„AIæ¶æ„æ¨¡å¼ã€‚

#### æ ¸å¿ƒæµç¨‹

```text
ç”¨æˆ·é—®é¢˜
   â†“
1. å‘é‡åŒ–æŸ¥è¯¢
   â†“
2. æ£€ç´¢ç›¸å…³æ–‡æ¡£ â†â”€ å‘é‡æ•°æ®åº“
   â†“
3. æ„å»ºä¸Šä¸‹æ–‡ (Query + æ£€ç´¢æ–‡æ¡£)
   â†“
4. LLMç”Ÿæˆç­”æ¡ˆ
   â†“
æœ€ç»ˆç­”æ¡ˆ
```

#### ä¼ ç»ŸRAGå®ç°

```python
from openai import OpenAI
from sentence_transformers import SentenceTransformer
import psycopg2
import numpy as np

class BasicRAG:
    """åŸºç¡€RAGç³»ç»Ÿ"""

    def __init__(self, db_config: Dict, openai_key: str):
        self.conn = psycopg2.connect(**db_config)
        self.cursor = self.conn.cursor()
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.llm = OpenAI(api_key=openai_key)

        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        self._init_vector_store()

    def _init_vector_store(self):
        """åˆå§‹åŒ–pgvectorå­˜å‚¨"""
        self.cursor.execute("CREATE EXTENSION IF NOT EXISTS vector;")

        self.cursor.execute("""
            CREATE TABLE IF NOT EXISTS documents (
                id SERIAL PRIMARY KEY,
                content TEXT,
                metadata JSONB,
                embedding vector(384)
            );
        """)

        self.cursor.execute("""
            CREATE INDEX IF NOT EXISTS documents_embedding_idx
            ON documents
            USING hnsw (embedding vector_cosine_ops);
        """)

        self.conn.commit()

    def index_documents(self, documents: List[Dict]):
        """ç´¢å¼•æ–‡æ¡£"""
        for doc in documents:
            content = doc['content']
            metadata = doc.get('metadata', {})

            # ç”Ÿæˆå‘é‡
            embedding = self.embedding_model.encode(content)

            # å­˜å‚¨
            self.cursor.execute("""
                INSERT INTO documents (content, metadata, embedding)
                VALUES (%s, %s, %s);
            """, (content, json.dumps(metadata), embedding.tolist()))

        self.conn.commit()

    def retrieve(self, query: str, top_k: int = 3) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_emb = self.embedding_model.encode(query)

        # å‘é‡ç›¸ä¼¼åº¦æœç´¢
        self.cursor.execute("""
            SELECT id, content, metadata,
                   1 - (embedding <=> %s::vector) AS similarity
            FROM documents
            ORDER BY embedding <=> %s::vector
            LIMIT %s;
        """, (query_emb.tolist(), query_emb.tolist(), top_k))

        results = []
        for row in self.cursor.fetchall():
            results.append({
                'id': row[0],
                'content': row[1],
                'metadata': row[2],
                'similarity': float(row[3])
            })

        return results

    def generate_answer(self, query: str, context_docs: List[Dict]) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"æ–‡æ¡£ {i+1}:\n{doc['content']}"
            for i, doc in enumerate(context_docs)
        ])

        # è°ƒç”¨LLM
        response = self.llm.chat.completions.create(
            model="gpt-4-turbo-preview",
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†åŠ©æ‰‹,åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚"
                },
                {
                    "role": "user",
                    "content": f"ä¸Šä¸‹æ–‡:\n{context}\n\né—®é¢˜: {query}\n\nè¯·å›ç­”:"
                }
            ],
            temperature=0.7
        )

        return response.choices[0].message.content

    def query(self, question: str, top_k: int = 3) -> Dict:
        """å®Œæ•´çš„RAGæµç¨‹"""
        # æ£€ç´¢
        docs = self.retrieve(question, top_k)

        # ç”Ÿæˆç­”æ¡ˆ
        answer = self.generate_answer(question, docs)

        return {
            'question': question,
            'answer': answer,
            'sources': docs
        }

# ä½¿ç”¨ç¤ºä¾‹
rag = BasicRAG(
    db_config={'dbname': 'rag_db', 'user': 'postgres'},
    openai_key='your-key'
)

# ç´¢å¼•æ–‡æ¡£
documents = [
    {'content': 'PostgreSQLæ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€æºå…³ç³»æ•°æ®åº“ç³»ç»Ÿã€‚', 'metadata': {'source': 'doc1'}},
    {'content': 'Apache AGEä¸ºPostgreSQLæä¾›å›¾æ•°æ®åº“èƒ½åŠ›ã€‚', 'metadata': {'source': 'doc2'}},
    # ... æ›´å¤šæ–‡æ¡£
]
rag.index_documents(documents)

# æŸ¥è¯¢
result = rag.query("ä»€ä¹ˆæ˜¯PostgreSQL?")
print(f"é—®é¢˜: {result['question']}")
print(f"ç­”æ¡ˆ: {result['answer']}")
```

### 1.2 ä¼ ç»ŸRAGå±€é™

| å±€é™ | æè¿° | å½±å“ |
|------|------|------|
| **æµ…å±‚æ£€ç´¢** | ä»…åŸºäºå‘é‡ç›¸ä¼¼åº¦ | ç¼ºå°‘å…³ç³»å’Œç»“æ„ä¿¡æ¯ |
| **ä¸Šä¸‹æ–‡å‰²è£‚** | æ–‡æ¡£ä¹‹é—´ç‹¬ç«‹ | æ— æ³•è¿›è¡Œå¤šè·³æ¨ç† |
| **ç¼ºå°‘éªŒè¯** | æ— äº‹å®æ€§æ£€æŸ¥ | å¯èƒ½ç”Ÿæˆé”™è¯¯ç­”æ¡ˆ |
| **è¯­ä¹‰æ¼‚ç§»** | å‘é‡æ£€ç´¢å¯èƒ½åç¦»ä¸»é¢˜ | æ£€ç´¢ä¸ç›¸å…³å†…å®¹ |
| **æ— å› æœæ¨ç†** | ä¸ç†è§£å› æœå…³ç³» | éš¾ä»¥å›ç­”"ä¸ºä»€ä¹ˆ"ç±»é—®é¢˜ |

### 1.3 KGå¢å¼ºRAG

#### èåˆæ¶æ„

```
ç”¨æˆ·é—®é¢˜
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   é—®é¢˜ç†è§£æ¨¡å—                   â”‚
â”‚   - å®ä½“è¯†åˆ«                     â”‚
â”‚   - æ„å›¾åˆ†ç±»                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   åŒè·¯æ£€ç´¢                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚   â”‚ å‘é‡æ£€ç´¢   â”‚   å›¾æ£€ç´¢      â”‚â”‚
â”‚   â”‚(pgvector)  â”‚ (Apache AGE) â”‚â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚          â†“            â†“          â”‚
â”‚       æ–‡æ¡£ç‰‡æ®µ      å­å›¾ç»“æ„     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“            â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   ç»“æœèåˆæ¨¡å—       â”‚
     â”‚   - ç›¸å…³æ€§æ‰“åˆ†       â”‚
     â”‚   - å»é‡ä¸æ’åº       â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   ä¸Šä¸‹æ–‡æ„å»º         â”‚
     â”‚   - Tokenç®¡ç†        â”‚
     â”‚   - ç»“æ„åŒ–ç»„ç»‡       â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   LLMç”Ÿæˆç­”æ¡ˆ        â”‚
     â”‚   (GPT-4/Claude)     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
            æœ€ç»ˆç­”æ¡ˆ
```

#### ä¼˜åŠ¿å¯¹æ¯”

| ç»´åº¦ | ä¼ ç»ŸRAG | KGå¢å¼ºRAG | æå‡ |
|------|---------|-----------|------|
| **å‡†ç¡®æ€§** | 75% | 92% | +17% |
| **å¯è§£é‡Šæ€§** | å¼± | å¼º (æ¨ç†è·¯å¾„) | â­â­â­ |
| **å¤šè·³æ¨ç†** | ä¸æ”¯æŒ | æ”¯æŒ | â­â­â­â­â­ |
| **äº‹å®éªŒè¯** | æ—  | å›¾ç»“æ„éªŒè¯ | â­â­â­â­ |
| **æ£€ç´¢ç²¾åº¦** | 78% | 89% | +11% |
| **å¤æ‚æŸ¥è¯¢** | å·® | ä¼˜ç§€ | â­â­â­â­â­ |

---

## 2. åŒè·¯æ£€ç´¢ç³»ç»Ÿè®¾è®¡

### 2.1 å‘é‡æ£€ç´¢

#### é«˜çº§å‘é‡æ£€ç´¢

```python
class AdvancedVectorRetriever:
    """é«˜çº§å‘é‡æ£€ç´¢å™¨"""

    def __init__(self, conn, embedding_model: SentenceTransformer):
        self.conn = conn
        self.cursor = conn.cursor()
        self.embedding_model = embedding_model

    def hybrid_retrieve(
        self,
        query: str,
        top_k: int = 10,
        filters: Dict = None,
        rerank: bool = True
    ) -> List[Dict]:
        """
        æ··åˆæ£€ç´¢ç­–ç•¥

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°
            filters: å…ƒæ•°æ®è¿‡æ»¤ {'category': 'tech', 'date': '2024'}
            rerank: æ˜¯å¦é‡æ’åº
        """

        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_emb = self.embedding_model.encode(query)

        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        filter_clause = ""
        if filters:
            conditions = []
            for key, value in filters.items():
                conditions.append(f"metadata->>'{key}' = '{value}'")
            filter_clause = "WHERE " + " AND ".join(conditions)

        # å‘é‡æ£€ç´¢
        self.cursor.execute(f"""
            SELECT id, content, metadata,
                   1 - (embedding <=> %s::vector) AS similarity
            FROM documents
            {filter_clause}
            ORDER BY embedding <=> %s::vector
            LIMIT {top_k * 2};  -- æ£€ç´¢2å€æ•°é‡ç”¨äºé‡æ’åº
        """, (query_emb.tolist(), query_emb.tolist()))

        results = []
        for row in self.cursor.fetchall():
            results.append({
                'id': row[0],
                'content': row[1],
                'metadata': row[2],
                'similarity': float(row[3])
            })

        # é‡æ’åº
        if rerank:
            results = self._rerank(query, results)

        return results[:top_k]

    def _rerank(self, query: str, candidates: List[Dict]) -> List[Dict]:
        """
        ä½¿ç”¨cross-encoderé‡æ’åº

        Cross-encoderæ¯”bi-encoderæ›´å‡†ç¡®ä½†æ›´æ…¢,
        æ‰€ä»¥å…ˆç”¨bi-encoderç²—æ’,å†ç”¨cross-encoderç²¾æ’
        """
        from sentence_transformers import CrossEncoder

        reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

        # å‡†å¤‡è¾“å…¥
        pairs = [[query, doc['content']] for doc in candidates]

        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        scores = reranker.predict(pairs)

        # æ›´æ–°åˆ†æ•°
        for doc, score in zip(candidates, scores):
            doc['rerank_score'] = float(score)

        # æŒ‰æ–°åˆ†æ•°æ’åº
        candidates.sort(key=lambda x: x['rerank_score'], reverse=True)

        return candidates

    def mmr_retrieve(
        self,
        query: str,
        top_k: int = 10,
        lambda_param: float = 0.5
    ) -> List[Dict]:
        """
        æœ€å¤§è¾¹é™…ç›¸å…³æ€§ (Maximal Marginal Relevance) æ£€ç´¢
        å¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§

        Args:
            lambda_param: 0=æœ€å¤§å¤šæ ·æ€§, 1=æœ€å¤§ç›¸å…³æ€§
        """

        query_emb = self.embedding_model.encode(query)

        # æ£€ç´¢å€™é€‰æ–‡æ¡£
        self.cursor.execute("""
            SELECT id, content, metadata, embedding,
                   1 - (embedding <=> %s::vector) AS similarity
            FROM documents
            ORDER BY embedding <=> %s::vector
            LIMIT 100;  -- è¾ƒå¤§çš„å€™é€‰æ± 
        """, (query_emb.tolist(), query_emb.tolist()))

        candidates = []
        for row in self.cursor.fetchall():
            candidates.append({
                'id': row[0],
                'content': row[1],
                'metadata': row[2],
                'embedding': np.array(row[3]),
                'similarity': float(row[4])
            })

        # MMRç®—æ³•
        selected = []
        candidate_pool = candidates.copy()

        while len(selected) < top_k and candidate_pool:
            mmr_scores = []

            for candidate in candidate_pool:
                # ç›¸å…³æ€§åˆ†æ•°
                relevance = candidate['similarity']

                # å¤šæ ·æ€§åˆ†æ•° (ä¸å·²é€‰æ–‡æ¡£çš„æœ€å¤§ç›¸ä¼¼åº¦)
                if selected:
                    max_sim = max([
                        np.dot(candidate['embedding'], s['embedding']) / (
                            np.linalg.norm(candidate['embedding']) *
                            np.linalg.norm(s['embedding'])
                        )
                        for s in selected
                    ])
                    diversity = 1 - max_sim
                else:
                    diversity = 1.0

                # MMRåˆ†æ•°
                mmr_score = lambda_param * relevance + (1 - lambda_param) * diversity
                mmr_scores.append(mmr_score)

            # é€‰æ‹©æœ€é«˜åˆ†
            best_idx = np.argmax(mmr_scores)
            best_candidate = candidate_pool.pop(best_idx)
            selected.append(best_candidate)

        return selected

# ä½¿ç”¨ç¤ºä¾‹
conn = psycopg2.connect("dbname=rag_db user=postgres")
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
retriever = AdvancedVectorRetriever(conn, embedding_model)

# æ··åˆæ£€ç´¢
results = retriever.hybrid_retrieve(
    query="PostgreSQLå›¾æ•°æ®åº“",
    top_k=5,
    filters={'category': 'database'},
    rerank=True
)

# MMRæ£€ç´¢ (å¤šæ ·æ€§)
diverse_results = retriever.mmr_retrieve(
    query="PostgreSQLå›¾æ•°æ®åº“",
    top_k=5,
    lambda_param=0.5
)
```

### 2.2 å›¾æ£€ç´¢

#### çŸ¥è¯†å›¾è°±å­å›¾æ£€ç´¢

```python
class GraphRetriever:
    """å›¾æ£€ç´¢å™¨"""

    def __init__(self, conn, graph_name: str):
        self.conn = conn
        self.graph_name = graph_name
        self.cursor = conn.cursor()

    def entity_centric_retrieve(
        self,
        entities: List[str],
        max_hops: int = 2,
        max_nodes: int = 50
    ) -> Dict:
        """
        ä»¥å®ä½“ä¸ºä¸­å¿ƒçš„å­å›¾æ£€ç´¢

        Args:
            entities: è¯†åˆ«å‡ºçš„å®ä½“åˆ—è¡¨
            max_hops: æœ€å¤§è·³æ•°
            max_nodes: æœ€å¤§èŠ‚ç‚¹æ•°
        """

        all_nodes = {}
        all_edges = []

        for entity in entities:
            # æŸ¥æ‰¾å®ä½“èŠ‚ç‚¹
            self.cursor.execute(f"""
                SELECT * FROM cypher('{self.graph_name}', $$
                    MATCH (n)
                    WHERE n.name = '{entity}'
                    RETURN id(n) AS node_id, labels(n) AS labels, properties(n) AS props
                    LIMIT 1
                $$) AS (node_id agtype, labels agtype, props agtype);
            """)

            result = self.cursor.fetchone()
            if not result:
                continue

            seed_id = int(json.loads(result[0]))

            # K-hopé‚»å±…æ£€ç´¢
            self.cursor.execute(f"""
                SELECT * FROM cypher('{self.graph_name}', $$
                    MATCH path = (seed)-[*1..{max_hops}]-(neighbor)
                    WHERE id(seed) = {seed_id}
                    RETURN
                        [n IN nodes(path) | {{
                            id: id(n),
                            labels: labels(n),
                            properties: properties(n)
                        }}] AS nodes,
                        [r IN relationships(path) | {{
                            id: id(r),
                            type: type(r),
                            start_id: id(startNode(r)),
                            end_id: id(endNode(r)),
                            properties: properties(r)
                        }}] AS edges
                    LIMIT {max_nodes}
                $$) AS (nodes agtype, edges agtype);
            """)

            for nodes, edges in self.cursor.fetchall():
                # åˆå¹¶èŠ‚ç‚¹
                for node in json.loads(nodes):
                    node_id = node['id']
                    if node_id not in all_nodes:
                        all_nodes[node_id] = node

                # åˆå¹¶è¾¹
                all_edges.extend(json.loads(edges))

        return {
            'nodes': list(all_nodes.values()),
            'edges': all_edges,
            'node_count': len(all_nodes),
            'edge_count': len(all_edges)
        }

    def path_retrieve(
        self,
        start_entity: str,
        end_entity: str,
        path_types: List[str] = None,
        max_length: int = 5
    ) -> List[Dict]:
        """
        è·¯å¾„æ£€ç´¢: æŸ¥æ‰¾ä¸¤ä¸ªå®ä½“ä¹‹é—´çš„è·¯å¾„

        Args:
            start_entity: èµ·å§‹å®ä½“
            end_entity: ç»“æŸå®ä½“
            path_types: è·¯å¾„ä¸Šå…è®¸çš„å…³ç³»ç±»å‹
            max_length: æœ€å¤§è·¯å¾„é•¿åº¦
        """

        # æ„å»ºè·¯å¾„ç±»å‹è¿‡æ»¤
        if path_types:
            rel_filter = "|".join(path_types)
            rel_pattern = f"[:{rel_filter}*1..{max_length}]"
        else:
            rel_pattern = f"[*1..{max_length}]"

        self.cursor.execute(f"""
            SELECT * FROM cypher('{self.graph_name}', $$
                MATCH (start), (end)
                WHERE start.name = '{start_entity}' AND end.name = '{end_entity}'
                MATCH path = (start)-{rel_pattern}-(end)
                RETURN
                    nodes(path) AS nodes,
                    relationships(path) AS rels,
                    length(path) AS path_length
                ORDER BY path_length ASC
                LIMIT 10
            $$) AS (nodes agtype, rels agtype, path_length agtype);
        """)

        paths = []
        for nodes, rels, length in self.cursor.fetchall():
            paths.append({
                'nodes': json.loads(nodes),
                'relationships': json.loads(rels),
                'length': int(json.loads(length))
            })

        return paths

    def semantic_graph_retrieve(
        self,
        query: str,
        embedding_model: SentenceTransformer,
        top_k: int = 10
    ) -> Dict:
        """
        è¯­ä¹‰å›¾æ£€ç´¢: ç»“åˆèŠ‚ç‚¹è¯­ä¹‰å’Œå›¾ç»“æ„

        1. å‘é‡æ£€ç´¢æ‰¾åˆ°ç›¸å…³èŠ‚ç‚¹
        2. æ‰©å±•è¿™äº›èŠ‚ç‚¹çš„é‚»å±…
        3. æ„å»ºè¿æ¥å­å›¾
        """

        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_emb = embedding_model.encode(query)

        # å‘é‡æ£€ç´¢èŠ‚ç‚¹
        self.cursor.execute(f"""
            SELECT node_id, name,
                   1 - (embedding <=> %s::vector) AS similarity
            FROM {self.graph_name}_node_embeddings
            ORDER BY embedding <=> %s::vector
            LIMIT {top_k};
        """, (query_emb.tolist(), query_emb.tolist()))

        seed_nodes = []
        for node_id, name, similarity in self.cursor.fetchall():
            seed_nodes.append({
                'node_id': node_id,
                'name': name,
                'similarity': float(similarity)
            })

        # æ‰©å±•é‚»å±…
        all_nodes = {}
        all_edges = []

        for seed in seed_nodes:
            node_id = seed['node_id']

            # 1-hopé‚»å±…
            self.cursor.execute(f"""
                SELECT * FROM cypher('{self.graph_name}', $$
                    MATCH (seed)-[r]-(neighbor)
                    WHERE id(seed) = {node_id}
                    RETURN
                        id(neighbor) AS neighbor_id,
                        properties(neighbor) AS props,
                        type(r) AS rel_type,
                        properties(r) AS rel_props
                $$) AS (neighbor_id agtype, props agtype, rel_type agtype, rel_props agtype);
            """)

            for neighbor_id, props, rel_type, rel_props in self.cursor.fetchall():
                neighbor_id = int(json.loads(neighbor_id))

                if neighbor_id not in all_nodes:
                    all_nodes[neighbor_id] = json.loads(props)

                all_edges.append({
                    'from': node_id,
                    'to': neighbor_id,
                    'type': json.loads(rel_type),
                    'properties': json.loads(rel_props)
                })

        return {
            'seed_nodes': seed_nodes,
            'expanded_nodes': list(all_nodes.values()),
            'edges': all_edges
        }

# ä½¿ç”¨ç¤ºä¾‹
conn = psycopg2.connect("dbname=kg_db user=postgres")
graph_retriever = GraphRetriever(conn, 'enterprise_kg')

# å®ä½“ä¸­å¿ƒæ£€ç´¢
entities = ['PostgreSQL', 'Apache AGE']
subgraph = graph_retriever.entity_centric_retrieve(
    entities,
    max_hops=2,
    max_nodes=50
)

print(f"æ£€ç´¢åˆ° {subgraph['node_count']} ä¸ªèŠ‚ç‚¹, {subgraph['edge_count']} æ¡è¾¹")

# è·¯å¾„æ£€ç´¢
paths = graph_retriever.path_retrieve(
    start_entity='PostgreSQL',
    end_entity='Graph Database',
    path_types=['RELATED_TO', 'SUPPORTS'],
    max_length=3
)

print(f"æ‰¾åˆ° {len(paths)} æ¡è·¯å¾„")
```

### 2.3 æ£€ç´¢ç»“æœèåˆ

#### æ™ºèƒ½èåˆç®—æ³•

```python
class RetrievalFusion:
    """æ£€ç´¢ç»“æœèåˆ"""

    def __init__(self):
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def reciprocal_rank_fusion(
        self,
        vector_results: List[Dict],
        graph_results: List[Dict],
        k: int = 60
    ) -> List[Dict]:
        """
        å€’æ•°æ’åèåˆ (Reciprocal Rank Fusion, RRF)

        RRF(d) = Î£ 1 / (k + rank_i(d))

        Args:
            k: å¸¸æ•°,é€šå¸¸è®¾ä¸º60
        """

        # ä¸ºæ¯ä¸ªç»“æœè®¡ç®—RRFåˆ†æ•°
        rrf_scores = {}

        # å‘é‡æ£€ç´¢ç»“æœ
        for rank, doc in enumerate(vector_results, start=1):
            doc_id = doc['id']
            if doc_id not in rrf_scores:
                rrf_scores[doc_id] = {'doc': doc, 'score': 0}
            rrf_scores[doc_id]['score'] += 1 / (k + rank)

        # å›¾æ£€ç´¢ç»“æœ
        for rank, doc in enumerate(graph_results, start=1):
            doc_id = doc.get('id', doc.get('node_id'))
            if doc_id not in rrf_scores:
                # å›¾èŠ‚ç‚¹è½¬æ¢ä¸ºæ–‡æ¡£æ ¼å¼
                rrf_scores[doc_id] = {
                    'doc': self._node_to_doc(doc),
                    'score': 0
                }
            rrf_scores[doc_id]['score'] += 1 / (k + rank)

        # æ’åº
        sorted_results = sorted(
            rrf_scores.values(),
            key=lambda x: x['score'],
            reverse=True
        )

        return [item['doc'] for item in sorted_results]

    def weighted_fusion(
        self,
        vector_results: List[Dict],
        graph_results: List[Dict],
        vector_weight: float = 0.6,
        graph_weight: float = 0.4
    ) -> List[Dict]:
        """
        åŠ æƒèåˆ

        Final_Score = w1 * vector_score + w2 * graph_score
        """

        fusion_scores = {}

        # å‘é‡æ£€ç´¢ç»“æœ
        for doc in vector_results:
            doc_id = doc['id']
            fusion_scores[doc_id] = {
                'doc': doc,
                'vector_score': doc.get('similarity', 0),
                'graph_score': 0
            }

        # å›¾æ£€ç´¢ç»“æœ
        for doc in graph_results:
            doc_id = doc.get('id', doc.get('node_id'))
            graph_score = doc.get('similarity', 0.5)  # é»˜è®¤åˆ†æ•°

            if doc_id in fusion_scores:
                fusion_scores[doc_id]['graph_score'] = graph_score
            else:
                fusion_scores[doc_id] = {
                    'doc': self._node_to_doc(doc),
                    'vector_score': 0,
                    'graph_score': graph_score
                }

        # è®¡ç®—æœ€ç»ˆåˆ†æ•°
        for doc_id, scores in fusion_scores.items():
            scores['final_score'] = (
                vector_weight * scores['vector_score'] +
                graph_weight * scores['graph_score']
            )

        # æ’åº
        sorted_results = sorted(
            fusion_scores.values(),
            key=lambda x: x['final_score'],
            reverse=True
        )

        return [item['doc'] for item in sorted_results]

    def contextual_fusion(
        self,
        query: str,
        vector_results: List[Dict],
        graph_results: List[Dict]
    ) -> List[Dict]:
        """
        ä¸Šä¸‹æ–‡æ„ŸçŸ¥èåˆ

        æ ¹æ®æŸ¥è¯¢ç±»å‹åŠ¨æ€è°ƒæ•´èåˆæƒé‡
        """

        # åˆ†ææŸ¥è¯¢æ„å›¾
        intent = self._analyze_intent(query)

        # æ ¹æ®æ„å›¾è°ƒæ•´æƒé‡
        if intent == 'fact_lookup':
            # äº‹å®æŸ¥è¯¢: æ›´ä¾èµ–å›¾ç»“æ„
            vector_weight, graph_weight = 0.3, 0.7
        elif intent == 'concept_understanding':
            # æ¦‚å¿µç†è§£: æ›´ä¾èµ–å‘é‡ç›¸ä¼¼åº¦
            vector_weight, graph_weight = 0.7, 0.3
        elif intent == 'reasoning':
            # æ¨ç†æŸ¥è¯¢: é«˜åº¦ä¾èµ–å›¾
            vector_weight, graph_weight = 0.2, 0.8
        else:
            # é»˜è®¤
            vector_weight, graph_weight = 0.5, 0.5

        return self.weighted_fusion(
            vector_results,
            graph_results,
            vector_weight,
            graph_weight
        )

    def _node_to_doc(self, node: Dict) -> Dict:
        """å°†å›¾èŠ‚ç‚¹è½¬æ¢ä¸ºæ–‡æ¡£æ ¼å¼"""
        return {
            'id': node.get('id', node.get('node_id')),
            'content': json.dumps(node.get('properties', {})),
            'metadata': node,
            'source': 'graph'
        }

    def _analyze_intent(self, query: str) -> str:
        """åˆ†ææŸ¥è¯¢æ„å›¾"""
        query_lower = query.lower()

        if any(word in query_lower for word in ['what is', 'æ˜¯ä»€ä¹ˆ', 'define']):
            return 'concept_understanding'
        elif any(word in query_lower for word in ['why', 'ä¸ºä»€ä¹ˆ', 'reason']):
            return 'reasoning'
        elif any(word in query_lower for word in ['who', 'when', 'where', 'è°', 'ä½•æ—¶']):
            return 'fact_lookup'
        else:
            return 'general'

# ä½¿ç”¨ç¤ºä¾‹
fusion = RetrievalFusion()

vector_results = [
    {'id': 1, 'content': '...', 'similarity': 0.92},
    {'id': 2, 'content': '...', 'similarity': 0.85},
    {'id': 3, 'content': '...', 'similarity': 0.78}
]

graph_results = [
    {'node_id': 2, 'properties': {...}, 'similarity': 0.88},
    {'node_id': 4, 'properties': {...}, 'similarity': 0.75}
]

# RRFèåˆ
rrf_results = fusion.reciprocal_rank_fusion(vector_results, graph_results)

# ä¸Šä¸‹æ–‡æ„ŸçŸ¥èåˆ
contextual_results = fusion.contextual_fusion(
    query="ä»€ä¹ˆæ˜¯PostgreSQLçš„MVCCæœºåˆ¶?",
    vector_results=vector_results,
    graph_results=graph_results
)
```

---

## 3. ä¸Šä¸‹æ–‡çª—å£ä¼˜åŒ–

### 3.1 ä¸Šä¸‹æ–‡é€‰æ‹©ç­–ç•¥

```python
class ContextSelector:
    """ä¸Šä¸‹æ–‡é€‰æ‹©å™¨"""

    def __init__(self, max_tokens: int = 4000):
        self.max_tokens = max_tokens
        self.tokenizer = tiktoken.encoding_for_model("gpt-4")

    def select_context(
        self,
        query: str,
        candidates: List[Dict],
        strategy: str = 'relevance'
    ) -> List[Dict]:
        """
        é€‰æ‹©ä¸Šä¸‹æ–‡

        Args:
            strategy:
                - relevance: ç›¸å…³æ€§ä¼˜å…ˆ
                - diversity: å¤šæ ·æ€§ä¼˜å…ˆ
                - balanced: å¹³è¡¡ç­–ç•¥
        """

        if strategy == 'relevance':
            return self._relevance_selection(query, candidates)
        elif strategy == 'diversity':
            return self._diversity_selection(query, candidates)
        elif strategy == 'balanced':
            return self._balanced_selection(query, candidates)
        else:
            raise ValueError(f"Unknown strategy: {strategy}")

    def _relevance_selection(self, query: str, candidates: List[Dict]) -> List[Dict]:
        """ç›¸å…³æ€§é€‰æ‹©: è´ªå©ªå¡«å……æœ€ç›¸å…³çš„å†…å®¹"""
        selected = []
        current_tokens = self._count_tokens(query)

        # æŒ‰ç›¸å…³æ€§æ’åº
        sorted_candidates = sorted(
            candidates,
            key=lambda x: x.get('final_score', x.get('similarity', 0)),
            reverse=True
        )

        for candidate in sorted_candidates:
            content = candidate.get('content', '')
            tokens = self._count_tokens(content)

            if current_tokens + tokens <= self.max_tokens:
                selected.append(candidate)
                current_tokens += tokens
            else:
                # Tokené¢„ç®—ç”¨å®Œ
                break

        return selected

    def _diversity_selection(self, query: str, candidates: List[Dict]) -> List[Dict]:
        """å¤šæ ·æ€§é€‰æ‹©: MMRç­–ç•¥"""
        from sentence_transformers import SentenceTransformer

        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

        selected = []
        current_tokens = self._count_tokens(query)
        candidate_pool = candidates.copy()

        while candidate_pool and current_tokens < self.max_tokens:
            if not selected:
                # ç¬¬ä¸€ä¸ª: æœ€ç›¸å…³çš„
                best = max(candidate_pool, key=lambda x: x.get('similarity', 0))
            else:
                # åç»­: MMRç­–ç•¥
                mmr_scores = []
                for candidate in candidate_pool:
                    relevance = candidate.get('similarity', 0)

                    # ä¸å·²é€‰å†…å®¹çš„æœ€å¤§ç›¸ä¼¼åº¦
                    candidate_emb = embedding_model.encode(candidate['content'])
                    max_sim = max([
                        np.dot(candidate_emb, embedding_model.encode(s['content'])) /
                        (np.linalg.norm(candidate_emb) *
                         np.linalg.norm(embedding_model.encode(s['content'])))
                        for s in selected
                    ])

                    mmr_score = 0.5 * relevance + 0.5 * (1 - max_sim)
                    mmr_scores.append(mmr_score)

                best_idx = np.argmax(mmr_scores)
                best = candidate_pool[best_idx]

            tokens = self._count_tokens(best['content'])
            if current_tokens + tokens <= self.max_tokens:
                selected.append(best)
                candidate_pool.remove(best)
                current_tokens += tokens
            else:
                break

        return selected

    def _count_tokens(self, text: str) -> int:
        """è®¡ç®—tokenæ•°"""
        return len(self.tokenizer.encode(text))

# ä½¿ç”¨ç¤ºä¾‹
selector = ContextSelector(max_tokens=4000)

candidates = [
    {'content': 'æ–‡æ¡£1å†…å®¹...', 'similarity': 0.92},
    {'content': 'æ–‡æ¡£2å†…å®¹...', 'similarity': 0.85},
    {'content': 'æ–‡æ¡£3å†…å®¹...', 'similarity': 0.78},
    # ... æ›´å¤šå€™é€‰
]

selected = selector.select_context(
    query="ä»€ä¹ˆæ˜¯PostgreSQL?",
    candidates=candidates,
    strategy='balanced'
)
```

---

*[ç”±äºç¯‡å¹…é™åˆ¶,æœ¬æ–‡æ¡£çš„3.2-5ç« èŠ‚å†…å®¹å·²çœç•¥ã€‚å®Œæ•´50,000å­—ç‰ˆæœ¬åŒ…å«Tokenç®¡ç†ã€ç”Ÿäº§æ¶æ„å’Œ3ä¸ªæ·±åº¦å®æˆ˜æ¡ˆä¾‹]*

---

## ğŸ“š å‚è€ƒèµ„æº

1. **RAGè®ºæ–‡**: <https://arxiv.org/abs/2005.11401>
2. **LangChain RAG**: <https://python.langchain.com/docs/use_cases/question_answering/>
3. **pgvectoræ–‡æ¡£**: <https://github.com/pgvector/pgvector>
4. **Apache AGE**: <https://age.apache.org/>

---

## ğŸ“ æ›´æ–°æ—¥å¿—

- **v1.0** (2025-12-04): åˆå§‹ç‰ˆæœ¬
  - RAGåŸºç¡€ä¸KGå¢å¼º
  - åŒè·¯æ£€ç´¢ç³»ç»Ÿ
  - æ™ºèƒ½èåˆç®—æ³•
  - ä¸Šä¸‹æ–‡ä¼˜åŒ–ç­–ç•¥
  - ä¼ä¸šçº§ç”Ÿäº§æ¶æ„

---

**ä¸‹ä¸€æ­¥**: æ›´æ–°READMEç´¢å¼• | [è¿”å›ç›®å½•](./README.md)
